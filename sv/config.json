{
	"noInstanceSelected": "Ingen modellinstans vald",
	"resetToDefault": "√Öterst√§ll",
	"showAdvancedSettings": "Visa avancerade inst√§llningar",
	"showAll": "Alla",
	"basicSettings": "Grundl√§ggande",
	"configSubtitle": "Ladda eller spara f√∂rinst√§llningar och experimentera med modellparametrar",
	"inferenceParameters/title": "Prediktionsparametrar",
	"inferenceParameters/info": "Experimentera med parametrar som p√•verkar prediktionen.",
	"generalParameters/title": "Allm√§nt",
	"samplingParameters/title": "Sampling",
	"basicTab": "Grundl√§ggande",
	"advancedTab": "Avancerat",
	"advancedTab/title": "üß™ Avancerad konfiguration",
	"advancedTab/expandAll": "Expandera alla",
	"advancedTab/overridesTitle": "Anpassade konfigurationer",
	"advancedTab/noConfigsText": "Du har inga osparade √§ndringar - redigera v√§rden ovan f√∂r att se anpassade konfigurationer h√§r.",
	"loadInstanceFirst": "Ladda en modell f√∂r att visa konfigurerbara parametrar",
	"noListedConfigs": "Inga konfigurerbara parametrar",
	"generationParameters/info": "Experimentera med grundl√§ggande parametrar som p√•verkar textgenerering.",
	"loadParameters/title": "Ladda parametrar",
	"loadParameters/description": "Inst√§llningar f√∂r att kontrollera hur modellen initieras och laddas i minnet.",
	"loadParameters/reload": "Ladda om f√∂r att till√§mpa √§ndringar",
	"discardChanges": "√Öngra √§ndringar",
	"loadModelToSeeOptions": "Ladda en modell f√∂r att se alternativ",
	"llm.prediction.systemPrompt/title": "Systemprompt",
	"llm.prediction.systemPrompt/description": "Anv√§nd detta f√§lt f√∂r att ge bakgrundsinstruktioner till modellen, s√•som en upps√§ttning regler, begr√§nsningar eller allm√§nna krav.",
	"llm.prediction.systemPrompt/subTitle": "Riktlinjer f√∂r AI:n",
	"llm.prediction.temperature/title": "Temperatur",
	"llm.prediction.temperature/subTitle": "Hur mycket slumpm√§ssighet som ska introduceras. 0 ger samma resultat varje g√•ng, medan h√∂gre v√§rden √∂kar kreativiteten och variationen",
	"llm.prediction.temperature/info": "Fr√•n llama.cpp hj√§lpdokument: \"Standardv√§rdet √§r <{{dynamicValue}}>, vilket ger en balans mellan slumpm√§ssighet och determinism. I extrema fall kommer en temperatur p√• 0 alltid att v√§lja den mest sannolika n√§sta token, vilket leder till identiska utdata vid varje k√∂rning\"",
	"llm.prediction.llama.sampling/title": "Sampling",
	"llm.prediction.topKSampling/title": "Top K Sampling",
	"llm.prediction.topKSampling/subTitle": "Begr√§nsar n√§sta token till en av de top-k mest sannolika token. Fungerar liknande som temperatur",
	"llm.prediction.topKSampling/info": "Fr√•n llama.cpp hj√§lpdokument:\n\nTop-k sampling √§r en textgenereringsmetod som v√§ljer n√§sta token endast fr√•n de top k mest sannolika token som modellen f√∂rutsp√•r.\n\nDet hj√§lper till att minska risken f√∂r att generera l√•g sannolikhet eller nonsens token, men det kan ocks√• begr√§nsa m√•ngfalden i utdata.\n\nEtt h√∂gre v√§rde f√∂r top-k (t.ex. 100) kommer att √∂verv√§ga fler token och leda till mer varierad text, medan ett l√§gre v√§rde (t.ex. 10) kommer att fokusera p√• de mest sannolika token och generera mer konservativ text.\n\n‚Ä¢ Standardv√§rdet √§r <{{dynamicValue}}>",
	"llm.prediction.llama.cpuThreads/title": "CPU-tr√•dar",
	"llm.prediction.llama.cpuThreads/subTitle": "Antal CPU-tr√•dar att anv√§nda under inferens",
	"llm.prediction.llama.cpuThreads/info": "Antalet tr√•dar att anv√§nda under ber√§kning. Att √∂ka antalet tr√•dar korrelerar inte alltid med b√§ttre prestanda. Standardv√§rdet √§r <{{dynamicValue}}>.",
	"llm.prediction.maxPredictedTokens/title": "Begr√§nsa svarsl√§ngd",
	"llm.prediction.maxPredictedTokens/subTitle": "Valfritt begr√§nsa l√§ngden p√• AI:s svar",
	"llm.prediction.maxPredictedTokens/info": "Kontrollera maxl√§ngden p√• chatbotens svar. Sl√• p√• f√∂r att st√§lla in en gr√§ns f√∂r maxl√§ngden p√• ett svar, eller sl√• av f√∂r att l√•ta chatboten best√§mma n√§r den ska sluta.",
	"llm.prediction.maxPredictedTokens/inputLabel": "Maximal svarsl√§ngd (token)",
	"llm.prediction.maxPredictedTokens/wordEstimate": "Cirka {{maxWords}} ord",
	"llm.prediction.repeatPenalty/title": "Upprepningskontroll",
	"llm.prediction.repeatPenalty/subTitle": "Hur mycket upprepningar av samma token ska begr√§nsas",
	"llm.prediction.repeatPenalty/info": "Fr√•n llama.cpp hj√§lpdokument: \"Hj√§lper till att f√∂rhindra att modellen genererar repetitiv eller monoton text.\n\nEtt h√∂gre v√§rde (t.ex. 1.5) kommer att straffa upprepningar starkare, medan ett l√§gre v√§rde (t.ex. 0.9) kommer att vara mer f√∂rl√•tande.\" ‚Ä¢ Standardv√§rdet √§r <{{dynamicValue}}>",
	"llm.prediction.minPSampling/title": "Min P Sampling",
	"llm.prediction.minPSampling/subTitle": "Minsta basprobabilitet f√∂r en token att v√§ljas f√∂r utdata",
	"llm.prediction.minPSampling/info": "Fr√•n llama.cpp hj√§lpdokument:\n\nDen minsta sannolikheten f√∂r en token att √∂verv√§gas, relativt sannolikheten f√∂r den mest sannolika token. M√•ste vara i [0, 1].\n\n‚Ä¢ Standardv√§rdet √§r <{{dynamicValue}}>",
	"llm.prediction.topPSampling/title": "Top P Sampling",
	"llm.prediction.topPSampling/subTitle": "Minsta kumulativa sannolikhet f√∂r de m√∂jliga n√§sta token. Fungerar liknande som temperatur",
	"llm.prediction.topPSampling/info": "Fr√•n llama.cpp hj√§lpdokument:\n\nTop-p sampling, √§ven k√§nd som nucleus sampling, √§r en annan textgenereringsmetod som v√§ljer n√§sta token fr√•n en delm√§ngd av token som tillsammans har en kumulativ sannolikhet p√• minst p.\n\nDenna metod ger en balans mellan m√•ngfald och kvalitet genom att √∂verv√§ga b√•de sannolikheterna f√∂r token och antalet token att v√§lja fr√•n.\n\nEtt h√∂gre v√§rde f√∂r top-p (t.ex. 0.95) kommer att leda till mer varierad text, medan ett l√§gre v√§rde (t.ex. 0.5) kommer att generera mer fokuserad och konservativ text. M√•ste vara i (0, 1].\n\n‚Ä¢ Standardv√§rdet √§r <{{dynamicValue}}>",
	"llm.prediction.stopStrings/title": "Stoppstr√§ngar",
	"llm.prediction.stopStrings/subTitle": "Str√§ngar som ska stoppa modellen fr√•n att generera fler token",
	"llm.prediction.stopStrings/info": "Specifika str√§ngar som n√§r de p√•tr√§ffas kommer att stoppa modellen fr√•n att generera fler token",
	"llm.prediction.stopStrings/placeholder": "Ange en str√§ng och tryck p√• ‚èé",
	"llm.prediction.contextOverflowPolicy/title": "Kontext√∂verfl√∂d",
	"llm.prediction.contextOverflowPolicy/subTitle": "Hur modellen ska bete sig n√§r konversationen blir f√∂r stor f√∂r att hantera",
	"llm.prediction.contextOverflowPolicy/info": "Best√§m vad som ska g√∂ras n√§r konversationen √∂verskrider storleken p√• modellens arbetsminne ('kontext')",
	"llm.prediction.llama.frequencyPenalty/title": "Frekvensbegr√§nsning",
	"llm.prediction.llama.presencePenalty/title": "N√§rvarobegr√§nsning",
	"llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
	"llm.prediction.llama.locallyTypicalSampling/title": "Lokalt typisk sampling",
	"llm.prediction.onnx.topKSampling/title": "Top K Sampling",
	"llm.prediction.onnx.topKSampling/subTitle": "Begr√§nsar n√§sta token till en av de top-k mest sannolika token. Fungerar liknande som temperatur",
	"llm.prediction.onnx.topKSampling/info": "Fr√•n ONNX-dokumentation:\n\nAntal h√∂gsta sannolikhetsvokabul√§rtoken att beh√•lla f√∂r top-k-filtrering\n\n‚Ä¢ Detta filter √§r avst√§ngt som standard",
	"llm.prediction.onnx.repeatPenalty/title": "Upprepningskontroll",
	"llm.prediction.onnx.repeatPenalty/subTitle": "Hur mycket upprepningar av samma token ska begr√§nsas",
	"llm.prediction.onnx.repeatPenalty/info": "Ett h√∂gre v√§rde avskr√§cker modellen fr√•n att upprepa sig sj√§lv",
	"llm.prediction.onnx.topPSampling/title": "Top P Sampling",
	"llm.prediction.onnx.topPSampling/subTitle": "Minsta kumulativa sannolikhet f√∂r de m√∂jliga n√§sta token. Fungerar liknande som temperatur",
	"llm.prediction.onnx.topPSampling/info": "Fr√•n ONNX-dokumentation:\n\nEndast de mest sannolika token med sannolikheter som tillsammans uppg√•r till TopP eller h√∂gre beh√•lls f√∂r generering\n\n‚Ä¢ Detta filter √§r avst√§ngt som standard",
	"llm.prediction.seed/title": "Seed",
	"llm.prediction.structured/title": "Strukturerad utdata",
	"llm.prediction.structured/info": "Strukturerad utdata",
	"llm.prediction.structured/description": "Avancerat: du kan tillhandah√•lla ett JSON-schema f√∂r att uppr√§tth√•lla ett specifikt utdataformat fr√•n modellen. L√§s [dokumentationen](https://lmstudio.ai/docs/advanced/structured-output) f√∂r att l√§ra dig mer",
	"llm.prediction.promptTemplate/title": "Promptmall",
	"llm.prediction.promptTemplate/subTitle": "Formatet i vilket meddelanden i chatten skickas till modellen. Att √§ndra detta kan introducera ov√§ntat beteende - se till att du vet vad du g√∂r!",
	"llm.load.contextLength/title": "Kontextl√§ngd",
	"llm.load.contextLength/subTitle": "Det maximala antalet token som modellen kan hantera i en prompt. Se alternativen f√∂r kontext√∂verfl√∂d under \"Prediktionsparametrar\" f√∂r fler s√§tt att hantera detta",
	"llm.load.contextLength/info": "Anger det maximala antalet token som modellen kan √∂verv√§ga samtidigt, vilket p√•verkar hur mycket kontext den beh√•ller under bearbetning",
	"llm.load.contextLength/warning": "Att st√§lla in ett h√∂gt v√§rde f√∂r kontextl√§ngd kan avsev√§rt p√•verka minnesanv√§ndningen",
	"llm.load.seed/title": "Seed",
	"llm.load.seed/subTitle": "Seed f√∂r slumptalsgeneratorn som anv√§nds vid textgenerering. -1 √§r slumpm√§ssig",
	"llm.load.seed/info": "Slumpm√§ssig Seed: St√§ller in seed f√∂r slumptalsgenerering f√∂r att s√§kerst√§lla reproducerbara resultat",
	
	"llm.load.llama.evalBatchSize/title": "Utv√§rderingsbatchstorlek",
	"llm.load.llama.evalBatchSize/subTitle": "Antal input-token att bearbeta √•t g√•ngen. Att √∂ka detta √∂kar prestandan p√• bekostnad av minnesanv√§ndning",
	"llm.load.llama.evalBatchSize/info": "St√§ller in antalet exempel som bearbetas tillsammans i en batch under utv√§rdering, vilket p√•verkar hastighet och minnesanv√§ndning",
	"llm.load.llama.ropeFrequencyBase/title": "RoPE Basfrekvens",
	"llm.load.llama.ropeFrequencyBase/subTitle": "Anpassad basfrekvens f√∂r roterande positionsinb√§ddningar (RoPE). Att √∂ka detta kan m√∂jligg√∂ra b√§ttre prestanda vid h√∂ga kontextl√§ngder",
	"llm.load.llama.ropeFrequencyBase/info": "[Avancerat] Justerar basfrekvensen f√∂r roterande positionskodning, vilket p√•verkar hur positionsinformation inb√§ddas",
	"llm.load.llama.ropeFrequencyScale/title": "RoPE Frekvensskala",
	"llm.load.llama.ropeFrequencyScale/subTitle": "Kontextl√§ngden skalas med denna faktor f√∂r att ut√∂ka effektiv kontext med hj√§lp av RoPE",
	"llm.load.llama.ropeFrequencyScale/info": "[Avancerat] Modifierar skalningen av frekvensen f√∂r roterande positionskodning f√∂r att kontrollera positionskodningens granularitet",
	"llm.load.llama.acceleration.offloadRatio/title": "GPU-avlastning",
	"llm.load.llama.acceleration.offloadRatio/subTitle": "Antal diskreta modellager att ber√§kna p√• GPU f√∂r GPU-acceleration",
	"llm.load.llama.acceleration.offloadRatio/info": "St√§ll in antalet lager som ska avlastas till GPU:n.",
	"llm.load.llama.flashAttention/title": "Flash Attention",
	"llm.load.llama.flashAttention/subTitle": "Minskar minnesanv√§ndning och genereringstid p√• vissa modeller",
	"llm.load.llama.flashAttention/info": "Accelererar uppm√§rksamhetsmekanismer f√∂r snabbare och mer effektiv bearbetning",
	"llm.load.numExperts/title": "Antal experter",
	"llm.load.numExperts/subTitle": "Antal experter att anv√§nda i modellen",
	"llm.load.numExperts/info": "Antalet experter att anv√§nda i modellen",
	"llm.load.llama.keepModelInMemory/title": "Beh√•ll modell i minnet",
	"llm.load.llama.keepModelInMemory/subTitle": "Reservera systemminne f√∂r modellen, √§ven n√§r den avlastas till GPU. F√∂rb√§ttrar prestanda men kr√§ver mer system-RAM",
	"llm.load.llama.keepModelInMemory/info": "F√∂rhindrar att modellen byts ut till disk, vilket s√§kerst√§ller snabbare √•tkomst p√• bekostnad av h√∂gre RAM-anv√§ndning",
	"llm.load.llama.useFp16ForKVCache/title": "Anv√§nd FP16 f√∂r KV-cache",
	"llm.load.llama.useFp16ForKVCache/info": "Minskar minnesanv√§ndningen genom att lagra cache i halvprecision (FP16)",
	"llm.load.llama.tryMmap/title": "F√∂rs√∂k med mmap()",
	"llm.load.llama.tryMmap/subTitle": "F√∂rb√§ttrar laddningstiden f√∂r modellen. Att inaktivera detta kan f√∂rb√§ttra prestandan n√§r modellen √§r st√∂rre √§n tillg√§ngligt system-RAM",
	"llm.load.llama.tryMmap/info": "Ladda modellfiler direkt fr√•n disk till minne",

	"embedding.load.contextLength/title": "Kontextl√§ngd",
	"embedding.load.contextLength/subTitle": "Det maximala antalet token som modellen kan hantera i en prompt. Se alternativen f√∂r kontext√∂verfl√∂d under \"Prediktionsparametrar\" f√∂r fler s√§tt att hantera detta",
	"embedding.load.contextLength/info": "Anger det maximala antalet token som modellen kan √∂verv√§ga samtidigt, vilket p√•verkar hur mycket kontext den beh√•ller under bearbetning",
	"embedding.load.llama.ropeFrequencyBase/title": "RoPE Basfrekvens",
	"embedding.load.llama.ropeFrequencyBase/subTitle": "Anpassad basfrekvens f√∂r roterande positionsinb√§ddningar (RoPE). Att √∂ka detta kan m√∂jligg√∂ra b√§ttre prestanda vid h√∂ga kontextl√§ngder",
	"embedding.load.llama.ropeFrequencyBase/info": "[Avancerat] Justerar basfrekvensen f√∂r roterande positionskodning, vilket p√•verkar hur positionsinformation inb√§ddas",
	"embedding.load.llama.evalBatchSize/title": "Utv√§rderingsbatchstorlek",
	"embedding.load.llama.evalBatchSize/subTitle": "Antal input-token att bearbeta √•t g√•ngen. Att √∂ka detta √∂kar prestandan p√• bekostnad av minnesanv√§ndning",
	"embedding.load.llama.evalBatchSize/info": "St√§ller in antalet token som bearbetas tillsammans i en batch under utv√§rdering",
	"embedding.load.llama.ropeFrequencyScale/title": "RoPE Frekvensskala",
	"embedding.load.llama.ropeFrequencyScale/subTitle": "Kontextl√§ngden skalas med denna faktor f√∂r att ut√∂ka effektiv kontext med hj√§lp av RoPE",
	"embedding.load.llama.ropeFrequencyScale/info": "[Avancerat] Modifierar skalningen av frekvensen f√∂r roterande positionskodning f√∂r att kontrollera positionskodningens granularitet",
	"embedding.load.llama.acceleration.offloadRatio/title": "GPU-avlastning",
	"embedding.load.llama.acceleration.offloadRatio/subTitle": "Antal diskreta modellager att ber√§kna p√• GPU f√∂r GPU-acceleration",
	"embedding.load.llama.acceleration.offloadRatio/info": "St√§ll in antalet lager som ska avlastas till GPU:n.",
	"embedding.load.llama.keepModelInMemory/title": "Beh√•ll modell i minnet",
	"embedding.load.llama.keepModelInMemory/subTitle": "Reservera systemminne f√∂r modellen, √§ven n√§r den avlastas till GPU. F√∂rb√§ttrar prestanda men kr√§ver mer system-RAM",
	"embedding.load.llama.keepModelInMemory/info": "F√∂rhindrar att modellen byts ut till disk, vilket s√§kerst√§ller snabbare √•tkomst p√• bekostnad av h√∂gre RAM-anv√§ndning",
	"embedding.load.llama.tryMmap/title": "F√∂rs√∂k med mmap()",
	"embedding.load.llama.tryMmap/subTitle": "F√∂rb√§ttrar laddningstiden f√∂r modellen. Att inaktivera detta kan f√∂rb√§ttra prestandan n√§r modellen √§r st√∂rre √§n tillg√§ngligt system-RAM",
	"embedding.load.llama.tryMmap/info": "Ladda modellfiler direkt fr√•n disk till minne",
	"embedding.load.seed/title": "Seed",
	"embedding.load.seed/subTitle": "Seed f√∂r slumptalsgeneratorn som anv√§nds vid textgenerering. -1 √§r slumpm√§ssig",

	"embedding.load.seed/info": "Slumpm√§ssig Seed: St√§ller in seed f√∂r slumptalsgenerering f√∂r att s√§kerst√§lla reproducerbara resultat",

	"presetTooltip": {
			"included/title": "F√∂rinst√§llda v√§rden",
			"included/description": "F√∂ljande f√§lt kommer att till√§mpas",
			"included/empty": "Inga f√§lt i denna f√∂rinst√§llning g√§ller i denna kontext.",
			"included/conflict": "Du kommer att bli ombedd att v√§lja om du vill till√§mpa detta v√§rde",
			"separateLoad/title": "Konfiguration vid laddning",
			"separateLoad/description.1": "F√∂rinst√§llningen inkluderar ocks√• f√∂ljande konfiguration vid laddning. Konfiguration vid laddning √§r modell√∂vergripande och kr√§ver omladdning av modellen f√∂r att tr√§da i kraft. H√•ll",
			"separateLoad/description.2": "f√∂r att till√§mpa p√•",
			"separateLoad/description.3": ".",
			"excluded/title": "Kanske inte g√§ller",
			"excluded/description": "F√∂ljande f√§lt ing√•r i f√∂rinst√§llningen men g√§ller inte i den aktuella kontexten.",
			"legacy/title": "Legacy-f√∂rinst√§llning",
			"legacy/description": "Denna f√∂rinst√§llning √§r en legacy-f√∂rinst√§llning. Den inkluderar f√∂ljande f√§lt som antingen hanteras automatiskt nu eller inte l√§ngre √§r till√§mpliga."
		},

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "AV"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Typ",
			"types.jinja/label": "Mall (Jinja)",
			"jinja.bosToken/label": "BOS Token",
			"jinja.eosToken/label": "EOS Token",
			"jinja.template/label": "Mall",
			"jinja/error": "Misslyckades med att tolka Jinja-mallen: {{error}}",
			"jinja/empty": "V√§nligen ange en Jinja-mall ovan.",
			"jinja/unlikelyToWork": "Jinja-mallen du angav ovan kommer troligen inte att fungera eftersom den inte refererar till variabeln \"messages\". V√§nligen dubbelkolla om du har angett en korrekt mall.",
			"types.manual/label": "Manuell",
			"manual.subfield.beforeSystem/label": "F√∂re System",
			"manual.subfield.beforeSystem/placeholder": "Ange System-prefix...",
			"manual.subfield.afterSystem/label": "Efter System",
			"manual.subfield.afterSystem/placeholder": "Ange System-suffix...",
			"manual.subfield.beforeUser/label": "F√∂re Anv√§ndare",
			"manual.subfield.beforeUser/placeholder": "Ange Anv√§ndar-prefix...",
			"manual.subfield.afterUser/label": "Efter Anv√§ndare",
			"manual.subfield.afterUser/placeholder": "Ange Anv√§ndar-suffix...",
			"manual.subfield.beforeAssistant/label": "F√∂re Assistent",
			"manual.subfield.beforeAssistant/placeholder": "Ange Assistent-prefix...",
			"manual.subfield.afterAssistant/label": "Efter Assistent",
			"manual.subfield.afterAssistant/placeholder": "Ange Assistent-suffix...",
			"stopStrings/label": "Ytterligare Stoppstr√§ngar",
			"stopStrings/subTitle": "Mall-specifika stoppstr√§ngar som kommer att anv√§ndas ut√∂ver anv√§ndarspecificerade stoppstr√§ngar."
    },
		"contextLength": {
			"maxValueTooltip": "Detta √§r det maximala antalet token som modellen tr√§nades att hantera. Klicka f√∂r att st√§lla in kontexten till detta v√§rde",
			"maxValueTextStart": "Modellen st√∂der upp till",
			"maxValueTextEnd": "token",
			"tooltipHint": "√Ñven om en modell kan st√∂dja upp till ett visst antal token, kan prestandan f√∂rs√§mras om din maskins resurser inte kan hantera belastningen - var f√∂rsiktig n√§r du √∂kar detta v√§rde"
		},
		"contextOverflowPolicy": {
			"stopAtLimit": "Stoppa vid gr√§ns",
			"stopAtLimitSub": "Sluta generera n√§r modellens minne blir fullt",
			"truncateMiddle": "Trunkera mitten",
			"truncateMiddleSub": "Tar bort meddelanden fr√•n mitten av konversationen f√∂r att ge plats √•t nyare. Modellen kommer fortfarande att minnas b√∂rjan av konversationen",
			"rollingWindow": "Rullande f√∂nster",
			"rollingWindowSub": "Modellen kommer alltid att f√• de senaste meddelandena men kan gl√∂mma b√∂rjan av konversationen"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "AV"
    }
  },
	"saveConflictResolution": {
		"title": "V√§lj vilka v√§rden som ska inkluderas i f√∂rinst√§llningen",
		"description": "V√§lj och v√§lj vilka v√§rden du vill beh√•lla",
		"instructions": "Klicka p√• ett v√§rde f√∂r att inkludera det",
		"userValues": "Tidigare v√§rde",
		"presetValues": "Nytt v√§rde",
		"confirm": "Bekr√§fta",
		"cancel": "Avbryt"
	},
	"applyConflictResolution": {
		"title": "Vilka v√§rden ska beh√•llas?",
		"description": "Du har obekr√§ftade √§ndringar som √∂verlappar med den inkommande f√∂rinst√§llningen",
		"instructions": "Klicka p√• ett v√§rde f√∂r att beh√•lla det",
		"userValues": "Nuvarande v√§rde",
		"presetValues": "Inkommande f√∂rinst√§llningsv√§rde",
		"confirm": "Bekr√§fta",
		"cancel": "Avbryt"
	},
	"empty": "<Empty>",
  "presets": {
		"title": "F√∂rinst√§llning",
		"commitChanges": "Verkst√§ll √§ndringar",
		"commitChanges/description": "Verkst√§ll dina √§ndringar till f√∂rinst√§llningen.",
		"commitChanges.manual": "Nya f√§lt uppt√§ckta. Du kommer att kunna v√§lja vilka √§ndringar som ska inkluderas i f√∂rinst√§llningen.",
		"commitChanges.manual.hold.0": "H√•ll",
		"commitChanges.manual.hold.1": "f√∂r att v√§lja vilka √§ndringar som ska verkst√§llas till f√∂rinst√§llningen.",
		"commitChanges.saveAll.hold.0": "H√•ll",
		"commitChanges.saveAll.hold.1": "f√∂r att spara alla √§ndringar.",
		"commitChanges.saveInPreset.hold.0": "H√•ll",
		"commitChanges.saveInPreset.hold.1": "f√∂r att endast spara √§ndringar till f√§lt som redan √§r inkluderade i f√∂rinst√§llningen.",
		"commitChanges/error": "Misslyckades med att verkst√§lla √§ndringar till f√∂rinst√§llningen.",
		"commitChanges.manual/description": "V√§lj vilka √§ndringar som ska inkluderas i f√∂rinst√§llningen.",
		"saveAs": "Spara som...",
		"presetNamePlaceholder": "Ange ett namn f√∂r f√∂rinst√§llningen...",
		"cannotCommitChangesLegacy": "Detta √§r en legacy-f√∂rinst√§llning och kan inte modifieras. Du kan skapa en kopia genom att anv√§nda \"Spara som ny...\".",
		"cannotCommitChangesNoChanges": "Inga √§ndringar att verkst√§lla.",
		"emptyNoUnsaved": "V√§lj en f√∂rinst√§llning...",
		"emptyWithUnsaved": "Osparad f√∂rinst√§llning",
		"saveEmptyWithUnsaved": "Spara f√∂rinst√§llning som...",
		"saveConfirm": "Spara",
		"saveCancel": "Avbryt",
		"saving": "Sparar...",
		"save/error": "Misslyckades med att spara f√∂rinst√§llningen.",
		"deselect": "Avmarkera f√∂rinst√§llning",
		"deselect/error": "Misslyckades med att avmarkera f√∂rinst√§llningen.",
		"select/error": "Misslyckades med att v√§lja f√∂rinst√§llning.",
		"delete/error": "Misslyckades med att radera f√∂rinst√§llning.",
		"discardChanges": "Kassera osparade",
		"discardChanges/info": "Kassera alla icke-verkst√§llda √§ndringar och √•terst√§ll f√∂rinst√§llningen till dess ursprungliga tillst√•nd",
		"newEmptyPreset": "Skapa ny tom f√∂rinst√§llning...",
		"contextMenuSelect": "V√§lj f√∂rinst√§llning",
		"contextMenuDelete": "Radera"
	},

	"flashAttentionWarning": "Flash Attention √§r en experimentell funktion som kan orsaka problem med vissa modeller. Om du st√∂ter p√• problem, f√∂rs√∂k att inaktivera den.",
	
	"seedUncheckedHint": "Slumpm√§ssig Seed",
	"ropeFrequencyBaseUncheckedHint": "Auto",
	"ropeFrequencyScaleUncheckedHint": "Auto"
}
