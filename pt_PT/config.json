{
  "noInstanceSelected": "Nenhum modelo selecionado",
  "resetToDefault": "Reiniciar para o padrão",
  "showAdvancedSettings": "Mostrar configurações avançadas",
  "showAll": "Mostrar tudo",
  "basicSettings": "Configurações básicas",
  "configSubtitle": "Carregar ou gravar presets e experimentar com modificações de parâmetros do modelo",
  "inferenceParameters/title": "Parâmetros de previsão",
  "inferenceParameters/info": "Experimente com parâmetros que impactam a previsão.",
  "generalParameters/title": "Geral",
  "samplingParameters/title": "Amostragem",
  "basicTab": "Básico",
  "advancedTab": "Avançado",
  "advancedTab/title": "Configuração avançada",
  "advancedTab/expandAll": "Expandir tudo",
  "advancedTab/overridesTitle": "Modificações de configuração",
  "advancedTab/noConfigsText": "Não há alterações por gravar - edite os valores acima para ver as modificações aqui.",
  "loadInstanceFirst": "Carregar um modelo para visualizar parâmetros configuráveis",
  "noListedConfigs": "Não há parâmetros configuráveis",
  "generationParameters/info": "Experimente com parâmetros básicos que impactam a geração de texto.",
  "loadParameters/title": "Carregar Parâmetros",
  "loadParameters/description": "Configurações para controlar a forma como o modelo é inicializado e carregado na memória.",
  "loadParameters/reload": "Recarregar para aplicar alterações",
  "discardChanges": "Cancelar alterações",
  "llm.prediction.systemPrompt/title": "Prompt do Sistema",
  "llm.prediction.systemPrompt/description": "Use este campo para fornecer instruções de fundo ao modelo, como um conjunto de regras, restrições ou requisitos gerais. Este campo é também frequentemente referido como o \"prompt do sistema\".",
  "llm.prediction.systemPrompt/subTitle": "Diretrizes para a IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quantos tokens aleatórios introduzir. 0 resultará no mesmo resultado sempre, enquanto valores mais altos aumentarão a criatividade e a variação.",
  "llm.prediction.temperature/info": "De acordo com os documentos de ajuda de llama.cpp: \"O valor predefinido é <{{dynamicValue}}>, que fornece um equilíbrio entre aleatoriedade e determinismo. Uma temperatura de 0 escolherá sempre o token mais provável, levando a saídas idênticas em cada execução\"",
  "llm.prediction.llama.sampling/title": "Amostragem",
  "llm.prediction.llama.topKSampling/title": "Amostragem Top K",
  "llm.prediction.llama.topKSampling/subTitle": "Limita o próximo token a um dos k tokens mais prováveis. Atua de forma semelhante à temperatura.",
  "llm.prediction.llama.topKSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA amostragem top-k é um método de geração de texto que seleciona o próximo token apenas dos k tokens mais prováveis previstos pelo modelo.\n\nIsto ajuda a reduzir o risco de gerar tokens de baixa probabilidade ou sem sentido, mas pode também limitar a diversidade do texto gerado.\n\nUm valor maior para top-k (por exemplo, 100) considerará mais tokens e levará a textos mais diversos, enquanto um valor menor (por exemplo, 10) concentrar-se-á nos tokens mais prováveis e gerará textos mais previsíveis.\n\n• O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads de CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Número de threads de CPU a usar durante a inferência.",
  "llm.prediction.llama.cpuThreads/info": "O número de threads a usar durante o processamento. Aumentar o número de threads nem sempre se reflete num melhor desempenho. O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.maxPredictedTokens/title": "Limitar tamanho da resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente, limite o comprimento da resposta da IA.",
  "llm.prediction.maxPredictedTokens/info": "Controle o tamanho máximo da resposta do chatbot. Ative para definir um limite no tamanho máximo de uma resposta ou desative para deixar que o chatbot decida quando parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Comprimento máximo da resposta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Por volta de {{maxWords}} palavras",
  "llm.prediction.llama.repeatPenalty/title": "Penalização por repetição",
  "llm.prediction.llama.repeatPenalty/subTitle": "Quantos a desencorajar repetir o mesmo token.",
  "llm.prediction.llama.repeatPenalty/info": "De acordo com os documentos de ajuda de llama.cpp: \"Ajuda a prevenir o modelo de gerar texto repetitivo ou monótono.\n\nUm valor maior (por exemplo, 1,5) penalizará as repetições de forma mais vincada, enquanto um valor menor (por exemplo, 0,9) será mais permissivo.\" • O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Amostragem Min P",
  "llm.prediction.llama.minPSampling/subTitle": "Probabilidade mínima para um token ser selecionado para saída.",
  "llm.prediction.llama.minPSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA probabilidade mínima para um token ser considerado em relação à probabilidade do token mais provável. Deve estar em [0, 1].\n\n• O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Amostragem Top P",
  "llm.prediction.llama.topPSampling/subTitle": "Probabilidade acumulada mínima para os possíveis próximos tokens. Atua de forma semelhante à temperatura.",
  "llm.prediction.llama.topPSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA amostragem top-p, também conhecida como amostragem de núcleo, é outro método de geração de texto que apenas seleciona o próximo token de um conjunto de tokens cujas probabilidades acumuladas são pelo menos p.\n\nEste método fornece um equilíbrio entre diversidade e qualidade considerando tanto as probabilidades dos tokens quanto o tamanho de tokens para amostragem.\n\nUm valor mais elevado para top-p (por exemplo, 0,95) levará a textos mais diversos, enquanto um valor menor (por exemplo, 0,5) gerará textos mais focados e previsíveis. Deve estar em (0, 1].\n\n• O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Strings de paragem",
  "llm.prediction.stopStrings/subTitle": "Strings que devem parar o modelo de gerar mais tokens.",
  "llm.prediction.stopStrings/info": "Strings específicas que, quando encontradas, farão com que o modelo pare de gerar mais tokens.",
  "llm.prediction.stopStrings/placeholder": "Insira uma string e pressione ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Interrupção de chat",
  "llm.prediction.contextOverflowPolicy/subTitle": "Como o modelo deve comportar-se quando a conversa cresce demais para lidar.",
  "llm.prediction.contextOverflowPolicy/info": "Decide o que fazer quando o chat excede o tamanho da memória do modelo ('contexto')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Parar no limite",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Para de gerar uma vez que a memória do modelo se encha.",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Truncar meio",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Remove mensagens do meio do chat para dar espaço para novas. O modelo ainda lembrará o início da chat.",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Mais recente",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "O modelo obterá sempre as últimas mensagens, mas pode esquecer o início da chat.",
  "llm.prediction.llama.frequencyPenalty/title": "Penalização por frequência",
  "llm.prediction.llama.presencePenalty/title": "Penalização por presença",
  "llm.prediction.llama.tailFreeSampling/title": "Amostragem Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem localmente típica",
  "llm.prediction.mlx.repeatPenalty/title": "Penalização por repetição",
  "llm.prediction.mlx.repeatPenalty/subTitle": "Quantos a desencorajar a repetir o mesmo token.",
  "llm.prediction.mlx.repeatPenalty/info": "Um valor maior desencoraja que o modelo se repita.",
  "llm.prediction.onnx.topKSampling/title": "Amostragem Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita o próximo token a um dos tokens top-k mais prováveis. Atua de forma semelhante à temperatura.",
  "llm.prediction.onnx.topKSampling/info": "De acordo com a documentação ONNX:\n\nNúmero de tokens de vocabulário mais prováveis para manter na filtragem top-k\n\n• Este filtro está desligado por princípio",
  "llm.prediction.onnx.repeatPenalty/title": "Penalização por repetição",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Quantos a desencorajar a repetir o mesmo token.",
  "llm.prediction.onnx.repeatPenalty/info": "Um valor maior desencoraja que o modelo se repita.",
  "llm.prediction.onnx.topPSampling/title": "Amostragem Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilidade acumulada mínima para os possíveis próximos tokens. Atua de forma semelhante à temperatura.",
  "llm.prediction.onnx.topPSampling/info": "De acordo com a documentação ONNX:\n\nSó os tokens mais prováveis com probabilidades que somam até TopP ou mais são mantidos para geração\n\n• Este filtro está desligado por princípio",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Geração estruturada",
  "llm.prediction.structured/info": "Geração estruturada",
  "llm.prediction.promptTemplate/title": "Modelo de prompt",
  "llm.prediction.promptTemplate/subTitle": "O formato em que as mensagens no chat são enviadas ao modelo. Mudar isso pode introduzir comportamento inesperado - certifique-se de saber o que está fazendo!",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Ocorreu um erro ao processar modelo Jinja: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manual",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Antes do sistema",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "Insira prefixo do sistema...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Depois do sistema",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "Insira sufixo do sistema...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Antes do utilizador",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "Insira prefixo do utilizador...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "Depois do utilizador",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "Insira sufixo do utilizador...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Antes do assistente",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "Insira prefixo do assistente...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Depois do assistente",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "Insira sufixo do assistente...",
  "llm.prediction.promptTemplate.stopStrings/label": "Strings de paragem adicionais",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "Strings de paragem específicas do modelo que serão usadas além das strings de paragem especificadas pelo utilizador",

  "llm.load.contextLength/title": "Tamanho do contexto",
  "llm.load.contextLength/subTitle": "O número máximo de tokens que o modelo pode atender em uma única solicitação. Verifique as opções de sobrecarga de conversa sob \"Parâmetros de inferência\" para mais formas de gerenciar isso.",
  "llm.load.contextLength/info": "Especifica o número máximo de tokens que o modelo pode considerar ao mesmo tempo, afetando o tamanho do contexto retido durante o processamento",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "A seed para o gerador de números aleatórios usado na geração de texto. -1 é aleatório.",
  "llm.load.seed/info": "Seed aleatória: Define a seed para geração de números aleatórios para garantir resultados reprodutíveis",

  "llm.load.llama.evalBatchSize/title": "Tamanho do lote de avaliação",
  "llm.load.llama.evalBatchSize/subTitle": "Número de tokens de entrada a processar por vez. Aumentar isso aumenta o desempenho ao custo da utilização de memória.",
  "llm.load.llama.evalBatchSize/info": "Define o número de exemplos processados em conjunto num lote durante a avaliação, afetando velocidade e uso de memória",
  "llm.load.llama.ropeFrequencyBase/title": "Frequência base da Embedding Posicional Rotativo (RoPE)",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frequência base personalizada para embeddings posicionais rotativos (RoPE). Aumentar isso pode habilitar melhor desempenho em contextos de comprimento alto.",
  "llm.load.llama.ropeFrequencyBase/info": "[Avançado] Ajusta a frequência base para Embedding Posicional Rotativo, afetando como informações posicionais são incorporadas",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequência da Embedding Posicional Rotativo (RoPE)",
  "llm.load.llama.ropeFrequencyScale/subTitle": "O comprimento do contexto é escalado por esse fator para estender o contexto efetivo usando RoPE.",
  "llm.load.llama.ropeFrequencyScale/info": "[Avançado] Modifica a escala da frequência para Embedding Posicional Rotativo (RoPE) para controlar a granularidade da codificação posicionai",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Número de camadas do modelo discretas a calcular no GPU para aceleração de GPU.",
  "llm.load.llama.acceleration.offloadRatio/info": "Defina o número de camadas a offloadar para o GPU.",
  "llm.load.llama.flashAttention/title": "Atenção Flash",
  "llm.load.llama.flashAttention/subTitle": "Reduz a utilização de memória e tempo de geração em alguns modelos.",
  "llm.load.llama.flashAttention/info": "Acelera mecanismos de atenção para um processamento mais rápido e eficiente",
  "llm.load.llama.keepModelInMemory/title": "Manter modelo em memória",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserva memória do sistema para o modelo, mesmo quando offload é efetuado para o GPU. Melhora o desempenho mas exige mais RAM do sistema.",
  "llm.load.llama.keepModelInMemory/info": "Previne que o modelo seja alocado para o disco, garantindo acesso mais rápido mas com a desvantagem de usar mais memória RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de memória utilizando a cache com metade da precisão (FP16)",
  "llm.load.llama.tryMmap/title": "Tentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Quando desligado pode melhorar o desempenho quando o modelo é maior que a RAM disponível no sistema.",
  "llm.load.llama.tryMmap/info": "Carrega ficheiros do modelo diretamente do disco para a memória",

  "embedding.load.contextLength/title": "Tamanho do contexto",
  "embedding.load.contextLength/subTitle": "O número máximo de tokens que o modelo pode atender em uma única solicitação. Verifique as opções de Overflow de conversa sob \"Parâmetros de inferência\" para mais formas de resolver este problema.",
  "embedding.load.contextLength/info": "Especifica o número máximo de tokens que o modelo pode considerar ao mesmo tempo, afetando o tamanho do contexto retido durante o processamento",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequência Base da Embedding Posicional Rotativo (RoPE)",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frequência base personalizada para embeddings posicionais rotativos (RoPE). Aumentar isso pode habilitar melhor desempenho em contextos de comprimento alto.",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avançado] Ajusta a frequência base para Embedding Posicional Rotativo, afetando como informações posicionais são incorporadas",
  "embedding.load.llama.evalBatchSize/title": "Tamanho da Lote de Avaliação",
  "embedding.load.llama.evalBatchSize/subTitle": "Número de tokens de entrada a processar por vez. Aumentar isso aumenta o desempenho ao custo da utilização de memória.",
  "embedding.load.llama.evalBatchSize/info": "Define o número de tokens processados juntos em um lote durante a avaliação",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequência da Embedding Posicional Rotativo (RoPE)",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "O comprimento do contexto é escalado por esse fator para estender o contexto efetivo usando RoPE.",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avançado] Modifica a escala da frequência para Embedding Posicional Rotativo (RoPE) para controlar a granularidade da codificação posicionai",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Número de camadas do modelo discretas a calcular no GPU para aceleração de GPU.",
  "embedding.load.llama.acceleration.offloadRatio/info": "Defina o número de camadas a offloadar para o GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Manter modelo em memória",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve memória do sistema para o modelo, mesmo quando offloadado para GPU. Melhora o desempenho mas exige mais RAM do sistema.",
  "embedding.load.llama.keepModelInMemory/info": "Previne que o modelo seja alocado para o disco, garantindo acesso mais rápido mas com a desvantagem de usar mais memória RAM",
  "embedding.load.llama.tryMmap/title": "Tentar mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desabilitar isso pode melhorar o desempenho quando o modelo é maior que a RAM disponível no sistema.",
  "embedding.load.llama.tryMmap/info": "Carrega os ficheiros do modelo diretamente do disco para a memória",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "A seed para o gerador de números aleatórios usado na geração de texto. -1 é aleatório.",
  "embedding.load.seed/info": "Seed aleatória: Define a seed para geração de números aleatórios para garantir resultados reprodutíveis"
}
