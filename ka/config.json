{
  "noInstanceSelected": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒ¡áƒšáƒ˜ áƒáƒ áƒ©áƒ”áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ",
  "resetToDefault": "áƒ©áƒáƒ›áƒáƒ§áƒ áƒ",
  "showAdvancedSettings": "áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—áƒ˜ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ",
  "showAll": "áƒ§áƒ•áƒ”áƒšáƒ",
  "basicSettings": "áƒ¡áƒáƒ‘áƒáƒ–áƒ˜áƒ¡áƒ",
  "configSubtitle": "Load or save presets and experiment with model parameter overrides",
  "inferenceParameters/title": "Prediction Parameters",
  "inferenceParameters/info": "Experiment with parameters that impact the prediction.",
  "generalParameters/title": "áƒ–áƒáƒ’áƒáƒ“áƒ˜",
  "samplingParameters/title": "áƒáƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜",
  "basicTab": "áƒ¡áƒáƒ‘áƒáƒ–áƒ˜áƒ¡áƒ",
  "advancedTab": "áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—",
  "advancedTab/title": "ğŸ§ª áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—áƒ˜ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜",
  "advancedTab/expandAll": "áƒ§áƒ•áƒ”áƒšáƒáƒ¡ áƒ©áƒáƒ›áƒáƒ¨áƒšáƒ",
  "advancedTab/overridesTitle": "áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¤áƒáƒ áƒ•áƒ”áƒ‘áƒ˜",
  "advancedTab/noConfigsText": "You have no unsaved changes - edit values above to see overrides here.",
  "loadInstanceFirst": "Load a model to view configurable parameters",
  "noListedConfigs": "áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒáƒ“áƒ˜ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”",
  "generationParameters/info": "Experiment with basic parameters which impact text generation.",
  "loadParameters/title": "áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ",
  "loadParameters/description": "Settings to control the way the model is initialized and loaded into memory.",
  "loadParameters/reload": "áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒšáƒáƒ“",
  "loadParameters/reload/error": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
  "discardChanges": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒªáƒ˜áƒšáƒ”áƒ‘áƒ",
  "loadModelToSeeOptions": "Load a model to see options",
  "schematicsError.title": "The config schematics contains errors in the following fields:",
  "manifestSections": {
    "structuredOutput/title": "áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒ›áƒ¥áƒáƒœáƒ” áƒ’áƒáƒ›áƒáƒ¢áƒáƒœáƒ",
    "speculativeDecoding/title": "áƒ¡áƒáƒ”áƒ™áƒ£áƒšáƒáƒªáƒ˜áƒ£áƒ áƒ˜ áƒ“áƒ”áƒ™áƒáƒ“áƒ˜áƒ áƒ”áƒ‘áƒ",
    "sampling/title": "áƒáƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜",
    "settings/title": "áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ",
    "toolUse/title": "áƒ®áƒ”áƒšáƒ¡áƒáƒ¬áƒ§áƒáƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ",
    "promptTemplate/title": "Prompt Template",
    "customFields/title": "áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ•áƒ”áƒšáƒ”áƒ‘áƒ˜"
  },
  "llm.prediction.systemPrompt/title": "áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒ§áƒ•áƒáƒœáƒ˜áƒ¡ áƒ–áƒáƒšáƒ˜",
  "llm.prediction.systemPrompt/description": "Use this field to provide background instructions to the model, such as a set of rules, constraints, or general requirements.",
  "llm.prediction.systemPrompt/subTitle": "Guidelines for the AI",
  "llm.prediction.systemPrompt/openEditor": "áƒ áƒ”áƒ“áƒáƒ¥áƒ¢áƒáƒ áƒ˜",
  "llm.prediction.systemPrompt/closeEditor": "áƒ áƒ”áƒ“áƒáƒ¥áƒ¢áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ®áƒ£áƒ áƒ•áƒ",
  "llm.prediction.systemPrompt/openedEditor": "áƒ’áƒáƒ˜áƒ®áƒ¡áƒœáƒ áƒ áƒ”áƒ“áƒáƒ¥áƒ¢áƒáƒ áƒ¨áƒ˜...",
  "llm.prediction.systemPrompt/edit": "áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ£áƒ áƒ˜ áƒ›áƒáƒ—áƒ®áƒáƒ•áƒœáƒ˜áƒ¡ áƒ•áƒ”áƒšáƒ˜áƒ¡ áƒ©áƒáƒ¡áƒ¬áƒáƒ áƒ”áƒ‘áƒ...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "áƒ˜áƒœáƒ¡áƒ¢áƒ áƒ£áƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ..",
  "llm.prediction.systemPrompt/addInstructions": "áƒ˜áƒœáƒ¡áƒ¢áƒ áƒ£áƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ..",
  "llm.prediction.temperature/title": "áƒ¢áƒ”áƒ›áƒáƒ”áƒ áƒáƒ¢áƒ£áƒ áƒ",
  "llm.prediction.temperature/subTitle": "How much randomness to introduce. 0 will yield the same result every time, while higher values will increase creativity and variance",
  "llm.prediction.temperature/info": "From llama.cpp help docs: \"The default value is <{{dynamicValue}}>, which provides a balance between randomness and determinism. At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run\"",
  "llm.prediction.llama.sampling/title": "áƒáƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜",
  "llm.prediction.topKSampling/title": "Top K-áƒ˜áƒ¡ áƒ¡áƒ”áƒ›áƒáƒšáƒ˜áƒœáƒ’áƒ˜",
  "llm.prediction.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.topKSampling/info": "From llama.cpp help docs:\n\nTop-k sampling is a text generation method that selects the next token only from the top k most likely tokens predicted by the model.\n\nIt helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output.\n\nA higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text.\n\nâ€¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU-áƒ˜áƒ¡ áƒœáƒáƒ™áƒáƒ“áƒ”áƒ‘áƒ˜",
  "llm.prediction.llama.cpuThreads/subTitle": "Number of CPU threads to use during inference",
  "llm.prediction.llama.cpuThreads/info": "The number of threads to use during computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ¡ áƒ¡áƒ˜áƒ’áƒ áƒ«áƒ˜áƒ¡ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ•áƒ",
  "llm.prediction.maxPredictedTokens/subTitle": "Optionally cap the length of the AI's response",
  "llm.prediction.maxPredictedTokens/info": "Control the max length of the chatbot's response. Turn on to set a limit on the max length of a response, or turn off to let the chatbot decide when to stop.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximum response length (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "áƒ“áƒáƒáƒ®áƒš {{maxWords}} áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ",
  "llm.prediction.repeatPenalty/title": "áƒ’áƒáƒ›áƒ”áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¯áƒáƒ áƒ˜áƒ›áƒ",
  "llm.prediction.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.repeatPenalty/info": "From llama.cpp help docs: \"Helps prevent the model from generating repetitive or monotonous text.\n\nA higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.\" â€¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P Sampling",
  "llm.prediction.minPSampling/subTitle": "Minimum base probability for a token to be selected for output",
  "llm.prediction.minPSampling/info": "From llama.cpp help docs:\n\nThe minimum probability for a token to be considered, relative to the probability of the most likely token. Must be in [0, 1].\n\nâ€¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P-áƒ˜áƒ¡ áƒ¡áƒ”áƒ›áƒáƒšáƒ˜áƒœáƒ’áƒ˜",
  "llm.prediction.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.topPSampling/info": "From llama.cpp help docs:\n\nTop-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p.\n\nThis method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from.\n\nA higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. Must be in (0, 1].\n\nâ€¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "áƒ’áƒáƒ©áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ”áƒ‘áƒ˜",
  "llm.prediction.stopStrings/subTitle": "Strings that should stop the model from generating more tokens",
  "llm.prediction.stopStrings/info": "Specific strings that when encountered will stop the model from generating more tokens",
  "llm.prediction.stopStrings/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ˜ áƒ“áƒ áƒ“áƒáƒáƒ­áƒ˜áƒ áƒ”áƒ— áƒ¦áƒ˜áƒšáƒáƒ™áƒ¡ â",
  "llm.prediction.contextOverflowPolicy/title": "áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒ’áƒáƒ“áƒáƒ˜áƒ•áƒ¡áƒ!",
  "llm.prediction.contextOverflowPolicy/subTitle": "How the model should behave when the conversation grows too large for it to handle",
  "llm.prediction.contextOverflowPolicy/info": "Decide what to do when the conversation exceeds the size of the model's working memory ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency Penalty",
  "llm.prediction.llama.presencePenalty/title": "áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ˜áƒ¡ áƒ¯áƒáƒ áƒ˜áƒ›áƒ",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.llama.xtcProbability/title": "XTC Sampling Probability",
  "llm.prediction.llama.xtcProbability/subTitle": "The XTC (Exclude Top Choices) sampler will only be activated with this probability per generated token. XTC sampling can boost creativity and reduce clichÃ©s",
  "llm.prediction.llama.xtcProbability/info": "XTC (Exclude Top Choices) sampling will only be activated with this probability, per generated token. XTC sampling usually boosts creativity and reduces clichÃ©s",
  "llm.prediction.llama.xtcThreshold/title": "XTC Sampling Threshold",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.llama.xtcThreshold/info": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.mlx.topKSampling/title": "Top K-áƒ˜áƒ¡ áƒ¡áƒ”áƒ›áƒáƒšáƒ˜áƒœáƒ’áƒ˜",
  "llm.prediction.mlx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.mlx.topKSampling/info": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/title": "Top K-áƒ˜áƒ¡ áƒ¡áƒ”áƒ›áƒáƒšáƒ˜áƒœáƒ’áƒ˜",
  "llm.prediction.onnx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/info": "From ONNX documentation:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\nâ€¢ This filter is turned off by default",
  "llm.prediction.onnx.repeatPenalty/title": "áƒ’áƒáƒ›áƒ”áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¯áƒáƒ áƒ˜áƒ›áƒ",
  "llm.prediction.onnx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.onnx.repeatPenalty/info": "áƒ áƒáƒª áƒ›áƒáƒ¦áƒáƒšáƒ˜áƒ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ, áƒ›áƒ˜áƒ— áƒ£áƒ¤áƒ áƒ áƒ¨áƒ”áƒ”áƒªáƒ“áƒ”áƒ‘áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜, áƒáƒ  áƒ’áƒáƒ›áƒ”áƒáƒ áƒ“áƒ”áƒ¡",
  "llm.prediction.onnx.topPSampling/title": "Top P-áƒ˜áƒ¡ áƒ¡áƒ”áƒ›áƒáƒšáƒ˜áƒœáƒ’áƒ˜",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topPSampling/info": "From ONNX documentation:\n\nOnly the most probable tokens with probabilities that add up to TopP or higher are kept for generation\n\nâ€¢ This filter is turned off by default",
  "llm.prediction.seed/title": "áƒ¡áƒ˜áƒ“áƒ˜",
  "llm.prediction.structured/title": "áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒ›áƒ¥áƒáƒœáƒ” áƒ’áƒáƒ›áƒáƒ¢áƒáƒœáƒ",
  "llm.prediction.structured/info": "áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒ›áƒ¥áƒáƒœáƒ” áƒ’áƒáƒ›áƒáƒ¢áƒáƒœáƒ",
  "llm.prediction.structured/description": "Advanced: you can provide a [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) to enforce a particular output format from the model. Read the [documentation](https://lmstudio.ai/docs/advanced/structured-output) to learn more",
  "llm.prediction.tools/title": "áƒ®áƒ”áƒšáƒ¡áƒáƒ¬áƒ§áƒáƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ",
  "llm.prediction.tools/description": "Advanced: you can provide a JSON-compliant list of tools for the model to request calls to. Read the [documentation](https://lmstudio.ai/docs/advanced/tool-use) to learn more",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pass this through the request body as `tools` when using the server API",
  "llm.prediction.promptTemplate/title": "Prompt Template",
  "llm.prediction.promptTemplate/subTitle": "The format in which messages in chat are sent to the model. Changing this may introduce unexpected behavior - make sure you know what you're doing!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Draft Tokens to Generate",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "The number of tokens to generate with the draft model per main model token. Find the sweet spot of compute vs. reward",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Drafting Probability Cutoff",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue drafting until a token's probability falls below this threshold. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min Draft Size",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Drafts smaller than this will be ignored by the main model. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max Draft Size",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Max number of tokens allowed in a draft. Ceiling if all token probs are > the cutoff. Lower values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.draftModel/title": "Draft Model",
  "llm.prediction.reasoning.parsing/title": "Reasoning Section Parsing",
  "llm.prediction.reasoning.parsing/subTitle": "How to parse reasoning sections in the model's output",
  "llm.load.mainGpu/title": "áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ GPU",
  "llm.load.mainGpu/subTitle": "áƒáƒ áƒ˜áƒáƒ áƒ˜áƒ¢áƒ”áƒ¢áƒ˜áƒ¡ áƒ›áƒ¥áƒáƒœáƒ” GPU áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ—áƒ•áƒšáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡",
  "llm.load.mainGpu/placeholder": "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ GPU...",
  "llm.load.splitStrategy/title": "áƒ’áƒáƒ§áƒáƒ¤áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒáƒ¢áƒ”áƒ’áƒ˜áƒ",
  "llm.load.splitStrategy/subTitle": "áƒ áƒáƒ’áƒáƒ  áƒ’áƒáƒ˜áƒ§áƒáƒ¤áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ—áƒ•áƒšáƒ”áƒ‘áƒ˜ GPU-áƒ”áƒ‘áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡",
  "llm.load.splitStrategy/placeholder": "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ’áƒáƒ§áƒáƒ¤áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒáƒ¢áƒ”áƒ’áƒ˜áƒ...",
  "llm.load.offloadKVCacheToGpu/title": "KV áƒ™áƒ”áƒ¨áƒ˜áƒ¡ áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒ GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜",
  "llm.load.offloadKVCacheToGpu/subTitle": "KV áƒ™áƒ”áƒ¨áƒ˜áƒ¡ áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒ GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜. áƒ–áƒ áƒ“áƒ˜áƒ¡ áƒ¬áƒáƒ áƒ›áƒáƒ“áƒáƒ‘áƒáƒ¡, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ›áƒ”áƒ¢áƒ˜ GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ áƒ¡áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ",
  "load.gpuStrictVramCap/title": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜ áƒ’áƒáƒ¢áƒáƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ•áƒ",
  "load.gpuStrictVramCap.customSubTitleOff": "áƒ’áƒáƒ›áƒáƒ áƒ—: áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ–áƒ˜áƒáƒ áƒ”áƒ‘áƒ£áƒš áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜ áƒ’áƒáƒ¢áƒáƒœáƒ˜áƒ¡ áƒ“áƒáƒ¨áƒ•áƒ”áƒ‘áƒ, áƒ—áƒ£ GPU-áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ˜áƒšáƒ˜ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ áƒ¡áƒáƒ•áƒ¡áƒ”áƒ",
  "load.gpuStrictVramCap.customSubTitleOn": "áƒ©áƒáƒ áƒ—: áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒáƒ•áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒáƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ˜áƒšáƒ˜ GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ–áƒ” áƒ“áƒ RAM-áƒ–áƒ”. áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¡ áƒ¯áƒ”áƒ  áƒ™áƒ˜áƒ“áƒ”áƒ• áƒ¨áƒ”áƒ£áƒ«áƒšáƒ˜áƒ áƒ’áƒáƒ–áƒ˜áƒáƒ áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ.",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ£áƒšáƒ˜áƒ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ˜áƒšáƒ˜ GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ–áƒ”. áƒ’áƒáƒ¢áƒáƒœáƒ˜áƒšáƒ˜ áƒ¤áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒœáƒáƒ›áƒ“áƒ•áƒ˜áƒšáƒ˜ áƒ áƒáƒáƒ“áƒ”áƒœáƒáƒ‘áƒ áƒªáƒ•áƒáƒšáƒ”áƒ‘áƒáƒ“áƒ˜áƒ",
  "load.allGpusDisabledWarning": "áƒáƒ›áƒŸáƒáƒ›áƒáƒ“ áƒ§áƒ•áƒ”áƒšáƒ GPU áƒ’áƒáƒ›áƒáƒ áƒ—áƒ£áƒšáƒ˜áƒ. áƒ—áƒ£ áƒ’áƒœáƒ”áƒ‘áƒáƒ•áƒ— áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒ, áƒ©áƒáƒ áƒ—áƒ”áƒ— áƒ”áƒ áƒ—áƒ˜ áƒ›áƒáƒ˜áƒœáƒª",
  "llm.load.contextLength/title": "áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¡áƒ˜áƒ’áƒ áƒ«áƒ”",
  "llm.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "llm.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "llm.load.contextLength/warning": "Setting a high value for context length can significantly impact memory usage",
  "llm.load.seed/title": "áƒ¡áƒ˜áƒ“áƒ˜",
  "llm.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random",
  "llm.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",
  "llm.load.numCpuExpertLayersRatio/title": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒœáƒáƒ«áƒáƒšáƒáƒ“áƒ”áƒ•áƒ˜ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ CPU-áƒ–áƒ”",
  "llm.load.numCpuExpertLayersRatio/subTitle": "áƒ›áƒáƒ®áƒ“áƒ”áƒ‘áƒ áƒ—áƒ£ áƒáƒ áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒœáƒáƒ«áƒáƒšáƒáƒ“áƒ”áƒ•áƒ˜ áƒ’áƒáƒ¢áƒáƒœáƒ CPU-áƒ˜áƒ¡ RAM-áƒ¨áƒ˜. áƒ–áƒáƒ’áƒáƒ•áƒ¡ VRAM-áƒ¡ áƒ“áƒ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ, áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ GPU-áƒ–áƒ” áƒœáƒáƒ¬áƒ˜áƒšáƒáƒ‘áƒ áƒ˜áƒ• áƒ’áƒáƒ¢áƒáƒœáƒáƒ–áƒ” áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜áƒª áƒ™áƒ˜ áƒ˜áƒ§áƒáƒ¡. áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ, áƒ—áƒ£ áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ›áƒ—áƒšáƒ˜áƒáƒœáƒáƒ“ áƒ”áƒ¢áƒ”áƒ•áƒ VRAM-áƒ¨áƒ˜.",
  "llm.load.numCpuExpertLayersRatio/info": "áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒ”áƒ‘áƒ¡, áƒ›áƒáƒ®áƒ“áƒ”áƒ‘áƒ áƒ—áƒ£ áƒáƒ áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¬áƒáƒœáƒ˜áƒ¡ áƒ¤áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒ CPU-áƒ˜áƒ¡ RAM-áƒ¨áƒ˜. áƒ¢áƒáƒ•áƒ”áƒ‘áƒ¡ áƒ§áƒ£áƒ áƒáƒ“áƒ¦áƒ”áƒ‘áƒ˜áƒ¡ áƒ¤áƒ”áƒœáƒ”áƒ‘áƒ¡ GPU-áƒ–áƒ”, áƒ áƒ˜áƒ—áƒ˜áƒª áƒ–áƒáƒ’áƒáƒ•áƒ¡ VRAM-áƒ¡ áƒ¬áƒáƒ áƒ›áƒáƒ“áƒáƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ¡áƒáƒ¦áƒ”áƒ‘ áƒ“áƒáƒœáƒ”áƒ–áƒ” áƒ¨áƒ”áƒœáƒáƒ áƒ©áƒ£áƒœáƒ”áƒ‘áƒ˜áƒ—",
  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "llm.load.llama.evalBatchSize/info": "Sets the number of examples processed together in one batch during evaluation, affecting speed and memory usage",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "llm.load.llama.acceleration.offloadRatio/title": "áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¢áƒáƒœáƒ GPU-áƒ–áƒ”",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "llm.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Decreases memory usage and generation time on some models",
  "llm.load.llama.flashAttention/info": "áƒáƒ©áƒ¥áƒáƒ áƒ”áƒ‘áƒ¡ áƒ§áƒ£áƒ áƒáƒ“áƒ¦áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ¥áƒªáƒ”áƒ•áƒ˜áƒ¡ áƒ›áƒ”áƒ¥áƒáƒœáƒ˜áƒ–áƒ›áƒ”áƒ‘áƒ¡ áƒ£áƒ¤áƒ áƒ áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜ áƒ“áƒ áƒ”áƒ¤áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ˜ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡",
  "llm.load.numExperts/title": "áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ áƒáƒáƒ“áƒ”áƒœáƒáƒ‘áƒ",
  "llm.load.numExperts/subTitle": "Number of experts to use in the model",
  "llm.load.numExperts/info": "The number of experts to use in the model",
  "llm.load.llama.keepModelInMemory/title": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ“áƒáƒ¢áƒáƒ•áƒ”áƒ‘áƒ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "llm.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "llm.load.llama.useFp16ForKVCache/title": "Use FP16 For KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Reduces memory usage by storing cache in half-precision (FP16)",
  "llm.load.llama.tryMmap/title": "mmap()-áƒ˜áƒ¡ áƒªáƒ“áƒ",
  "llm.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "llm.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU áƒœáƒáƒ™áƒáƒ“áƒ˜áƒ¡ áƒáƒ£áƒšáƒ˜áƒ¡ áƒ–áƒáƒ›áƒ",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Number of CPU threads to allocate to the thread pool used for model computation",
  "llm.load.llama.cpuThreadPoolSize/info": "The number of CPU threads to allocate to the thread pool used for model computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K áƒ™áƒ”áƒ¨áƒ˜áƒ¡ áƒ“áƒáƒ™áƒ•áƒáƒœáƒ¢áƒ•áƒ˜áƒ¡ áƒ¢áƒ˜áƒáƒ˜",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/title": "V Cache Quantization Type",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "âš ï¸ You must disable this value if Flash Attention is not enabled",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Can only be turned on when Flash Attention is enabled",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "âš ï¸ You must disable flash attention when using F32",
  "llm.load.mlx.kvCacheBits/title": "KV áƒ™áƒ”áƒ¨áƒ˜áƒ¡ áƒ“áƒáƒ™áƒ•áƒáƒœáƒ¢áƒ•áƒ",
  "llm.load.mlx.kvCacheBits/subTitle": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Context Length setting is ignored when using KV Cache Quantization",
  "llm.load.mlx.kvCacheGroupSize/title": "KV Cache Quantization: Group Size",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Group size during quantization operation for the KV cache. Higher group size reduces memory usage but may decrease quality",
  "llm.load.mlx.kvCacheGroupSize/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV Cache Quantization: Start quantizing when ctx crosses this length",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantization/title": "KV áƒ™áƒ”áƒ¨áƒ˜áƒ¡ áƒ“áƒáƒ™áƒ•áƒáƒœáƒ¢áƒ•áƒ",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize the model's KV cache. This may result in faster generation and lower memory footprint,\nat the expense of the quality of the model output.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV cache quantization bits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Number of bits to quantize the KV cache to",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "áƒ‘áƒ˜áƒ¢áƒ˜",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "áƒ¯áƒ’áƒ£áƒ¤áƒ˜áƒ¡ áƒ–áƒáƒ›áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒáƒ¢áƒ”áƒ’áƒ˜áƒ",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "áƒ¡áƒ˜áƒ–áƒ£áƒ¡áƒ¢áƒ”",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "áƒ“áƒáƒ‘áƒáƒšáƒáƒœáƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "áƒ¡áƒ¬áƒ áƒáƒ¤áƒáƒ“",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Advanced: Quantized 'matmul group size' configuration\n\nâ€¢ Accuracy = group size 32\nâ€¢ Balanced = group size 64\nâ€¢ Speedy = group size 128",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Start quantizing when ctx reaches this length",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "When the context reaches this amount of tokens,\nbegin quantizing the KV cache",
  "embedding.load.contextLength/title": "áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¡áƒ˜áƒ’áƒ áƒ«áƒ”",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.acceleration.offloadRatio/title": "áƒ“áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¢áƒáƒœáƒ GPU-áƒ–áƒ”",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ“áƒáƒ¢áƒáƒ•áƒ”áƒ‘áƒ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒáƒ¨áƒ˜",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "mmap()-áƒ˜áƒ¡ áƒªáƒ“áƒ",
  "embedding.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "embedding.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "embedding.load.seed/title": "áƒ¡áƒ˜áƒ“áƒ˜",
  "embedding.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random seed",
  "embedding.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",
  "presetTooltip": {
    "included/title": "áƒ¬áƒ˜áƒœáƒ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜",
    "included/description": "The following fields will be applied",
    "included/empty": "No fields of this preset apply in this context.",
    "included/conflict": "You will be asked to choose whether to apply this value",
    "separateLoad/title": "áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒ“áƒ áƒáƒ˜áƒ¡ áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "áƒ áƒáƒ› áƒ’áƒáƒ“áƒáƒ¢áƒáƒ áƒ“áƒ”áƒ¡ áƒ áƒáƒ–áƒ”",
    "separateLoad/description.3": ".",
    "excluded/title": "áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ, áƒáƒ  áƒ”áƒ™áƒ£áƒ—áƒ•áƒœáƒáƒ“áƒ”áƒ¡",
    "excluded/description": "The following fields are included in the preset but does not apply in the current context.",
    "legacy/title": "áƒ›áƒáƒ«áƒ•áƒ”áƒšáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable.",
    "button/publish": "áƒ°áƒáƒ‘áƒ–áƒ” áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ",
    "button/pushUpdate": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ áƒ°áƒáƒ‘áƒ–áƒ”",
    "button/noChangesToPush": "áƒ’áƒáƒ¡áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”",
    "button/export": "áƒ’áƒáƒ¢áƒáƒœáƒ",
    "hubLabel": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ Hub-áƒ“áƒáƒœ. áƒáƒ•áƒ¢áƒáƒ áƒ˜ {{user}}",
    "ownHubLabel": "áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ Hub-áƒ“áƒáƒœ"
  },
  "customInputs": {
    "string": {
      "emptyParagraph": "<áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜>"
    },
    "checkboxNumeric": {
      "off": "áƒ’áƒáƒ›áƒáƒ áƒ—"
    },
    "llamaCacheQuantizationType": {
      "off": "áƒ’áƒáƒ›áƒáƒ áƒ—"
    },
    "mlxKvCacheBits": {
      "off": "áƒ’áƒáƒ›áƒáƒ áƒ—"
    },
    "stringArray": {
      "empty": "<áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜>"
    },
    "llmPromptTemplate": {
      "type": "áƒ¢áƒ˜áƒáƒ˜",
      "types.jinja/label": "áƒ¨áƒáƒ‘áƒšáƒáƒœáƒ˜ (Jinja)",
      "jinja.bosToken/label": "BOS áƒ¢áƒáƒ™áƒ”áƒœáƒ˜",
      "jinja.eosToken/label": "EOS áƒ¢áƒáƒ™áƒ”áƒœáƒ˜",
      "jinja.template/label": "áƒ¨áƒáƒ‘áƒšáƒáƒœáƒ˜",
      "jinja/error": "Jinja-áƒ˜áƒ¡ áƒœáƒ˜áƒ›áƒ£áƒ¨áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {{error}}",
      "jinja/empty": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ–áƒ”áƒ›áƒáƒ— Jinja-áƒ˜áƒ¡ áƒ¨áƒáƒ‘áƒšáƒáƒœáƒ˜.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "áƒ®áƒ”áƒšáƒ˜áƒ—",
      "manual.subfield.beforeSystem/label": "áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒáƒ›áƒ“áƒ”",
      "manual.subfield.beforeSystem/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ£áƒ áƒ˜ áƒáƒ áƒ”áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜...",
      "manual.subfield.afterSystem/label": "áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’",
      "manual.subfield.afterSystem/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ£áƒ áƒ˜ áƒ¡áƒ£áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜...",
      "manual.subfield.beforeUser/label": "áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒáƒ›áƒ“áƒ”",
      "manual.subfield.beforeUser/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒ˜áƒ¡ áƒáƒ áƒ”áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜...",
      "manual.subfield.afterUser/label": "áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’",
      "manual.subfield.afterUser/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒ˜áƒ¡ áƒ¡áƒ£áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜...",
      "manual.subfield.beforeAssistant/label": "áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ›áƒ“áƒ”",
      "manual.subfield.beforeAssistant/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ˜áƒ¡ áƒáƒ áƒ”áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜...",
      "manual.subfield.afterAssistant/label": "áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’",
      "manual.subfield.afterAssistant/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ˜áƒ¡ áƒ¡áƒ£áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜...",
      "stopStrings/label": "áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—áƒ˜ áƒ’áƒáƒ©áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ”áƒ‘áƒ˜",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "This is the maximum number of tokens the model was trained to handle. Click to set the context to this value",
      "maxValueTextStart": "áƒ›áƒáƒ“áƒ”áƒšáƒ¡ áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ áƒáƒ¥áƒ•áƒ¡",
      "maxValueTextEnd": "áƒ¢áƒáƒ™áƒ”áƒœáƒ˜",
      "tooltipHint": "While a model may support up to a certain number of tokens, performance may deteriorate if your machine's resources cannot handle the load - use caution when increasing this value"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "áƒ’áƒáƒ©áƒ”áƒ áƒ”áƒ‘áƒ áƒ–áƒ¦áƒ•áƒáƒ áƒ—áƒáƒœ",
      "stopAtLimitSub": "Stop generating once the model's memory gets full",
      "truncateMiddle": "áƒ¨áƒ£áƒáƒ¡ áƒ¬áƒáƒ™áƒ•áƒ”áƒ—áƒ",
      "truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
      "rollingWindow": "áƒ›áƒªáƒ£áƒ áƒáƒ•áƒ˜ áƒ¤áƒáƒœáƒ¯áƒáƒ áƒ",
      "rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "áƒ›áƒáƒ¥áƒ¡",
      "off": "áƒ’áƒáƒ›áƒáƒ áƒ—"
    },
    "gpuSplitStrategy": {
      "evenly": "áƒ—áƒáƒœáƒáƒ‘áƒ áƒáƒ“",
      "favorMainGpu": "áƒáƒ áƒ˜áƒáƒ áƒ˜áƒ¢áƒ”áƒ¢áƒ˜ áƒ›áƒ—áƒáƒ•áƒáƒ  GPU-áƒ–áƒ”"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ, áƒ áƒáƒ’áƒáƒ  áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡",
      "placeholder": "Select a compatible draft model",
      "noCompatible": "No compatible draft models found for your current model selection",
      "stillLoading": "Identifying compatible draft models...",
      "notCompatible": "The selected draft model (<draft/>) is not compatible with the current model selection (<current/>).",
      "off": "áƒ’áƒáƒ›áƒáƒ áƒ—",
      "loadModelToSeeOptions": "Load model <keyboard-shortcut /> to see compatible options",
      "compatibleWithNumberOfModels": "Recommended for at least {{dynamicValue}} of your models",
      "recommendedForSomeModels": "áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ áƒ–áƒáƒ’áƒ˜áƒ”áƒ áƒ—áƒ˜ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡",
      "recommendedForLlamaModels": "áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ Llama-áƒ˜áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡",
      "recommendedForQwenModels": "áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ Qwen-áƒ˜áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡",
      "onboardingModal": {
        "introducing": "áƒ¬áƒáƒ áƒ›áƒáƒ’áƒ˜áƒ“áƒ’áƒ”áƒœáƒ—",
        "speculativeDecoding": "áƒ¡áƒáƒ”áƒ™áƒ£áƒšáƒáƒªáƒ˜áƒ£áƒ áƒ˜ áƒ“áƒ”áƒ™áƒáƒ“áƒ˜áƒ áƒ”áƒ‘áƒ",
        "firstStepBody": "Inference speedup for <custom-span>llama.cpp</custom-span> and <custom-span>MLX</custom-span> models",
        "secondStepTitle": "Inference Speedup with Speculative Decoding",
        "secondStepBody": "Speculative Decoding is a technique involving the collaboration of two models:\n - A larger \"main\" model\n - A smaller \"draft\" model\n\nDuring generation, the draft model rapidly proposes tokens for the larger main model to verify. Verifying tokens is a much faster process than actually generating them, which is the source of the speed gains. **Generally, the larger the size difference between the main model and the draft model, the greater the speed-up**.\n\nTo maintain quality, the main model only accepts tokens that align with what it would have generated itself, enabling the response quality of the larger model at faster inference speeds. Both models must share the same vocabulary.",
        "draftModelRecommendationsTitle": "Draft model recommendations",
        "basedOnCurrentModels": "áƒ—áƒ¥áƒ•áƒ”áƒœáƒ¡ áƒáƒ›áƒŸáƒáƒ›áƒ˜áƒœáƒ“áƒ”áƒš áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ–áƒ” áƒ“áƒáƒ§áƒ áƒ“áƒœáƒáƒ‘áƒ˜áƒ—",
        "close": "áƒ“áƒáƒ®áƒ£áƒ áƒ•áƒ",
        "next": "áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’áƒ˜",
        "done": "áƒ›áƒ–áƒáƒ“áƒáƒ"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Please load a model first <model-badge /> ",
      "errorEngineNotSupported": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine (<key/>) and reload the model to use this feature.",
      "errorEngineNotSupported/noKey": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine and reload the model to use this feature."
    },
    "llmReasoningParsing": {
      "startString/label": "áƒ“áƒáƒ¬áƒ§áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ˜",
      "startString/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ“áƒáƒ¬áƒ§áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ˜...",
      "endString/label": "áƒ“áƒáƒ¡áƒ áƒ£áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ˜",
      "endString/placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ“áƒáƒ¡áƒ áƒ£áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ˜..."
    }
  },
  "saveConflictResolution": {
    "title": "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ—, áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜ áƒ©áƒáƒ˜áƒ¡áƒ›áƒ”áƒ‘áƒ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜",
    "description": "Pick and choose which values to keep",
    "instructions": "áƒ“áƒáƒáƒ¬áƒ™áƒáƒáƒ£áƒœáƒ”áƒ— áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒáƒ–áƒ” áƒ›áƒ˜áƒ¡ áƒ©áƒáƒ¡áƒáƒ¡áƒ›áƒ”áƒšáƒáƒ“",
    "userValues": "áƒ¬áƒ˜áƒœáƒ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ",
    "presetValues": "áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’áƒ˜ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ",
    "confirm": "áƒ“áƒáƒ“áƒáƒ¡áƒ¢áƒ£áƒ áƒ”áƒ‘áƒ",
    "cancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ"
  },
  "applyConflictResolution": {
    "title": "áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜ áƒ“áƒáƒ•áƒ˜áƒ¢áƒáƒ•áƒ?",
    "description": "You have uncommitted changes which overlap with the incoming Preset",
    "instructions": "áƒ“áƒáƒáƒ¬áƒ™áƒáƒáƒ£áƒœáƒ”áƒ— áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒáƒ–áƒ” áƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒœáƒáƒ áƒ©áƒ£áƒœáƒ”áƒ‘áƒšáƒáƒ“",
    "userValues": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ” áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ",
    "presetValues": "áƒ¨áƒ”áƒ›áƒáƒ›áƒáƒ•áƒáƒšáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ",
    "confirm": "áƒ“áƒáƒ“áƒáƒ¡áƒ¢áƒ£áƒ áƒ”áƒ‘áƒ",
    "cancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ"
  },
  "empty": "<áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜>",
  "noModelSelected": "áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒáƒ áƒ©áƒ”áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ",
  "apiIdentifier.label": "API-áƒ˜áƒ¡ áƒ˜áƒ“áƒ”áƒœáƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ˜",
  "apiIdentifier.hint": "Optionally provide an identifier for this model. This will be used in API requests. Leave blank to use the default identifier.",
  "idleTTL.label": "áƒáƒ•áƒ¢áƒáƒ’áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ, áƒ—áƒ£ áƒ£áƒ¥áƒ›áƒ”áƒ (TTL)",
  "idleTTL.hint": "If set, the model will be automatically unloaded after being idle for the specified amount of time.",
  "idleTTL.mins": "áƒ¬áƒ—",
  "presets": {
    "title": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜",
    "saveChanges": "áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ",
    "saveChanges/description": "áƒ¨áƒ”áƒ˜áƒœáƒáƒ®áƒ”áƒ— áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜.",
    "saveChanges.manual": "áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒšáƒ˜áƒ áƒáƒ®áƒáƒšáƒ˜ áƒ•áƒ”áƒšáƒ”áƒ‘áƒ˜. áƒ¨áƒ”áƒ’áƒ”áƒ«áƒšáƒ”áƒ‘áƒáƒ—, áƒáƒ˜áƒ áƒ©áƒ˜áƒáƒ—, áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ áƒ’áƒœáƒ”áƒ‘áƒáƒ•áƒ—, áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜ áƒ©áƒáƒ¡áƒ•áƒáƒ—",
    "saveChanges.manual.hold.0": "áƒ’áƒ”áƒ­áƒ˜áƒ áƒáƒ—",
    "saveChanges.manual.hold.1": "áƒ áƒáƒ› áƒáƒ˜áƒ áƒ©áƒ˜áƒáƒ—, áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ áƒ’áƒœáƒ”áƒ‘áƒáƒ•áƒ—, áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜ áƒ©áƒáƒ¡áƒ•áƒáƒ—.",
    "saveChanges.saveAll.hold.0": "áƒ’áƒ”áƒ­áƒ˜áƒ áƒáƒ—",
    "saveChanges.saveAll.hold.1": "áƒ áƒáƒ› áƒ©áƒáƒ¡áƒ•áƒáƒ— áƒ§áƒ•áƒ”áƒšáƒ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ.",
    "saveChanges.saveInPreset.hold.0": "áƒ’áƒ”áƒ­áƒ˜áƒ áƒáƒ—",
    "saveChanges.saveInPreset.hold.1": "áƒ áƒáƒ› áƒ©áƒáƒ¡áƒ•áƒáƒ—, áƒ›áƒ®áƒáƒšáƒáƒ“, áƒ˜áƒ¡ áƒ•áƒ”áƒšáƒ”áƒ‘áƒ˜, áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒª áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜ áƒ£áƒ™áƒ•áƒ” áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ¡.",
    "saveChanges/error": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ.",
    "saveChanges.manual/description": "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ—, áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜ áƒ’áƒœáƒ”áƒ‘áƒáƒ•áƒ—, áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜ áƒ©áƒáƒ¡áƒ•áƒáƒ—.",
    "saveAs": "áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ, áƒ áƒáƒ’áƒáƒ áƒª áƒáƒ®áƒáƒšáƒ˜...",
    "presetNamePlaceholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ˜...",
    "cannotCommitChangesLegacy": "This is a legacy preset and cannot be modified. You can create a copy by using \"Save As New...\".",
    "cannotSaveChangesNoChanges": "áƒ¨áƒ”áƒ¡áƒáƒœáƒáƒ®áƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”.",
    "emptyNoUnsaved": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒáƒ áƒ©áƒ”áƒ•áƒ...",
    "emptyWithUnsaved": "áƒ¨áƒ”áƒ£áƒœáƒáƒ®áƒáƒ•áƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜",
    "saveEmptyWithUnsaved": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ, áƒ áƒáƒ’áƒáƒ áƒª...",
    "saveConfirm": "áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ",
    "saveCancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ",
    "saving": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ...",
    "save/error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ.",
    "deselect": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ›áƒáƒœáƒ˜áƒ¨áƒ•áƒœáƒ˜áƒ¡ áƒ›áƒáƒ®áƒ¡áƒœáƒ",
    "deselect/error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ›áƒáƒœáƒ˜áƒ¨áƒ•áƒœáƒ˜áƒ¡ áƒ›áƒáƒ®áƒ¡áƒœáƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ.",
    "select/error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒáƒ áƒ©áƒ”áƒ•áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ.",
    "delete/error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¬áƒáƒ¨áƒšáƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ.",
    "discardChanges": "áƒ¨áƒ”áƒ£áƒœáƒáƒ®áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒªáƒ˜áƒšáƒ”áƒ‘áƒ",
    "discardChanges/info": "Discard all uncommitted changes and restore the preset to its original state",
    "newEmptyPreset": "+ áƒáƒ®áƒáƒšáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜",
    "importPreset": "áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ",
    "contextMenuCopyIdentifier": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ˜áƒ“áƒ”áƒœáƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ˜áƒ¡ áƒ™áƒáƒáƒ˜áƒ áƒ”áƒ‘áƒ",
    "contextMenuSelect": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ",
    "contextMenuDelete": "áƒ¬áƒáƒ¨áƒšáƒ...",
    "contextMenuShare": "áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ...",
    "contextMenuOpenInHub": "áƒœáƒáƒ®áƒ•áƒ áƒ°áƒáƒ‘áƒ–áƒ”",
    "contextMenuPullFromHub": "áƒ£áƒáƒ®áƒšáƒ”áƒ¡áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ",
    "contextMenuPushChanges": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ áƒ°áƒáƒ‘áƒ–áƒ”",
    "contextMenuPushingChanges": "áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ...",
    "contextMenuPushedChanges": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ£áƒšáƒ˜áƒ",
    "contextMenuExport": "áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ’áƒáƒ¢áƒáƒœáƒ",
    "contextMenuRevealInExplorer": "áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ›áƒáƒ áƒ—áƒ•áƒ”áƒšáƒ¨áƒ˜",
    "contextMenuRevealInFinder": "áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ Finder-áƒ¨áƒ˜",
    "share": {
      "title": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "áƒ›áƒ¤áƒšáƒáƒ‘áƒ”áƒšáƒ˜",
      "uploadAs": "áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ áƒ¨áƒ”áƒ˜áƒ¥áƒ›áƒœáƒ”áƒ‘áƒ áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ— {{name}}",
      "presetNameLabel": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ˜",
      "descriptionLabel": "áƒáƒ¦áƒ¬áƒ”áƒ áƒ (áƒáƒ áƒáƒ¡áƒáƒ•áƒáƒšáƒ“áƒ”áƒ‘áƒ£áƒšáƒ)",
      "loading": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ...",
      "success": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ’áƒáƒ“áƒáƒ˜áƒªáƒ",
      "presetIsLive": "<preset-name /> áƒáƒ®áƒšáƒ áƒ°áƒáƒ‘áƒ–áƒ”áƒ!",
      "close": "áƒ“áƒáƒ®áƒ£áƒ áƒ•áƒ",
      "confirmViewOnWeb": "áƒœáƒáƒ®áƒ•áƒ áƒ•áƒ”áƒ‘áƒ–áƒ”",
      "confirmCopy": "URL-áƒ˜áƒ¡ áƒ™áƒáƒáƒ˜áƒ áƒ”áƒ‘áƒ",
      "confirmCopied": "áƒ“áƒáƒ™áƒáƒáƒ˜áƒ áƒ“áƒ!",
      "pushedToHub": "áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ áƒ’áƒáƒ“áƒáƒ˜áƒ’áƒ–áƒáƒ•áƒœáƒ Hub-áƒ–áƒ”",
      "descriptionPlaceholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒáƒ¦áƒ¬áƒ”áƒ áƒ...",
      "willBePublic": "áƒ’áƒáƒ›áƒáƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ— áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜, áƒ áƒáƒ› áƒ˜áƒ¡ áƒ¡áƒáƒ¯áƒáƒ áƒ áƒ’áƒáƒ®áƒ“áƒ”áƒ¡",
      "willBePrivate": "áƒáƒ› áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¡, áƒ›áƒ®áƒáƒšáƒáƒ“, áƒ—áƒ¥áƒ•áƒ”áƒœ áƒ“áƒáƒ˜áƒœáƒáƒ®áƒáƒ•áƒ—",
      "willBeOrgVisible": "áƒáƒ› áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¡ áƒáƒ áƒ’áƒáƒœáƒ˜áƒ–áƒáƒªáƒ˜áƒáƒ¨áƒ˜ áƒ§áƒ•áƒ”áƒšáƒ áƒ“áƒáƒ˜áƒœáƒáƒ®áƒáƒ•áƒ¡.",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "privateUsageReached": "áƒ›áƒ˜áƒ¦áƒ¬áƒ”áƒ£áƒšáƒ˜áƒ áƒáƒ˜áƒ áƒáƒ“áƒ˜ áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ áƒáƒáƒ“áƒ”áƒœáƒáƒ‘áƒ˜áƒ¡ áƒ–áƒ¦áƒ•áƒáƒ áƒ˜",
      "continueInBrowser": "áƒ‘áƒ áƒáƒ£áƒ–áƒ”áƒ áƒ¨áƒ˜ áƒ’áƒáƒ’áƒ áƒ«áƒ”áƒšáƒ”áƒ‘áƒ",
      "confirmShareButton": "áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ",
      "error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ áƒ°áƒáƒ‘áƒ–áƒ”",
      "title/success": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ’áƒáƒœáƒáƒ®áƒšáƒ“áƒ",
      "subtitle": "Make changes to <custom-preset-name /> and push them to the Hub",
      "descriptionLabel": "áƒáƒ¦áƒ¬áƒ”áƒ áƒ",
      "descriptionPlaceholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒáƒ¦áƒ¬áƒ”áƒ áƒ...",
      "loading": "áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ...",
      "cancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "áƒ’áƒáƒœáƒáƒ®áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ¬áƒáƒ“áƒ”áƒ‘áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
      "confirmUpdateButton": "áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ"
    },
    "resolve": {
      "title": "áƒ™áƒáƒœáƒ¤áƒšáƒ˜áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ­áƒ áƒ...",
      "tooltip": "áƒ›áƒáƒ“áƒáƒšáƒ˜áƒ¡ áƒ’áƒáƒ®áƒ¡áƒœáƒ Hub-áƒ˜áƒ¡ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒáƒ¡áƒ—áƒáƒœ áƒ¡áƒ®áƒ•áƒáƒáƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¡áƒáƒ­áƒ áƒ”áƒšáƒáƒ“"
    },
    "loginToManage": {
      "title": "áƒ¡áƒáƒ›áƒáƒ áƒ—áƒáƒ•áƒáƒ“ áƒ¨áƒ”áƒ“áƒ˜áƒ—..."
    },
    "downloadFromHub": {
      "title": "áƒ’áƒáƒ“áƒ›áƒáƒ¬áƒ”áƒ áƒ",
      "downloading": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ’áƒáƒ“áƒ›áƒáƒ¬áƒ”áƒ áƒ...",
      "success": "áƒ’áƒáƒ“áƒ›áƒáƒ¬áƒ”áƒ áƒ˜áƒšáƒ˜áƒ!",
      "error": "áƒ’áƒáƒ“áƒ›áƒáƒ¬áƒ”áƒ áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ"
    },
    "push": {
      "title": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ",
      "pushing": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ...",
      "success": "áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ",
      "tooltip": "áƒšáƒáƒ™áƒáƒšáƒ£áƒ áƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ Hub-áƒ–áƒ” áƒáƒ áƒ¡áƒ”áƒ‘áƒ£áƒš áƒ•áƒ”áƒ áƒ¡áƒ˜áƒáƒ–áƒ”",
      "error": "áƒ’áƒáƒ“áƒáƒªáƒ”áƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ"
    },
    "saveAsNewModal": {
      "title": "áƒ•áƒáƒ˜! Hub-áƒ–áƒ” áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒšáƒ˜ áƒáƒ áƒáƒ",
      "confirmSaveAsNewDescription": "áƒ’áƒœáƒ”áƒ‘áƒáƒ•áƒ— áƒáƒ› áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒáƒ®áƒšáƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ˜áƒ— áƒ’áƒáƒ¢áƒáƒœáƒ?",
      "confirmButton": "áƒ’áƒáƒ›áƒáƒ¥áƒ•áƒ”áƒ§áƒœáƒ”áƒ‘áƒ áƒáƒ®áƒšáƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ˜áƒ—"
    },
    "pull": {
      "title": "áƒ£áƒáƒ®áƒšáƒ”áƒ¡áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ",
      "error": "áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
      "contextMenuErrorMessage": "áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
      "success": "áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ˜áƒšáƒ˜áƒ",
      "pulling": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ...",
      "upToDate": "áƒ’áƒáƒœáƒáƒ®áƒšáƒ”áƒ‘áƒ£áƒšáƒ˜áƒ!",
      "unsavedChangesModal": {
        "title": "áƒ’áƒáƒ¥áƒ•áƒ— áƒ¨áƒ”áƒ£áƒœáƒáƒ®áƒáƒ•áƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜.",
        "bodyContent": "áƒ“áƒáƒ¨áƒáƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ“áƒáƒœ áƒ’áƒáƒ›áƒáƒ—áƒ®áƒáƒ•áƒ áƒšáƒáƒ™áƒáƒšáƒ£áƒ  áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ¡ áƒ—áƒáƒ•áƒ–áƒ” áƒ’áƒáƒ“áƒáƒáƒ¬áƒ”áƒ áƒ¡. áƒ’áƒáƒ•áƒáƒ’áƒ áƒ«áƒ”áƒšáƒ?",
        "confirmButton": "áƒ¨áƒ”áƒ£áƒœáƒáƒ®áƒáƒ•áƒ˜ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ—áƒáƒ•áƒ–áƒ” áƒ’áƒáƒ“áƒáƒ¬áƒ”áƒ áƒ"
      }
    },
    "import": {
      "title": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ",
      "dragPrompt": "Drag and drop preset JSON files or <custom-link>select from your computer</custom-link>",
      "remove": "áƒ¬áƒáƒ¨áƒšáƒ",
      "cancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ",
      "importPreset_zero": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ",
      "importPreset_one": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ",
      "importPreset_other": "{{count}} áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ",
      "selectDialog": {
        "title": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒáƒ áƒ©áƒ”áƒ•áƒ (.json)",
        "button": "áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ"
      },
      "error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
      "resultsModal": {
        "titleSuccessSection_one": "áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ˜áƒšáƒ˜áƒ 1 áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜",
        "titleSuccessSection_other": "áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ˜áƒšáƒ˜áƒ {{count}} áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜",
        "titleFailSection_zero": "0",
        "titleFailSection_one": "({{count}} áƒ©áƒáƒ•áƒáƒ áƒ“áƒ)",
        "titleFailSection_other": "({{count}} áƒ©áƒáƒ•áƒáƒ áƒ“áƒ)",
        "titleAllFailed": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
        "importMore": "áƒ›áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ",
        "close": "áƒ›áƒ–áƒáƒ“áƒáƒ",
        "successBadge": "áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ",
        "alreadyExistsBadge": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜ áƒ£áƒ™áƒ•áƒ” áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ¡",
        "errorBadge": "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ",
        "invalidFileBadge": "áƒáƒ áƒáƒ¡áƒ¬áƒáƒ áƒ˜ áƒ¤áƒáƒ˜áƒšáƒ˜",
        "otherErrorBadge": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ",
        "errorViewDetailsButton": "áƒ“áƒ”áƒ¢áƒáƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒœáƒáƒ®áƒ•áƒ",
        "seeError": "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ˜áƒ¡ áƒœáƒáƒ®áƒ•áƒ",
        "noName": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”",
        "useInChat": "áƒ©áƒáƒ¢áƒ¨áƒ˜ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ"
      },
      "importFromUrl": {
        "button": "áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ URL-áƒ“áƒáƒœ...",
        "title": "áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ URL-áƒ“áƒáƒœ",
        "back": "áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ",
        "cancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ",
        "loading": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ”áƒáƒ‘áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¢áƒáƒœáƒ...",
        "error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ“áƒ›áƒáƒ¬áƒ”áƒ áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ."
      }
    },
    "download": {
      "title": "<preset-name />-áƒ˜áƒ¡ áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ LM Studio-áƒ˜áƒ¡ áƒ°áƒáƒ‘áƒ˜áƒ“áƒáƒœ",
      "subtitle": "Save <custom-name /> to your presets. Doing so you will allow you to use this preset in the app",
      "button": "áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ",
      "button/loading": "áƒ›áƒ˜áƒ¦áƒ”áƒ‘áƒ...",
      "cancel": "áƒ’áƒáƒ£áƒ¥áƒ›áƒ”áƒ‘áƒ",
      "error": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ“áƒ›áƒáƒ¬áƒ”áƒ áƒ áƒ©áƒáƒ•áƒáƒ áƒ“áƒ."
    },
    "inclusiveness": {
      "speculativeDecoding": "áƒáƒ áƒ”áƒ¡áƒ”áƒ¢áƒ¨áƒ˜ áƒ©áƒáƒ¡áƒ›áƒ"
    }
  },
  "flashAttentionWarning": "Flash Attention is an experimental feature that may cause issues with some models. If you encounter problems, try disabling it.",
  "llamaKvCacheQuantizationWarning": "KV Cache Quantization is an experimental feature that may cause issues with some models. Flash Attention must be enabled for V cache quantization. If you encounter problems, reset to the default \"F16\".",
  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "áƒáƒ•áƒ¢áƒ",
  "ropeFrequencyScaleUncheckedHint": "áƒáƒ•áƒ¢áƒ",
  "hardware": {
    "environmentVariables": "áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ”áƒ‘áƒ˜",
    "environmentVariables.info": "áƒ—áƒ£ áƒáƒ  áƒ˜áƒªáƒ˜áƒ—, áƒ áƒáƒ¡ áƒáƒ™áƒ”áƒ—áƒ”áƒ‘áƒ—, áƒ“áƒáƒ¢áƒáƒ•áƒ”áƒ— áƒœáƒáƒ’áƒ£áƒšáƒ˜áƒ¡áƒ®áƒ›áƒ”áƒ•áƒ˜ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜",
    "environmentVariables.reset": "áƒœáƒáƒ’áƒ£áƒšáƒ˜áƒ¡áƒ®áƒ›áƒ”áƒ•áƒ–áƒ” áƒ©áƒáƒ›áƒáƒ§áƒ áƒ",
    "gpus.information": "áƒ—áƒ¥áƒ•áƒ”áƒœáƒ¡ áƒ›áƒáƒœáƒ¥áƒáƒœáƒáƒ–áƒ” áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒšáƒ˜ áƒ’áƒ áƒáƒ¤áƒ˜áƒ™áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒ‘áƒáƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ¡ (GPU) áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ",
    "gpuSettings": {
      "editMaxCapacity": "áƒ›áƒáƒ¥áƒ¡. áƒ¢áƒ”áƒ•áƒáƒ“áƒáƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¡áƒ¬áƒáƒ áƒ”áƒ‘áƒ",
      "hideEditMaxCapacity": "áƒ›áƒáƒ¥áƒ¡. áƒ¢áƒ”áƒ•áƒáƒ“áƒáƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¡áƒ¬áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒáƒšáƒ•áƒ",
      "allOffWarning": "áƒ§áƒ•áƒ”áƒšáƒ GPU áƒ’áƒáƒ›áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒáƒœ áƒ’áƒáƒ—áƒ˜áƒ¨áƒ£áƒšáƒ˜áƒ. áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¡áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒáƒ“ áƒ“áƒáƒ áƒ¬áƒ›áƒ£áƒœáƒ“áƒ˜áƒ—, áƒ áƒáƒ› áƒ®áƒ”áƒšáƒ›áƒ˜áƒ¡áƒáƒ¬áƒ•áƒ“áƒáƒ›áƒ˜áƒ áƒ”áƒ áƒ—áƒ˜ GPU áƒ›áƒáƒ˜áƒœáƒª",
      "split": {
        "title": "áƒ¡áƒ¢áƒ áƒáƒ¢áƒ”áƒ’áƒ˜áƒ",
        "placeholder": "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— GPU-áƒ˜áƒ¡ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ",
        "options": {
          "generalDescription": "áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒ”áƒ—, áƒ áƒáƒ’áƒáƒ  áƒ›áƒáƒ®áƒ“áƒ”áƒ‘áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ GPU-áƒ¨áƒ˜",
          "evenly": {
            "title": "áƒ—áƒáƒœáƒáƒ‘áƒ áƒáƒ“ áƒ’áƒáƒ§áƒáƒ¤áƒ",
            "description": "áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ—áƒáƒœáƒáƒ‘áƒ áƒáƒ“ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ GPU-áƒ”áƒ‘áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡"
          },
          "priorityOrder": {
            "title": "áƒáƒ áƒ˜áƒáƒ áƒ˜áƒ¢áƒ”áƒ¢áƒ˜áƒ¡ áƒ›áƒ˜áƒ›áƒ“áƒ”áƒ•áƒ áƒáƒ‘áƒ",
            "description": "áƒ’áƒáƒ“áƒáƒáƒ—áƒ áƒ˜áƒ”áƒ— áƒáƒ áƒ˜áƒáƒ áƒ˜áƒ¢áƒ”áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒªáƒ•áƒšáƒ”áƒšáƒáƒ“. áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ áƒ”áƒªáƒ“áƒ”áƒ‘áƒ, áƒ›áƒ”áƒ¢áƒ˜ áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¡ GPU-áƒ”áƒ‘áƒ–áƒ”, áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒª áƒ¡áƒ˜áƒ˜áƒ¡ áƒ“áƒáƒ¡áƒáƒ¬áƒ§áƒ˜áƒ¡áƒ¨áƒ˜áƒ"
          },
          "custom": {
            "title": "áƒ®áƒ”áƒšáƒ˜áƒ—",
            "description": "áƒ›áƒ”áƒ®áƒ¡áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ",
            "maxAllocation": "áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒáƒšáƒ£áƒ áƒ˜ áƒ’áƒáƒ›áƒáƒ§áƒáƒ¤áƒ"
          }
        }
      },
      "deviceId.info": "áƒáƒ› áƒ›áƒáƒ¬áƒ§áƒáƒ‘áƒ˜áƒšáƒáƒ‘áƒ˜áƒ¡ áƒ£áƒœáƒ˜áƒ™áƒáƒšáƒ£áƒ áƒ˜ áƒ˜áƒ“áƒ”áƒœáƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ˜",
      "changesOnlyAffectNewlyLoadedModels": "áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜, áƒ›áƒ®áƒáƒšáƒáƒ“, áƒáƒ®áƒšáƒáƒ“ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ£áƒš áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ¡ áƒ”áƒ®áƒ”áƒ‘áƒ",
      "toggleGpu": "GPU-áƒ˜áƒ¡ áƒ©áƒáƒ áƒ—áƒ•áƒ/áƒ’áƒáƒ›áƒáƒ áƒ—áƒ•áƒ"
    }
  },
  "load.gpuSplitConfig/title": "GPU-áƒ˜áƒ¡ áƒ’áƒáƒ§áƒáƒ¤áƒ˜áƒ¡ áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ",
  "envVars/title": "áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ˜áƒ¡ áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ",
  "envVars": {
    "select": {
      "placeholder": "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ˜...",
      "noOptions": "áƒ›áƒ”áƒ¢áƒ˜ áƒ®áƒ”áƒšáƒ›áƒ˜áƒ¡áƒáƒ¬áƒ•áƒ“áƒáƒ›áƒ˜ áƒáƒ áƒáƒ",
      "filter": {
        "placeholder": "áƒ«áƒ”áƒ‘áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¤áƒ˜áƒšáƒ¢áƒ•áƒ áƒ",
        "resultsFound_zero": "áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”",
        "resultsFound_one": "áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒšáƒ˜áƒ áƒ”áƒ áƒ—áƒ˜ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜",
        "resultsFound_other": "áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒšáƒ˜áƒ {{count}} áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜"
      }
    },
    "inputValue": {
      "placeholder": "áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ"
    },
    "values": {
      "title": "áƒ›áƒ˜áƒ›áƒ“áƒ˜áƒœáƒáƒ áƒ” áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜"
    }
  }
}
