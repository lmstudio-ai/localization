{
  "noInstanceSelected": "≈Ω√°dn√° instance modelu nen√≠ vybr√°na",
  "resetToDefault": "Reset",
  "showAdvancedSettings": "Zobrazit pokroƒçil√© nastaven√≠",
  "showAll": "V≈°e",
  "basicSettings": "Z√°kladn√≠",
  "configSubtitle": "Nahr√°t nebo ulo≈æit p≈ôedvolby a experimentovat s p≈ôekryty parametr≈Ø modelu",
  "inferenceParameters/title": "Parametry p≈ôedpovƒõdi",
  "inferenceParameters/info": "Experimentovat s parametry, kter√© ovliv≈àuj√≠ p≈ôedpovƒõƒè.",
  "generalParameters/title": "Obecn√©",
  "samplingParameters/title": "Vyberov√°n√≠",
  "basicTab": "Z√°kladn√≠",
  "advancedTab": "Pokroƒçil√©",
  "advancedTab/title": "üß™ Pokroƒçil√© konfigurace",
  "advancedTab/expandAll": "Rozbalit v≈°e",
  "advancedTab/overridesTitle": "P≈ôepis konfigurace",
  "advancedTab/noConfigsText": "Nem√°te ≈æ√°dn√© neulo≈æen√© zmƒõny - upravte hodnoty v√Ω≈°e, abyste vidƒõli p≈ôekryty zde.",
  "loadInstanceFirst": "Nahr√°t model, abyste vidƒõli konfigurovateln√© parametry",
  "noListedConfigs": "≈Ω√°dn√© konfigurovateln√© parametry",
  "generationParameters/info": "Experimentovat s z√°kladn√≠mi parametry, kter√© ovliv≈àuj√≠ generov√°n√≠ textu.",
  "loadParameters/title": "Nahr√°t parametry",
  "loadParameters/description": "Nastaven√≠ pro ≈ô√≠zen√≠ zp≈Øsobu inicializace a naƒçten√≠ modelu do pamƒõti.",
  "loadParameters/reload": "P≈ôelo≈æit pro pou≈æit√≠ zmƒõn",
  "discardChanges": "Zahodit zmƒõny",
  "llm.prediction.systemPrompt/title": "Syst√©mov√Ω p≈ô√≠kaz",
  "llm.prediction.systemPrompt/description": "Pou≈æijte toto pole k poskytnut√≠ pozad√≠ instrukc√≠ modelu, jako je nap≈ô√≠klad sadou pravidel, omezen√≠ nebo obecn√Ωch po≈æadavk≈Ø. Toto pole je tak√© ƒçasto oznaƒçov√°no jako \"syst√©mov√Ω prompt\".",
  "llm.prediction.systemPrompt/subTitle": "Pokyny pro AI",
  "llm.prediction.temperature/title": "Teplota",
  "llm.prediction.temperature/subTitle": "Kolik n√°hodnosti m√° p≈ôidat. 0 v≈ædy d√° stejn√Ω v√Ωsledek, zat√≠mco vy≈°≈°√≠ hodnoty zv√Ω≈°√≠ kreativitu a variabilitu",
  "llm.prediction.temperature/info": "Z dokumentace llama.cpp: \"V√Ωchoz√≠ hodnota je <{{dynamicValue}}>, co≈æ poskytuje rovnov√°hu mezi n√°hodnost√≠ a determinismem. Na kraji, teplota 0 v≈ædy vybere nejbli≈æ≈°√≠ n√°sleduj√≠c√≠ token, co≈æ vede k tomu, ≈æe ka≈æd√Ω v√Ωstup bude stejn√Ω\"",
  "llm.prediction.llama.sampling/title": "Vyberov√°n√≠",
  "llm.prediction.topKSampling/title": "Top K Sampling",
  "llm.prediction.topKSampling/subTitle": "Omez√≠ n√°sleduj√≠c√≠ token na jeden z nejv√≠ce pravdƒõpodobn√Ωch token≈Ø top-k. P≈Øsob√≠ podobnƒõ jako teplota",
  "llm.prediction.topKSampling/info": "Z dokumentace llama.cpp:\n\nTop-k sampling je metoda generov√°n√≠ textu, kter√° vyb√≠r√° n√°sleduj√≠c√≠ token pouze z nejv√≠ce pravdƒõpodobn√Ωch token≈Ø, kter√© model p≈ôedpov√≠d√°.\n\nPom√°h√° sn√≠≈æit riziko generov√°n√≠ n√≠zkoprocentn√≠ch nebo nesmysln√Ωch token≈Ø, ale tak√© omezuje diverzitu v√Ωstupu.\n\nVy≈°≈°√≠ hodnota pro top-k (nap≈ô. 100) zv√°≈æ√≠ o v√≠ce token≈Ø a vede k v√≠ce diverzn√≠mu textu, zat√≠mco ni≈æ≈°√≠ hodnota (nap≈ô. 10) se bude soust≈ôedit na nejv√≠ce pravdƒõpodobn√© tokeny a generovat v√≠ce konzervativn√≠ text.\n\n‚Ä¢ V√Ωchoz√≠ hodnota je <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU vl√°kna",
  "llm.prediction.llama.cpuThreads/subTitle": "Poƒçet vl√°ken CPU, kter√© se maj√≠ pou≈æ√≠t p≈ôi inferenci",
  "llm.prediction.llama.cpuThreads/info": "Poƒçet vl√°ken, kter√© se maj√≠ pou≈æ√≠t p≈ôi v√Ωpoƒçtu. Zv√Ω≈°en√≠ poƒçtu vl√°ken nez√°vis√≠ na tom, zda v√Ωkon je lep≈°√≠. V√Ωchoz√≠ je <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Omezen√≠ d√©lky odpovƒõdi",
  "llm.prediction.maxPredictedTokens/subTitle": "Volitelnƒõ omezte d√©lku odpovƒõdi AI",
  "llm.prediction.maxPredictedTokens/info": "Ovl√°d√°n√≠ maxim√°ln√≠ d√©lky odpovƒõdi chatbota. Zapnƒõte pro omezen√≠ maxim√°ln√≠ d√©lky odpovƒõdi, nebo vypnƒõte pro povolen√≠ chatbota, aby rozhodl, kdy skonƒçit.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maxim√°ln√≠ d√©lka odpovƒõdi (token≈Ø)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "O d√©lce {{maxWords}} slov",
  "llm.prediction.repeatPenalty/title": "Odst≈ôeƒèovac√≠ pokuta",
  "llm.prediction.repeatPenalty/subTitle": "Nakolik m√° b√Ωt opakov√°n√≠ stejn√©ho tokenu potlaƒçeno",
  "llm.prediction.repeatPenalty/info": "Z dokumentace llama.cpp: \"Pom√°h√° p≈ôedch√°zet modelu generov√°n√≠ opakuj√≠c√≠ho se nebo monotonn√≠ho textu.\n\nVy≈°≈°√≠ hodnota (nap≈ô. 1,5) bude v√≠ce penlizovat opakov√°n√≠, zat√≠mco ni≈æ≈°√≠ hodnota (nap≈ô. 0,9) bude m√©nƒõ p≈ô√≠sn√°.\" ‚Ä¢ V√Ωchoz√≠ hodnota je <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Mininim√°ln√≠ P Sampling",
  "llm.prediction.minPSampling/subTitle": "Mininim√°ln√≠ z√°kladn√≠ pravdƒõpodobnost pro v√Ωbƒõr tokenu pro v√Ωstup",
  "llm.prediction.minPSampling/info": "Z dokumentace llama.cpp:\n\nMinim√°ln√≠ pravdƒõpodobnost pro token, aby byl zva≈æov√°n, vzhledem k pravdƒõpodobnosti nejv√≠ce pravdƒõpodobn√©ho tokenu. Mus√≠ b√Ωt v rozmez√≠ [0, 1].\n\n‚Ä¢ V√Ωchoz√≠ hodnota je <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Vzorkov√°n√≠ Top P",
  "llm.prediction.topPSampling/subTitle": "Mininim√°ln√≠ kumulativn√≠ pravdƒõpodobnost pro mo≈æn√© n√°sleduj√≠c√≠ tokeny. P≈Øsob√≠ podobnƒõ jako teplota",
  "llm.prediction.topPSampling/info": "Z dokumentace llama.cpp:\n\nTop-p sampling, tak√© zn√°m√© jako nucleus sampling, je dal≈°√≠ metodou generov√°n√≠ textu, kter√° vyb√≠r√° n√°sleduj√≠c√≠ token z podmno≈æiny token≈Ø, kter√© spoleƒçnƒõ maj√≠ kumulativn√≠ pravdƒõpodobnost nejm√©nƒõ p.\n\nTato metoda poskytuje rovnov√°hu mezi diverzitou a kvalitou, zva≈æuj√≠c√≠ pravdƒõpodobnosti token≈Ø a poƒçet token≈Ø, ze kter√Ωch se m√° vz√≠t vzorek.\n\nVy≈°≈°√≠ hodnota pro top-p (nap≈ô. 0,95) vede k v√≠ce diverzn√≠mu textu, zat√≠mco ni≈æ≈°√≠ hodnota (nap≈ô. 0,5) generuje m√©nƒõ konzervativn√≠ a koncentrovan√Ω text. Mus√≠ b√Ωt v rozmez√≠ (0, 1].\n\n‚Ä¢ V√Ωchoz√≠ hodnota je <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "≈òetƒõzce pro zastaven√≠",
  "llm.prediction.stopStrings/subTitle": "≈òetƒõzce, kter√© by mƒõly zastavit model generov√°n√≠ dal≈°√≠ch token≈Ø",
  "llm.prediction.stopStrings/info": "Konkr√©tn√≠ ≈ôetƒõzce, kter√© p≈ôi sv√©m nalezen√≠ zastav√≠ model generov√°n√≠ dal≈°√≠ch token≈Ø",
  "llm.prediction.stopStrings/placeholder": "Vlo≈æte ≈ôetƒõzec a stisknƒõte ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "P≈ôeteƒçen√≠ konverzace",
  "llm.prediction.contextOverflowPolicy/subTitle": "Jak se model m√° chovat, kdy≈æ konverzace p≈ôestane b√Ωt vhodn√° pro jeho zpracov√°n√≠",
  "llm.prediction.contextOverflowPolicy/info": "Rozhodnƒõte, co se stane, kdy≈æ konverzace p≈ôekroƒç√≠ velikost pracovn√≠ pamƒõti modelu ('context')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Zastavit na limitu",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Zastav√≠ generov√°n√≠, kdy≈æ je model pln√Ω",
  "customInputs.contextOverflowPolicy.truncateMiddle": "Odsek√°vat st≈ôed",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Odstran√≠ zpr√°vy z st≈ôedu konverzace, aby se vytvo≈ôilo m√≠sto pro nov√©. Model si v≈°ak pamatuje zaƒç√°tek konverzace",
  "customInputs.contextOverflowPolicy.rollingWindow": "Rolovac√≠ okno",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "Model bude v≈ædy dost√°vat nƒõkolik nejnovƒõj≈°√≠ch zpr√°v, ale m≈Ø≈æe zapomenout na zaƒç√°tek konverzace",
  "llm.prediction.llama.frequencyPenalty/title": "Frekvence pokuty",
  "llm.prediction.llama.presencePenalty/title": "P≈ô√≠tomnost pokuty",
  "llm.prediction.llama.tailFreeSampling/title": "Vzorek bez ocasu",
  "llm.prediction.llama.locallyTypicalSampling/title": "M√≠stn√≠ typick√Ω vzorek",
  "llm.prediction.onnx.topKSampling/title": "Vzorkov√°n√≠ Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Omez√≠ n√°sleduj√≠c√≠ token na jeden z nejv√≠ce pravdƒõpodobn√Ωch token≈Ø top-k. P≈Øsob√≠ podobnƒõ jako teplota",
  "llm.prediction.onnx.topKSampling/info": "Z dokumentace ONNX:\n\nPoƒçet nejv√≠ce pravdƒõpodobn√Ωch slovn√≠kov√Ωch token≈Ø, kter√© se maj√≠ ponechat pro top-k-filtrov√°n√≠\n\n‚Ä¢ Tento filtr je vypnut√Ω",
  "llm.prediction.onnx.repeatPenalty/title": "Opakov√°n√≠ pokuty",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Kolik tolik m√° b√Ωt opakov√°n√≠ stejn√©ho tokenu potlaƒçeno",
  "llm.prediction.onnx.repeatPenalty/info": "Vy≈°≈°√≠ hodnota potlaƒç√≠ opakov√°n√≠ stejn√©ho tokenu",
  "llm.prediction.onnx.topPSampling/title": "Vzorkov√°n√≠ Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Mininim√°ln√≠ kumulativn√≠ pravdƒõpodobnost pro mo≈æn√© n√°sleduj√≠c√≠ tokeny. P≈Øsob√≠ podobnƒõ jako teplota",
  "llm.prediction.onnx.topPSampling/info": "Z dokumentace ONNX:\n\nPro generov√°n√≠ jsou ponech√°ny pouze nejpravdƒõpodobnƒõj≈°√≠ tokeny s pravdƒõpodobnost√≠ TopP nebo vy≈°≈°√≠.\n\n‚Ä¢ Tento filtr je vypnut√Ω",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Strukturovan√Ω v√Ωstup",
  "llm.prediction.structured/info": "Strukturovan√Ω v√Ωstup",
  "llm.prediction.promptTemplate/title": "≈†ablona p≈ô√≠kazu",
  "llm.prediction.promptTemplate/subTitle": "Form√°t, ve kter√©m jsou zpr√°vy v konverzaci odesl√°ny modelu. Zmƒõna tohoto form√°tu m≈Ø≈æe zp≈Øsobit neoƒçek√°van√© chov√°n√≠ - ujistƒõte se, ≈æe v√≠te, co dƒõl√°te!",

  "llm.load.contextLength/title": "D√©lka kontextu",
  "llm.load.contextLength/subTitle": "Nejvƒõt≈°√≠ poƒçet token≈Ø, kter√© model m≈Ø≈æe zpracovat v jedn√© zpr√°vƒõ. Pod√≠vejte se na mo≈ænosti p≈ôeteƒçen√≠ konverzace pod \"Parametry p≈ôedpovƒõdi\" pro dal≈°√≠ zp≈Øsoby ≈ô√≠zen√≠ tohoto limitu",
  "llm.load.contextLength/info": "Urƒçuje maxim√°ln√≠ poƒçet token≈Ø, kter√© model m≈Ø≈æe zpracovat najednou, co≈æ ovliv≈àuje, kolik kontextu si model pamatuje bƒõhem zpracov√°n√≠",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "Seed pro generov√°n√≠ n√°hodn√Ωch ƒç√≠sel. -1 je n√°hodn√Ω seed",
  "llm.load.seed/info": "N√°hodn√Ω seed: Nastav√≠ seed pro generov√°n√≠ n√°hodn√Ωch ƒç√≠sel pro zaji≈°tƒõn√≠ reprodukovateln√Ωch v√Ωsledk≈Ø",

  "llm.load.llama.evalBatchSize/title": "Velikost d√°vky vyhodnocen√≠",
  "llm.load.llama.evalBatchSize/subTitle": "Poƒçet vstupn√≠ch token≈Ø, kter√© se maj√≠ zpracovat najednou. Zv√Ω≈°en√≠ tohoto ƒç√≠sla zvy≈°uje v√Ωkon, ale zvy≈°uje tak√© vyu≈æit√≠ pamƒõti",
  "llm.load.llama.evalBatchSize/info": "Nastav√≠ poƒçet token≈Ø zpracovan√Ωch spoleƒçnƒõ v jedn√© d√°vce bƒõhem vyhodnocen√≠, co≈æ ovliv≈àuje rychlost a vyu≈æit√≠ pamƒõti",
  "llm.load.llama.ropeFrequencyBase/title": "Z√°kladn√≠ frekvence RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Vlastn√≠ z√°kladn√≠ frekvence pro rotaci polohov√©ho vkl√°d√°n√≠ (RoPE). Zv√Ω≈°en√≠ tohoto ƒç√≠sla m≈Ø≈æe umo≈ænit lep≈°√≠ v√Ωkon p≈ôi vysok√Ωch d√©lk√°ch kontextu",
  "llm.load.llama.ropeFrequencyBase/info": "[Pokroƒçil√©] Nastav√≠ z√°kladn√≠ frekvenci pro rotaci polohov√©ho vkl√°d√°n√≠, co≈æ ovliv≈àuje, jak je polohov√° informace vlo≈æena",
  "llm.load.llama.ropeFrequencyScale/title": "Mƒõ≈ô√≠tko frekvence RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "D√©lka kontextu je vyn√°sobena t√≠mto faktorem pro roz≈°√≠≈ôen√≠ efektivn√≠ho kontextu pomoc√≠ RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Pokroƒçil√©] Upravuje mƒõ≈ô√≠tko frekvence pro rotaci polohov√©ho vkl√°d√°n√≠, co≈æ ovliv≈àuje, jak je polohov√° informace vlo≈æena",
  "llm.load.llama.acceleration.offloadRatio/title": "Odlo≈æen√≠ na GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Poƒçet diskr√©tn√≠ch vrstev modelu, kter√© se maj√≠ vypoƒç√≠tat na GPU pro GPU zrychlen√≠",
  "llm.load.llama.acceleration.offloadRatio/info": "Nastav√≠ poƒçet vrstev, kter√© se maj√≠ vypoƒç√≠tat na GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Sn√≠≈æ√≠ vyu≈æit√≠ pamƒõti a ƒças generov√°n√≠ u nƒõkter√Ωch model≈Ø",
  "llm.load.llama.flashAttention/info": "Zrychl√≠ mechanismy pozornosti pro rychlej≈°√≠ a efektivn√≠ zpracov√°n√≠",
  "llm.load.llama.keepModelInMemory/title": "Uchovat model v pamƒõti",
  "llm.load.llama.keepModelInMemory/subTitle": "Rezervovat syst√©movou pamƒõ≈• pro model, i kdy≈æ je odlo≈æen na GPU. Zlep≈°uje v√Ωkon, ale vy≈æaduje v√≠ce syst√©mov√© RAM",
  "llm.load.llama.keepModelInMemory/info": "P≈ôedch√°z√≠ v√Ωmƒõnƒõ modelu na disk, zaji≈°≈•uje rychlej≈°√≠ p≈ô√≠stup za cenu vy≈°≈°√≠ho vyu≈æit√≠ RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Pou≈æ√≠t FP16 pro KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Sn√≠≈æ√≠ vyu≈æit√≠ pamƒõti t√≠m, ≈æe ukl√°d√° cache v polovinƒõ p≈ôesnosti (FP16)",
  "llm.load.llama.tryMmap/title": "Pokusit se o mmap()",
  "llm.load.llama.tryMmap/subTitle": "Zlep≈°√≠ ƒças naƒç√≠t√°n√≠ modelu. Vypnut√≠ tohoto m≈Ø≈æe zlep≈°it v√Ωkon, kdy≈æ je model vƒõt≈°√≠ ne≈æ dostupn√° syst√©mov√° RAM",
  "llm.load.llama.tryMmap/info": "Naƒçte soubory modelu p≈ô√≠mo z disku do pamƒõti",

  "embedding.load.contextLength/title": "D√©lka kontextu",
  "embedding.load.contextLength/subTitle": "Nejvƒõt≈°√≠ poƒçet token≈Ø, kter√© model m≈Ø≈æe zpracovat v jedn√© zpr√°vƒõ. Pod√≠vejte se na mo≈ænosti p≈ôeteƒçen√≠ konverzace pod \"Parametry p≈ôedpovƒõdi\" pro dal≈°√≠ zp≈Øsoby ≈ô√≠zen√≠ tohoto limitu",
  "embedding.load.contextLength/info": "Urƒçuje maxim√°ln√≠ poƒçet token≈Ø, kter√© model m≈Ø≈æe zpracovat najednou, co≈æ ovliv≈àuje, kolik kontextu si model pamatuje bƒõhem zpracov√°n√≠",
  "embedding.load.llama.ropeFrequencyBase/title": "Z√°kladn√≠ frekvence RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Vlastn√≠ z√°kladn√≠ frekvence pro rotaci polohov√©ho vkl√°d√°n√≠ (RoPE). Zv√Ω≈°en√≠ tohoto ƒç√≠sla m≈Ø≈æe umo≈ænit lep≈°√≠ v√Ωkon p≈ôi vysok√Ωch d√©lk√°ch kontextu",
  "embedding.load.llama.ropeFrequencyBase/info": "[Pokroƒçil√©] Upravuje z√°kladn√≠ frekvenci pro rotaci polohov√©ho vkl√°d√°n√≠, co≈æ ovliv≈àuje, jak je polohov√° informace vlo≈æena",
  "embedding.load.llama.evalBatchSize/title": "Velikost d√°vky vyhodnocen√≠",
  "embedding.load.llama.evalBatchSize/subTitle": "Poƒçet vstupn√≠ch token≈Ø, kter√© se maj√≠ zpracovat najednou. Zv√Ω≈°en√≠ tohoto ƒç√≠sla zvy≈°uje v√Ωkon, ale zvy≈°uje tak√© vyu≈æit√≠ pamƒõti",
  "embedding.load.llama.evalBatchSize/info": "Nastav√≠ poƒçet token≈Ø zpracovan√Ωch spoleƒçnƒõ v jedn√© d√°vce bƒõhem vyhodnocen√≠, co≈æ ovliv≈àuje rychlost a vyu≈æit√≠ pamƒõti",
  "embedding.load.llama.ropeFrequencyScale/title": "Mƒõ≈ô√≠tko frekvence RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "D√©lka kontextu je vyn√°sobena t√≠mto faktorem pro roz≈°√≠≈ôen√≠ efektivn√≠ho kontextu pomoc√≠ RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Pokroƒçil√©] Upravuje mƒõ≈ô√≠tko frekvence pro rotaci polohov√©ho vkl√°d√°n√≠, co≈æ ovliv≈àuje, jak je polohov√° informace vlo≈æena",
  "embedding.load.llama.acceleration.offloadRatio/title": "Odlo≈æen√≠ na GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Poƒçet diskr√©tn√≠ch vrstev modelu, kter√© se maj√≠ vypoƒç√≠tat na GPU pro GPU zrychlen√≠",
  "embedding.load.llama.acceleration.offloadRatio/info": "Nastav√≠ poƒçet vrstev, kter√© se maj√≠ vypoƒç√≠tat na GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Uchovat model v pamƒõti",
  "embedding.load.llama.keepModelInMemory/subTitle": "Rezervovat syst√©movou pamƒõ≈• pro model, i kdy≈æ je odlo≈æen na GPU. Zlep≈°uje v√Ωkon, ale vy≈æaduje v√≠ce syst√©mov√© RAM",
  "embedding.load.llama.keepModelInMemory/info": "P≈ôedch√°z√≠ v√Ωmƒõnƒõ modelu na disk, zaji≈°≈•uje rychlej≈°√≠ p≈ô√≠stup za cenu vy≈°≈°√≠ho vyu≈æit√≠ RAM",
  "embedding.load.llama.tryMmap/title": "Pokusit se o mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Zlep≈°√≠ ƒças naƒç√≠t√°n√≠ modelu. Vypnut√≠ tohoto m≈Ø≈æe zlep≈°it v√Ωkon, kdy≈æ je model vƒõt≈°√≠ ne≈æ dostupn√° syst√©mov√° RAM",
  "embedding.load.llama.tryMmap/info": "Naƒçte soubory modelu p≈ô√≠mo z disku do pamƒõti",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "Seed pro generov√°n√≠ n√°hodn√Ωch ƒç√≠sel. -1 je n√°hodn√Ω seed",

  "embedding.load.seed/info": "N√°hodn√Ω seed: Nastav√≠ seed pro generov√°n√≠ n√°hodn√Ωch ƒç√≠sel pro zaji≈°tƒõn√≠ reprodukovateln√Ωch v√Ωsledk≈Ø",

  "customInputs": {
    "contextLength": {
      "maxValueTooltip": "Toto je maxim√°ln√≠ poƒçet token≈Ø, kter√© model byl tr√©nov√°n na zpracov√°n√≠. Kliknƒõte pro nastaven√≠ kontextu na tuto hodnotu",
      "maxValueTextStart": "Model podporuje a≈æ",
      "maxValueTextEnd": "token≈Ø"
    },
    "llmPromptTemplate": {
      "types.jinja/label": "Jinja",
      "jinja/error": "Nepoda≈ôilo se analyzovat ≈°ablonu Jinja: {{error}}",
      "types.manual/label": "Manu√°ln√≠",
      "manual.subfield.beforeSystem/label": "P≈ôed syst√©mem",
      "manual.subfield.beforeSystem/placeholder": "Vlo≈æte prefix pro syst√©m...",
      "manual.subfield.afterSystem/label": "Po syst√©mu",
      "manual.subfield.afterSystem/placeholder": "Vlo≈æte suffix pro syst√©m...",
      "manual.subfield.beforeUser/label": "P≈ôed u≈æivatelem",
      "manual.subfield.beforeUser/placeholder": "Vlo≈æte prefix pro u≈æivatele...",
      "manual.subfield.afterUser/label": "Po u≈æivateli",
      "manual.subfield.afterUser/placeholder": "Vlo≈æte suffix pro u≈æivatele...",
      "manual.subfield.beforeAssistant/label": "P≈ôed asistentem",
      "manual.subfield.beforeAssistant/placeholder": "Vlo≈æte prefix pro asistenta...",
      "manual.subfield.afterAssistant/label": "Po asistentovi",
      "manual.subfield.afterAssistant/placeholder": "Vlo≈æte suffix pro asistenta...",
      "stopStrings/label": "Dal≈°√≠ ≈ôetƒõzce pro zastaven√≠",
      "stopStrings/subTitle": "≈òetƒõzce, kter√© se maj√≠ pou≈æ√≠t k zastaven√≠ modelu generov√°n√≠ dal≈°√≠ch token≈Ø"
    }
  },

  "flashAttentionWarning": "Flash Attention je experiment√°ln√≠ funkce, kter√° m≈Ø≈æe zp≈Øsobit probl√©my s nƒõkter√Ωmi modely. Pokud naraz√≠te na probl√©my, zkuste ji vypnout."
}
