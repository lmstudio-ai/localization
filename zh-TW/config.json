{
  "noInstanceSelected": "未選擇模型實例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "顯示進階設定",
  "showAll": "全部",
  "basicSettings": "基本",
  "configSubtitle": "載入或儲存預設值，並嘗試使用模型參數覆蓋",
  "inferenceParameters/title": "預測參數",
  "inferenceParameters/info": "嘗試使用影響預測的參數。",
  "generalParameters/title": "一般",
  "samplingParameters/title": "取樣",
  "basicTab": "基本",
  "advancedTab": "進階",
  "advancedTab/title": "🧪 進階設定",
  "advancedTab/expandAll": "全部展開",
  "advancedTab/overridesTitle": "設定覆蓋",
  "advancedTab/noConfigsText": "您沒有未儲存的變更 - 編輯上面的值以在此處查看覆蓋。",
  "loadInstanceFirst": "載入模型以查看可設定的參數",
  "noListedConfigs": "沒有可設定的參數",
  "generationParameters/info": "嘗試使用影響文字產生的基本參數。",
  "loadParameters/title": "載入參數",
  "loadParameters/description": "控制模型初始化和載入到記憶體的方式的設定。",
  "loadParameters/reload": "重新載入以套用變更",
  "loadParameters/reload/error": "重新載入模型失敗",
  "discardChanges": "捨棄變更",
  "loadModelToSeeOptions": "載入模型以查看選項",
  "schematicsError.title": "設定示意圖在以下欄位中包含錯誤：",
  "manifestSections": {
    "structuredOutput/title": "結構化輸出",
    "speculativeDecoding/title": "推測解碼",
    "sampling/title": "取樣",
    "settings/title": "設定",
    "toolUse/title": "工具使用",
    "promptTemplate/title": "提示範本"
  },

  "llm.prediction.systemPrompt/title": "系統提示",
  "llm.prediction.systemPrompt/description": "使用此欄位向模型提供背景指示，例如一組規則、約束或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "溫度",
  "llm.prediction.temperature/subTitle": "引入多少隨機性。0 將每次產生相同的結果，而較高的值將增加創造力和差異",
  "llm.prediction.temperature/info": "來自 llama.cpp 說明文件：\"預設值為 <{{dynamicValue}}>，它在隨機性和確定性之間提供平衡。在極端情況下，溫度為 0 將始終選擇最可能的下一個 Token，從而在每次運行中產生相同的輸出\"",
  "llm.prediction.llama.sampling/title": "取樣",
  "llm.prediction.topKSampling/title": "Top K 取樣",
  "llm.prediction.topKSampling/subTitle": "將下一個 Token 限制為最可能的 top-k Token 之一。作用類似於溫度",
  "llm.prediction.topKSampling/info": "來自 llama.cpp 說明文件：\n\nTop-k 取樣是一種文字產生方法，它僅從模型預測的最可能的 top k 個 Token 中選擇下一個 Token。\n\n它有助於降低產生低機率或無意義 Token 的風險，但也可能限制輸出的多樣性。\n\ntop-k 的較高值（例如 100）將考慮更多 Token 並導致更多樣化的文字，而較低的值（例如 10）將專注於最可能的 Token 並產生更保守的文字。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 執行緒",
  "llm.prediction.llama.cpuThreads/subTitle": "推論期間要使用的 CPU 執行緒數",
  "llm.prediction.llama.cpuThreads/info": "計算期間要使用的執行緒數。增加執行緒數並不總是與更好的效能相關。預設值為 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制回應長度",
  "llm.prediction.maxPredictedTokens/subTitle": "可選擇地限制 AI 回應的長度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天機器人回應的最大長度。開啟以設定回應最大長度的限制，或關閉以讓聊天機器人決定何時停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（Token）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 個字",
  "llm.prediction.repeatPenalty/title": "重複懲罰",
  "llm.prediction.repeatPenalty/subTitle": "在多大程度上阻止重複相同的 Token",
  "llm.prediction.repeatPenalty/info": "來自 llama.cpp 說明文件：\"有助於防止模型產生重複或單調的文字。\n\n較高的值（例如 1.5）將更強烈地懲罰重複，而較低的值（例如 0.9）將更寬容。\" • 預設值為 <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P 取樣",
  "llm.prediction.minPSampling/subTitle": "Token 被選為輸出的最小基本機率",
  "llm.prediction.minPSampling/info": "來自 llama.cpp 說明文件：\n\n相對於最可能 Token 的機率，Token 被考慮的最小機率。必須在 [0, 1] 中。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P 取樣",
  "llm.prediction.topPSampling/subTitle": "可能的下一個 Token 的最小累積機率。作用類似於溫度",
  "llm.prediction.topPSampling/info": "來自 llama.cpp 說明文件：\n\nTop-p 取樣，也稱為核心取樣，是另一種文字產生方法，它從一組 Token 中選擇下一個 Token，這些 Token 的累積機率總和至少為 p。\n\n此方法透過考慮 Token 的機率和要取樣的 Token 數量，在多樣性和品質之間提供平衡。\n\ntop-p 的較高值（例如 0.95）將導致更多樣化的文字，而較低的值（例如 0.5）將產生更集中和保守的文字。必須在 (0, 1] 中。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字串",
  "llm.prediction.stopStrings/subTitle": "應阻止模型產生更多 Token 的字串",
  "llm.prediction.stopStrings/info": "遇到時將阻止模型產生更多 Token 的特定字串",
  "llm.prediction.stopStrings/placeholder": "輸入字串並按下 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢位",
  "llm.prediction.contextOverflowPolicy/subTitle": "當對話變得太大而無法處理時，模型應如何表現",
  "llm.prediction.contextOverflowPolicy/info": "決定當對話超過模型工作記憶體（「上下文」）的大小時該怎麼做",
  "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
  "llm.prediction.llama.presencePenalty/title": "存在懲罰",
  "llm.prediction.llama.tailFreeSampling/title": "無尾取樣",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型取樣",
  "llm.prediction.llama.xtcProbability/title": "XTC 取樣機率",
  "llm.prediction.llama.xtcProbability/subTitle": "XTC（排除頂級選擇）取樣器僅會以每個產生的 Token 的機率啟用。XTC 取樣可以提高創造力並減少陳腔濫調",
  "llm.prediction.llama.xtcProbability/info": "XTC（排除頂級選擇）取樣僅會以每個產生的 Token 的機率啟用。XTC 取樣通常會提高創造力並減少陳腔濫調",
  "llm.prediction.llama.xtcThreshold/title": "XTC 取樣閾值",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除頂級選擇）閾值。在 `xtc-probability` 的機會下，搜尋機率在 `xtc-threshold` 和 0.5 之間的 Token，並移除所有此類 Token，除了機率最低的 Token",
  "llm.prediction.llama.xtcThreshold/info": "XTC（排除頂級選擇）閾值。在 `xtc-probability` 的機會下，搜尋機率在 `xtc-threshold` 和 0.5 之間的 Token，並移除所有此類 Token，除了機率最低的 Token",
  "llm.prediction.mlx.topKSampling/title": "Top K 取樣",
  "llm.prediction.mlx.topKSampling/subTitle": "將下一個 Token 限制為最可能的 top-k Token 之一。作用類似於溫度",
  "llm.prediction.mlx.topKSampling/info": "將下一個 Token 限制為最可能的 top-k Token 之一。作用類似於溫度",
  "llm.prediction.onnx.topKSampling/title": "Top K 取樣",
  "llm.prediction.onnx.topKSampling/subTitle": "將下一個 Token 限制為最可能的 top-k Token 之一。作用類似於溫度",
  "llm.prediction.onnx.topKSampling/info": "來自 ONNX 文件：\n\n要保留用於 top-k 過濾的最高機率詞彙 Token 數\n\n• 預設情況下，此過濾器已關閉",
  "llm.prediction.onnx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.onnx.repeatPenalty/subTitle": "在多大程度上阻止重複相同的 Token",
  "llm.prediction.onnx.repeatPenalty/info": "較高的值會阻止模型重複自身",
  "llm.prediction.onnx.topPSampling/title": "Top P 取樣",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一個 Token 的最小累積機率。作用類似於溫度",
  "llm.prediction.onnx.topPSampling/info": "來自 ONNX 文件：\n\n僅保留機率加起來達到 TopP 或更高的最可能的 Token 用於產生\n\n• 預設情況下，此過濾器已關閉",
  "llm.prediction.seed/title": "種子",
  "llm.prediction.structured/title": "結構化輸出",
  "llm.prediction.structured/info": "結構化輸出",
  "llm.prediction.structured/description": "進階：您可以提供一個 [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) 來強制模型產生特定的輸出格式。閱讀 [文件](https://lmstudio.ai/docs/advanced/structured-output) 以了解更多資訊",
  "llm.prediction.tools/title": "工具使用",
  "llm.prediction.tools/description": "進階：您可以提供一個符合 JSON 規範的工具清單，供模型要求呼叫。閱讀 [文件](https://lmstudio.ai/docs/advanced/tool-use) 以了解更多資訊",
  "llm.prediction.tools/serverPageDescriptionAddon": "使用伺服器 API 時，將其作為 `tools` 透過請求主體傳遞",
  "llm.prediction.promptTemplate/title": "提示範本",
  "llm.prediction.promptTemplate/subTitle": "聊天中訊息傳送到模型的格式。變更此設定可能會導致意外行為 - 請確保您知道自己在做什麼！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "要產生的草稿 Token",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "每個主要模型 Token 要使用草稿模型產生的 Token 數。找到計算與獎勵之間的最佳平衡點",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿機率截止",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "繼續起草，直到 Token 的機率低於此閾值。較高的值通常意味著較低的風險，較低的獎勵",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿大小",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "主要模型將忽略小於此值的草稿。較高的值通常意味著較低的風險，較低的獎勵",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿大小",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "草稿中允許的最大 Token 數。如果所有 Token 機率都 > 截止值，則為上限。較低的值通常意味著較低的風險，較低的獎勵",
  "llm.prediction.speculativeDecoding.draftModel/title": "草稿模型",
  "llm.prediction.reasoning.parsing/title": "推理區段解析",
  "llm.prediction.reasoning.parsing/subTitle": "如何在模型的輸出中解析推理區段",

  "llm.load.contextLength/title": "上下文長度",
  "llm.load.contextLength/subTitle": "模型在一個提示中可以處理的最大 Token 數量。請參閱「推論參數」下的「對話溢位」選項，以取得更多管理此項目的方法",
  "llm.load.contextLength/info": "指定模型一次可以考慮的最大 Token 數量，影響其在處理過程中保留多少上下文",
  "llm.load.contextLength/warning": "為上下文長度設定較高的值可能會顯著影響記憶體使用量",
  "llm.load.seed/title": "種子",
  "llm.load.seed/subTitle": "用於文字生成中隨機數生成器的種子。-1 為隨機",
  "llm.load.seed/info": "隨機種子：設定隨機數生成的種子，以確保可重現的結果",

  "llm.load.llama.evalBatchSize/title": "評估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次處理的輸入 Token 數量。增加此值會提高效能，但會增加記憶體使用量",
  "llm.load.llama.evalBatchSize/info": "設定在評估期間一次在一個批次中一起處理的範例數量，影響速度和記憶體使用量",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 頻率基底",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋轉位置嵌入 (RoPE) 的自訂基底頻率。增加此值可能會在高上下文長度下實現更好的效能",
  "llm.load.llama.ropeFrequencyBase/info": "[進階] 調整旋轉位置編碼的基底頻率，影響位置資訊的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 頻率比例",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此因子縮放，以使用 RoPE 擴展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[進階] 修改旋轉位置編碼的頻率縮放，以控制位置編碼粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "要在 GPU 上計算以進行 GPU 加速的離散模型層數",
  "llm.load.llama.acceleration.offloadRatio/info": "設定要卸載到 GPU 的層數。",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "減少某些模型上的記憶體使用量和生成時間",
  "llm.load.llama.flashAttention/info": "加速注意力機制，以實現更快、更有效率的處理",
  "llm.load.numExperts/title": "專家數量",
  "llm.load.numExperts/subTitle": "要在模型中使用的專家數量",
  "llm.load.numExperts/info": "要在模型中使用的專家數量",
  "llm.load.llama.keepModelInMemory/title": "將模型保留在記憶體中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使卸載到 GPU，也為模型保留系統記憶體。提高效能，但需要更多系統 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型交換到磁碟，確保更快的存取速度，但會增加 RAM 使用量",
  "llm.load.llama.useFp16ForKVCache/title": "將 FP16 用於 KV 快取",
  "llm.load.llama.useFp16ForKVCache/info": "透過以半精度 (FP16) 儲存快取來減少記憶體使用量",
  "llm.load.llama.tryMmap/title": "嘗試 mmap()",
  "llm.load.llama.tryMmap/subTitle": "改善模型的載入時間。當模型大於可用的系統 RAM 時，停用此功能可能會提高效能",
  "llm.load.llama.tryMmap/info": "直接從磁碟載入模型檔案到記憶體",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU 執行緒池大小",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "要分配給用於模型計算的執行緒池的 CPU 執行緒數",
  "llm.load.llama.cpuThreadPoolSize/info": "要分配給用於模型計算的執行緒池的 CPU 執行緒數。增加執行緒數並不總是與更好的效能相關。預設值為 <{{dynamicValue}}>。",
  "llm.load.llama.kCacheQuantizationType/title": "K 快取量化類型",
  "llm.load.llama.kCacheQuantizationType/subTitle": "較低的值會減少記憶體使用量，但可能會降低品質。效果在不同模型之間差異很大。",
  "llm.load.llama.vCacheQuantizationType/title": "V 快取量化類型",
  "llm.load.llama.vCacheQuantizationType/subTitle": "較低的值會減少記憶體使用量，但可能會降低品質。效果在不同模型之間差異很大。",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 如果未啟用 Flash Attention，您必須停用此值",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "只能在啟用 Flash Attention 時開啟",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 使用 F32 時，您必須停用 flash attention",
  "llm.load.mlx.kvCacheBits/title": "KV 快取量化",
  "llm.load.mlx.kvCacheBits/subTitle": "KV 快取應量化到的位元數",
  "llm.load.mlx.kvCacheBits/info": "KV 快取應量化到的位元數",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "使用 KV 快取量化時，會忽略上下文長度設定",
  "llm.load.mlx.kvCacheGroupSize/title": "KV 快取量化：群組大小",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "KV 快取量化操作期間的群組大小。較高的群組大小會減少記憶體使用量，但可能會降低品質",
  "llm.load.mlx.kvCacheGroupSize/info": "KV 快取應量化到的位元數",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV 快取量化：當 ctx 跨越此長度時開始量化",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "開始量化 KV 快取的上下文長度閾值",
  "llm.load.mlx.kvCacheQuantizationStart/info": "開始量化 KV 快取的上下文長度閾值",
  "llm.load.mlx.kvCacheQuantization/title": "KV 快取量化",
  "llm.load.mlx.kvCacheQuantization/subTitle": "量化模型的 KV 快取。這可能會導致更快的生成速度和更低的記憶體佔用量，\n但會犧牲模型輸出的品質。",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV 快取量化位元",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "將 KV 快取量化到的位元數",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "位元",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "群組大小策略",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "準確性",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "平衡",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "快速",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "進階：量化 'matmul 群組大小' 配置\n\n• 準確性 = 群組大小 32\n• 平衡 = 群組大小 64\n• 快速 = 群組大小 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "當 ctx 達到此長度時開始量化",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "當上下文達到此 Token 數量時，\n開始量化 KV 快取",  

  "embedding.load.contextLength/title": "上下文長度",
  "embedding.load.contextLength/subTitle": "模型在一個提示中可以處理的最大 Token 數量。請參閱「推論參數」下的「對話溢位」選項，以取得更多管理此項目的方法",
  "embedding.load.contextLength/info": "指定模型一次可以考慮的最大 Token 數量，影響其在處理過程中保留多少上下文",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 頻率基底",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋轉位置嵌入 (RoPE) 的自訂基底頻率。增加此值可能會在高上下文長度下實現更好的效能",
  "embedding.load.llama.ropeFrequencyBase/info": "[進階] 調整旋轉位置編碼的基底頻率，影響位置資訊的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "評估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次處理的輸入 Token 數量。增加此值會提高效能，但會增加記憶體使用量",
  "embedding.load.llama.evalBatchSize/info": "設定在評估期間一次在一個批次中一起處理的 Token 數量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 頻率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此因子縮放，以使用 RoPE 擴展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[進階] 修改旋轉位置編碼的頻率縮放，以控制位置編碼粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "要在 GPU 上計算以進行 GPU 加速的離散模型層數",
  "embedding.load.llama.acceleration.offloadRatio/info": "設定要卸載到 GPU 的層數。",
  "embedding.load.llama.keepModelInMemory/title": "將模型保留在記憶體中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使卸載到 GPU，也為模型保留系統記憶體。提高效能，但需要更多系統 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型交換到磁碟，確保更快的存取速度，但會增加 RAM 使用量",
  "embedding.load.llama.tryMmap/title": "嘗試 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "改善模型的載入時間。當模型大於可用的系統 RAM 時，停用此功能可能會提高效能",
  "embedding.load.llama.tryMmap/info": "直接從磁碟載入模型檔案到記憶體",
  "embedding.load.seed/title": "種子",
  "embedding.load.seed/subTitle": "用於文字生成中隨機數生成器的種子。-1 為隨機種子",

  "embedding.load.seed/info": "隨機種子：設定隨機數生成的種子，以確保可重現的結果",  

  "presetTooltip": {
    "included/title": "預設值",
    "included/description": "將套用下列欄位",
    "included/empty": "此預設的欄位在此環境中不適用。",
    "included/conflict": "系統將詢問您是否要套用此值",
    "separateLoad/title": "載入時間配置",
    "separateLoad/description.1": "預設也包含下列載入時間配置。載入時間配置是模型範圍的，需要重新載入模型才能生效。按住",
    "separateLoad/description.2": "以套用至",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不適用",
    "excluded/description": "下列欄位包含在預設中，但在目前環境中不適用。",
    "legacy/title": "舊版預設",
    "legacy/description": "此預設是舊版預設。它包含下列欄位，這些欄位現在要嘛自動處理，要嘛不再適用。",
    "button/publish": "發佈至 Hub",
    "button/pushUpdate": "將變更推送至 Hub",
    "button/export": "匯出"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空白>"
    },
    "checkboxNumeric": {
      "off": "關閉"
    },
    "llamaCacheQuantizationType": {
      "off": "關閉"
    },
    "mlxKvCacheBits": {
      "off": "關閉"
    },
    "stringArray": {
      "empty": "<空白>"
    },
    "llmPromptTemplate": {
      "type": "類型",
      "types.jinja/label": "範本 (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "範本",
      "jinja/error": "無法剖析 Jinja 範本: {{error}}",
      "jinja/empty": "請在上方輸入 Jinja 範本。",
      "jinja/unlikelyToWork": "您在上方提供的 Jinja 範本不大可能有效，因為它未參考變數 \"messages\"。請仔細檢查您是否輸入了正確的範本。",
      "types.manual/label": "手動",
      "manual.subfield.beforeSystem/label": "在系統之前",
      "manual.subfield.beforeSystem/placeholder": "輸入系統前綴...",
      "manual.subfield.afterSystem/label": "在系統之後",
      "manual.subfield.afterSystem/placeholder": "輸入系統後綴...",
      "manual.subfield.beforeUser/label": "在使用者之前",
      "manual.subfield.beforeUser/placeholder": "輸入使用者前綴...",
      "manual.subfield.afterUser/label": "在使用者之後",
      "manual.subfield.afterUser/placeholder": "輸入使用者後綴...",
      "manual.subfield.beforeAssistant/label": "在助理之前",
      "manual.subfield.beforeAssistant/placeholder": "輸入助理前綴...",
      "manual.subfield.afterAssistant/label": "在助理之後",
      "manual.subfield.afterAssistant/placeholder": "輸入助理後綴...",
      "stopStrings/label": "其他停止字串",
      "stopStrings/subTitle": "除了使用者指定的停止字串之外，還將使用的範本特定停止字串。"
    },
    "contextLength": {
      "maxValueTooltip": "這是模型經過訓練可以處理的最大 Token 數量。按一下以將上下文設定為此值",
      "maxValueTextStart": "模型最多支援",
      "maxValueTextEnd": "個 Token",
      "tooltipHint": "雖然模型可能支援最多一定數量的 Token，但如果您的機器資源無法處理負載，效能可能會下降 - 增加此值時請謹慎"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "在限制處停止",
      "stopAtLimitSub": "一旦模型的記憶體已滿，就停止生成",
      "truncateMiddle": "截斷中間",
      "truncateMiddleSub": "從對話中間移除訊息，以為較新的訊息騰出空間。模型仍然會記住對話的開頭",
      "rollingWindow": "滾動視窗",
      "rollingWindowSub": "模型將始終獲得最近的幾個訊息，但可能會忘記對話的開頭"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "關閉"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "平均",
      "favorMainGpu": "偏好主要 GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "閱讀其運作方式",
      "placeholder": "選取相容的草稿模型",
      "noCompatible": "找不到與您目前選取的模型相容的草稿模型",
      "stillLoading": "正在識別相容的草稿模型...",
      "notCompatible": "選取的草稿模型 (<draft/>) 與目前選取的模型 (<current/>) 不相容。",
      "off": "關閉",
      "loadModelToSeeOptions": "載入模型 <keyboard-shortcut /> 以查看相容的選項",
      "compatibleWithNumberOfModels": "建議用於至少 {{dynamicValue}} 個您的模型",
      "recommendedForSomeModels": "建議用於某些模型",
      "recommendedForLlamaModels": "建議用於 Llama 模型",
      "recommendedForQwenModels": "建議用於 Qwen 模型",
      "onboardingModal": {
        "introducing": "介紹",
        "speculativeDecoding": "推測解碼",
        "firstStepBody": "<custom-span>llama.cpp</custom-span> 和 <custom-span>MLX</custom-span> 模型的推論加速",
        "secondStepTitle": "使用推測解碼加速推論",
        "secondStepBody": "推測解碼是一種涉及兩個模型協作的技術：\n - 一個較大的「主要」模型\n - 一個較小的「草稿」模型\n\n在生成期間，草稿模型會快速提出 Token，供較大的主要模型驗證。驗證 Token 比實際生成它們快得多，這是速度提升的來源。**通常，主要模型和草稿模型之間的大小差異越大，速度提升就越大**。\n\n為了保持品質，主要模型只接受與它自己會生成的 Token 對齊的 Token，從而在更快的推論速度下實現較大模型的響應品質。兩個模型必須共享相同的詞彙表。",
        "draftModelRecommendationsTitle": "草稿模型建議",
        "basedOnCurrentModels": "根據您目前的模型",
        "close": "關閉",
        "next": "下一步",
        "done": "完成"
      },
      "speculativeDecodingLoadModelToSeeOptions": "請先載入模型 <model-badge /> ",
      "errorEngineNotSupported": "推測解碼需要引擎 {{engineName}} 的至少 {{minVersion}} 版本。請更新引擎 (<key/>) 並重新載入模型以使用此功能。",
      "errorEngineNotSupported/noKey": "推測解碼需要引擎 {{engineName}} 的至少 {{minVersion}} 版本。請更新引擎並重新載入模型以使用此功能。"
    },
    "llmReasoningParsing": {
      "startString/label": "開始字串",
      "startString/placeholder": "輸入開始字串...",
      "endString/label": "結束字串",
      "endString/placeholder": "輸入結束字串..."
    } 
  },
    "saveConflictResolution": {
      "title": "選擇要包含在預設中的值",
      "description": "挑選並選擇要保留的值",
      "instructions": "點擊一個值以包含它",
      "userValues": "先前的值",
      "presetValues": "新的值",
      "confirm": "確認",
      "cancel": "取消"
    }, 
      "applyConflictResolution": {
        "title": "要保留哪些值？",
        "description": "您有未提交的變更與傳入的預設重疊",
        "instructions": "點擊一個值以保留它",
        "userValues": "目前的值",
        "presetValues": "傳入的預設值",
        "confirm": "確認",
        "cancel": "取消"
      },
    "empty": "<空白>",
    "noModelSelected": "未選取模型",
    "apiIdentifier.label": "API 識別碼",
    "apiIdentifier.hint": "您可以選擇性地為此模型提供一個識別碼。這將用於 API 請求。留空以使用預設識別碼。",
    "idleTTL.label": "閒置時自動卸載 (TTL)",
    "idleTTL.hint": "如果設定，模型將在閒置指定的時間後自動卸載。",
    "idleTTL.mins": "分鐘" ,   

    "presets": {
      "title": "預設",
      "commitChanges": "提交變更",
      "commitChanges/description": "將您的變更提交到預設。",
      "commitChanges.manual": "偵測到新的欄位。您將能夠選擇要包含在預設中的變更。",
      "commitChanges.manual.hold.0": "按住",
      "commitChanges.manual.hold.1": "以選擇要提交到預設的變更。",
      "commitChanges.saveAll.hold.0": "按住",
      "commitChanges.saveAll.hold.1": "以儲存所有變更。",
      "commitChanges.saveInPreset.hold.0": "按住",
      "commitChanges.saveInPreset.hold.1": "以僅儲存對已包含在預設中的欄位的變更。",
      "commitChanges/error": "無法將變更提交到預設。",
      "commitChanges.manual/description": "選擇要包含在預設中的變更。",
      "saveAs": "另存為新...",
      "presetNamePlaceholder": "輸入預設的名稱...",
      "cannotCommitChangesLegacy": "這是一個舊版預設，無法修改。您可以使用「另存為新...」建立副本。",
      "cannotCommitChangesNoChanges": "沒有要提交的變更。",
      "emptyNoUnsaved": "選取一個預設...",
      "emptyWithUnsaved": "未儲存的預設",
      "saveEmptyWithUnsaved": "將預設另存為...",
      "saveConfirm": "儲存",
      "saveCancel": "取消",
      "saving": "正在儲存...",
      "save/error": "無法儲存預設。",
      "deselect": "取消選取預設",
      "deselect/error": "無法取消選取預設。",
      "select/error": "無法選取預設。",
      "delete/error": "無法刪除預設。",
      "discardChanges": "捨棄未儲存的變更",
      "discardChanges/info": "捨棄所有未提交的變更，並將預設還原到其原始狀態",
      "newEmptyPreset": "+ 新預設",
      "importPreset": "匯入",
      "contextMenuSelect": "套用預設",
      "contextMenuDelete": "刪除...",
      "contextMenuShare": "發布...",
      "contextMenuOpenInHub": "在 Hub 上檢視",
      "contextMenuPushChanges": "將變更推送到 Hub",
      "contextMenuPushingChanges": "正在推送...",
      "contextMenuPushedChanges": "已推送變更",
      "contextMenuExport": "匯出檔案",
      "contextMenuRevealInExplorer": "在檔案總管中顯示",
      "contextMenuRevealInFinder": "在 Finder 中顯示",
      "share": {
        "title": "發布預設",
        "action": "分享您的預設，供其他人下載、喜歡和 Fork",
        "presetOwnerLabel": "擁有者",
        "uploadAs": "您的預設將建立為 {{name}}",
        "presetNameLabel": "預設名稱",
        "descriptionLabel": "描述 (選填)",
        "loading": "正在發布...",
        "success": "預設已成功推送",
        "presetIsLive": "<preset-name /> 現在已在 Hub 上線！",
        "close": "關閉",
        "confirmViewOnWeb": "在網路上檢視",
        "confirmCopy": "複製網址",
        "confirmCopied": "已複製！",
        "pushedToHub": "您的預設已推送到 Hub",
        "descriptionPlaceholder": "輸入描述...",
        "willBePublic": "發布您的預設將使其公開",
        "publicSubtitle": "您的預設是 <custom-bold>公開</custom-bold>。其他人可以在 lmstudio.ai 上下載和 Fork 它",
        "confirmShareButton": "發布",
        "error": "無法發布預設",
        "createFreeAccount": "在 Hub 中建立一個免費帳戶以發布預設"
      },
      "update": {
        "title": "將變更推送到 Hub",
        "title/success": "預設已成功更新",
        "subtitle": "變更 <custom-preset-name /> 並將其推送到 Hub",
        "descriptionLabel": "描述",
        "descriptionPlaceholder": "輸入描述...",
        "loading": "正在推送...",
        "cancel": "取消",
        "createFreeAccount": "在 Hub 中建立一個免費帳戶以發布預設",
        "error": "無法推送更新",
        "confirmUpdateButton": "推送"
      },
      "import": {
        "title": "從檔案匯入預設",
        "dragPrompt": "拖放預設 JSON 檔案或 <custom-link>從您的電腦選取</custom-link>",
        "remove": "移除",
        "cancel": "取消",
        "importPreset_zero": "匯入預設",
        "importPreset_one": "匯入預設",
        "importPreset_other": "匯入 {{count}} 個預設",
        "selectDialog": {
          "title": "選取預設檔案 (.json)",
          "button": "匯入"
        },
        "error": "無法匯入預設",
        "resultsModal": {
          "titleSuccessSection_one": "已成功匯入 1 個預設",
          "titleSuccessSection_other": "已成功匯入 {{count}} 個預設",
          "titleFailSection_zero": "",
          "titleFailSection_one": "({{count}} 個失敗)",
          "titleFailSection_other": "({{count}} 個失敗)",
          "titleAllFailed": "無法匯入預設",
          "importMore": "匯入更多",
          "close": "完成",
          "successBadge": "成功",
          "alreadyExistsBadge": "預設已存在",
          "errorBadge": "錯誤",
          "invalidFileBadge": "無效的檔案",
          "otherErrorBadge": "無法匯入預設",
          "errorViewDetailsButton": "檢視詳細資訊",
          "seeError": "查看錯誤",
          "noName": "沒有預設名稱",
          "useInChat": "在聊天中使用"
        },
        "importFromUrl": {
          "button": "從網址匯入...",
          "title": "從網址匯入",
          "back": "從檔案匯入...",
          "action": "貼上您要匯入的預設的 LM Studio Hub 網址",
          "invalidUrl": "無效的網址。請確保您貼上的是正確的 LM Studio Hub 網址。",
          "tip": "您可以使用 LM Studio Hub 中的 {{buttonName}} 按鈕直接安裝預設",
          "confirm": "匯入",
          "cancel": "取消",
          "loading": "正在匯入...",
          "error": "無法下載預設。"
        }
      },
      "download": {
        "title": "從 LM Studio Hub 下載 <preset-name />",
        "subtitle": "將 <custom-name /> 儲存到您的預設。這樣您就可以在應用程式中使用此預設",
        "button": "下載",
        "button/loading": "正在下載...",
        "cancel": "取消",
        "error": "無法下載預設。"
      },
      "inclusiveness": {
        "speculativeDecoding": "包含在預設中"
      }
    },
    
    "flashAttentionWarning": "Flash Attention 是一項實驗性功能，可能會導致某些模型出現問題。如果遇到問題，請嘗試停用它。",
    "llamaKvCacheQuantizationWarning": "KV 快取量化是一項實驗性功能，可能會導致某些模型出現問題。必須啟用 Flash Attention 才能進行 V 快取量化。如果遇到問題，請重設為預設值「F16」。",
  
    "seedUncheckedHint": "隨機種子",
    "ropeFrequencyBaseUncheckedHint": "自動",
    "ropeFrequencyScaleUncheckedHint": "自動",
  
    "hardware": {
      "advancedGpuSettings": "進階 GPU 設定",
      "advancedGpuSettings.info": "如果您不確定，請將這些設定保留為預設值",
      "advancedGpuSettings.reset": "重設為預設值",
      "environmentVariables": {
        "title": "環境變數",
        "description": "模型生命週期中的作用中環境變數。",
        "key.placeholder": "選取變數...",
        "value.placeholder": "值"
      },
      "mainGpu": {
        "title": "主要 GPU",
        "description": "優先用於模型計算的 GPU。",
        "placeholder": "選取主要 GPU..."
      },
      "splitStrategy": {
        "title": "分割策略",
        "description": "如何在 GPU 之間分割模型計算。",
        "placeholder": "選取分割策略..."
      }
    }
  } 
     