{
  "noInstanceSelected": "未選擇模型實例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "顯示進階設定",
  "showAll": "全部",
  "basicSettings": "基礎",
  "configSubtitle": "載入或儲存預設並測試模型參數覆蓋",
  "inferenceParameters/title": "生成參數",
  "inferenceParameters/info": "測試影響生成結果的參數。",
  "generalParameters/title": "一般參數",
  "samplingParameters/title": "取樣",
  "basicTab": "基礎",
  "advancedTab": "進階",
  "advancedTab/title": "🧪 進階配置",
  "advancedTab/expandAll": "全部展開",
  "advancedTab/overridesTitle": "配置覆蓋",
  "advancedTab/noConfigsText": "你尚無未儲存的變更 - 編輯上方的值以查看覆蓋設定。",
  "loadInstanceFirst": "載入模型以檢視可配置的參數",
  "noListedConfigs": "無可配置參數",
  "generationParameters/info": "測試影響文本生成的基礎參數。",
  "loadParameters/title": "載入參數",
  "loadParameters/description": "控制模型初始化和載入到記憶體中的設置。",
  "loadParameters/reload": "重新載入以應用變更",
  "discardChanges": "捨棄變更",
  "llm.prediction.systemPrompt/title": "系統提示詞",
  "llm.prediction.systemPrompt/description": "使用此欄位為模型提供背景指示，例如一組規則、約束或一般需求。此欄位也通常被稱為“系統提示詞”。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "溫度",
  "llm.prediction.temperature/subTitle": "引入多少隨機性。0 將每次都生成相同結果，較高值將增加創造力和變異性",
  "llm.prediction.temperature/info": "根據 llama.cpp 說明：“預設值為 <{{dynamicValue}}>，在隨機性與決定性之間達到平衡。極端情況下，溫度為 0 時，總是選擇最可能的下一個 token，每次運行生成相同的輸出結果。”",
  "llm.prediction.llama.sampling/title": "取樣",
  "llm.prediction.llama.topKSampling/title": "Top K 取樣",
  "llm.prediction.llama.topKSampling/subTitle": "將下一個 token 限制為前 k 個最可能的 token。與溫度相似",
  "llm.prediction.llama.topKSampling/info": "來自 llama.cpp 說明文檔：\n\nTop-k 取樣是一種文本生成方法，從模型預測的前 k 個最可能的 token 中選擇下一個 token。\n\n它有助於降低生成低概率或無意義 token 的風險，但也可能限制輸出的多樣性。\n\n較高的 top-k 值（如 100）將考慮更多 token，導致生成文本更加多樣；較低的值（如 10）將專注於最可能的 token，生成的文本更加保守。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 執行緒",
  "llm.prediction.llama.cpuThreads/subTitle": "推理過程中使用的 CPU 執行緒數",
  "llm.prediction.llama.cpuThreads/info": "在計算期間使用的執行緒數。增加執行緒數量並不總是意味著性能提升。預設值為 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制回應長度",
  "llm.prediction.maxPredictedTokens/subTitle": "選擇性限制 AI 的回應長度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天機器人回應的最大長度。開啟後可設置回應的最大長度，關閉則讓聊天機器人決定何時停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（token）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "大約 {{maxWords}} 字",
  "llm.prediction.llama.repeatPenalty/title": "重複懲罰",
  "llm.prediction.llama.repeatPenalty/subTitle": "懲罰重複 token 的程度",
  "llm.prediction.llama.repeatPenalty/info": "根據 llama.cpp 說明文檔：“有助於防止模型生成重複或單調的文本。\n\n較高的值（如 1.5）將更強烈地懲罰重複，而較低的值（如 0.9）則會更寬容。”• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Min P 取樣",
  "llm.prediction.llama.minPSampling/subTitle": "選擇輸出的 token 所需的最小基礎概率",
  "llm.prediction.llama.minPSampling/info": "來自 llama.cpp 說明文檔：\n\n選擇 token 所需的最小概率，與最可能的 token 的概率相關。範圍必須在 [0, 1] 之間。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P 取樣",
  "llm.prediction.llama.topPSampling/subTitle": "可能的下一個 token 的最小累積概率。與溫度相似",
  "llm.prediction.llama.topPSampling/info": "來自 llama.cpp 說明文檔：\n\nTop-p 取樣，也被稱為核取樣，是另一種文本生成方法，從累積概率至少為 p 的 token 子集選擇下一個 token。\n\n此方法通過考慮 token 的概率和可供取樣的 token 數量，在多樣性和質量之間達到平衡。\n\n較高的 top-p 值（如 0.95）將導致生成文本更加多樣；較低的值（如 0.5）將生成更專注和保守的文本。範圍必須在 (0, 1] 之間。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字串",
  "llm.prediction.stopStrings/subTitle": "模型應停止生成更多 token 的字串",
  "llm.prediction.stopStrings/info": "當遇到特定字串時，將停止生成 token。",
  "llm.prediction.stopStrings/placeholder": "輸入一個字串並按下 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "對話溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "當對話超出模型能處理的大小時，模型應如何應對",
  "llm.prediction.contextOverflowPolicy/info": "當對話超過模型的工作記憶體大小（‘上下文’）時，選擇如何處理",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "達到限制時停止",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "當模型的記憶體滿了時停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "截取中間",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "從對話的中間刪除訊息，為新訊息騰出空間。模型仍然會記住對話的開頭",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滾動窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型將始終獲取最新的幾則訊息，但可能忘記對話的開頭",
  "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
  "llm.prediction.llama.presencePenalty/title": "存在懲罰",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free 取樣",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型取樣",
  "llm.prediction.mlx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.mlx.repeatPenalty/subTitle": "懲罰重複 token 的程度",
  "llm.prediction.mlx.repeatPenalty/info": "較高的值會抑制模型自我重複",
  "llm.prediction.onnx.topKSampling/title": "Top K 取樣",
  "llm.prediction.onnx.topKSampling/subTitle": "將下一個 token 限制為前 k 個最可能的 token。與溫度相似",
  "llm.prediction.onnx.topKSampling/info": "來自 ONNX 說明文檔：\n\nTop-k 取樣是通過保留最高概率的詞彙 token 來進行篩選的\n\n• 此篩選器預設關閉",
  "llm.prediction.onnx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.onnx.repeatPenalty/subTitle": "懲罰重複 token 的程度",
  "llm.prediction.onnx.repeatPenalty/info": "較高的值會抑制模型自我重複",
  "llm.prediction.onnx.topPSampling/title": "Top P 取樣",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一個 token 的最小累積概率。與溫度相似",
  "llm.prediction.onnx.topPSampling/info": "來自 ONNX 說明文檔：\n\n只保留累積概率達到 TopP 或更高的最可能 token\n\n• 此篩選器預設關閉",
  "llm.prediction.seed/title": "隨機種子",
  "llm.prediction.structured/title": "結構化輸出",
  "llm.prediction.structured/info": "結構化輸出",
  "llm.prediction.promptTemplate/title": "提示詞模板",
  "llm.prediction.promptTemplate/subTitle": "將訊息以何種格式發送給模型。更改此設置可能會導致意外行為——請確保你知道自己在做什麼！",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "無法解析 Jinja 模板：{{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手動",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "系統之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "輸入系統前綴...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "系統之後",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "輸入系統後綴...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "使用者之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "輸入使用者前綴...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "使用者之後",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "輸入使用者後綴...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "助手之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "輸入助手前綴...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "助手之後",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "輸入助手後綴...",
  "llm.prediction.promptTemplate.stopStrings/label": "額外的停止字串",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "模板專用的停止字串，將與使用者指定的停止字串一起使用。",
  "llm.load.contextLength/title": "上下文長度",
  "llm.load.contextLength/subTitle": "模型在一則提示詞中可以處理的最大 token 數。檢視“生成參數”下的對話溢出選項以了解更多管理方式",
  "llm.load.contextLength/info": "指定模型一次可考慮的最大 token 數，影響它在處理過程中保留的上下文",
  "llm.load.seed/title": "隨機種子",
  "llm.load.seed/subTitle": "文本生成中使用的隨機數生成器的種子。-1 為隨機",
  "llm.load.seed/info": "隨機種子：設置隨機數生成的種子以確保結果可重複。",
  "llm.load.llama.evalBatchSize/title": "評估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次處理的輸入 token 數。增加這個數字會提高性能，但會消耗更多的記憶體",
  "llm.load.llama.evalBatchSize/info": "設置評估期間一起處理的範例數量，影響速度和記憶體使用量。",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 基頻",
  "llm.load.llama.ropeFrequencyBase/subTitle": "自定義 RoPE（旋轉位置嵌入）的基頻。增加這個值可能會提高高上下文長度下的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[進階] 調整旋轉位置編碼的基頻，影響位置信息的嵌入",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 頻率縮放",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此比例進行縮放，以延長 RoPE 的有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[進階] 修改旋轉位置編碼的頻率縮放，控制位置編碼的粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "指定要在 GPU 上計算的模型層數，以加速 GPU 計算",
  "llm.load.llama.acceleration.offloadRatio/info": "設置要卸載到 GPU 的層數。",
  "llm.load.llama.flashAttention/title": "閃存注意力",
  "llm.load.llama.flashAttention/subTitle": "減少記憶體使用並加速某些模型的生成",
  "llm.load.llama.flashAttention/info": "加速注意力機制以進行更快速且高效的處理",
  "llm.load.llama.keepModelInMemory/title": "將模型保留在記憶體中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使模型已卸載到 GPU，也會保留系統記憶體中的模型。提高性能，但需要更多的系統記憶體",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交換到磁碟中，確保更快的存取，但會消耗更多的記憶體",
  "llm.load.llama.useFp16ForKVCache/title": "將 FP16 用於 KV 快取",
  "llm.load.llama.useFp16ForKVCache/info": "通過以半精度（FP16）存儲快取來減少記憶體使用量",
  "llm.load.llama.tryMmap/title": "嘗試使用 mmap()",
  "llm.load.llama.tryMmap/subTitle": "提高模型載入速度。若模型大於可用系統記憶體，禁用此選項可能會提高性能",
  "llm.load.llama.tryMmap/info": "直接從磁碟加載模型檔案到記憶體中",
  "embedding.load.contextLength/title": "上下文長度",
  "embedding.load.contextLength/subTitle": "模型在一則提示詞中可以處理的最大 token 數。檢視“生成參數”下的對話溢出選項以了解更多管理方式",
  "embedding.load.contextLength/info": "指定模型一次可考慮的最大 token 數，影響它在處理過程中保留的上下文",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 基頻",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "自定義 RoPE（旋轉位置嵌入）的基頻。增加這個值可能會提高高上下文長度下的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[進階] 調整旋轉位置編碼的基頻，影響位置信息的嵌入",
  "embedding.load.llama.evalBatchSize/title": "評估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次處理的 token 數量。增加這個數字會提高性能，但會消耗更多的記憶體",
  "embedding.load.llama.evalBatchSize/info": "設置評估期間處理的 token 數量，影響速度和記憶體使用量。",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 頻率縮放",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此比例進行縮放，以延長 RoPE 的有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[進階] 修改旋轉位置編碼的頻率縮放，控制位置編碼的粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "指定要在 GPU 上計算的模型層數，以加速 GPU 計算",
  "embedding.load.llama.acceleration.offloadRatio/info": "設置要卸載到 GPU 的層數。",
  "embedding.load.llama.keepModelInMemory/title": "將模型保留在記憶體中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使模型已卸載到 GPU，也會保留系統記憶體中的模型。提高性能，但需要更多的系統記憶體",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交換到磁碟中，確保更快的存取，但會消耗更多的記憶體",
  "embedding.load.llama.tryMmap/title": "嘗試使用 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "提高模型載入速度。若模型大於可用系統記憶體，禁用此選項可能會提高性能",
  "embedding.load.llama.tryMmap/info": "直接從磁碟加載模型檔案到記憶體中",
  "embedding.load.seed/title": "隨機種子",
  "embedding.load.seed/subTitle": "文本生成中使用的隨機數生成器的種子。-1 為隨機",
  "embedding.load.seed/info": "隨機種子：設置隨機數生成的種子以確保結果可重複。",
  "customInputs": {
    "contextLength": {
      "maxValueTooltip": "這是模型被訓練處理的最大 token 數。點擊以將上下文設置為此值",
      "maxValueTextStart": "模型支持最多",
      "maxValueTextEnd": "個 token"
    }
  },
  "flashAttentionWarning": "閃存注意力是一項實驗性功能，可能會在某些模型上引發問題。如果遇到問題，請嘗試禁用它。"
}
