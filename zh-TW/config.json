{
    "noInstanceSelected": "未選擇模型例項",
    "resetToDefault": "重置",
    "showAdvancedSettings": "顯示高階設定",
    "showAll": "全部",
    "basicSettings": "基本",
    "configSubtitle": "載入或儲存預設值並試驗模型引數覆蓋",
    "inferenceParameters/title": "預測引數",
    "inferenceParameters/info": "試驗影響預測的引數。",
    "generalParameters/title": "一般",
    "samplingParameters/title": "取樣",
    "basicTab": "基本",
    "advancedTab": "高階",
    "advancedTab/title": "🧪 高階設定",
    "advancedTab/expandAll": "全部展開",
    "advancedTab/overridesTitle": "設定覆蓋",
    "advancedTab/noConfigsText": "您尚未儲存任何變更 - 請在上方編輯值以檢視這裡的覆蓋設定。",
    "loadInstanceFirst": "載入模型以檢視可配置引數",
    "noListedConfigs": "無可配置引數",
    "generationParameters/info": "試驗影響文字生成的基礎引數。",
    "loadParameters/title": "載入引數",
    "loadParameters/description": "控制模型初始化與載入到記憶體的方式設定。",
    "loadParameters/reload": "重新載入以應用變更",
    "loadParameters/reload/error": "重新載入模型失敗",
    "discardChanges": "丟棄變更",
    "loadModelToSeeOptions": "載入模型以檢視選項",
    "schematicsError.title": "配置示意圖下列欄位存在錯誤：",
    "manifestSections": {
        "structuredOutput/title": "結構化輸出",
        "speculativeDecoding/title": "預測解碼",
        "sampling/title": "取樣",
        "settings/title": "設定",
        "toolUse/title": "工具使用",
        "promptTemplate/title": "提示範本"
    },
    "llm.prediction.systemPrompt/title": "系統提示",
    "llm.prediction.systemPrompt/description": "使用此欄位提供背景說明給模型，例如一組規則、限制或一般要求。",
    "llm.prediction.systemPrompt/subTitle": "AI的使用準則",
    "llm.prediction.temperature/title": "溫度",
    "llm.prediction.temperature/subTitle": "要引入多少隨機性。0將每次產生相同結果，較高值會增加創作性和變異性",
    "llm.prediction.temperature/info": "來自llama.cpp幫助檔案：「預設值為<{{dynamicValue}}>，在隨機性與確定性之間取得平衡。在極端情況下，溫度為0時將總是選擇最可能的下一個令牌，導致每次執行結果相同」",
    "llm.prediction.llama.sampling/title": "取樣",
    "llm.prediction.topKSampling/title": "Top K 取樣",
    "llm.prediction.topKSampling/subTitle": "將下一個令牌限制在最可能的前k個令牌中。行為類似於溫度",
    "llm.prediction.topKSampling/info": "來自llama.cpp幫助檔案：\n\nTop-k取樣是一種文字生成方法，僅從模型預測的最前k個最可能令牌中選擇下一個令牌。\n\n它有助於減少產生低機率或無意義令牌的風險，但可能會限制輸出的多樣性。\n\nTop-k的值越高（例如100），考慮的令牌越多，導致文字更多樣，而值越低（例如10）則聚焦於最可能的令牌，產生更保守的文字。\n\n• 預設值為<{{dynamicValue}}>",
    "llm.prediction.llama.cpuThreads/title": "CPU 連數",
    "llm.prediction.llama.cpuThreads/subTitle": "推論期間使用的 CPU 執行緒數目",
    "llm.prediction.llama.cpuThreads/info": "計算期間使用的執行緒數目。增加執行緒數目未必會提高效能。預設值為 <{{dynamicValue}}>。",
    "llm.prediction.maxPredictedTokens/title": "限制回應長度",
    "llm.prediction.maxPredictedTokens/subTitle": "可選地限制 AI 的回應長度",
    "llm.prediction.maxPredictedTokens/info": "控制聊天機器人的回應最大長度。開啟以設定回應最大長度限制，關閉則讓聊天機器人決定停止的時間點。",
    "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（標記）",
    "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 個字",
    "llm.prediction.repeatPenalty/title": "重複懲罰",
    "llm.prediction.repeatPenalty/subTitle": "要 discourage 重複的相同標記程度",
    "llm.prediction.repeatPenalty/info": "來自 llama.cpp 說明檔案：\n\n較高的值（例如 1.5）會更強烈懲罰重複，而較低的值（例如 0.9）則較為寬容。預設值為 <{{dynamicValue}}>",
    "llm.prediction.minPSampling/title": "最小 P 取樣",
    "llm.prediction.minPSampling/subTitle": "標記被選為輸出的最小基礎機率",
    "llm.prediction.minPSampling/info": "來自 llama.cpp 說明檔案：\n\n相對於最可能標記的機率，標記需被考慮的最小機率。必須在 [0, 1] 內。\n\n預設值為 <{{dynamicValue}}>",
    "llm.prediction.topPSampling/title": "頂部 P 取樣",
    "llm.prediction.topPSampling/subTitle": "可能的下一個標記的最小累積機率。類似於溫度",
    "llm.prediction.topPSampling/info": "來自 llama.cpp 幫助檔案：\n\nTop-p 取樣，也稱為核取樣，是另一種文字生成方法，會從一個總機率達到至少 p 的 tokens 子集裡選擇下一個 token。\n\n此方法透過考量 tokens 的機率與取樣的 tokens 數量， achieving 異或與品質的平衡。\n\nTop-p 的值越高（例如 0.95），生成的文字就會越 diverse；值越低（例如 0.5），生成的文字會更聚焦且保守。該值須在 (0, 1] 內。\n\n• 預設值為 <{{dynamicValue}}>",
    "llm.prediction.stopStrings/title": "停止字串",
    "llm.prediction.stopStrings/subTitle": "應停止模型生成更多 tokens 的字串",
    "llm.prediction.stopStrings/info": "當遇到特定字串時，應停止模型生成更多 tokens",
    "llm.prediction.stopStrings/placeholder": "輸入一個字串並按下 ⏎",
    "llm.prediction.contextOverflowPolicy/title": "上下文溢位",
    "llm.prediction.contextOverflowPolicy/subTitle": "當對話變得太大以致模型無法處理時，模型應如何反應",
    "llm.prediction.contextOverflowPolicy/info": "決定當對話超過模型工作記憶的大小時應採取的行動（「上下文」）",
    "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
    "llm.prediction.llama.presencePenalty/title": "存在懲罰",
    "llm.prediction.llama.tailFreeSampling/title": "尾部取樣",
    "llm.prediction.llama.locallyTypicalSampling/title": "區域性典型取樣",
    "llm.prediction.llama.xtcProbability/title": "XTC取樣機率",
    "llm.prediction.llama.xtcProbability/subTitle": "XTC（排除最優選）取樣器將在每個生成的記號中以此機率啟動。XTC取樣可以提升創造力並減少陳詞濫調",
    "llm.prediction.llama.xtcProbability/info": "XTC（排除最優選）取樣器將在每個生成的記號中以此機率啟動。XTC取樣通常可以提升創造力並減少陳詞濫調",
    "llm.prediction.llama.xtcThreshold/title": "XTC取樣閥值",
    "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除最優選）閥值。以`xtc-probability`的機率，搜尋機率在`xtc-threshold`與0.5之間的標記，並將除機率最低的標記外的所有標記移除",
    "llm.prediction.llama.xtcThreshold/info": "XTC（排除優選專案）閾值。以 `xtc-機率` 的機率，搜尋機率在 `xtc-閾值` 與 0.5 之間的 token，並僅保留機率最低的那個 token",
    "llm.prediction.mlx.topKSampling/title": "Top K 抽樣",
    "llm.prediction.mlx.topKSampling/subTitle": "將下一個 token 限制為最可能的 top-k 個 token 中的一個。類似於溫度係數",
    "llm.prediction.mlx.topKSampling/info": "將下一個 token 限制為最可能的 top-k 個 token 中的一個。類似於溫度係數",
    "llm.prediction.onnx.topKSampling/title": "Top K 取樣",
    "llm.prediction.onnx.topKSampling/subTitle": "將下一個 token 限制為最可能的 top-k 個 token 中的一個。類似於溫度係數",
    "llm.prediction.onnx.topKSampling/info": "From ONNX 文件說明：\n\n保留 top-k 過濾中最高機率的語彙標記數量\n\n• 預設情況下這個過濾器為關閉狀態",
    "llm.prediction.onnx.repeatPenalty/title": "重複懲罰係數",
    "llm.prediction.onnx.repeatPenalty/subTitle": "抑制重複使用相同 token 的程度",
    "llm.prediction.onnx.repeatPenalty/info": "值越高，模型越不容易重複自身",
    "llm.prediction.onnx.topPSampling/title": "Top P 取樣",
    "llm.prediction.onnx.topPSampling/subTitle": "可能的下一個 token 的最小累計機率。類似於溫度係數",
    "llm.prediction.onnx.topPSampling/info": "From ONNX 文件說明：\n\n只保留機率總和達到 TopP 或更高的最可能 token\n\n• 預設情況下這個過濾器為關閉狀態",
    "llm.prediction.seed/title": "seed",
    "llm.prediction.structured/title": "結構化輸出",
    "llm.prediction.structured/info": "結構化輸出",
    "llm.prediction.structured/description": "高階：您可以提供 [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) 來強制要求模型以特定格式輸出。閱讀 [檔案](https://lmstudio.ai/docs/advanced/structured-output) 獲得更多資訊",
    "llm.prediction.tools/title": "工具使用",
    "llm.prediction.tools/description": "高階：您可以提供一串符合 JSON 格式的工具供模型請求呼叫。閱讀 [檔案](https://lmstudio.ai/docs/advanced/tool-use) 獲得更多資訊",
    "llm.prediction.tools/serverPageDescriptionAddon": "使用伺服器 API 時，請以 `tools` 的形式傳遞這組資料",
    "llm.prediction.promptTemplate/title": "提示範本",
    "llm.prediction.promptTemplate/subTitle": "聊天訊息傳送給模型的格式。修改此設定可能導致不預期的行為 - 請確保您知道自己在做什麼！",
    "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "草稿生成 token 數",
    "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "每一個主模型 token 生成的草稿 token 數量。尋求計算效能與回報之間的平衡點",
    "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿機率門檻",
    "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "繼續草稿直到某個 token 的機率低於此門檻。值越高，風險越低，回報越少",
    "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿大小",
    "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "草稿小於此數量將被主模型忽略。值越高，風險越低，回報越少",
    "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿大小",
    "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "設定草案中允許的token數量上限。若所有token機率均高於截斷值，則會設為上限。較低的值通常意味著較低的風險，但獎勵也較低",
    "llm.prediction.speculativeDecoding.draftModel/title": "草案模型",
    "llm.prediction.reasoning.parsing/title": "維度分析區域解析",
    "llm.prediction.reasoning.parsing/subTitle": "如何解析模型輸出中的維度分析區域",
    "llm.load.contextLength/title": "上下文長度",
    "llm.load.contextLength/subTitle": "設定模型在一對提示中可關注的token數量上限。請參閱「推理引數」下的對話溢位選項以瞭解更多管理方式",
    "llm.load.contextLength/info": "指定模型一次可處理的token數量上限，影響其處理時的記憶保留量",
    "llm.load.contextLength/warning": "設定較高的上下文長度值會顯著影響記憶使用量",
    "llm.load.seed/title": "離子",
    "llm.load.seed/subTitle": "用於文字生成的隨機數生成器的離子。-1代表隨機",
    "llm.load.seed/info": "隨機離子：設定隨機數生成的離子以確保結果可重複",
    "llm.load.llama.evalBatchSize/title": "餐評批次大小",
    "llm.load.llama.evalBatchSize/subTitle": "一次處理的輸入token數量。增加此值會提升效能，但需消耗更多記憶體",
    "llm.load.llama.evalBatchSize/info": "設定評估時一次處理的範例數量，影響速度與記憶體使用量",
    "llm.load.llama.ropeFrequencyBase/title": "RoPE頻率基數",
    "llm.load.llama.ropeFrequencyBase/subTitle": "自定義 rotary positional embeddings (RoPE) 的頻率基數。增加此值可能有助於在高上下文長度下取得更好的表現",
    "llm.load.llama.ropeFrequencyBase/info": "[高階] 調整 rotary positional encoding 的頻率基數，影響位置資訊的嵌入方式",
    "llm.load.llama.ropeFrequencyScale/title": "RoPE頻率係數",
    "llm.load.llama.ropeFrequencyScale/subTitle": "上下文長度會乘以此係數以擴充套件使用 RoPE 的有效上下文",
    "llm.load.llama.ropeFrequencyScale/info": "[高階] 調整 rotary positional encoding 的頻率係數以控制位置編碼精細度",
    "llm.load.llama.acceleration.offloadRatio/title": "GPU解除安裝",
    "llm.load.llama.acceleration.offloadRatio/subTitle": "設定在 GPU 上計算的離散模型層數以實現 GPU 加速",
    "llm.load.llama.acceleration.offloadRatio/info": "設定解除安裝至 GPU 的層數",
    "llm.load.llama.flashAttention/title": "Flash 注意力",
    "llm.load.llama.flashAttention/subTitle": "在部分模型中減少記憶體使用與生成時間",
    "llm.load.llama.flashAttention/info": "加速注意力機制以實現更快更高效的處理",
    "llm.load.numExperts/title": "專家數量",
    "llm.load.numExperts/subTitle": "模型使用的專家數量",
    "llm.load.numExperts/info": "模型使用的專家數量",
    "llm.load.llama.keepModelInMemory/title": "留存模型記憶體",
    "llm.load.llama.keepModelInMemory/subTitle": "即使解除安裝至 GPU 都會保留系統記憶體給模型。提升效能但需要更多系統記憶體",
    "llm.load.llama.keepModelInMemory/info": "防止模型被換出至磁碟，確保更快的訪問速度，但需更多記憶體",
    "llm.load.llama.useFp16ForKVCache/title": "使用FP16儲存KV緩衝區",
    "llm.load.llama.useFp16ForKVCache/info": "透過以半精度（FP16）儲存緩衝區來減少記憶體使用量",
    "llm.load.llama.tryMmap/title": "試用mmap()",
    "llm.load.llama.tryMmap/subTitle": "提升模型載入時間。當模型大小超過系統記憶體時，關閉此設定可能提升效能",
    "llm.load.llama.tryMmap/info": "直接從硬碟載入模型檔案至記憶體",
    "llm.load.llama.cpuThreadPoolSize/title": "CPU執行緒池大小",
    "llm.load.llama.cpuThreadPoolSize/subTitle": "分配給模型計算用執行緒池的CPU執行緒數量",
    "llm.load.llama.cpuThreadPoolSize/info": "分配給模型計算用執行緒池的CPU執行緒數量。增加執行緒數量未必能提升效能。預設值為<{{dynamicValue}}>",
    "llm.load.llama.kCacheQuantizationType/title": "K緩衝區量化型別",
    "llm.load.llama.kCacheQuantizationType/subTitle": "較低的值能減少記憶體使用量，但可能降低品質。效果會因模型而異。",
    "llm.load.llama.vCacheQuantizationType/title": "V緩衝區量化型別",
    "llm.load.llama.vCacheQuantizationType/subTitle": "較低的值能減少記憶體使用量，但可能降低品質。效果會因模型而異。",
    "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 若未啟用Flash Attention，必須關閉此設定",
    "llm.load.llama.vCacheQuantizationType/disabledMessage": "僅能於啟用Flash Attention時啟動",
    "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 若使用F32，必須關閉Flash Attention",
    "llm.load.mlx.kvCacheBits/title": "KV緩衝區量化",
    "llm.load.mlx.kvCacheBits/subTitle": "KV緩衝區應量化為多少位元",
    "llm.load.mlx.kvCacheBits/info": "KV緩衝區應量化為多少位元",
    "llm.load.mlx.kvCacheBits/turnedOnWarning": "當使用KV緩衝區量化時，上下文長度設定將被忽略",
    "llm.load.mlx.kvCacheGroupSize/title": "KV緩衝區量化：群組大小",
    "llm.load.mlx.kvCacheGroupSize/subTitle": "量化操作期間KV緩衝區的群組大小。較大的群組大小能減少記憶體使用量，但可能降低品質",
    "llm.load.mlx.kvCacheGroupSize/info": "KV緩衝區應量化為多少位元",
    "llm.load.mlx.kvCacheQuantizationStart/title": "KV緩衝區量化：當上下文長度超過此長度時開始量化",
    "llm.load.mlx.kvCacheQuantizationStart/subTitle": "開始量化KV緩衝區的上下文長度閥值",
    "llm.load.mlx.kvCacheQuantizationStart/info": "開始量化KV緩衝區的上下文長度閥值",
    "llm.load.mlx.kvCacheQuantization/title": "KV緩衝區量化",
    "llm.load.mlx.kvCacheQuantization/subTitle": "量化模型的KV緩衝區。這可能會導致生成速度加快且記憶體佔用減少，\n但會以模型輸出的品質為代價。",
    "llm.load.mlx.kvCacheQuantization/bits/title": "KV緩衝區量化位數",
    "llm.load.mlx.kvCacheQuantization/bits/tooltip": "將KV緩衝區量化為多少位元",
    "llm.load.mlx.kvCacheQuantization/bits/bits": "位元",
    "llm.load.mlx.kvCacheQuantization/groupSize/title": "群組大小策略",
    "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "精度",
    "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "均衡",
    "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "速度優先",
    "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "5. 高階：量化 'matmul 群組大小' 設定\n4.\n3. • 精確度 = 群組大小 32\n2. • 平衡 = 群組大小 64\n1. • 速度 = 群組大小 128\n",
    "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "當 contexts 長度達標時開始量化",
    "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "2. 當 contexts 達到此語彙數量時，\n1. 開始量化 KV 快取",
    "embedding.load.contextLength/title": "上下文長度",
    "embedding.load.contextLength/subTitle": "模型在一個提示中所能關注的最大語彙數量。詳見「推理引數」下的對話 Overflow 選項，瞭解更多管理方式",
    "embedding.load.contextLength/info": "指定模型一次性所能考慮的最大語彙數量，影響處理過程中的上下文保留量",
    "embedding.load.llama.ropeFrequencyBase/title": "RoPE 頻率基礎",
    "embedding.load.llama.ropeFrequencyBase/subTitle": "自訂 RoPE 旋轉位置嵌入（RoPE）的基礎頻率。提升此值可能有助於在高 contexts 長度下取得更好表現",
    "embedding.load.llama.ropeFrequencyBase/info": "[高階] 調整 RoPE 旋轉位置編碼的基礎頻率，影響位置資訊的嵌入方式",
    "embedding.load.llama.evalBatchSize/title": "評估批次大小",
    "embedding.load.llama.evalBatchSize/subTitle": "每次處理的輸入語彙數量。提升此值會增加效能，但會增加記憶體使用量",
    "embedding.load.llama.evalBatchSize/info": "設定評估時每批次處理的語彙數量",
    "embedding.load.llama.ropeFrequencyScale/title": "RoPE 頻率比例",
    "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文長度會乘以這個係數，透過 RoPE 延伸有效上下文",
    "embedding.load.llama.ropeFrequencyScale/info": "[高階] 修改 RoPE 旋轉位置編碼的頻率比例，用以控制位置編碼的細節程度",
    "embedding.load.llama.acceleration.offloadRatio/title": "GPU 解卸",
    "embedding.load.llama.acceleration.offloadRatio/subTitle": "需要 GPU 計算的離散模型層數",
    "embedding.load.llama.acceleration.offloadRatio/info": "設定需要解卸到 GPU 的層數",
    "embedding.load.llama.keepModelInMemory/title": "保持模型在記憶體中",
    "embedding.load.llama.keepModelInMemory/subTitle": "優先保留系統記憶體給模型，即使解卸到 GPU。雖然提升效能，但需要更多系統 RAM",
    "embedding.load.llama.keepModelInMemory/info": "防止模型被換出到硬碟，確保更快的存取速度，以更高的 RAM 使用為代價",
    "embedding.load.llama.tryMmap/title": "堯 mmap()",
    "embedding.load.llama.tryMmap/subTitle": "改善模型的載入時間。關閉此設定可能在模型大於系統 RAM 時改善效能",
    "embedding.load.llama.tryMmap/info": "載入模型檔案直徑從硬碟到記憶體",
    "embedding.load.seed/title": "隨機種子",
    "embedding.load.seed/subTitle": "文字生成中使用的隨機數生成器的種子。-1 為隨機種子",
    "embedding.load.seed/info": "隨機種子：設定隨機數生成的種子，以確保結果可重複",
    "presetTooltip": {
        "included/title": "預設值",
        "included/description": "下面的專案將會被應用",
        "included/empty": "本預設的欄位在當前情境下均不適用。",
        "included/conflict": "您將被問 whether 要應用此值。",
        "separateLoad/title": "裝載時設定",
        "separateLoad/description.1": "本預設也包含以下裝載時設定。裝載時間設定為模型範圍內，需重新載入模型後才會生效。保留",
        "separateLoad/description.2": "適用於",
        "separateLoad/description.3": ".",
        "excluded/title": "可能不適用",
        "excluded/description": "以下欄位包含在預設中，但在當前情境下不適用。",
        "legacy/title": "舊預設",
        "legacy/description": "本預設為舊預設。它包含以下欄位，這些欄位目前已自動處理，或不再適用。",
        "button/publish": "發布到 Hub",
        "button/pushUpdate": "推送變更到 Hub",
        "button/export": "匯出"
    },
    "customInputs": {
        "string": {
            "emptyParagraph": "<空>"
        },
        "checkboxNumeric": {
            "off": "關閉"
        },
        "llamaCacheQuantizationType": {
            "off": "關閉"
        },
        "mlxKvCacheBits": {
            "off": "關閉"
        },
        "stringArray": {
            "empty": "<空>"
        },
        "llmPromptTemplate": {
            "type": "型別",
            "types.jinja/label": "模板 (Jinja)",
            "jinja.bosToken/label": "BOS 標記",
            "jinja.eosToken/label": "EOS 標記",
            "jinja.template/label": "模板",
            "jinja/error": "無法解析 Jinja 模板：{{錯誤}}",
            "jinja/empty": "請在上方輸入一個 Jinja 模板。",
            "jinja/unlikelyToWork": "您提供的上方 Jinja 模板很可能無法運作，因為它未引用變數「messages」。請確認是否輸入正確的模板。",
            "types.manual/label": "手動",
            "manual.subfield.beforeSystem/label": "系統前",
            "manual.subfield.beforeSystem/placeholder": "輸入系統字首…",
            "manual.subfield.afterSystem/label": "系統後",
            "manual.subfield.afterSystem/placeholder": "輸入系統字尾…",
            "manual.subfield.beforeUser/label": "使用者前",
            "manual.subfield.beforeUser/placeholder": "輸入使用者字首…",
            "manual.subfield.afterUser/label": "使用者後",
            "manual.subfield.afterUser/placeholder": "輸入使用者字尾…",
            "manual.subfield.beforeAssistant/label": "助理前",
            "manual.subfield.beforeAssistant/placeholder": "輸入助理字首…",
            "manual.subfield.afterAssistant/label": "助理後",
            "manual.subfield.afterAssistant/placeholder": "輸入助理字尾…",
            "stopStrings/label": "額外停止字串",
            "stopStrings/subTitle": "將用於額外的停止字串，這些字串將與使用者指定的停止字串一起使用。"
        },
        "contextLength": {
            "maxValueTooltip": "這是模型訓練的最大記憶體容量。點選以設定當前情境為此值",
            "maxValueTextStart": "模型支援至",
            "maxValueTextEnd": "記憶體",
            "tooltipHint": "雖然模型可能支援至一定數量的記憶體，但若您的機器資源無法處理負載，表現可能惡化 - 在增加此值時請謹慎"
        },
        "contextOverflowPolicy": {
            "stopAtLimit": "停止於極限",
            "stopAtLimitSub": "當模型記憶體滿時停止產生",
            "truncateMiddle": "截斷中間",
            "truncateMiddleSub": "從對話中間刪除訊息以為新訊息騰出空間。模型仍會記住對話的開始部分",
            "rollingWindow": "滑動視窗",
            "rollingWindowSub": "模型將始終取得最近幾條訊息，但可能遺忘對話的開始部分"
        },
        "llamaAccelerationOffloadRatio": {
            "max": "MAX",
            "off": "關閉"
        },
        "llamaAccelerationSplitStrategy": {
            "evenly": "均勻",
            "favorMainGpu": "優先主顯示卡"
        },
        "speculativeDecodingDraftModel": {
            "readMore": "閱讀其運作方式",
            "placeholder": "選擇一個相容的草案模型",
            "noCompatible": "未找到與您目前模型選擇相容的草稿模型",
            "stillLoading": "尋找相容的草稿模型…",
            "notCompatible": "所選的草稿模型(<draft/>)與目前的模型選擇(<current/>)不相容。",
            "off": "關閉",
            "loadModelToSeeOptions": "載入模型 <keyboard-shortcut /> 以檢視相容選項",
            "compatibleWithNumberOfModels": "至少適用於您模型中的 {{dynamicValue}} 個模型",
            "recommendedForSomeModels": "適用於部分模型",
            "recommendedForLlamaModels": "適用於Llama模型",
            "recommendedForQwenModels": "適用於Qwen模型",
            "onboardingModal": {
                "introducing": "引言",
                "speculativeDecoding": "猜測解碼",
                "firstStepBody": "針對 <custom-span>llama.cpp</custom-span> 與 <custom-span>MLX</custom-span> 模型的推論加速",
                "secondStepTitle": "搭配猜測解碼的推論加速",
                "secondStepBody": "猜測解碼是一種兩個模型協作的技術：\n - 一個較大的「主」模型\n - 一個較小的「草稿」模型\n\n在生成過程中，草稿模型快速提出標記供較大的主模型驗證。驗證標記的過程比實際生成標記更快，這是速度提升的來源。**一般來說，主模型與草稿模型之間的大小差異越大，速度提升越明顯**。\n\n為維持品質，主模型只接受與其自行生成的結果相符的標記，使較大的模型能在更快的推論速度下產生穩定的回應。兩者必須共享相同的詞彙。",
                "draftModelRecommendationsTitle": "草稿模型推薦",
                "basedOnCurrentModels": "基於您目前的模型",
                "close": "關閉",
                "next": "下一頁",
                "done": "完成"
            },
            "speculativeDecodingLoadModelToSeeOptions": "請先載入一個模型 <model-badge /> ",
            "errorEngineNotSupported": "猜測解碼需至少 {{minVersion}} 版本的引擎 {{engineName}}。請更新引擎 (<key/>) 並重新載入模型以使用此功能。",
            "errorEngineNotSupported/noKey": "猜測解碼需至少 {{minVersion}} 版本的引擎 {{engineName}}。請更新引擎並重新載入模型以使用此功能。"
        },
        "llmReasoningParsing": {
            "startString/label": "起始字串",
            "startString/placeholder": "輸入起始字串…",
            "endString/label": "結束字串",
            "endString/placeholder": "輸入結束字串…"
        }
    },
    "saveConflictResolution": {
        "title": "選擇要包含在預設值中的值",
        "description": "選擇要保留的值",
        "instructions": "點選選項以包含它",
        "userValues": "上一個值",
        "presetValues": "新值",
        "confirm": "確認",
        "cancel": "取消"
    },
    "applyConflictResolution": {
        "title": "要保留哪些值？",
        "description": "您有與即將合併的預設重疊的未提交變更",
        "instructions": "點選值以保留它",
        "userValues": "目前值",
        "presetValues": "即將合併的預設值",
        "confirm": "確認",
        "cancel": "取消"
    },
    "empty": "<空>",
    "noModelSelected": "未選擇模型",
    "apiIdentifier.label": "API識別碼",
    "apiIdentifier.hint": "可選提供此模型的識別碼。此識別碼將用於API請求。留空則使用預設識別碼。",
    "idleTTL.label": "閒置時自動解除安裝（TTL）",
    "idleTTL.hint": "若設定，模型在閒置指定時間後將自動解除安裝。",
    "idleTTL.mins": "分鐘",
    "presets": {
        "title": "預設",
        "commitChanges": "提交變更",
        "commitChanges/description": "將您的變更提交至預設。",
        "commitChanges.manual": "偵測到新欄位。您將能選擇哪些變更包含在預設中。",
        "commitChanges.manual.hold.0": "按住",
        "commitChanges.manual.hold.1": "選擇要提交至預設的變更。",
        "commitChanges.saveAll.hold.0": "按住",
        "commitChanges.saveAll.hold.1": "儲存所有變更。",
        "commitChanges.saveInPreset.hold.0": "按住",
        "commitChanges.saveInPreset.hold.1": "僅儲存已包含在預設中的欄位變更。",
        "commitChanges/error": "無法提交預設的變更。",
        "commitChanges.manual/description": "選擇要包含在預設中的變更。",
        "saveAs": "作為新預設儲存…",
        "presetNamePlaceholder": "輸入預設名稱…",
        "cannotCommitChangesLegacy": "此為遺留預設，無法修改。您可使用「作為新預設儲存…」建立複製。",
        "cannotCommitChangesNoChanges": "無變更可提交。",
        "emptyNoUnsaved": "選擇預設…",
        "emptyWithUnsaved": "尚未儲存的預設",
        "saveEmptyWithUnsaved": "作為預設儲存…",
        "saveConfirm": "儲存",
        "saveCancel": "取消",
        "saving": "儲存中…",
        "save/error": "無法儲存預設。",
        "deselect": "取消選中預設",
        "deselect/error": "無法取消選中預設。",
        "select/error": "無法選中預設。",
        "delete/error": "無法刪除預設。",
        "discardChanges": "刪除此未儲存的",
        "discardChanges/info": "刪除所有未提交的變更並恢復預設至原始狀態",
        "newEmptyPreset": "+ 新預設",
        "importPreset": "匯入",
        "contextMenuSelect": "套用預設",
        "contextMenuDelete": "刪除…",
        "contextMenuShare": "發布…",
        "contextMenuOpenInHub": "在中心檢視",
        "contextMenuPushChanges": "推送變更至中心",
        "contextMenuPushingChanges": "推送中…",
        "contextMenuPushedChanges": "變更已推送",
        "contextMenuExport": "匯出檔案",
        "contextMenuRevealInExplorer": "在檔案總管中顯示",
        "contextMenuRevealInFinder": "在Finder中顯示",
        "share": {
            "title": "發布預設",
            "action": "與他人分享您的預設，讓他們下載、點讚、並fork",
            "presetOwnerLabel": "擁有者",
            "uploadAs": "您的預設將會以{{name}}的名稱建立",
            "presetNameLabel": "預設名稱",
            "descriptionLabel": "說明（可選）",
            "loading": "發布中…",
            "success": "預設發布成功",
            "presetIsLive": "<預設名稱 /> 現在已上線於中心！",
            "close": "關閉",
            "confirmViewOnWeb": "在網頁檢視",
            "confirmCopy": "複製URL",
            "confirmCopied": "已複製！",
            "pushedToHub": "您的預設已推送至中心",
            "descriptionPlaceholder": "輸入說明…",
            "willBePublic": "發布您的預設會使其成為公公開",
            "publicSubtitle": "您的預設是<custom-bold>公公開</custom-bold>。其他人可在lmstudio.ai下載並複製它",
            "confirmShareButton": "釋出",
            "error": "無法釋出預設",
            "createFreeAccount": "在Hub中建立免費賬戶以釋出預設"
        },
        "update": {
            "title": "推送上Hub",
            "title/success": "預設成功更新",
            "subtitle": "修改<custom-preset-name />並推送上Hub",
            "descriptionLabel": "描述",
            "descriptionPlaceholder": "輸入描述…",
            "loading": "推送上…",
            "cancel": "取消",
            "createFreeAccount": "在Hub中建立免費賬戶以釋出預設",
            "error": "無法推送上更新",
            "confirmUpdateButton": "推送上"
        },
        "import": {
            "title": "從檔案匯入預設",
            "dragPrompt": "拖放預設JSON檔案或<custom-link>從你的電腦選擇</custom-link>",
            "remove": "移除",
            "cancel": "取消",
            "importPreset_zero": "匯入預設",
            "importPreset_one": "匯入預設",
            "importPreset_other": "匯入{{count}}個預設",
            "selectDialog": {
                "title": "選擇預設檔案(.json)",
                "button": "匯入"
            },
            "error": "無法匯入預設",
            "resultsModal": {
                "titleSuccessSection_one": "成功匯入1個預設",
                "titleSuccessSection_other": "成功匯入{{count}}個預設",
                "titleFailSection_zero": "",
                "titleFailSection_one": "（{{count}}失敗）",
                "titleFailSection_other": "（{{count}}失敗）",
                "titleAllFailed": "無法匯入預設",
                "importMore": "匯入更多",
                "close": "完成",
                "successBadge": "成功",
                "alreadyExistsBadge": "預設已存在",
                "errorBadge": "錯誤",
                "invalidFileBadge": "無效檔案",
                "otherErrorBadge": "無法匯入預設",
                "errorViewDetailsButton": "檢視細節",
                "seeError": "檢視錯誤",
                "noName": "無預設名稱",
                "useInChat": "用於對話"
            },
            "importFromUrl": {
                "button": "從URL匯入…",
                "title": "從URL匯入",
                "back": "從檔案匯入…",
                "action": "請在下方貼入您想要匯入的預設的LM Studio Hub URL",
                "invalidUrl": "無效URL。請確認您貼入的是正確的LM Studio Hub URL。",
                "tip": "您可以在LM Studio Hub中透過{{buttonName}}按鈕直接安裝此預設",
                "confirm": "匯入",
                "cancel": "取消",
                "loading": "正在匯入…",
                "error": "無法下載預設。"
            }
        },
        "download": {
            "title": "從LM Studio Hub拉取<preset-name />預設",
            "subtitle": "儲存<custom-name />至您的預設。這樣您就能在應用程式中使用此預設",
            "button": "拉取",
            "button/loading": "正在拉取…",
            "cancel": "取消",
            "error": "無法下載預設。"
        },
        "inclusiveness": {
            "speculativeDecoding": "包含於預設"
        }
    },
    "flashAttentionWarning": "快閃記憶體注意是實驗性功能，可能與某些模型產生問題。若遇到問題，請嘗試停用它。",
    "llamaKvCacheQuantizationWarning": "KV快取量化是實驗性功能，可能與某些模型產生問題。快閃記憶體注意需啟用才能使用V快取量化。若遇到問題，請恢復為預設的「F16」。",
    "seedUncheckedHint": "隨機種子",
    "ropeFrequencyBaseUncheckedHint": "自動",
    "ropeFrequencyScaleUncheckedHint": "自動",
    "hardware": {
        "advancedGpuSettings": "高階GPU設定",
        "advancedGpuSettings.info": "若有疑慮，請保留預設值",
        "advancedGpuSettings.reset": "恢復預設設定",
        "environmentVariables": {
            "title": "環境變數",
            "description": "在模型運作期間活用的環境變數。",
            "key.placeholder": "選擇變數…",
            "value.placeholder": "值"
        },
        "mainGpu": {
            "title": "主 GPU",
            "description": "為模型運算優先選擇的 GPU。",
            "placeholder": "選擇主 GPU…"
        },
        "splitStrategy": {
            "title": "分割策略",
            "description": "如何將模型運算分割至多個 GPU。",
            "placeholder": "選擇分割策略…"
        }
    }
}