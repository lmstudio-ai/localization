{
  "noInstanceSelected": "Не выбран ни один экземпляр модели",
  "resetToDefault": "Сбросить",
  "showAdvancedSettings": "Показать расширенные настройки",
  "showAll": "Все",
  "basicSettings": "Базовые",
  "configSubtitle": "Загружайте или сохраняйте предустановки, экспериментируйте с переопределением параметров модели",
  "inferenceParameters/title": "Параметры предсказания",
  "inferenceParameters/info": "Экспериментируйте с параметрами, влияющими на предсказание.",
  "generalParameters/title": "Общие",
  "samplingParameters/title": "Сэмплирование",
  "basicTab": "Базовые",
  "advancedTab": "Расширенные",
  "advancedTab/title": "🧪 Расширенная конфигурация",
  "advancedTab/expandAll": "Развернуть все",
  "advancedTab/overridesTitle": "Переопределения конфигурации",
  "advancedTab/noConfigsText": "У вас нет несохраненных изменений - отредактируйте значения выше, чтобы увидеть переопределения здесь.",
  "loadInstanceFirst": "Загрузите модель, чтобы просмотреть настраиваемые параметры",
  "noListedConfigs": "Нет настраиваемых параметров",
  "generationParameters/info": "Экспериментируйте с базовыми параметрами, влияющими на генерацию текста.",
  "loadParameters/title": "Загрузить параметры",
  "loadParameters/description": "Настройки для управления процессом инициализации и загрузки модели в память.",
  "loadParameters/reload": "Перезагрузить для применения изменений",
  "loadParameters/reload/error": "Не удалось перезагрузить модель",
  "discardChanges": "Отменить изменения",
  "loadModelToSeeOptions": "Загрузите модель, чтобы увидеть параметры",
  "schematicsError.title": "В схемах конфигурации обнаружены ошибки в следующих полях:",
  "manifestSections": {
    "structuredOutput/title": "Структурированный вывод",
    "speculativeDecoding/title": "Спекулятивное декодирование",
    "sampling/title": "Сэмплирование",
    "settings/title": "Настройки",
    "toolUse/title": "Использование инструментов",
    "promptTemplate/title": "Шаблон промпта",
    "customFields/title": "Пользовательские поля"
  },

  "llm.prediction.systemPrompt/title": "Системный промпт",
  "llm.prediction.systemPrompt/description": "Используйте это поле для предоставления модели фоновых инструкций, таких как набор правил, ограничений или общих требований.",
  "llm.prediction.systemPrompt/subTitle": "Руководство по ИИ",
  "llm.prediction.systemPrompt/openEditor": "Редактор",
  "llm.prediction.systemPrompt/closeEditor": "Закрыть редактор",
  "llm.prediction.systemPrompt/openedEditor": "Открыто в редакторе…",
  "llm.prediction.systemPrompt/edit": "Редактировать системный промпт...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "Добавить инструкции...",
  "llm.prediction.systemPrompt/addInstructions": "Добавить инструкции",
  "llm.prediction.temperature/title": "Температура",
  "llm.prediction.temperature/subTitle": "Сколько случайности добавить. 0 будет давать одинаковый результат каждый раз, более высокие значения повысят креативность и разнообразие",
  "llm.prediction.temperature/info": "Из документации llama.cpp: \"Значение по умолчанию <{{dynamicValue}}> обеспечивает баланс между случайностью и детерминизмом. В крайнем случае, температура 0 всегда будет выбирать наиболее вероятный следующий токен, приводя к идентичным результатам при каждом запуске\"",
  "llm.prediction.llama.sampling/title": "Сэмплирование",
  "llm.prediction.topKSampling/title": "Сэмплирование Top K",
  "llm.prediction.topKSampling/subTitle": "Ограничивает следующий токен одним из топ-k наиболее вероятных токенов. Работает аналогично температуре",
  "llm.prediction.topKSampling/info": "Из документации llama.cpp:\n\nСэмплирование Top-k - это метод генерации текста, который выбирает следующий токен только из числа top k наиболее вероятных токенов, предсказанных моделью.\n\nЭто помогает снизить риск генерации маловероятных или бессмысленных токенов, но может также ограничить разнообразие выходных данных.\n\nБолее высокое значение для top-k (например, 100) будет учитывать больше токенов и приводить к более разнообразному тексту, в то время как более низкое значение (например, 10) сфокусируется на наиболее вероятных токенах и генерирует более консервативный текст.\n\n• Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Потоки CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Количество потоков CPU для вывода",
  "llm.prediction.llama.cpuThreads/info": "Количество потоков, используемых при вычислениях. Увеличение количества потоков не всегда коррелирует с улучшением производительности. Значение по умолчанию: <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Ограничение длины ответа",
  "llm.prediction.maxPredictedTokens/subTitle": "Необязательное ограничение длины ответа ИИ",
  "llm.prediction.maxPredictedTokens/info": "Контролирует максимальную длину ответа чат-бота. Включите для установки ограничения на максимальную длину ответа или отключите, чтобы чат-бот самостоятельно определял, когда остановиться.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Максимальная длина ответа (в токенах)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Примерно слов: {{maxWords}}",
  "llm.prediction.repeatPenalty/title": "Штраф за повторение",
  "llm.prediction.repeatPenalty/subTitle": "Насколько снижать вероятность повторения одного и того же токена",
  "llm.prediction.repeatPenalty/info": "Из документации llama.cpp: Помогает предотвратить генерацию моделью повторяющегося или монотонного текста.\n\nБолее высокое значение (например, 1,5) сильнее штрафует за повторения, а более низкое (например, 0.9) - действует мягче.\" • Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Сэмплирование Min P",
  "llm.prediction.minPSampling/subTitle": "Минимальная базовая вероятность выбора токена для вывода",
  "llm.prediction.minPSampling/info": "Из документации llama.cpp:\n\nМинимальная вероятность токена для рассмотрения относительно вероятности наиболее вероятного токена. Должно быть в диапазоне [0, 1].\n\n• Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Сэмплирование Top P",
  "llm.prediction.topPSampling/subTitle": "Минимальная кумулятивная вероятность для возможных следующих токенов. Работает аналогично температуре",
  "llm.prediction.topPSampling/info": "Из документации llama.cpp:\n\nСэмплирование Top-p, также известное как nucleus sampling, - это ещё один метод генерации текста, который выбирает следующий токен из подмножества токенов, совокупная вероятность которых составляет не менее p.\n\nЭтот метод обеспечивает баланс между разнообразием и качеством, учитывая как вероятности токенов, так и количество токенов для выборки.\n\nБолее высокое значение для top-p (например, 0,95) приведёт к более разнообразному тексту, в то время как более низкое значение (например, 0,5) сгенерирует более узконаправленный и консервативный текст. Должно быть в диапазоне [0, 1].\n\n• Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Стоп-строки",
  "llm.prediction.stopStrings/subTitle": "Строки, при встрече которых модель прекращает генерацию токенов.",
  "llm.prediction.stopStrings/info": "Определённые строки, при встрече которых модель прекратит генерацию дополнительных токенов.",
  "llm.prediction.stopStrings/placeholder": "Введите строку и нажмите ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Переполнение контекста",
  "llm.prediction.contextOverflowPolicy/subTitle": "Как должна вести себя модель, когда разговор становится слишком большим для её обработки",
  "llm.prediction.contextOverflowPolicy/info": "Определите, что делать, когда разговор превышает размер рабочей памяти модели ('контекст')",
  "llm.prediction.llama.frequencyPenalty/title": "Штраф за повторение",
  "llm.prediction.llama.presencePenalty/title": "Штраф за появление",
  "llm.prediction.llama.tailFreeSampling/title": "Сэмплирование Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Локальное типичное сэмплирование",
  "llm.prediction.llama.xtcProbability/title": "Вероятность сэмплирования XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "Сэмплер XTC (Exclude Top Choices) активируется с указанной вероятностью для каждого генерируемого токена. Сэмплирование XTC может повысить креативность и уменьшить клише",
  "llm.prediction.llama.xtcProbability/info": "Сэмплер XTC (Exclude Top Choices) активируется с указанной вероятностью для каждого генерируемого токена. Сэмплирование XTC обычно повышает креативность и снижает количество клише",
  "llm.prediction.llama.xtcThreshold/title": "Порог сэмплирования XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Порог сэмплирования XTC (Exclude Top Choices). С указанной вероятностью `xtc-probability`, ищет токены с вероятностями между `xtc-threshold` и 0.5, и удаляет все такие токены, кроме наименее вероятного",
  "llm.prediction.llama.xtcThreshold/info": "Порог сэмплирования XTC (Exclude Top Choices). С указанной вероятностью `xtc-probability`, ищет токены с вероятностями между `xtc-threshold` и 0.5, и удаляет все такие токены, кроме наименее вероятного",
  "llm.prediction.mlx.topKSampling/title": "Сэмплирование Top K",
  "llm.prediction.mlx.topKSampling/subTitle": "Ограничивает следующий токен одним из топ-k наиболее вероятных токенов. Работает аналогично температуре",
  "llm.prediction.mlx.topKSampling/info": "Ограничивает следующий токен одним из топ-k наиболее вероятных токенов. Работает аналогично температуре",
  "llm.prediction.onnx.topKSampling/title": "Сэмплирование Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Ограничивает следующий токен одним из топ-k наиболее вероятных токенов. Работает аналогично температуре",
  "llm.prediction.onnx.topKSampling/info": "Из документации ONNX:\n\nКоличество токенов словаря с наивысшей вероятностью, которые следует сохранить для фильтрации Top-K.\n\n• Этот фильтр отключён по умолчанию.",
  "llm.prediction.onnx.repeatPenalty/title": "Штраф за повторение",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Насколько снижать вероятность повторения одного и того же токена",
  "llm.prediction.onnx.repeatPenalty/info": "Большие значения снижают склонность модели к повторениям",
  "llm.prediction.onnx.topPSampling/title": "Сэмплирование Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Минимальная кумулятивная вероятность для возможных следующих токенов. Работает аналогично температуре",
  "llm.prediction.onnx.topPSampling/info": "Из документации ONNX:\n\nТолько наиболее вероятные токены, совокупная вероятность которых достигает TopP или выше, сохраняются для генерации.\n\n• Этот фильтр отключён по умолчанию.",
  "llm.prediction.seed/title": "Сид",
  "llm.prediction.structured/title": "Структурированный вывод",
  "llm.prediction.structured/info": "Структурированный вывод",
  "llm.prediction.structured/description": "Дополнительно: можно задать [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) для принудительного форматирования вывода модели. Подробнее в [документации](https://lmstudio.ai/docs/advanced/structured-output).",
  "llm.prediction.tools/title": "Использование инструментов",
  "llm.prediction.tools/description": "Дополнительно: можно передать список инструментов, в формате JSON, которые модель сможет вызывать. Подробнее в [документации](https://lmstudio.ai/docs/advanced/tool-use)",
  "llm.prediction.tools/serverPageDescriptionAddon": "Передавать это в теле запроса как `tools` при использовании серверного API",
  "llm.prediction.promptTemplate/title": "Шаблон промпта",
  "llm.prediction.promptTemplate/subTitle": "Формат, в котором сообщения в чате отправляются модели. Изменение может привести к непредсказуемым последствиям - убедитесь, что вы понимаете, что делаете!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Токенов для черновика",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "Число токенов, генерируемых черновой моделью на каждый токен основной модели. Найдите оптимальный баланс между вычислительными ресурсами и результативностью",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Порог отсечения вероятности черновика",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Продолжать черновик, пока вероятность токена не опустится ниже этого значения. Более высокие значения обычно означают меньший риск и меньшую отдачу",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Мин. размер черновика",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Черновики меньшего размера будут игнорироваться основной моделью. Более высокие значения обычно означают меньший риск и меньшую отдачу",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Макс. размер черновика",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Максимальное количество токенов, разрешённое в черновике. Ограничение, если все вероятности токенов превышают пороговое значение. Более низкие значения обычно означают меньший риск и меньшую отдачу",
  "llm.prediction.speculativeDecoding.draftModel/title": "Черновая модель",
  "llm.prediction.reasoning.parsing/title": "Парсинг секций рассуждений",
  "llm.prediction.reasoning.parsing/subTitle": "Как парсить секции рассуждений в выходных данных модели",

  "llm.load.mainGpu/title": "Основной GPU",
  "llm.load.mainGpu/subTitle": "GPU для приоритетной обработки вычислений модели",
  "llm.load.mainGpu/placeholder": "Выбрать основной GPU...",
  "llm.load.splitStrategy/title": "Стратегия разделения",
  "llm.load.splitStrategy/subTitle": "Как распределить вычисления модели между GPU",
  "llm.load.splitStrategy/placeholder": "Выбрать стратегию разделения...",
  "llm.load.offloadKVCacheToGpu/title": "Выгрузить кэш KV в память GPU",
  "llm.load.offloadKVCacheToGpu/subTitle": "Выгрузить кэш KV в память GPU. Улучшает производительность, но требует больше памяти GPU.",
  "llm.load.numParallelSessions/title": "Максимальное количество одновременных предсказаний",
  "llm.load.numParallelSessions/subTitle": "Максимальное количество предсказаний, которые модель может выполнять одновременно. Скорость каждого отдельного предсказания может снижаться при увеличении параллельности, но каждое предсказание будет запускаться быстрее, и можно добиться более высокой общей пропускной способности.",
  "llm.load.useUnifiedKvCache/title": "Унифицированный кэш KV",
  "llm.load.useUnifiedKvCache/subTitle": "Управляет тем, используют ли одновременные предсказания единый кэш KV, экономя память. Отключение этой опции гарантирует, что каждое предсказание сможет использовать полную длину контекста, но при этом потребуется больше памяти.",
  "load.gpuStrictVramCap/title": "Ограничить выгрузку модели только в выделенную память GPU",
  "load.gpuStrictVramCap.customSubTitleOff": "ВЫКЛ: Разрешить выгрузку весов модели в общую память, если выделенная память GPU заполнена",
  "load.gpuStrictVramCap.customSubTitleOn": "ВКЛ: Система будет ограничивать выгрузку весов модели только в выделенную память GPU и оперативную память (RAM). Контекст все ещё может использовать общую память",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "Выгрузка модели ограничена выделенной памятью GPU. Фактическое количество выгруженных слоёв может отличаться.",
  "load.allGpusDisabledWarning": "Все GPU в настоящее время отключены. Включите хотя бы один параметр для выгрузки",

  "llm.load.contextLength/title": "Длина контекста",
  "llm.load.contextLength/subTitle": "Максимальное количество токенов, которое модель может обрабатывать в одном запросе. Смотрите опции управления переполнением в разделе \"Параметры вывода\" для получения дополнительных способов управления этим параметром",
  "llm.load.contextLength/info": "Определяет максимальное количество токенов, которое модель может учитывать одновременно, что влияет на объем контекста, который она сохраняет во время обработки",
  "llm.load.contextLength/warning": "Высокое значение длины контекста может существенно увеличить потребление памяти",
  "llm.load.seed/title": "Сид",
  "llm.load.seed/subTitle": "Сид для генератора случайных чисел, используемого при генерации текста. -1 означает случайное значение",
  "llm.load.seed/info": "Случайный сид: Устанавливает сид для генератора случайных чисел, чтобы обеспечить воспроизводимость результатов",
  "llm.load.numCpuExpertLayersRatio/title": "Количество слоёв, для которых следует принудительно перенести веса MoE на CPU",
  "llm.load.numCpuExpertLayersRatio/subTitle": "Количество слоёв, для которых следует принудительно перенести экспертов на CPU. Экономит видеопамять (VRAM) и может быть быстрее, чем частичная выгрузка на GPU. Не рекомендуется, если модель полностью помещается в видеопамять (VRAM).",
  "llm.load.numCpuExpertLayersRatio/info": "Указывает количество слоёв, для которых следует принудительно перенести экспертов на CPU. Оставляет слои внимания на GPU, экономя видеопамять (VRAM) при сохранении приемлемой скорости инференса.",

  "llm.load.llama.evalBatchSize/title": "Размер пакета оценки",
  "llm.load.llama.evalBatchSize/subTitle": "Число входных токенов для обработки за один проход. Увеличение повышает производительность за счёт использования памяти",
  "llm.load.llama.evalBatchSize/info": "Устанавливает количество примеров, обрабатываемых совместно в одном пакете при оценке, что влияет на скорость и потребление памяти",
  "llm.load.llama.ropeFrequencyBase/title": "Базовая частота RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Базовая частота для вращающихся позиционных эмбеддингов (RoPE). Увеличение этого значения может повысить производительность при большой длине контекста",
  "llm.load.llama.ropeFrequencyBase/info": "[Дополнительно] Настраивает базовую частоту для вращающихся позиционных эмбеддингов (RoPE), влияя на то, как встраивается позиционная информация",
  "llm.load.llama.ropeFrequencyScale/title": "Масштаб частоты RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Множитель длины контекста для расширения эффективного контекста с помощью RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Дополнительно] Изменяет масштабирование частоты вращающихся позиционных эмбеддингов (RoPE) для управления гранулярностью позиционного кодирования",
  "llm.load.llama.acceleration.offloadRatio/title": "Выгрузка на GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Число слоёв модели, вычисляемых на GPU для ускорения работы",
  "llm.load.llama.acceleration.offloadRatio/info": "Установите количество слоёв для выгрузки на GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Снижает потребление памяти и время генерации для некоторых моделей",
  "llm.load.llama.flashAttention/info": "Ускоряет механизмы внимания для более быстрой и эффективной обработки",
  "llm.load.numExperts/title": "Число экспертов",
  "llm.load.numExperts/subTitle": "Число экспертов, используемых в модели",
  "llm.load.numExperts/info": "Число экспертов, используемых в модели",
  "llm.load.llama.keepModelInMemory/title": "Держать модель в памяти",
  "llm.load.llama.keepModelInMemory/subTitle": "Резервировать системную память для модели, даже при выгрузке на GPU. Улучшает производительность, но требует больше оперативной памяти",
  "llm.load.llama.keepModelInMemory/info": "Не даёт выгружать модель на диск, обеспечивая более быстрый доступ за счёт увеличения потребления оперативной памяти",
  "llm.load.llama.useFp16ForKVCache/title": "Использовать FP16 для кэша KV",
  "llm.load.llama.useFp16ForKVCache/info": "Снижает использование памяти за счёт хранения кэша в половинной точности (FP16)",
  "llm.load.llama.tryMmap/title": "Использовать mmap()",
  "llm.load.llama.tryMmap/subTitle": "Ускоряет время загрузки модели. Отключение этой опции может повысить производительность, если размер модели превышает объем доступной оперативной памяти",
  "llm.load.llama.tryMmap/info": "Загружать файлы моделей непосредственно с диска в память",
  "llm.load.llama.cpuThreadPoolSize/title": "Размер пула потоков CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Число потоков CPU, выделяемых пулу потоков, используемому для вычислений модели",
  "llm.load.llama.cpuThreadPoolSize/info": "Число потоков CPU, выделяемых пулу потоков, используемому для вычислений модели. Увеличение количества потоков не всегда коррелирует с улучшением производительности. Значение по умолчанию: <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Тип квантования K-кэша",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Низкие значения уменьшают использование памяти, но могут снизить качество. Эффект существенно различается в зависимости от модели.",
  "llm.load.llama.vCacheQuantizationType/title": "Тип квантования V-кэша",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Низкие значения уменьшают использование памяти, но могут снизить качество. Эффект существенно различается в зависимости от модели.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ Необходимо отключить это значение, если Flash Attention не включён",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Можно включать только при включённой Flash Attention",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ Необходимо отключить Flash Attention при использовании F32",
  "llm.load.mlx.kvCacheBits/title": "Квантование KV-кэша",
  "llm.load.mlx.kvCacheBits/subTitle": "Число бит, до которых следует квантовать KV-кэш",
  "llm.load.mlx.kvCacheBits/info": "Число бит, до которых следует квантовать KV-кэш",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Настройка длины контекста игнорируется при использовании квантования KV-кэша",
  "llm.load.mlx.kvCacheGroupSize/title": "Квантование KV-кэша: Размер группы",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Размер группы при операции квантования для KV-кэша. Больший размер группы снижает потребление памяти, но может ухудшить качество.",
  "llm.load.mlx.kvCacheGroupSize/info": "Число бит, до которых следует квантовать KV-кэш",
  "llm.load.mlx.kvCacheQuantizationStart/title": "Квантование KV-кэша: Начать квантование, когда длина контекста превысит это значение",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Пороговое значение длины контекста для начала квантования KV-кэша",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Пороговое значение длины контекста для начала квантования KV-кэша",
  "llm.load.mlx.kvCacheQuantization/title": "Квантование KV-кэша",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Квантовать KV-кэш модели. Это может привести к более быстрой генерации и сниженному потреблению памяти,\nно за счёт качества выходных данных модели.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "Квантование KV-кэша в битах",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Число бит для квантования KV-кэша",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Биты",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Стратегия определения размера группы",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Точность",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Сбалансированный",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Быстрый",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Расширенные: Конфигурация размера группы матричного умножения (quantized 'matmul group size')\n\n• Точность = размер группы 32\n• Сбалансированный = размер группы 64\n• Быстрый = размер группы 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Начать квантование, когда длина контекста достигнет этого значения",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Когда длина контекста достигнет этого количества токенов,\nначать квантование KV-кэша",

  "embedding.load.contextLength/title": "Длина контекста",
  "embedding.load.contextLength/subTitle": "Максимальное количество токенов, которое модель может обрабатывать в одном запросе. Смотрите опции управления переполнением в разделе \"Параметры вывода\" для получения дополнительных способов управления этим параметром",
  "embedding.load.contextLength/info": "Определяет максимальное количество токенов, которое модель может учитывать одновременно, что влияет на объем контекста, который она сохраняет во время обработки",
  "embedding.load.llama.ropeFrequencyBase/title": "Базовая частота RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Базовая частота для вращающихся позиционных эмбеддингов (RoPE). Увеличение этого значения может повысить производительность при большой длине контекста",
  "embedding.load.llama.ropeFrequencyBase/info": "[Дополнительно] Настраивает базовую частоту для вращающихся позиционных эмбеддингов (RoPE), влияя на то, как встраивается позиционная информация",
  "embedding.load.llama.evalBatchSize/title": "Размер пакета оценки",
  "embedding.load.llama.evalBatchSize/subTitle": "Число входных токенов для обработки за один проход. Увеличение повышает производительность за счёт использования памяти",
  "embedding.load.llama.evalBatchSize/info": "Устанавливает количество токенов, обрабатываемых вместе в одном пакете во время оценки",
  "embedding.load.llama.ropeFrequencyScale/title": "Масштаб частоты RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Множитель длины контекста для расширения эффективного контекста с помощью RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Дополнительно] Изменяет масштабирование частоты вращающихся позиционных эмбеддингов (RoPE) для управления гранулярностью позиционного кодирования",
  "embedding.load.llama.acceleration.offloadRatio/title": "Выгрузка на GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Число слоёв модели, вычисляемых на GPU для ускорения работы",
  "embedding.load.llama.acceleration.offloadRatio/info": "Установите количество слоёв для выгрузки на GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Держать модель в памяти",
  "embedding.load.llama.keepModelInMemory/subTitle": "Резервировать системную память для модели, даже при выгрузке на GPU. Улучшает производительность, но требует больше оперативной памяти",
  "embedding.load.llama.keepModelInMemory/info": "Не даёт выгружать модель на диск, обеспечивая более быстрый доступ за счёт увеличения потребления оперативной памяти",
  "embedding.load.llama.tryMmap/title": "Использовать mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Ускоряет время загрузки модели. Отключение этой опции может повысить производительность, если размер модели превышает объем доступной оперативной памяти",
  "embedding.load.llama.tryMmap/info": "Загружать файлы моделей непосредственно с диска в память",
  "embedding.load.seed/title": "Сид",
  "embedding.load.seed/subTitle": "Сид для генератора случайных чисел, используемого при генерации текста. -1 означает случайный сид",

  "embedding.load.seed/info": "Случайный сид: Устанавливает сид для генератора случайных чисел, чтобы обеспечить воспроизводимость результатов",

  "presetTooltip": {
    "included/title": "Заданные значения",
    "included/description": "Следующие поля будут применены",
    "included/empty": "Ни одно из полей этой предустановки не применяется в данном контексте.",
    "included/conflict": "Вам будет предложено выбрать, применять ли это значение",
    "separateLoad/title": "Конфигурация времени загрузки",
    "separateLoad/description.1": "Предустановка также включает в себя следующую конфигурацию времени загрузки. Конфигурация времени загрузки является глобальной для модели и требует перезагрузки модели, чтобы изменения вступили в силу. Удерживайте",
    "separateLoad/description.2": "чтобы применить к",
    "separateLoad/description.3": ".",
    "excluded/title": "Может не применяться",
    "excluded/description": "Следующие поля включены в предустановку, но они не применяются в текущем контексте.",
    "legacy/title": "Устаревшая предустановка",
    "legacy/description": "Эта предустановка является устаревшей. Включает следующие поля, которые либо обрабатываются сейчас автоматически, либо больше не применимы.",
    "button/publish": "Опубликовать в Хаб",
    "button/pushUpdate": "Опубликовать изменения в Хабе",
    "button/noChangesToPush": "Нет изменений для публикации",
    "button/export": "Экспорт",
    "hubLabel": "Предустановка из Хаба от {{user}}",
    "ownHubLabel": "Ваша предустановка из Хаба"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Пусто>"
    },
    "checkboxNumeric": {
      "off": "ВЫКЛ"
    },
    "llamaCacheQuantizationType": {
      "off": "ВЫКЛ"
    },
    "mlxKvCacheBits": {
      "off": "ВЫКЛ"
    },
    "stringArray": {
      "empty": "<Пусто>"
    },
    "llmPromptTemplate": {
      "type": "Тип",
      "types.jinja/label": "Шаблон (Jinja)",
      "jinja.bosToken/label": "Токен BOS",
      "jinja.eosToken/label": "Токен EOS",
      "jinja.template/label": "Шаблон",
      "jinja/error": "Не удалось разобрать шаблон Jinja: {{error}}",
      "jinja/empty": "Пожалуйста, введите шаблон Jinja выше.",
      "jinja/unlikelyToWork": "Предоставленный вами шаблон Jinja, скорее всего, не будет работать, так как он не ссылается на переменную \"messages\". Пожалуйста, убедитесь, что вы правильно ввели шаблон.",
      "types.manual/label": "Ручной",
      "manual.subfield.beforeSystem/label": "Перед Системой",
      "manual.subfield.beforeSystem/placeholder": "Введите префикс Системы...",
      "manual.subfield.afterSystem/label": "После Системы",
      "manual.subfield.afterSystem/placeholder": "Введите суффикс Системы...",
      "manual.subfield.beforeUser/label": "Перед Пользователем",
      "manual.subfield.beforeUser/placeholder": "Введите префикс Пользователя...",
      "manual.subfield.afterUser/label": "После Пользователя",
      "manual.subfield.afterUser/placeholder": "Введите суффикс Пользователя...",
      "manual.subfield.beforeAssistant/label": "Перед Ассистентом",
      "manual.subfield.beforeAssistant/placeholder": "Введите префикс Ассистента...",
      "manual.subfield.afterAssistant/label": "После Ассистента",
      "manual.subfield.afterAssistant/placeholder": "Введите суффикс Ассистента...",
      "stopStrings/label": "Дополнительные стоп-строки",
      "stopStrings/subTitle": "Стоп-строки шаблона, которые будут использоваться в дополнение к стоп-строкам, указанным пользователем."
    },
    "contextLength": {
      "maxValueTooltip": "Это максимальное количество токенов, которое модель была обучена обрабатывать. Нажмите, чтобы установить контекст на это значение",
      "maxValueTextStart": "Модель поддерживает до",
      "maxValueTextEnd": "токенов",
      "tooltipHint": "Хотя модель может поддерживать определённое количество токенов, производительность может ухудшиться, если ресурсы вашего компьютера не справляются с нагрузкой - будьте осторожны при увеличении этого значения."
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Остановить при достижении лимита",
      "stopAtLimitSub": "Останавливать генерацию, когда память модели заполнена",
      "truncateMiddle": "Усечь посередине",
      "truncateMiddleSub": "Удаляет сообщения из середины разговора, чтобы освободить место для новых. Модель все равно будет помнить начало разговора",
      "rollingWindow": "Скользящее окно",
      "rollingWindowSub": "Модель всегда будет получать несколько последних сообщений, но может забыть начало разговора."
    },
    "llamaAccelerationOffloadRatio": {
      "max": "МАКС",
      "off": "ВЫКЛ"
    },
    "gpuSplitStrategy": {
      "evenly": "Равномерно",
      "favorMainGpu": "Предпочитать основной GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Прочитать, как это работает",
      "placeholder": "Выберите совместимую черновую модель",
      "noCompatible": "Не найдено ни одной совместимой черновой модели для выбранной вами основной модели",
      "stillLoading": "Определение совместимых черновых моделей...",
      "notCompatible": "Выбранная черновая модель (<draft/>) несовместима с текущей выбранной моделью (<current/>).",
      "off": "ВЫКЛ",
      "loadModelToSeeOptions": "Загрузить модель <keyboard-shortcut />, чтобы увидеть совместимые варианты.",
      "compatibleWithNumberOfModels": "Рекомендуется для не менее чем {{dynamicValue}} ваших моделей",
      "recommendedForSomeModels": "Рекомендуется для некоторых моделей",
      "recommendedForLlamaModels": "Рекомендуется для моделей Llama",
      "recommendedForQwenModels": "Рекомендуется для моделей Qwen",
      "onboardingModal": {
        "introducing": "Представляем",
        "speculativeDecoding": "Спекулятивное декодирование",
        "firstStepBody": "Ускорение инференса для моделей <custom-span>llama.cpp</custom-span> и <custom-span>MLX</custom-span>",
        "secondStepTitle": "Ускорение инференса с помощью спекулятивного декодирования",
        "secondStepBody": "Спекулятивное декодирование - это техника, основанная на взаимодействии двух моделей:\n - Большая \"основная\" модель\n - Меньшая \"черновая\" модель\n\nВо время генерации черновая модель быстро предлагает токены для проверки основной, более крупной модели. Проверка токенов - значительно более быстрый процесс, чем их фактическое генерирование, что и является источником прироста скорости. **Как правило, чем больше разница в размере между основной моделью и черновой моделью, тем выше ускорение.**\n\nДля поддержания качества основная модель принимает только токены, соответствующие тем, что она сгенерировала бы сама, обеспечивая качество ответа большей модели при более высокой скорости инференса. Обе модели должны использовать один и тот же словарь.",
        "draftModelRecommendationsTitle": "Рекомендации по черновой модели",
        "basedOnCurrentModels": "На основе ваших текущих моделей",
        "close": "Закрыть",
        "next": "Далее",
        "done": "Готово"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Сначала загрузите модель <model-badge />. ",
      "errorEngineNotSupported": "Спекулятивное декодирование требует как минимум версии {{minVersion}} движка {{engineName}}. Обновите движок (<key/>) и перезагрузите модель, чтобы использовать эту функцию.",
      "errorEngineNotSupported/noKey": "Спекулятивное декодирование требует как минимум версии {{minVersion}} движка {{engineName}}. Обновите движок и перезагрузите модель, чтобы использовать эту функцию."
    },
    "llmReasoningParsing": {
      "startString/label": "Начальная строка",
      "startString/placeholder": "Введите начальную строку...",
      "endString/label": "Конечная строка",
      "endString/placeholder": "Введите конечную строку..."
    }
  },
  "saveConflictResolution": {
    "title": "Выберите, какие значения включить в предустановку",
    "description": "Выберите из предложенного, какие значения сохранить",
    "instructions": "Нажмите на значение, чтобы включить его",
    "userValues": "Предыдущее значение",
    "presetValues": "Новое значение",
    "confirm": "Подтвердить",
    "cancel": "Отмена"
  },
  "applyConflictResolution": {
    "title": "Какие значения сохранить?",
    "description": "У вас есть незафиксированные изменения, которые перекрываются с входящей предустановкой",
    "instructions": "Нажмите на значение, чтобы сохранить его",
    "userValues": "Текущее значение",
    "presetValues": "Значение входящей предустановки",
    "confirm": "Подтвердить",
    "cancel": "Отмена"
  },
  "empty": "<Пусто>",
  "noModelSelected": "Ни одна модель не выбрана",
  "apiIdentifier.label": "Идентификатор API",
  "apiIdentifier.hint": "При желании укажите идентификатор для этой модели. Он будет использоваться в запросах к API. Оставьте пустым, чтобы использовать стандартный идентификатор.",
  "idleTTL.label": "Автоматическая выгрузка при бездействии (TTL)",
  "idleTTL.hint": "Если значение установлено, модель будет автоматически выгружена после бездействия в течение указанного времени.",
  "idleTTL.mins": "мин.",

  "presets": {
    "title": "Предустановка",
    "saveChanges": "Сохранить",
    "saveChanges/description": "Сохраните изменения в предустановке.",
    "saveChanges.manual": "Обнаружены новые поля. Вы сможете выбрать, какие изменения включить в предустановку.",
    "saveChanges.manual.hold.0": "Удерживайте",
    "saveChanges.manual.hold.1": "чтобы выбрать, какие изменения сохранить в предустановку.",
    "saveChanges.saveAll.hold.0": "Удерживайте",
    "saveChanges.saveAll.hold.1": "чтобы сохранить все изменения.",
    "saveChanges.saveInPreset.hold.0": "Удерживайте",
    "saveChanges.saveInPreset.hold.1": "для сохранения изменений только в тех полях, которые уже включены в предустановку.",
    "saveChanges/error": "Не удалось сохранить изменения в предустановке.",
    "saveChanges.manual/description": "Выберите, какие изменения включить в предустановку.",
    "saveAs": "Сохранить как новый...",
    "presetNamePlaceholder": "Введите имя для предустановки…",
    "cannotCommitChangesLegacy": "Это устаревшая предустановка и её нельзя изменять. Вы можете создать копию, используя \"Сохранить как новый…\".",
    "cannotSaveChangesNoChanges": "Нет изменений для сохранения.",
    "emptyNoUnsaved": "Выберите предустановку...",
    "emptyWithUnsaved": "Несохраненная предустановка",
    "saveEmptyWithUnsaved": "Сохранить предустановку как...",
    "saveConfirm": "Сохранить",
    "saveCancel": "Отмена",
    "saving": "Сохранение...",
    "save/error": "Не удалось сохранить предустановку.",
    "deselect": "Снять выделение с предустановки",
    "deselect/error": "Не удалось снять выделение с предустановки.",
    "select/error": "Не удалось выбрать предустановку.",
    "delete/error": "Не удалось удалить предустановку.",
    "discardChanges": "Отменить несохраненные изменения",
    "discardChanges/info": "Отбросить все несохраненные изменения и восстановить предустановку в исходное состояние",
    "newEmptyPreset": "+ Новая предустановка",
    "importPreset": "Импорт",
    "contextMenuCopyIdentifier": "Копировать идентификатор предустановки",
    "contextMenuSelect": "Применить предустановку",
    "contextMenuDelete": "Удалить...",
    "contextMenuShare": "Опубликовать...",
    "contextMenuOpenInHub": "Просмотр в Интернете",
    "contextMenuPullFromHub": "Получить последнюю версию",
    "contextMenuPushChanges": "Опубликовать изменения в Хабе",
    "contextMenuPushingChanges": "Публикация...",
    "contextMenuPushedChanges": "Изменения опубликованы",
    "contextMenuExport": "Экспорт файла",
    "contextMenuRevealInExplorer": "Показать в Проводнике",
    "contextMenuRevealInFinder": "Показать в Finder",
    "share": {
      "title": "Опубликовать предустановку",
      "action": "Поделитесь своей предустановкой с другими пользователями для её загрузки, оценки и создания форка",
      "presetOwnerLabel": "Владелец",
      "uploadAs": "Ваша предустановка будет создана как {{name}}",
      "presetNameLabel": "Название предустановки",
      "descriptionLabel": "Описание (необязательно)",
      "loading": "Публикация...",
      "success": "Предустановка успешно опубликована",
      "presetIsLive": "<preset-name /> теперь доступна на Хабе!",
      "close": "Закрыть",
      "confirmViewOnWeb": "Просмотр в Интернете",
      "confirmCopy": "Копировать URL",
      "confirmCopied": "Скопировано!",
      "pushedToHub": "Ваша предустановка была опубликована на Хабе.",
      "descriptionPlaceholder": "Введите описание...",
      "willBePublic": "Эта предустановка будет общедоступной. Любой пользователь в интернете сможет увидеть её.",
      "willBePrivate": "Только вы сможете видеть эту предустановку",
      "willBeOrgVisible": "Эта предустановка будет видна всем сотрудникам организации.",
      "publicSubtitle": "Ваша предустановка <custom-bold>общедоступна</custom-bold>. Другие пользователи смогут загружать и создавать её форк на lmstudio.ai",
      "privateUsageReached": "Превышен лимит частных предустановок.",
      "continueInBrowser": "Продолжить в браузере",
      "confirmShareButton": "Опубликовать",
      "error": "Не удалось опубликовать предустановку",
      "createFreeAccount": "Создайте бесплатную учётную запись в Хабе, чтобы публиковать предустановки"
    },
    "update": {
      "title": "Опубликовать изменения в Хабе",
      "title/success": "Предустановка успешно обновлена",
      "subtitle": "Внесите изменения в <custom-preset-name /> и отправьте их в Хаб",
      "descriptionLabel": "Описание",
      "descriptionPlaceholder": "Введите описание...",
      "loading": "Публикация...",
      "cancel": "Отмена",
      "createFreeAccount": "Создайте бесплатную учётную запись в Хабе, чтобы публиковать предустановки",
      "error": "Не удалось отправить обновление",
      "confirmUpdateButton": "Отправить"
    },
    "resolve": {
      "title": "Разрешить конфликты...",
      "tooltip": "Открыть модальное окно для разрешения конфликтов с версией в Хабе"
    },
    "loginToManage": {
      "title": "Войдите, чтобы управлять..."
    },
    "downloadFromHub": {
      "title": "Загрузить",
      "downloading": "Загрузка...",
      "success": "Загружено!",
      "error": "Не удалось загрузить"
    },
    "push": {
      "title": "Отправить изменения",
      "pushing": "Отправка...",
      "success": "Отправлено",
      "tooltip": "Отправьте локальные изменения в удалённую версию, размещённую на Хабе",
      "error": "Не удалось отправить"
    },
    "saveAsNewModal": {
      "title": "Ой! Не удалось найти предустановку на Хабе",
      "confirmSaveAsNewDescription": "Вы хотите опубликовать эту предустановку как новую?",
      "confirmButton": "Опубликовать как новую"
    },
    "pull": {
      "title": "Получить последнюю версию",
      "error": "Не удалось получить",
      "contextMenuErrorMessage": "Не удалось получить",
      "success": "Получено",
      "pulling": "Получение...",
      "upToDate": "Актуально!",
      "unsavedChangesModal": {
        "title": "У вас есть несохраненные изменения.",
        "bodyContent": "Получение данных из удалённого источника перезапишет ваши несохраненные изменения. Продолжить?",
        "confirmButton": "Перезаписать несохраненные изменения"
      }
    },
    "import": {
      "title": "Импортировать предустановку из файла",
      "dragPrompt": "Перетащите файлы предустановок (.tar.gz или preset.json) или <custom-link>выберите на компьютере</custom-link>",
      "remove": "Удалить",
      "cancel": "Отмена",
      "importPreset_zero": "Импорт предустановки",
      "importPreset_one": "Импорт предустановки",
      "importPreset_other": "Импорт предустановок: {{count}}",
      "selectDialog": {
        "title": "Выберите файл предустановки (preset.json или .tar.gz)",
        "button": "Импорт"
      },
      "error": "Не удалось импортировать предустановку",
      "resultsModal": {
        "titleSuccessSection_one": "Успешно импортировано предустановок: 1",
        "titleSuccessSection_other": "Успешно импортировано предустановок: {{count}}",
        "titleFailSection_zero": "",
        "titleFailSection_one": "(Не удалось: {{count}})",
        "titleFailSection_other": "(Не удалось: {{count}})",
        "titleAllFailed": "Не удалось импортировать предустановки",
        "importMore": "Импортировать ещё",
        "close": "Готово",
        "successBadge": "Успешно",
        "alreadyExistsBadge": "Предустановка уже существует",
        "errorBadge": "Ошибка",
        "invalidFileBadge": "Неверный файл",
        "otherErrorBadge": "Не удалось импортировать предустановку",
        "errorViewDetailsButton": "Просмотреть детали",
        "seeError": "Посмотреть ошибку",
        "noName": "Нет имени предустановки",
        "useInChat": "Использовать в чате"
      },
      "importFromUrl": {
        "button": "Импорт из URL...",
        "title": "Импорт из URL",
        "back": "Импорт из файла...",
        "action": "Вставьте URL Хаба LM Studio, содержащий предустановку, которую вы хотите импортировать, ниже.",
        "invalidUrl": "Неверный URL. Убедитесь, что вы вставляете правильный URL Хаба LM Studio.",
        "tip": "Вы можете установить предустановку непосредственно с помощью кнопки {{buttonName}} в LM Studio Hub",
        "confirm": "Импорт",
        "cancel": "Отмена",
        "loading": "Импортирование...",
        "error": "Не удалось загрузить предустановку."
      }
    },
    "download": {
      "title": "Загрузить <preset-name /> из LM Studio Hub",
      "subtitle": "Сохранить <custom-name /> в ваши предустановки. Это позволит вам использовать эту предустановку в приложении",
      "button": "Получить",
      "button/loading": "Получение...",
      "cancel": "Отмена",
      "error": "Не удалось загрузить предустановку."
    },
    "inclusiveness": {
      "speculativeDecoding": "Включить в предустановку"
    }
  },

  "flashAttentionWarning": "Flash Attention - экспериментальная функция, которая может вызывать проблемы с некоторыми моделями. При возникновении проблем попробуйте отключить её.",
  "llamaKvCacheQuantizationWarning": "Квантование KV-кэша - экспериментальная функция, которая может вызывать проблемы с некоторыми моделями. Для квантования KV-кэша необходимо включить Flash Attention. При возникновении проблем верните значение к значению по умолчанию \"F16\".",

  "seedUncheckedHint": "Случайный сид",
  "ropeFrequencyBaseUncheckedHint": "Автоматически",
  "ropeFrequencyScaleUncheckedHint": "Автоматически",

  "hardware": {
    "environmentVariables": "Переменные окружения",
    "environmentVariables.info": "Если вы не уверены в значении этих параметров, оставьте их значения по умолчанию.",
    "environmentVariables.reset": "Сбросить к значениям по умолчанию",

    "gpus.information": "Настройте графические процессоры (GPU), обнаруженные на вашем компьютере",
    "gpuSettings": {
      "editMaxCapacity": "Редактировать максимальную ёмкость",
      "hideEditMaxCapacity": "Скрыть редактирование максимальной ёмкости",
      "allOffWarning": "Все графические процессоры отключены или неактивны. Убедитесь, что выделено некоторое количество памяти GPU, для загрузки моделей.",
      "split": {
        "title": "Стратегия",
        "placeholder": "Выбрать распределение памяти GPU",
        "options": {
          "generalDescription": "Настройте порядок загрузки моделей на ваши GPU",
          "evenly": {
            "title": "Разделить поровну",
            "description": "Равномерно распределить память между GPU"
          },
          "priorityOrder": {
            "title": "Порядок приоритетов",
            "description": "Перетащите для изменения порядка приоритетов. Система постарается выделить больше памяти на GPU, указанных первыми."
          },
          "custom": {
            "title": "Пользовательский",
            "description": "Выделить память",
            "maxAllocation": "Максимальное выделение"
          }
        }
      },
      "deviceId.info": "Уникальный идентификатор этого устройства",
      "changesOnlyAffectNewlyLoadedModels": "Изменения будут применяться только к новым моделям",
      "toggleGpu": "Включить/Отключить GPU"
    }
  },

  "load.gpuSplitConfig/title": "Конфигурация разделения GPU",
  "envVars/title": "Установить переменную окружения",
  "envVars": {
    "select": {
      "placeholder": "Выбрать переменную окружения...",
      "noOptions": "Больше недоступно",
      "filter": {
        "placeholder": "Фильтр результатов поиска",
        "resultsFound_zero": "Результаты не найдены",
        "resultsFound_one": "Найдено результатов: 1",
        "resultsFound_other": "Найдено результатов: {{count}}"
      }
    },
    "inputValue": {
      "placeholder": "Введите значение"
    },
    "values": {
      "title": "Текущие значения"
    }
  }
}
