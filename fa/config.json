{
  "noInstanceSelected": "هیچ نمونه مدلی انتخاب نشده است",
  "resetToDefault": "بازنشانی",
  "showAdvancedSettings": "نمایش تنظیمات پیشرفته",
  "showAll": "همه",
  "basicSettings": "تنظیمات پایه",
  "configSubtitle": "بارگذاری یا ذخیره پیش فرض‌ها و آزمایش با تغییرات پارامترهای مدل",
  "inferenceParameters/title": "پارامترهای پیش‌بینی",
  "inferenceParameters/info": "پارامترهایی را که بر پیش‌بینی تأثیر می‌گذارند آزمایش کنید.",
  "generalParameters/title": "عمومی",
  "samplingParameters/title": "نمونه‌گیری",
  "basicTab": "پایه",
  "advancedTab": "پیشرفته",
  "advancedTab/title": "🧪 تنظیمات پیشرفته",
  "advancedTab/expandAll": "گسترش همه",
  "advancedTab/overridesTitle": "لغو تنظیمات",
  "advancedTab/noConfigsText": "شما هیچ تغییری ذخیره نشده‌ای ندارید - برای مشاهده لغوها مقادیر را ویرایش کنید.",
  "loadInstanceFirst": "برای مشاهده پارامترهای قابل تنظیم، یک مدل بارگیری کنید",
  "noListedConfigs": "هیچ پارامتر قابل تنظیمی وجود ندارد",
  "generationParameters/info": "با پارامترهای پایه که بر تولید متن تأثیر می‌گذارند آزمایش کنید.",
  "loadParameters/title": "پارامترهای بارگذاری",
  "loadParameters/description": "تنظیماتی برای کنترل بارگذاری مدل به حافظه.",
  "loadParameters/reload": "برای اعمال تغییرات بارگذاری مجدد کنید",
  "discardChanges": "لغو تغییرات",
  "loadModelToSeeOptions": "برای مشاهده گزینه‌ها مدلی را بارگذاری کنید",
  "llm.prediction.systemPrompt/title": "راهنمای سیستم",
  "llm.prediction.systemPrompt/description": "از این فیلد برای ارائه دستورالعمل‌های پس‌زمینه به مدل، نظیر یک مجموعه قوانین، محدودیت‌ها، یا الزامات عمومی استفاده کنید.",
  "llm.prediction.systemPrompt/subTitle": "راهنمایی برای AI",
  "llm.prediction.temperature/title": "دما",
  "llm.prediction.temperature/subTitle": "میزان تصادفی بودن. مقدار ۰ پاسخی یکسان در هر بار ارائه می‌دهد، در حالی که مقادیر بالاتر خلاقیت و تنوع را افزایش می‌دهد.",
  "llm.prediction.temperature/info": "از مستندات llama.cpp: «مقدار پیش‌فرض <{{dynamicValue}}> است، که تعادلی بین تصادفی بودن و قطعیت فراهم می‌کند. در حالت نهایی، دمای ۰ همیشه محتمل‌ترین توکن بعدی را انتخاب می‌کند و در هر بار خروجی‌هایی یکسان ایجاد می‌کند.»",
  "llm.prediction.llama.sampling/title": "نمونه‌گیری",
  "llm.prediction.topKSampling/title": "نمونه‌گیری Top K",
  "llm.prediction.topKSampling/subTitle": "توکن بعدی را به یکی از K توکن‌های محتمل‌تر محدود می‌کند. مشابه دما عمل می‌کند.",
  "llm.prediction.topKSampling/info": "از مستندات lama.cpp:\n\nTop-k نمونه‌گیری، یک روش تولید متن است که توکن بعدی را فقط از بین k توکن محتمل‌تر انتخاب می‌کند...\n\nاین روش کمک می‌کند تا خطر ایجاد توکن‌های کم احتمال یا بی‌معنی کاهش یابد، اما ممکن است به تنوع خروجی آسیب بزند.\n\nیک مقدار بالاتر برای top-k (مثلاً 100) تعداد بیشتری توکن را در نظر می‌گیرد و به متنی متنوع‌تر می‌انجامد، در حالی که یک مقدار پایین‌تر (مثلاً 10) تنها بر روی محتمل‌ترین توکن‌ها تمرکز می‌کند و متنی محافظه‌کارانه‌تر تولید می‌کند.\n\nمقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.prediction.llama.cpuThreads/title": "تعداد هسته‌های CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "تعداد هسته‌های CPU برای استفاده در زمان استنتاج",
  "llm.prediction.llama.cpuThreads/info": "تعداد هسته‌هایی که هنگام محاسبه استفاده می‌شوند. افزایش تعداد هسته‌ها همیشه بهبود عملکرد را تضمین نمی‌کند. مقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.prediction.maxPredictedTokens/title": "محدود کردن طول پاسخ",
  "llm.prediction.maxPredictedTokens/subTitle": "اختیاری: طول پاسخ AI را محدود کنید",
  "llm.prediction.maxPredictedTokens/info": "حداکثر طول پاسخ چت‌بات را کنترل کنید. برای تنظیم محدودیت طول روشن کنید، یا آن را خاموش کنید تا چت‌بات تصمیم بگیرد کی متوقف شود.",
  "llm.prediction.maxPredictedTokens/inputLabel": "حداکثر طول پاسخ (توکن‌ها)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "تقریباً {{maxWords}} کلمه",
  "llm.prediction.repeatPenalty/title": "تنبیه تکرار",
  "llm.prediction.repeatPenalty/subTitle": "میزان تشویق نکردن به تکرار یک توکن",
  "llm.prediction.repeatPenalty/info": "از مستندات llama.cpp: «این روش کمک می‌کند که مدل از تولید متن‌های تکراری یا یکنواخت خودداری کند.\n\nیک مقدار بالاتر (مثلاً 1.5) تکرار‌ها را با شدت بیشتری تنبیه می‌کند، در حالی که یک مقدار پایین‌تر (مثلاً 0.9) اندکی ملایم‌تر عمل می‌کند.»\n\nمقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.prediction.minPSampling/title": "نمونه‌گیری Min P",
  "llm.prediction.minPSampling/subTitle": "حداقل احتمال پایه برای اینکه یک توکن به‌عنوان خروجی انتخاب شود",
  "llm.prediction.minPSampling/info": "از مستندات llama.cpp:\n\nMin P تعیین حداقل احتمال یک توکن، نسبت به احتمال محتمل‌ترین توکن را تعیین می‌کند. این مقدار باید بین [0, 1] باشد.\n\nمقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.prediction.topPSampling/title": "نمونه‌گیری Top P",
  "llm.prediction.topPSampling/subTitle": "احتمال انباشت حداقلی برای توکن‌های ممکن بعدی. مشابه دما عمل می‌کند",
  "llm.prediction.topPSampling/info": "از مستندات llama.cpp:\n\nTop-p نمونه‌گیری، که به‌عنوان نمونه‌گیری هسته‌ای نیز شناخته می‌شود، یکی دیگر از روش‌های تولید متن است که توکن بعدی را از یک زیرمجموعه از توکن‌ها انتخاب می‌کند که به‌طور مشترک حداقل احتمال انباشت P را دارند.\n\nاین متد تعادلی بین تنوع و کیفیت ایجاد می‌کند...",
  "llm.prediction.stopStrings/title": "عبارت‌های توقف",
  "llm.prediction.stopStrings/subTitle": "عباراتی که باید باعث توقف تولید مدل شوند",
  "llm.prediction.stopStrings/info": "عبارات خاصی که وقتی شناسایی شوند مدل تولید توکن‌های بیشتر را متوقف می‌کند",
  "llm.prediction.stopStrings/placeholder": "یک عبارت وارد کنید و کلید ⏎ را فشار دهید",
  "llm.prediction.contextOverflowPolicy/title": "مدیریت سربار محتوایی",
  "llm.prediction.contextOverflowPolicy/subTitle": "وقتی مکالمه بیش از حد بزرگ شده و نمی‌تواند مدیریت شود، مدل باید چکار کند",
  "llm.prediction.contextOverflowPolicy/info": "تصمیم بگیرید که وقتی مکالمه سایز حافظه کاری مدل (زمینه) را بیشتر کرد، چه اتفاقی بیفتد",
  "llm.prediction.llama.frequencyPenalty/title": "تنبیه فرکانس",
  "llm.prediction.llama.presencePenalty/title": "تنبیه حضور",
  "llm.prediction.llama.tailFreeSampling/title": "نمونه‌گیری Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "نمونه‌گیری محلی",
  "llm.prediction.onnx.topKSampling/title": "نمونه‌گیری Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "توکن بعدی را به یکی از احتمال‌های K بیشتر محدود می‌کند. شبیه به دما رفتار می‌کند",
  "llm.prediction.onnx.topKSampling/info": "از مستندات ONNX:\n\n تعداد بیشتری توکن‌های محتم‌ترین انتخاب‌شده برای top-k فیلتر می‌شوند. به‌صورت پیش‌فرض غیرفعال است",
  "llm.prediction.onnx.repeatPenalty/title": "تنبیه تکرار",
  "llm.prediction.onnx.repeatPenalty/subTitle": "چقدر باید از تکرار همان توکن جلوگیری شود",
  "llm.prediction.onnx.repeatPenalty/info": "یک مقدار بالاتر مدل را بیشتر از تکرار جلوگیری می‌کند",
  "llm.prediction.onnx.topPSampling/title": "نمونه‌گیری Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "احتمال انباشت حداقلی برای توکن‌های ممکن بعدی. شبیه به دما عمل می‌کند",
  "llm.prediction.onnx.topPSampling/info": "از مستندات ONNX:\n\nفقط محتمل‌ترین توکن‌هایی که به احتمال انباشت P یا بیشتر از آن می‌رسند برای تولید نگه داشته می‌شوند",
  "llm.prediction.seed/title": "دانه تصادفی",
  "llm.prediction.structured/title": "خروجی ساختاریافته",
  "llm.prediction.structured/info": "خروجی ساختاریافته",
  "llm.prediction.structured/description": "پیشرفته: شما می‌توانید یک اسکیما JSON مشخص کنید تا فرمتی خاص را در خروجی مدل به کار بگیرد. برای اطلاعات بیشتر [مستندات](https://lmstudio.ai/docs/advanced/structured-output) را بخوانید.",
  "llm.prediction.promptTemplate/title": "قالب راهنما",
  "llm.prediction.promptTemplate/subTitle": "فرمتی که پیام‌ها در چت به مدل ارسال می‌شوند. تغییر این امر ممکن است رفتارهای غیرمنتظره ایجاد کند...",
  
  "llm.load.contextLength/title": "طول زمینه",
  "llm.load.contextLength/subTitle": "حداکثر تعداد توکنی که مدل می‌تواند در یک راهنما پردازش کند. برای روش‌های بیشتر به گزینه‌های \"نگهداری از متن\" مراجعه کنید.",
  "llm.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل می‌تواند به‌طور همزمان پردازش کند، که بر مقدار زمینه موردنظر در پردازش تأثیر می‌گذارد.",
  "llm.load.contextLength/warning": "تنظیم مقدار بالای طول زمینه می‌تواند بر مصرف حافظه تاثیر چشمگیری بگذارد",
  "llm.load.seed/title": "دانه تصادفی",
  "llm.load.seed/subTitle": "دانه برای مولد اعداد تصادفی که در تولید متن استفاده می‌شود. -1 به‌معنای تصادفی است.",
  "llm.load.seed/info": "دانه تصادفی: دانه‌ای برای مولد اعداد تصادفی تعیین می‌کند تا نتایج مشابهی به‌دست آورید.",
  
  "llm.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "llm.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی که به‌صورت همزمان پردازش می‌شوند. افزایش این مقدار عملکرد را بهبود می‌بخشد اما باعث مصرف حافظه بیشتر می‌شود",
  "llm.load.llama.evalBatchSize/info": "تعداد ورودی‌هایی را که در یک دسته پردازش می‌شوند تعیین می‌کند.",
  "llm.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "پایه فرکانس سفارشی برای جاسازی موقعیتی چرخشی (RoPE). افزایش این مقدار ممکن است توانایی عملکردی بیشتری هنگام طولانی‌شدن زمینه ایجاد کند",
  "llm.load.llama.ropeFrequencyBase/info": "[پیشرفته] پایه فرکانس‌ها برای جاسازی چرخشی تعدیل می‌شود، که بر نحوه جاسازی اطلاعات موقعیتی تأثیرگذار است.",
  "llm.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "طول زمینه به‌وسیله این فاکتور مقیاس‌بندی می‌شود تا زمینه مؤثر را با استفاده از RoPE گسترش دهد",
  "llm.load.llama.ropeFrequencyScale/info": "[پیشرفته] تنظیم مقیاس فرکانس برای جاسازی چرخشی برای کنترل دقت جاسازی موقعیتی",
  "llm.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل جداگانه که برای تسریع GPU محاسبه می‌شوند",
  "llm.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU منتقل شوند را تنظیم می‌کند.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "کاهش مصرف حافظه و زمان تولید در برخی مدل‌ها",
  "llm.load.llama.flashAttention/info": "مکانیسم‌های توجه را برای پردازش سریع‌تر و کارآمدتر تسریع می‌کند.",
  "llm.load.numExperts/title": "تعداد متخصصین",
  "llm.load.numExperts/subTitle": "تعداد متخصصین قابل استفاده در مدل",
  "llm.load.numExperts/info": "تعداد متخصصین قابل استفاده در مدل را تعیین می‌کند.",
  "llm.load.llama.keepModelInMemory/title": "نگه داشتن مدل در حافظه",
  "llm.load.llama.keepModelInMemory/subTitle": "ذخیره مدل در حافظه سیستم حتی هنگام انتقال به GPU. بهبود عملکرد اما نیاز به RAM بیشتری دارد.",
  "llm.load.llama.keepModelInMemory/info": "از انتقال مدل به حافظه دیسک جلوگیری می‌کند، و باعث دسترسی سریع‌تر می‌شود به هزینه استفاده بیشتر از RAM.",
  "llm.load.llama.useFp16ForKVCache/title": "استفاده از FP16 برای کیش KV",
  "llm.load.llama.useFp16ForKVCache/info": "مصرف حافظه را با ذخیره کردن کیش در نیمه‌دقت (FP16) کاهش می‌دهد.",
  "llm.load.llama.tryMmap/title": "استفاده از mmap()",
  "llm.load.llama.tryMmap/subTitle": "بهبود زمان بارگذاری برای مدل. غیرفعال‌کردن این گزینه ممکن است عملکرد را درصورتی که مدل بزرگ‌تر از RAM در دسترس سیستم باشد بهبود بخشد",
  "llm.load.llama.tryMmap/info": "بارگذاری فایل‌های مدل مستقیماً از دیسک به حافظه.",

  "embedding.load.contextLength/title": "طول زمینه",
  "embedding.load.contextLength/subTitle": "حداکثر تعداد توکنی که مدل می‌تواند در یک درخواست پردازش کند. گزینه‌های نگهداری متن را تحت \"پارامترهای استنتاج\" برای مدیریت آن مشاهده کنید.",
  "embedding.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل در یک واحد محاوره‌ای پردازش می‌کند را مشخص می‌کند، که بر چگونگی نگهداشت اطلاعات زمینه‌ای تأثیر می‌گذارد.",
  "embedding.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "پایه فرکانس سفارشی برای جاسازی موقعیتی چرخشی (RoPE). افزایش این مقدار ممکن است توانایی عملکرد را بهبود بخشد.",
  "embedding.load.llama.ropeFrequencyBase/info": "[پیشرفته] تعدیل پایه فرکانس‌ها برای جاسازی روی چرخشی، تأثیر اطلاعات موقعیتی را تعیین می‌کند.",
  "embedding.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "embedding.load.llama.evalBatchSize/subTitle": "تعداد توکن‌هایی که به‌صورت همزمان پردازش می‌شوند. افزایش این مورد عملکرد را بهبود می‌بخشد اما هزینه حافظه بیشتر است.",
  "embedding.load.llama.evalBatchSize/info": "تعداد توکن‌های پردازش‌شده در یک دسته را تعیین می‌کند.",
  "embedding.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "طول زمینه به‌وسیله این فاکتور مقیاس‌بندی می‌شود تا زمینه مؤثر را با RoPE گسترش می‌دهد",
  "embedding.load.llama.ropeFrequencyScale/info": "[پیشرفته] تنظیم مقیاس فرکانس برای جاسازی روی چرخشی برای سفارشی‌سازی دقت اطلاعات موقعیتی.",
  "embedding.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "تعداد مدل لایه‌های جداگانه که به GPU برای تسریع هدایت می‌شوند.",
  "embedding.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU منتقل شوند را تنظیم می‌کند.",
  "embedding.load.llama.keepModelInMemory/title": "نگهداشت مدل در حافظه",
  "embedding.load.llama.keepModelInMemory/subTitle": "مدل را در حافظه سیستم نگه دارید، حتی هنگام انتقال به GPU. منجر به بهبود عملکرد می‌شود اما نیاز به RAM بیشتری دارد.",
  "embedding.load.llama.keepModelInMemory/info": "از انتقال مدل به حافظه دیسک جلوگیری می‌کند، و باعث دسترسی سریع‌تر می‌شود به هزینه استفاده بیشتر از RAM.",
  "embedding.load.llama.tryMmap/title": "استفاده از mmap()",
  "embedding.load.llama.tryMmap/subTitle": "بهبود زمان بارگذاری مدل. غیرفعال‌کردن این گزینه ممکن است هنگام بزرگتر بودن مدل بیش از RAM موجود سیستم، عملکرد را بهبود ببخشد.",
  "embedding.load.llama.tryMmap/info": "بارگذاری فایل‌های مدل مستقیماً از دیسک به حافظه.",
  "embedding.load.seed/title": "دانه تصادفی",
  "embedding.load.seed/subTitle": "دانه برای مولد اعداد تصادفی که در تولید متن استفاده می‌شود. -1 به‌معنای تصادفی است.",
  "embedding.load.seed/info": "دانه تصادفی: دانه‌ای برای مولد اعداد تصادفی تنظیم می‌کند تا نتایج مشابهی به‌دست آید.",

  "presetTooltip": {
    "included/title": "مقادیر پیش‌فرض",
    "included/description": "فیلدهای زیر اعمال خواهند شد",
    "included/empty": "هیچ فیلدی از این پیش‌فرض در این زمینه اعمال نمی‌شود.",
    "included/conflict": "از شما خواسته خواهد شد که انتخاب کنید آیا این مقدار اعمال شود یا خیر",
    "separateLoad/title": "پیکربندی زمان بارگذاری",
    "separateLoad/description.1": "پیش‌فرض همچنین شامل پیکربندی‌های زمان بارگذاری زیر است. پیکربندی زمان بارگذاری مدلی سراسری است و برای تأثیرگذاری نیاز به باز‌بارگذاری مدل دارد. نگه دارید",
    "separateLoad/description.2": "تا اعمال شود برای",
    "separateLoad/description.3": ".",
    "excluded/title": "ممکن است اعمال نشود",
    "excluded/description": "فیلدهای زیر در پیش‌فرض گنجانده شده‌اند اما در زمینه کنونی اعمال نمی‌شوند.",
    "legacy/title": "پیش‌فرض قدیمی",
    "legacy/description": "این یک پیش‌فرض قدیمی است. شامل فیلدهای زیر می‌باشد که یا اکنون خودکار مدیریت می‌شوند یا دیگر قابل اعمال نیستند."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<خالی>"
    },
    "checkboxNumeric": {
      "off": "خاموش"
    },
    "stringArray": {
      "empty": "<خالی>"
    },
    "llmPromptTemplate": {
      "type": "نوع",
      "types.jinja/label": "الگو (Jinja)",
      "jinja.bosToken/label": "توکن BOS",
      "jinja.eosToken/label": "توکن EOS",
      "jinja.template/label": "الگو",
      "jinja/error": "خطا در تحلیل الگوی Jinja: {{error}}",
      "jinja/empty": "لطفاً یک الگوی Jinja وارد کنید.",
      "jinja/unlikelyToWork": "الگوی Jinja ارائه شده احتمالاً کار نخواهد کرد، چون به متغیر \"messages\" اشاره‌ای ندارد. لطفاً دوباره بررسی کنید که الگوی درست وارد کرده‌اید یا خیر.",
      "types.manual/label": "دستی",
      "manual.subfield.beforeSystem/label": "قبل از سیستم",
      "manual.subfield.beforeSystem/placeholder": "پیشوند سیستم را وارد کنید...",
      "manual.subfield.afterSystem/label": "بعد از سیستم",
      "manual.subfield.afterSystem/placeholder": "پسوند سیستم را وارد کنید...",
      "manual.subfield.beforeUser/label": "قبل از کاربر",
      "manual.subfield.beforeUser/placeholder": "پیشوند کاربر را وارد کنید...",
      "manual.subfield.afterUser/label": "بعد از کاربر",
      "manual.subfield.afterUser/placeholder": "پسوند کاربر را وارد کنید...",
      "manual.subfield.beforeAssistant/label": "قبل از دستیار",
      "manual.subfield.beforeAssistant/placeholder": "پیشوند دستیار را وارد کنید...",
      "manual.subfield.afterAssistant/label": "بعد از دستیار",
      "manual.subfield.afterAssistant/placeholder": "پسوند دستیار را وارد کنید...",
      "stopStrings/label": "عبارت‌های توقف اضافی",
      "stopStrings/subTitle": "عبارت‌های توقف مشخصی که به‌طور خاص با توجه به الگو در کنار دیگر عبارت‌های توقف کاربر استفاده می‌شوند."
    },
    "contextLength": {
      "maxValueTooltip": "این حداکثر تعداد توکنی است که مدل هنگام تمرین می‌تواند مدیریت کند. کلیک کنید تا زمینه به این مقدار تنظیم شود.",
      "maxValueTextStart": "مدل تا",
      "maxValueTextEnd": "توکن پشتیبانی می‌کند",
      "tooltipHint": "در حالی که ممکن است مدل تا حد مشخصی از تعداد توکن‌ها را پشتیبانی کند، عملکرد ممکن است در صورت ناتوانی منابع دستگاه شما کاهش یابد - با احتیاط از این مقدار استفاده کنید."
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "توقف در محدودیت",
      "stopAtLimitSub": "تولید را زمانی که حافظه مدل پر می‌شود متوقف کنید.",
      "truncateMiddle": "حذف از وسط",
      "truncateMiddleSub": "پیام‌های میان مکالمه را حذف می‌کند تا برای پیام‌های جدید فضای کافی ایجاد شود. مدل همچنان ابتدای مکالمه را به یاد می‌آورد.",
      "rollingWindow": "پنجره چرخشی",
      "rollingWindowSub": "مدل همیشه چند پیام اخیر را دریافت می‌کند اما ممکن است ابتدا مکالمه را فراموش کند."
    },
    "llamaAccelerationOffloadRatio": {
      "max": "حداکثر",
      "off": "خاموش"
    }
  },
  "saveConflictResolution": {
    "title": "انتخاب کنید که کدام مقادیر در پیش‌فرض گنجانده شوند",
    "description": "مقادیر مورد نظر برای حفظ را انتخاب کنید.",
    "instructions": "برای حفظ مقداری کلیک کنید",
    "userValues": "مقدار قبلی",
    "presetValues": "مقدار جدید",
    "confirm": "تأیید",
    "cancel": "لغو"
  },
  "applyConflictResolution": {
    "title": "کدام مقادیر را حفظ کنیم؟",
    "description": "شما تغییرات نا‌انجام‌شده‌ای دارید که با پیش‌فرض ورودی تداخل دارد.",
    "instructions": "برای حفظ یک مقدار کلیک کنید",
    "userValues": "مقدار فعلی",
    "presetValues": "مقدار پیش‌فرض ورودی",
    "confirm": "تأیید",
    "cancel": "لغو"
  },
  "empty": "<خالی>",
  "presets": {
    "title": "پیش‌فرض‌ها",
    "commitChanges": "تأیید تغییرات",
    "commitChanges/description": "تغییرات را به پیش‌فرض متعهد شوید.",
    "commitChanges.manual": "فیلدهای جدید شناسایی شدند. شما می‌توانید تغییرات را انتخاب کنید که کدام را در پیش‌فرض قرار دهید.",
    "commitChanges.manual.hold.0": "نگه دارید",
    "commitChanges.manual.hold.1": "تا تغییراتی که متعهد به پیش‌فرض می‌خواهید شوند را انتخاب کنید.",
    "commitChanges.saveAll.hold.0": "نگه دارید",
    "commitChanges.saveAll.hold.1": "تا همه تغییرات ذخیره شوند.",
    "commitChanges.saveInPreset.hold.0": "نگه دارید",
    "commitChanges.saveInPreset.hold.1": "تا فقط تغییراتی که در فیلدهای موجود پیش‌فرض هستند ذخیره شوند.",
    "commitChanges/error": "تأیید تغییرات به پیش‌فرض ناموفق بود.",
    "commitChanges.manual/description": "انتخاب کنید که کدام تغییرات را در پیش‌فرض گنجانده شوند.",
    "saveAs": "ذخیره به عنوان جدید...",
    "presetNamePlaceholder": "نامی برای پیش‌فرض وارد کنید...",
    "cannotCommitChangesLegacy": "این پیش‌فرض قدیمی است و نمی‌توان آن را تغییر داد. شما می‌توانید با استفاده از \"ذخیره به عنوان جدید...\" یک کپی ایجاد کنید.",
    "cannotCommitChangesNoChanges": "هیچ تغییری برای تأیید وجود ندارد.",
    "emptyNoUnsaved": "پیش‌فرضی را انتخاب کنید...",
    "emptyWithUnsaved": "پیش‌فرض ذخیره‌نشده",
    "saveEmptyWithUnsaved": "ذخیره پیش‌فرض به عنوان...",
    "saveConfirm": "ذخیره",
    "saveCancel": "لغو",
    "saving": "در حال ذخیره...",
    "save/error": "ذخیره پیش‌فرض ناموفق بود.",
    "deselect": "لغو انتخاب پیش‌فرض",
    "deselect/error": "لغو انتخاب پیش‌فرض ناموفق بود.",
    "select/error": "انتخاب پیش‌فرض ناموفق بود.",
    "delete/error": "حذف پیش‌فرض ناموفق بود.",
    "discardChanges": "لغو تغییرات ذخیره‌نشده",
    "discardChanges/info": "همه تغییرات نا‌انجام‌شده را لغو و پیش‌فرض را به حالت اولیه آن بازگردانید",
    "newEmptyPreset": "ایجاد پیش‌فرض خالی جدید...",
    "contextMenuSelect": "انتخاب پیش‌فرض",
    "contextMenuDelete": "حذف"
  },

  "flashAttentionWarning": "Flash Attention یک ویژگی تجربی است و ممکن است مشکلاتی با برخی مدل‌ها ایجاد کند. در صورت مواجهه با مشکل، سعی کنید آن را غیرفعال کنید.",

  "seedUncheckedHint": "دانه تصادفی",
  "ropeFrequencyBaseUncheckedHint": "به طور خودکار",
  "ropeFrequencyScaleUncheckedHint": "به طور خودکار"
}