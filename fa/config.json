{
  "noInstanceSelected": "هیچ نمونه‌ای از مدل انتخاب نشده است",
  "resetToDefault": "بازنشانی",
  "showAdvancedSettings": "نمایش تنظیمات پیشرفته",
  "showAll": "همه",
  "basicSettings": "پایه‌ای",
  "configSubtitle": "بارگذاری یا ذخیره تنظیمات و آزمایش تغییر پارامترهای مدل",
  "inferenceParameters/title": "پارامترهای پیش‌بینی",
  "inferenceParameters/info": "با پارامترهایی که بر پیش‌بینی تأثیر می‌گذارند، آزمایش کنید.",
  "generalParameters/title": "عمومی",
  "samplingParameters/title": "نمونه‌گیری",
  "basicTab": "پایه‌ای",
  "advancedTab": "پیشرفته",
  "advancedTab/title": "🧪 تنظیمات پیشرفته",
  "advancedTab/expandAll": "باز کردن همه",
  "advancedTab/overridesTitle": "جایگزینی تنظیمات",
  "advancedTab/noConfigsText": "هیچ تغییری ذخیره نشده است - برای مشاهده تغییرات، مقادیر بالا را ویرایش کنید.",
  "loadInstanceFirst": "برای مشاهده پارامترهای قابل تنظیم، یک مدل بارگذاری کنید",
  "noListedConfigs": "هیچ پارامتر قابل تنظیمی وجود ندارد",
  "generationParameters/info": "با پارامترهای پایه‌ای که بر تولید متن تأثیر می‌گذارند، آزمایش کنید.",
  "loadParameters/title": "بارگذاری پارامترها",
  "loadParameters/description": "تنظیماتی برای کنترل نحوه‌ی راه‌اندازی و بارگذاری مدل در حافظه.",
  "loadParameters/reload": "بارگذاری مجدد برای اعمال تغییرات",
  "loadParameters/reload/error": "بارگذاری مجدد مدل ناموفق بود",
  "discardChanges": "لغو تغییرات",
  "loadModelToSeeOptions": "برای مشاهده گزینه‌ها، یک مدل بارگذاری کنید",
  "schematicsError.title": "طرح تنظیمات دارای خطا در فیلدهای زیر است:",
  "manifestSections": {
    "structuredOutput/title": "خروجی ساختاریافته",
    "speculativeDecoding/title": "رمزگشایی احتمالی",
    "sampling/title": "نمونه‌گیری",
    "settings/title": "تنظیمات",
    "toolUse/title": "استفاده از ابزار",
    "promptTemplate/title": "الگوی ورودی"
  },

  "llm.prediction.systemPrompt/title": "پیام سیستمی",
  "llm.prediction.systemPrompt/description": "از این قسمت برای ارائه دستورالعمل‌های زمینه‌ای به مدل استفاده کنید، مانند مجموعه‌ای از قوانین، محدودیت‌ها یا الزامات کلی.",
  "llm.prediction.systemPrompt/subTitle": "راهنمایی‌های مربوط به هوش مصنوعی",

  "llm.prediction.temperature/title": "دما",
  "llm.prediction.temperature/subTitle": "میزان تصادفی بودن در خروجی. مقدار ۰ همیشه نتیجه یکسانی تولید می‌کند، در حالی که مقادیر بالاتر خلاقیت و تنوع را افزایش می‌دهند.",
  "llm.prediction.temperature/info": "از مستندات llama.cpp: «مقدار پیش‌فرض <{{dynamicValue}}> است که تعادل بین تصادفی بودن و تعیین‌پذیری را فراهم می‌کند. در حالت افراطی، دمای ۰ همیشه محتمل‌ترین توکن بعدی را انتخاب می‌کند، که منجر به خروجی‌های یکسان در هر اجرا می‌شود.»",

  "llm.prediction.llama.sampling/title": "نمونه‌گیری",

  "llm.prediction.topKSampling/title": "نمونه‌گیری Top-K",
  "llm.prediction.topKSampling/subTitle": "محدود کردن انتخاب توکن بعدی به k توکن محتمل برتر. عملکردی مشابه دما دارد.",
  "llm.prediction.topKSampling/info": "از مستندات llama.cpp:\n\nنمونه‌گیری Top-K روشی برای تولید متن است که در آن توکن بعدی فقط از بین k توکن محتمل برتر انتخاب می‌شود.\n\nاین روش احتمال تولید توکن‌های کم‌احتمال یا غیرمنطقی را کاهش می‌دهد اما ممکن است تنوع خروجی را نیز محدود کند.\n\nمقدار بالاتر برای k (مثلاً ۱۰۰) باعث افزایش تنوع متن می‌شود، در حالی که مقدار پایین‌تر (مثلاً ۱۰) خروجی را محافظه‌کارانه‌تر می‌کند.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است.",

  "llm.prediction.llama.cpuThreads/title": "تعداد رشته‌های پردازنده (CPU Threads)",
  "llm.prediction.llama.cpuThreads/subTitle": "تعداد رشته‌های پردازنده برای استفاده در حین استنتاج",
  "llm.prediction.llama.cpuThreads/info": "تعداد رشته‌هایی که در طول پردازش استفاده می‌شوند. افزایش تعداد رشته‌ها همیشه به معنی عملکرد بهتر نیست. مقدار پیش‌فرض <{{dynamicValue}}> است.",

  "llm.prediction.maxPredictedTokens/title": "محدودیت طول پاسخ",
  "llm.prediction.maxPredictedTokens/subTitle": "امکان محدود کردن طول پاسخ مدل",
  "llm.prediction.maxPredictedTokens/info": "حداکثر طول پاسخ چت‌بات را کنترل کنید. این گزینه را فعال کنید تا طول پاسخ محدود شود، یا غیرفعال کنید تا مدل خودش تصمیم بگیرد چه زمانی متوقف شود.",
  "llm.prediction.maxPredictedTokens/inputLabel": "حداکثر طول پاسخ (بر حسب توکن)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "حدود {{maxWords}} کلمه",

  "llm.prediction.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.repeatPenalty/subTitle": "میزان کاهش احتمال تکرار یک توکن",
  "llm.prediction.repeatPenalty/info": "از مستندات llama.cpp:\n\n«به جلوگیری از تولید متن تکراری یا یکنواخت کمک می‌کند.\n\nمقدار بالاتر (مثلاً ۱.۵) تکرارها را بیشتر جریمه می‌کند، در حالی که مقدار پایین‌تر (مثلاً ۰.۹) ملایم‌تر است.»\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است.",

  "llm.prediction.minPSampling/title": "نمونه‌گیری Min P",
  "llm.prediction.minPSampling/subTitle": "حداقل احتمال پایه برای انتخاب یک توکن",
  "llm.prediction.minPSampling/info": "از مستندات llama.cpp:\n\nحداقل احتمالی که یک توکن باید داشته باشد تا در خروجی در نظر گرفته شود، نسبت به محتمل‌ترین توکن. مقدار باید در بازه [۰, ۱] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است.",

  "llm.prediction.topPSampling/title": "نمونه‌گیری Top-P",
  "llm.prediction.topPSampling/subTitle": "حداقل احتمال انباشته برای انتخاب توکن‌های بعدی. عملکردی مشابه دما دارد.",
  "llm.prediction.topPSampling/info": "از مستندات llama.cpp:\n\nنمونه‌گیری Top-P که به عنوان نمونه‌گیری هسته‌ای نیز شناخته می‌شود، یک روش تولید متن است که توکن بعدی را از بین مجموعه‌ای از توکن‌ها انتخاب می‌کند که مجموع احتمال آن‌ها حداقل p است.\n\nاین روش تعادل بین تنوع و کیفیت را برقرار می‌کند.\n\nمقدار بالاتر (مثلاً ۰.۹۵) باعث تولید متن متنوع‌تر و مقدار پایین‌تر (مثلاً ۰.۵) باعث تولید متن محافظه‌کارانه‌تر می‌شود.\n\nمقدار باید در بازه (۰, ۱] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است.",

  "llm.prediction.stopStrings/title": "عبارات توقف",
  "llm.prediction.stopStrings/subTitle": "عباراتی که باعث توقف تولید متن توسط مدل می‌شوند",
  "llm.prediction.stopStrings/info": "عبارات خاصی که در صورت مواجهه با آن‌ها، مدل از ادامه تولید متن بازمی‌ایستد.",
  "llm.prediction.stopStrings/placeholder": "یک عبارت وارد کنید و ⏎ را فشار دهید.",

  "llm.prediction.contextOverflowPolicy/title": "مدیریت سرریز حافظه",
  "llm.prediction.contextOverflowPolicy/subTitle": "نحوه رفتار مدل در صورت بیش از حد بزرگ شدن مکالمه",
  "llm.prediction.contextOverflowPolicy/info": "تعیین کنید که وقتی حجم مکالمه از حافظه کاری مدل (context) فراتر رفت، چه اقدامی انجام شود.",

  "llm.prediction.seed/title": "مقدار اولیه (Seed)",

  "llm.prediction.structured/title": "خروجی ساختاریافته",
  "llm.prediction.structured/info": "خروجی ساختاریافته",
  "llm.prediction.structured/description": "پیشرفته: می‌توانید یک [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) ارائه دهید تا مدل را مجبور به پیروی از یک قالب خروجی خاص کنید. برای اطلاعات بیشتر، [مستندات](https://lmstudio.ai/docs/advanced/structured-output) را بخوانید.",

  "llm.prediction.tools/title": "استفاده از ابزار",
  "llm.prediction.tools/description": "پیشرفته: می‌توانید یک لیست JSON از ابزارهایی ارائه دهید که مدل بتواند از آن‌ها درخواست کند. برای اطلاعات بیشتر، [مستندات](https://lmstudio.ai/docs/advanced/tool-use) را مطالعه کنید.",
  "llm.prediction.tools/serverPageDescriptionAddon": "در هنگام استفاده از API سرور، این مقدار را به عنوان `tools` در بدنه درخواست ارسال کنید.",

  "llm.prediction.promptTemplate/title": "قالب درخواست",
  "llm.prediction.promptTemplate/subTitle": "فرمت ارسال پیام‌ها به مدل. تغییر این مقدار ممکن است باعث رفتارهای غیرمنتظره شود، بنابراین قبل از تغییر، اطمینان حاصل کنید که می‌دانید چه کاری انجام می‌دهید.",

  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "تعداد توکن‌های پیشنویس",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "تعداد توکن‌هایی که باید با مدل پیشنویس در ازای هر توکن مدل اصلی تولید شوند. تعادل بین محاسبات و پاداش را پیدا کنید.",

  "llm.prediction.reasoning.parsing/title": "تجزیه و تحلیل استدلال",
  "llm.prediction.reasoning.parsing/subTitle": "نحوه پردازش بخش‌های استدلال در خروجی مدل",

  "llm.prediction.llama.frequencyPenalty/title": "جریمه فرکانس",
  "llm.prediction.llama.presencePenalty/title": "جریمه حضور",
  "llm.prediction.llama.tailFreeSampling/title": "نمونه‌برداری بدون دنباله",
  "llm.prediction.llama.locallyTypicalSampling/title": "نمونه‌برداری معمولی محلی",
  "llm.prediction.llama.xtcProbability/title": "احتمال نمونه‌برداری XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "نمونه‌بردار XTC (حذف انتخاب‌های برتر) تنها با این احتمال در هر توکن تولیدی فعال می‌شود. نمونه‌برداری XTC می‌تواند خلاقیت را افزایش داده و کلیشه‌ها را کاهش دهد",
  "llm.prediction.llama.xtcProbability/info": "نمونه‌برداری XTC (حذف انتخاب‌های برتر) تنها با این احتمال در هر توکن تولیدی فعال می‌شود. این نمونه‌برداری معمولاً خلاقیت را افزایش داده و کلیشه‌ها را کاهش می‌دهد",
  "llm.prediction.llama.xtcThreshold/title": "آستانه نمونه‌برداری XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "آستانه XTC (حذف انتخاب‌های برتر). با احتمال `xtc-probability`، جستجو برای توکن‌هایی با احتمال‌های بین `xtc-threshold` و 0.5 انجام می‌شود و تمامی این توکن‌ها به جز کم‌ترین احتمالی‌ها حذف می‌شوند",
  "llm.prediction.llama.xtcThreshold/info": "آستانه XTC (حذف انتخاب‌های برتر). با احتمال `xtc-probability`، جستجو برای توکن‌هایی با احتمال‌های بین `xtc-threshold` و 0.5 انجام می‌شود و تمامی این توکن‌ها به جز کم‌ترین احتمالی‌ها حذف می‌شوند",
  "llm.prediction.mlx.topKSampling/title": "نمونه‌برداری برتر K",
  "llm.prediction.mlx.topKSampling/subTitle": "محدود کردن توکن بعدی به یکی از توکن‌های با بیشترین احتمال برتر K. مشابه دما عمل می‌کند",
  "llm.prediction.mlx.topKSampling/info": "محدود کردن توکن بعدی به یکی از توکن‌های با بیشترین احتمال برتر K. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topKSampling/title": "نمونه‌برداری برتر K",
  "llm.prediction.onnx.topKSampling/subTitle": "محدود کردن توکن بعدی به یکی از توکن‌های با بیشترین احتمال برتر K. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topKSampling/info": "از مستندات ONNX:\n\nتعداد توکن‌های با بالاترین احتمال لغت‌نامه که برای فیلتر کردن برتر K نگه‌داری می‌شوند\n\n• این فیلتر به‌طور پیش‌فرض غیرفعال است",
  "llm.prediction.onnx.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.onnx.repeatPenalty/subTitle": "میزان جلوگیری از تکرار همان توکن",
  "llm.prediction.onnx.repeatPenalty/info": "مقدار بالاتر، مدل را از تکرار خود جلوگیری می‌کند",
  "llm.prediction.onnx.topPSampling/title": "نمونه‌برداری برتر P",
  "llm.prediction.onnx.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های ممکن بعدی. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topPSampling/info": "از مستندات ONNX:\n\nفقط توکن‌های با بالاترین احتمال که احتمال‌هایشان جمعاً به مقدار TopP یا بیشتر برسد، برای تولید نگه‌داری می‌شوند\n\n• این فیلتر به‌طور پیش‌فرض غیرفعال است",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "حد آستانه احتمال طراحی",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "ادامه طراحی تا زمانی که احتمال یک توکن زیر این آستانه برود. مقادیر بالاتر معمولاً به معنی ریسک کمتر و پاداش کمتر است",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "حداقل اندازه پیش‌نویس برای در نظر گرفتن",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "پیش‌نویس‌هایی که کوچکتر از این مقدار باشند توسط مدل اصلی نادیده گرفته می‌شوند. مقادیر بالاتر معمولاً به معنی ریسک کمتر و پاداش کمتر است",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "حداکثر اندازه پیش‌نویس",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "حداکثر تعداد توکن‌های مجاز در پیش‌نویس. سقف اگر تمام احتمالات توکن‌ها بیشتر از آستانه برود. مقادیر پایین‌تر معمولاً به معنی ریسک کمتر و پاداش کمتر است",
  "llm.prediction.speculativeDecoding.draftModel/title": "مدل پیش‌نویس",

  "llm.load.contextLength/title": "طول متن",
  "llm.load.contextLength/subTitle": "حداکثر تعداد توکن‌هایی که مدل می‌تواند در یک درخواست به آن‌ها توجه کند. برای روش‌های بیشتری برای مدیریت این مورد، گزینه‌های \"پراکندگی مکالمه\" را در \"پارامترهای استنباط\" مشاهده کنید",
  "llm.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل می‌تواند یک‌باره در نظر بگیرد را مشخص می‌کند که بر میزان زمینه‌ای که در طول پردازش حفظ می‌شود تأثیر می‌گذارد",
  "llm.load.contextLength/warning": "تنظیم مقدار بالا برای طول متن می‌تواند تأثیر قابل توجهی بر استفاده از حافظه داشته باشد",
  "llm.load.seed/title": "بذر",
  "llm.load.seed/subTitle": "بذر برای تولید عدد تصادفی استفاده‌شده در تولید متن. مقدار -1 به معنی تصادفی بودن است",
  "llm.load.seed/info": "بذر تصادفی: بذر برای تولید اعداد تصادفی را تنظیم می‌کند تا نتایج قابل بازتولید را تضمین کند",

  "llm.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "llm.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی که به صورت همزمان پردازش می‌شوند. افزایش این مقدار باعث افزایش عملکرد با هزینه مصرف حافظه می‌شود",
  "llm.load.llama.evalBatchSize/info": "تعداد مثال‌هایی که به صورت همزمان در یک دسته در حین ارزیابی پردازش می‌شوند، که بر سرعت و مصرف حافظه تأثیر می‌گذارد",
  "llm.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "پایه فرکانس سفارشی برای جاسازی موقعیتی چرخشی (RoPE). افزایش این مقدار می‌تواند عملکرد بهتری در طول‌های زمینه بلندتر فراهم کند",
  "llm.load.llama.ropeFrequencyBase/info": "[پیشرفته] پایه فرکانس برای رمزگذاری موقعیتی چرخشی را تنظیم می‌کند که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",
  "llm.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "طول زمینه با این ضریب مقیاس می‌شود تا زمینه مؤثر را با استفاده از RoPE گسترش دهد",
  "llm.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس‌بندی فرکانس برای رمزگذاری موقعیتی چرخشی را تغییر می‌دهد تا جزئیات رمزگذاری موقعیتی را کنترل کند",
  "llm.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل که برای شتاب‌دهی GPU محاسبه می‌شوند",
  "llm.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU منتقل شوند را تنظیم می‌کند",
  "llm.load.llama.flashAttention/title": "توجه فوری",
  "llm.load.llama.flashAttention/subTitle": "مصرف حافظه و زمان تولید را در برخی مدل‌ها کاهش می‌دهد",
  "llm.load.llama.flashAttention/info": "مکانیزم‌های توجه را برای پردازش سریع‌تر و کارآمدتر تسریع می‌کند",
  "llm.load.numExperts/title": "تعداد کارشناسان",
  "llm.load.numExperts/subTitle": "تعداد کارشناسانی که باید در مدل استفاده شوند",
  "llm.load.numExperts/info": "تعداد کارشناسانی که باید در مدل استفاده شوند",
  "llm.load.llama.keepModelInMemory/title": "نگه داشتن مدل در حافظه",
  "llm.load.llama.keepModelInMemory/subTitle": "حافظه سیستم را برای مدل رزرو می‌کند، حتی زمانی که به GPU منتقل می‌شود. عملکرد را بهبود می‌بخشد اما به RAM بیشتری نیاز دارد",
  "llm.load.llama.keepModelInMemory/info": "مدل را از تعویض به دیسک جلوگیری می‌کند و دسترسی سریع‌تری را با هزینه مصرف بیشتر RAM فراهم می‌کند",
  "llm.load.llama.useFp16ForKVCache/title": "استفاده از FP16 برای کش KV",
  "llm.load.llama.useFp16ForKVCache/info": "مصرف حافظه را با ذخیره کش در دقت نیمه (FP16) کاهش می‌دهد",
  "llm.load.llama.tryMmap/title": "استفاده از mmap()",
  "llm.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن این ممکن است عملکرد را زمانی که مدل از حافظه سیستم بزرگ‌تر است، بهبود بخشد",
  "llm.load.llama.tryMmap/info": "فایل‌های مدل را مستقیماً از دیسک به حافظه بارگذاری می‌کند",
  "llm.load.llama.cpuThreadPoolSize/title": "اندازه استخر رشته CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "تعداد رشته‌های CPU که برای استخر رشته‌ها برای محاسبات مدل تخصیص داده می‌شود",
  "llm.load.llama.cpuThreadPoolSize/info": "تعداد رشته‌های CPU که برای استخر رشته‌ها برای محاسبات مدل تخصیص داده می‌شود. افزایش تعداد رشته‌ها همیشه با عملکرد بهتر همراه نیست. مقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.load.llama.kCacheQuantizationType/title": "نوع کمی‌سازی کش K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "مقادیر پایین‌تر مصرف حافظه را کاهش می‌دهند اما ممکن است کیفیت را کاهش دهند. اثر این مقادیر در مدل‌های مختلف متفاوت است.",
  "llm.load.llama.vCacheQuantizationType/title": "نوع کمی‌سازی کش V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "مقادیر پایین‌تر مصرف حافظه را کاهش می‌دهند اما ممکن است کیفیت را کاهش دهند. اثر این مقادیر در مدل‌های مختلف متفاوت است.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ شما باید این مقدار را غیرفعال کنید اگر توجه فوری فعال نباشد",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "فقط زمانی می‌توان آن را فعال کرد که توجه فوری فعال باشد",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ شما باید توجه فوری را هنگام استفاده از F32 غیرفعال کنید",

  "llm.load.mlx.kvCacheBits/title": "کمی‌سازی کش KV",
  "llm.load.mlx.kvCacheBits/subTitle": "تعداد بیت‌هایی که کش KV باید به آن کمی‌سازی شود",
  "llm.load.mlx.kvCacheBits/info": "تعداد بیت‌هایی که کش KV باید به آن کمی‌سازی شود",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "تنظیم طول زمینه زمانی که از کمی‌سازی کش KV استفاده می‌شود نادیده گرفته می‌شود",
  "llm.load.mlx.kvCacheGroupSize/title": "کمی‌سازی کش KV: اندازه گروه",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "اندازه گروه در حین عملیات کمی‌سازی برای کش KV. اندازه گروه بزرگ‌تر مصرف حافظه را کاهش می‌دهد اما ممکن است کیفیت را کاهش دهد",
  "llm.load.mlx.kvCacheGroupSize/info": "تعداد بیت‌هایی که کش KV باید به آن کمی‌سازی شود",
  "llm.load.mlx.kvCacheQuantizationStart/title": "کمی‌سازی کش KV: شروع کمی‌سازی زمانی که طول زمینه از این مقدار عبور کند",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "آستانه طول زمینه برای شروع کمی‌سازی کش KV",
  "llm.load.mlx.kvCacheQuantizationStart/info": "آستانه طول زمینه برای شروع کمی‌سازی کش KV",
  "llm.load.mlx.kvCacheQuantization/title": "کمی‌سازی کش KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "کمی‌سازی کش KV مدل. این ممکن است منجر به تولید سریع‌تر و کاهش حجم حافظه شود،\nاما به قیمت کاهش کیفیت خروجی مدل.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "بیت‌های کمی‌سازی کش KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "تعداد بیت‌ها برای کمی‌سازی کش KV",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "بیت‌ها",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "استراتژی اندازه گروه",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "دقت",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "متوازن",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "سریع",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "پیشرفته: پیکربندی اندازه گروه 'matmul کمی‌سازی' \n\n• دقت = اندازه گروه 32\n• متوازن = اندازه گروه 64\n• سریع = اندازه گروه 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "شروع کمی‌سازی زمانی که طول زمینه به این مقدار برسد",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "زمانی که طول زمینه به این تعداد توکن رسید،\nکمی‌سازی کش KV را شروع کنید",

  "embedding.load.contextLength/title": "طول زمینه",
  "embedding.load.contextLength/subTitle": "حداکثر تعداد توکن‌هایی که مدل می‌تواند در یک درخواست به آن توجه کند. برای روش‌های بیشتر در مدیریت این موضوع، به گزینه‌های «انفجار مکالمه» در زیر \"پارامترهای استنتاج\" مراجعه کنید",
  "embedding.load.contextLength/info": "تعداد حداکثری توکن‌هایی که مدل می‌تواند یک‌جا در نظر بگیرد، که بر میزان اطلاعات زمینه‌ای که مدل در حین پردازش نگه می‌دارد تأثیر می‌گذارد",
  "embedding.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای جاسازی موقعیتی چرخشی (RoPE). افزایش این مقدار می‌تواند عملکرد بهتری در طول‌های زمینه بالا فراهم کند",
  "embedding.load.llama.ropeFrequencyBase/info": "[پیشرفته] پایه فرکانس برای رمزگذاری موقعیتی چرخشی را تنظیم می‌کند که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",
  "embedding.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "embedding.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی که به صورت همزمان پردازش می‌شوند. افزایش این مقدار باعث افزایش عملکرد با هزینه مصرف حافظه می‌شود",
  "embedding.load.llama.evalBatchSize/info": "تعداد توکن‌هایی که به صورت همزمان در یک دسته در حین ارزیابی پردازش می‌شوند",
  "embedding.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "طول زمینه با این ضریب مقیاس می‌شود تا زمینه مؤثر را با استفاده از RoPE گسترش دهد",
  "embedding.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس‌بندی فرکانس برای رمزگذاری موقعیتی چرخشی را تغییر می‌دهد تا جزئیات رمزگذاری موقعیتی را کنترل کند",
  "embedding.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل که برای شتاب‌دهی GPU محاسبه می‌شوند",
  "embedding.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU منتقل شوند را تنظیم می‌کند",
  "embedding.load.llama.keepModelInMemory/title": "نگه داشتن مدل در حافظه",
  "embedding.load.llama.keepModelInMemory/subTitle": "حافظه سیستم را برای مدل رزرو می‌کند، حتی زمانی که به GPU منتقل می‌شود. عملکرد را بهبود می‌بخشد اما به RAM بیشتری نیاز دارد",
  "embedding.load.llama.keepModelInMemory/info": "مدل را از تعویض به دیسک جلوگیری می‌کند و دسترسی سریع‌تری را با هزینه مصرف بیشتر RAM فراهم می‌کند",
  "embedding.load.llama.tryMmap/title": "استفاده از mmap()",
  "embedding.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن این ممکن است عملکرد را زمانی که مدل از حافظه سیستم بزرگ‌تر است، بهبود بخشد",
  "embedding.load.llama.tryMmap/info": "فایل‌های مدل را مستقیماً از دیسک به حافظه بارگذاری می‌کند",
  "embedding.load.seed/title": "بذر",
  "embedding.load.seed/subTitle": "بذر برای تولید اعداد تصادفی که در تولید متن استفاده می‌شود. -1 به معنی بذر تصادفی است",
  "embedding.load.seed/info": "بذر تصادفی: بذر برای تولید اعداد تصادفی تنظیم می‌شود تا نتایج تکرارپذیر حاصل شود",

  "presetTooltip": {
    "included/title": "مقادیر از پیش تنظیم شده",
    "included/description": "فیلدهای زیر اعمال خواهند شد",
    "included/empty": "هیچ فیلد از این پیش‌تنظیم در این زمینه اعمال نمی‌شود.",
    "included/conflict": "از شما خواسته می‌شود که تصمیم بگیرید آیا این مقدار را اعمال کنید یا نه",
    "separateLoad/title": "پیکربندی زمان بارگذاری",
    "separateLoad/description.1": "پیش‌تنظیم شامل پیکربندی زمان بارگذاری زیر نیز هست. پیکربندی زمان بارگذاری برای مدل به‌طور کلی است و برای اعمال آن نیاز به بارگذاری مجدد مدل دارد. نگه دارید",
    "separateLoad/description.2": "برای اعمال به",
    "separateLoad/description.3": ".",
    "excluded/title": "ممکن است اعمال نشود",
    "excluded/description": "فیلدهای زیر در پیش‌تنظیم گنجانده شده‌اند اما در زمینه فعلی اعمال نمی‌شوند.",
    "legacy/title": "پیش‌تنظیم قدیمی",
    "legacy/description": "این پیش‌تنظیم یک پیش‌تنظیم قدیمی است. شامل فیلدهای زیر می‌شود که یا به‌طور خودکار حالا مدیریت می‌شوند یا دیگر قابل اعمال نیستند.",
    "button/publish": "انتشار در هاب",
    "button/pushUpdate": "ارسال تغییرات به هاب",
    "button/export": "صادرات"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<خالی>"
    },
    "checkboxNumeric": {
      "off": "خاموش"
    },
    "llamaCacheQuantizationType": {
      "off": "خاموش"
    },
    "mlxKvCacheBits": {
      "off": "خاموش"
    },
    "stringArray": {
      "empty": "<خالی>"
    },
    "llmPromptTemplate": {
      "type": "نوع",
      "types.jinja/label": "قالب (Jinja)",
      "jinja.bosToken/label": "توکن BOS",
      "jinja.eosToken/label": "توکن EOS",
      "jinja.template/label": "قالب",
      "jinja/error": "خطا در تجزیه قالب Jinja: {{error}}",
      "jinja/empty": "لطفاً یک قالب Jinja وارد کنید.",
      "jinja/unlikelyToWork": "قالب Jinja که وارد کرده‌اید احتمالاً کار نخواهد کرد زیرا به متغیر \"messages\" ارجاع نمی‌دهد. لطفاً دوباره بررسی کنید که آیا قالب صحیح وارد شده است یا نه.",
      "types.manual/label": "دستی",
      "manual.subfield.beforeSystem/label": "قبل از سیستم",
      "manual.subfield.beforeSystem/placeholder": "پیشوند سیستم را وارد کنید...",
      "manual.subfield.afterSystem/label": "بعد از سیستم",
      "manual.subfield.afterSystem/placeholder": "پسوند سیستم را وارد کنید...",
      "manual.subfield.beforeUser/label": "قبل از کاربر",
      "manual.subfield.beforeUser/placeholder": "پیشوند کاربر را وارد کنید...",
      "manual.subfield.afterUser/label": "بعد از کاربر",
      "manual.subfield.afterUser/placeholder": "پسوند کاربر را وارد کنید...",
      "manual.subfield.beforeAssistant/label": "قبل از دستیار",
      "manual.subfield.beforeAssistant/placeholder": "پیشوند دستیار را وارد کنید...",
      "manual.subfield.afterAssistant/label": "بعد از دستیار",
      "manual.subfield.afterAssistant/placeholder": "پسوند دستیار را وارد کنید...",
      "stopStrings/label": "رشته‌های توقف اضافی",
      "stopStrings/subTitle": "رشته‌های توقف خاص قالب که علاوه بر رشته‌های توقف مشخص شده توسط کاربر استفاده خواهند شد."
    },
    "contextLength": {
      "maxValueTooltip": "این بیشترین تعداد توکن‌هایی است که مدل برای پردازش آن آموزش دیده است. برای تنظیم طول زمینه به این مقدار، کلیک کنید",
      "maxValueTextStart": "مدل تا",
      "maxValueTextEnd": "توکن را پشتیبانی می‌کند",
      "tooltipHint": "اگرچه یک مدل ممکن است تا تعداد خاصی از توکن‌ها را پشتیبانی کند، اما ممکن است عملکرد کاهش یابد اگر منابع سیستم شما قادر به پردازش آن بار نباشد - هنگام افزایش این مقدار احتیاط کنید"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "ایستادن در حد",
      "stopAtLimitSub": "تولید را زمانی که حافظه مدل پر شود متوقف کنید",
      "truncateMiddle": "حذف از وسط",
      "truncateMiddleSub": "پیام‌ها را از وسط مکالمه حذف می‌کند تا فضا برای پیام‌های جدید ایجاد شود. مدل همچنان ابتدای مکالمه را به یاد می‌آورد",
      "rollingWindow": "پنجره چرخشی",
      "rollingWindowSub": "مدل همیشه جدیدترین پیام‌ها را دریافت می‌کند، اما ممکن است ابتدای مکالمه را فراموش کند"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "حداکثر",
      "off": "خاموش"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "یکنواخت",
      "favorMainGpu": "ترجیح به GPU اصلی"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "بخوانید چگونه کار می‌کند",
      "placeholder": "یک مدل پیش‌نویس سازگار را انتخاب کنید",
      "noCompatible": "مدل‌های پیش‌نویس سازگاری برای انتخاب مدل فعلی شما پیدا نشد",
      "stillLoading": "در حال شناسایی مدل‌های پیش‌نویس سازگار...",
      "notCompatible": "مدل پیش‌نویس انتخاب شده (<draft/>) با انتخاب مدل فعلی (<current/>) سازگار نیست.",
      "off": "خاموش",
      "loadModelToSeeOptions": "مدل <keyboard-shortcut /> را بارگذاری کنید تا گزینه‌های سازگار را مشاهده کنید",
      "compatibleWithNumberOfModels": "برای حداقل {{dynamicValue}} از مدل‌های شما توصیه شده است",
      "recommendedForSomeModels": "برای برخی مدل‌ها توصیه شده است",
      "recommendedForLlamaModels": "برای مدل‌های Llama توصیه شده است",
      "recommendedForQwenModels": "برای مدل‌های Qwen توصیه شده است",
      "onboardingModal": {
        "introducing": "معرفی",
        "speculativeDecoding": "رمزگشایی احتمالی",
        "firstStepBody": "شتاب‌دهی به سرعت استنتاج برای مدل‌های <custom-span>llama.cpp</custom-span> و <custom-span>MLX</custom-span>",
        "secondStepTitle": "شتاب‌دهی به سرعت استنتاج با رمزگشایی احتمالی",
        "secondStepBody": "رمزگشایی احتمالی یک تکنیک است که شامل همکاری دو مدل است:\n - یک مدل \"اصلی\" بزرگتر\n - یک مدل \"پیش‌نویس\" کوچکتر\n\nدر طول تولید، مدل پیش‌نویس به سرعت توکن‌هایی را برای مدل اصلی پیشنهاد می‌دهد تا آن‌ها را تأیید کند. تأیید توکن‌ها فرآیندی سریع‌تر از تولید واقعی آن‌ها است که منبع افزایش سرعت است. **به طور کلی، هرچه تفاوت اندازه بین مدل اصلی و مدل پیش‌نویس بیشتر باشد، سرعت بیشتر خواهد بود**.\n\nبرای حفظ کیفیت، مدل اصلی تنها توکن‌هایی را قبول می‌کند که با آنچه خود تولید کرده باشد تطابق داشته باشد، که این باعث می‌شود پاسخ‌های مدل بزرگتر با سرعت‌های استنتاج سریع‌تر حفظ شود. هر دو مدل باید واژگان یکسانی داشته باشند.",
        "draftModelRecommendationsTitle": "توصیه‌های مدل پیش‌نویس",
        "basedOnCurrentModels": "بر اساس مدل‌های فعلی شما",
        "close": "بستن",
        "next": "بعدی",
        "done": "تمام"
      },
      "speculativeDecodingLoadModelToSeeOptions": "لطفاً ابتدا یک مدل بارگذاری کنید <model-badge /> ",
      "errorEngineNotSupported": "رمزگشایی احتمالی به حداقل نسخه {{minVersion}} از موتور {{engineName}} نیاز دارد. لطفاً موتور را به‌روزرسانی کرده و مدل را دوباره بارگذاری کنید تا از این ویژگی استفاده کنید.",
      "errorEngineNotSupported/noKey": "رمزگشایی احتمالی به حداقل نسخه {{minVersion}} از موتور {{engineName}} نیاز دارد. لطفاً موتور را به‌روزرسانی کرده و مدل را دوباره بارگذاری کنید تا از این ویژگی استفاده کنید."
    },
    "llmReasoningParsing": {
      "startString/label": "رشته شروع",
      "startString/placeholder": "رشته شروع را وارد کنید...",
      "endString/label": "رشته پایان",
      "endString/placeholder": "رشته پایان را وارد کنید..."
    }
  },
  "saveConflictResolution": {
    "title": "انتخاب کنید که کدام مقادیر را در پیش‌تنظیم گنجانده شود",
    "description": "مقادیر را انتخاب کرده و نگه دارید",
    "instructions": "برای گنجاندن یک مقدار روی آن کلیک کنید",
    "userValues": "مقدار قبلی",
    "presetValues": "مقدار جدید",
    "confirm": "تأیید",
    "cancel": "لغو"
  },
  "applyConflictResolution": {
    "title": "کدام مقادیر را نگه دارید؟",
    "description": "شما تغییرات غیرمؤکد شده‌ای دارید که با پیش‌تنظیم‌های وارد شده همپوشانی دارد",
    "instructions": "برای نگه داشتن یک مقدار روی آن کلیک کنید",
    "userValues": "مقدار کنونی",
    "presetValues": "مقدار پیش‌تنظیم وارد شده",
    "confirm": "تأیید",
    "cancel": "لغو"
  },
  {
    "empty": "<خالی>",
    "noModelSelected": "مدلی انتخاب نشده است",
    "apiIdentifier.label": "شناسه API",
    "apiIdentifier.hint": "اختیاری است که شناسه‌ای برای این مدل وارد کنید. این شناسه در درخواست‌های API استفاده خواهد شد. برای استفاده از شناسه پیش‌فرض، آن را خالی بگذارید.",
    "idleTTL.label": "بارگذاری خودکار در صورت بی‌فعالیت بودن (TTL)",
    "idleTTL.hint": "اگر تنظیم شود، مدل پس از مدت زمانی که بی‌فعالیت بوده به طور خودکار بارگذاری می‌شود.",
    "idleTTL.mins": "دقیقه",

    "presets": {
      "title": "پیش‌تنظیم",
      "commitChanges": "اعمال تغییرات",
      "commitChanges/description": "تغییرات خود را به پیش‌تنظیم اعمال کنید.",
      "commitChanges.manual": "فیلدهای جدید شناسایی شدند. شما می‌توانید انتخاب کنید که کدام تغییرات به پیش‌تنظیم اضافه شوند.",
      "commitChanges.manual.hold.0": "نگه دارید",
      "commitChanges.manual.hold.1": "برای انتخاب تغییراتی که باید به پیش‌تنظیم اعمال شوند.",
      "commitChanges.saveAll.hold.0": "نگه دارید",
      "commitChanges.saveAll.hold.1": "برای ذخیره تمام تغییرات.",
      "commitChanges.saveInPreset.hold.0": "نگه دارید",
      "commitChanges.saveInPreset.hold.1": "برای ذخیره فقط تغییرات فیلدهایی که در پیش‌تنظیم موجود هستند.",
      "commitChanges/error": "در اعمال تغییرات به پیش‌تنظیم خطا رخ داده است.",
      "commitChanges.manual/description": "انتخاب کنید که کدام تغییرات به پیش‌تنظیم اضافه شوند.",
      "saveAs": "ذخیره به عنوان جدید...",
      "presetNamePlaceholder": "نامی برای پیش‌تنظیم وارد کنید...",
      "cannotCommitChangesLegacy": "این پیش‌تنظیم از نسخه قدیمی است و نمی‌توان آن را تغییر داد. شما می‌توانید با استفاده از گزینه \"ذخیره به عنوان جدید...\" یک کپی از آن ایجاد کنید.",
      "cannotCommitChangesNoChanges": "تغییری برای اعمال وجود ندارد.",
      "emptyNoUnsaved": "یک پیش‌تنظیم انتخاب کنید...",
      "emptyWithUnsaved": "پیش‌تنظیم ذخیره‌نشده",
      "saveEmptyWithUnsaved": "ذخیره پیش‌تنظیم به عنوان...",
      "saveConfirm": "ذخیره",
      "saveCancel": "لغو",
      "saving": "در حال ذخیره...",
      "save/error": "در ذخیره پیش‌تنظیم خطا رخ داده است.",
      "deselect": "لغو انتخاب پیش‌تنظیم",
      "deselect/error": "در لغو انتخاب پیش‌تنظیم خطا رخ داده است.",
      "select/error": "در انتخاب پیش‌تنظیم خطا رخ داده است.",
      "delete/error": "در حذف پیش‌تنظیم خطا رخ داده است.",
      "discardChanges": "دور انداختن تغییرات ذخیره‌نشده",
      "discardChanges/info": "تمام تغییرات اعمال‌نشده را دور بیندازید و پیش‌تنظیم را به حالت اولیه خود برگردانید.",
      "newEmptyPreset": "+ پیش‌تنظیم جدید",
      "importPreset": "وارد کردن",
      "contextMenuSelect": "اعمال پیش‌تنظیم",
      "contextMenuDelete": "حذف...",
      "contextMenuShare": "انتشار...",
      "contextMenuOpenInHub": "مشاهده در Hub",
      "contextMenuPushChanges": "اعمال تغییرات به Hub",
      "contextMenuPushingChanges": "در حال اعمال...",
      "contextMenuPushedChanges": "تغییرات اعمال شد",
      "contextMenuExport": "صادرات فایل",
      "contextMenuRevealInExplorer": "نمایش در کاوشگر فایل",
      "contextMenuRevealInFinder": "نمایش در Finder",
      "share": {
        "title": "انتشار پیش‌تنظیم",
        "action": "پیش‌تنظیم خود را برای دیگران به اشتراک بگذارید تا آن را دانلود کنند، پسندیده یا فورک کنند.",
        "presetOwnerLabel": "مالک",
        "uploadAs": "پیش‌تنظیم شما به عنوان {{name}} ایجاد خواهد شد",
        "presetNameLabel": "نام پیش‌تنظیم",
        "descriptionLabel": "توضیحات (اختیاری)",
        "loading": "در حال انتشار...",
        "success": "پیش‌تنظیم با موفقیت منتشر شد",
        "presetIsLive": "<نام پیش‌تنظیم /> اکنون در Hub زنده است!",
        "close": "بستن",
        "confirmViewOnWeb": "مشاهده در وب",
        "confirmCopy": "کپی URL",
        "confirmCopied": "کپی شد!",
        "pushedToHub": "پیش‌تنظیم شما به Hub ارسال شد",
        "descriptionPlaceholder": "توضیح وارد کنید...",
        "willBePublic": "انتشار پیش‌تنظیم شما آن را عمومی خواهد کرد",
        "publicSubtitle": "پیش‌تنظیم شما <custom-bold>عمومی</custom-bold> است. دیگران می‌توانند آن را دانلود و فورک کنند در lmstudio.ai",
        "confirmShareButton": "انتشار",
        "error": "در انتشار پیش‌تنظیم خطا رخ داده است",
        "createFreeAccount": "برای انتشار پیش‌تنظیم‌ها یک حساب رایگان در Hub ایجاد کنید"
      },
      "update": {
        "title": "اعمال تغییرات به Hub",
        "title/success": "پیش‌تنظیم با موفقیت به روز شد",
        "subtitle": "تغییرات <custom-preset-name /> را اعمال کنید و آن‌ها را به Hub ارسال کنید",
        "descriptionLabel": "توضیحات",
        "descriptionPlaceholder": "توضیح وارد کنید...",
        "loading": "در حال ارسال...",
        "cancel": "لغو",
        "createFreeAccount": "برای انتشار پیش‌تنظیم‌ها یک حساب رایگان در Hub ایجاد کنید",
        "error": "در ارسال تغییرات خطا رخ داده است",
        "confirmUpdateButton": "ارسال"
      },
      "import": {
        "title": "وارد کردن پیش‌تنظیم از فایل",
        "dragPrompt": "فایل‌های JSON پیش‌تنظیم را بکشید و رها کنید یا <custom-link>از کامپیوتر خود انتخاب کنید</custom-link>",
        "remove": "حذف",
        "cancel": "لغو",
        "importPreset_zero": "وارد کردن پیش‌تنظیم",
        "importPreset_one": "وارد کردن پیش‌تنظیم",
        "importPreset_other": "وارد کردن {{count}} پیش‌تنظیم",
        "selectDialog": {
          "title": "فایل پیش‌تنظیم (.json) را انتخاب کنید",
          "button": "وارد کردن"
        },
        "error": "در وارد کردن پیش‌تنظیم خطا رخ داده است",
        "resultsModal": {
          "titleSuccessSection_one": "1 پیش‌تنظیم با موفقیت وارد شد",
          "titleSuccessSection_other": "{{count}} پیش‌تنظیم با موفقیت وارد شد",
          "titleFailSection_zero": "",
          "titleFailSection_one": "({{count}} ناموفق)",
          "titleFailSection_other": "({{count}} ناموفق)",
          "titleAllFailed": "وارد کردن پیش‌تنظیم‌ها با شکست مواجه شد",
          "importMore": "وارد کردن بیشتر",
          "close": "تمام",
          "successBadge": "موفق",
          "alreadyExistsBadge": "پیش‌تنظیم قبلاً وجود دارد",
          "errorBadge": "خطا",
          "invalidFileBadge": "فایل نامعتبر",
          "otherErrorBadge": "در وارد کردن پیش‌تنظیم خطا رخ داده است",
          "errorViewDetailsButton": "مشاهده جزئیات",
          "seeError": "مشاهده خطا",
          "noName": "نام پیش‌تنظیم وجود ندارد",
          "useInChat": "استفاده در چت"
        },
        "importFromUrl": {
          "button": "وارد کردن از URL...",
          "title": "وارد کردن از URL",
          "back": "وارد کردن از فایل...",
          "action": "URL پیش‌تنظیم را از Hub وارد کنید که می‌خواهید وارد کنید",
          "invalidUrl": "URL نامعتبر است. لطفاً اطمینان حاصل کنید که URL صحیح را وارد می‌کنید.",
          "tip": "شما می‌توانید پیش‌تنظیم را مستقیماً با دکمه {{buttonName}} در LM Studio Hub نصب کنید",
          "confirm": "وارد کردن",
          "cancel": "لغو",
          "loading": "در حال وارد کردن...",
          "error": "در دانلود پیش‌تنظیم خطا رخ داده است."
        }
      },
      "download": {
        "title": "کشیدن <preset-name /> از LM Studio Hub",
        "subtitle": "ذخیره <custom-name /> به پیش‌تنظیم‌های شما. با این کار شما اجازه می‌دهید که از این پیش‌تنظیم در اپلیکیشن استفاده کنید",
        "button": "کشیدن",
        "button/loading": "در حال کشیدن...",
        "cancel": "لغو",
        "error": "در دانلود پیش‌تنظیم خطا رخ داده است."
      },
      "inclusiveness": {
        "speculativeDecoding": "شامل در پیش‌تنظیم"
      }
    },

    "flashAttentionWarning": "Flash Attention یک ویژگی آزمایشی است که ممکن است برای برخی از مدل‌ها مشکلاتی ایجاد کند. اگر با مشکلی مواجه شدید، می‌توانید آن را غیرفعال کنید.",
    "llamaKvCacheQuantizationWarning": "KV Cache Quantization یک ویژگی آزمایشی است که ممکن است برای برخی از مدل‌ها مشکلاتی ایجاد کند. برای فعال‌سازی V cache quantization، باید Flash Attention فعال باشد. اگر با مشکلی مواجه شدید، آن را به مقدار پیش‌فرض \"F16\" بازنشانی کنید.",

    "seedUncheckedHint": "دانه تصادفی",
    "ropeFrequencyBaseUncheckedHint": "خودکار",
    "ropeFrequencyScaleUncheckedHint": "خودکار",

    "hardware": {
      "advancedGpuSettings": "تنظیمات پیشرفته GPU",
      "advancedGpuSettings.info": "اگر مطمئن نیستید، این تنظیمات را به حالت پیش‌فرض رها کنید",
      "advancedGpuSettings.reset": "بازنشانی به حالت پیش‌فرض",
      "environmentVariables": {
        "title": "متغیرهای محیطی",
        "description": "متغیرهای محیطی فعال در طول عمر مدل.",
        "key.placeholder": "متغیر را انتخاب کنید...",
        "value.placeholder": "مقدار"
      },
      "mainGpu": {
        "title": "GPU اصلی",
        "description": "GPU ای که برای محاسبات مدل اولویت دارد.",
        "placeholder": "GPU اصلی را انتخاب کنید..."
      },
      "splitStrategy": {
        "title": "استراتژی تقسیم",
        "description": "چگونه محاسبات مدل بین GPU ها تقسیم شود.",
        "placeholder": "استراتژی تقسیم را انتخاب کنید..."
      }
    }

  }