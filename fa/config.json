{
  "noInstanceSelected": "هیچ نمونه مدلی انتخاب نشده است",
  "resetToDefault": "بازنشانی",
  "showAdvancedSettings": "نمایش تنظیمات پیشرفته",
  "showAll": "همه",
  "basicSettings": "پایه",
  "configSubtitle": "بارگذاری یا ذخیره پیش‌تنظیم‌ها و آزمایش با جایگزین‌های پارامتر مدل",
  "inferenceParameters/title": "پارامترهای پیش‌بینی",
  "inferenceParameters/info": "آزمایش با پارامترهایی که بر پیش‌بینی تأثیر می‌گذارند.",
  "generalParameters/title": "عمومی",
  "samplingParameters/title": "نمونه‌برداری",
  "basicTab": "پایه",
  "advancedTab": "پیشرفته",
  "advancedTab/title": "🧪 پیکربندی پیشرفته",
  "advancedTab/expandAll": "گسترش همه",
  "advancedTab/overridesTitle": "جایگزین‌های پیکربندی",
  "advancedTab/noConfigsText": "شما هیچ تغییر ذخیره نشده‌ای ندارید - مقادیر بالا را ویرایش کنید تا جایگزین‌ها را اینجا ببینید.",
  "loadInstanceFirst": "یک مدل را بارگذاری کنید تا پارامترهای قابل تنظیم را مشاهده کنید",
  "noListedConfigs": "پارامتر قابل تنظیمی وجود ندارد",
  "generationParameters/info": "آزمایش با پارامترهای پایه که بر تولید متن تأثیر می‌گذارند.",
  "loadParameters/title": "پارامترهای بارگذاری",
  "loadParameters/description": "تنظیماتی برای کنترل نحوه راه‌اندازی و بارگذاری مدل در حافظه.",
  "loadParameters/reload": "برای اعمال تغییرات، مجدداً بارگذاری کنید",
  "discardChanges": "لغو تغییرات",
  "loadModelToSeeOptions": "یک مدل را بارگذاری کنید تا گزینه‌ها را ببینید",

  "llm.prediction.systemPrompt/title": "پیام سیستم",
  "llm.prediction.systemPrompt/description": "از این فیلد برای ارائه دستورالعمل‌های پس‌زمینه به مدل استفاده کنید، مانند مجموعه‌ای از قوانین، محدودیت‌ها یا الزامات کلی.",
  "llm.prediction.systemPrompt/subTitle": "دستورالعمل‌ها برای هوش مصنوعی",
  "llm.prediction.temperature/title": "دما",
  "llm.prediction.temperature/subTitle": "چقدر تصادفی بودن اضافه شود. 0 هر بار همان نتیجه را خواهد داد، در حالی که مقادیر بالاتر خلاقیت و تنوع را افزایش می‌دهند",
  "llm.prediction.temperature/info": "از اسناد راهنمای llama.cpp: \"مقدار پیش‌فرض <{{dynamicValue}}> است، که تعادلی بین تصادفی بودن و قطعیت ایجاد می‌کند. در حالت حدی، دمای 0 همیشه محتمل‌ترین توکن بعدی را انتخاب می‌کند و منجر به خروجی‌های یکسان در هر اجرا می‌شود\"",
  "llm.prediction.llama.sampling/title": "نمونه‌برداری",
  "llm.prediction.topKSampling/title": "نمونه‌برداری Top K",
  "llm.prediction.topKSampling/subTitle": "توکن بعدی را به یکی از k توکن با بیشترین احتمال محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.topKSampling/info": "از اسناد راهنمای llama.cpp:\n\nنمونه‌برداری top-k روشی برای تولید متن است که توکن بعدی را فقط از بین k توکن با بیشترین احتمال پیش‌بینی شده توسط مدل انتخاب می‌کند.\n\nاین روش کمک می‌کند تا خطر تولید توکن‌های با احتمال کم یا بی‌معنی کاهش یابد، اما ممکن است تنوع خروجی را نیز محدود کند.\n\nمقدار بالاتر برای top-k (مثلاً 100) توکن‌های بیشتری را در نظر می‌گیرد و منجر به متن متنوع‌تر می‌شود، در حالی که مقدار پایین‌تر (مثلاً 10) روی محتمل‌ترین توکن‌ها تمرکز می‌کند و متن محافظه‌کارانه‌تری تولید می‌کند.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",

  "llm.prediction.llama.cpuThreads/title": "تعداد رشته‌های CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "تعداد رشته‌های CPU برای استفاده در طول استنتاج",
  "llm.prediction.llama.cpuThreads/info": "تعداد رشته‌هایی که در طول محاسبات استفاده می‌شوند. افزایش تعداد رشته‌ها همیشه با عملکرد بهتر همبستگی ندارد. پیش‌فرض <{{dynamicValue}}> است.",

  "llm.prediction.maxPredictedTokens/title": "محدودیت طول پاسخ",
  "llm.prediction.maxPredictedTokens/subTitle": "محدود کردن اختیاری طول پاسخ هوش مصنوعی",
  "llm.prediction.maxPredictedTokens/info": "کنترل حداکثر طول پاسخ ربات گفتگو. روشن کنید تا محدودیتی برای حداکثر طول پاسخ تعیین کنید، یا خاموش کنید تا ربات گفتگو خودش تصمیم بگیرد چه زمانی متوقف شود.",
  "llm.prediction.maxPredictedTokens/inputLabel": "حداکثر طول پاسخ (توکن)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "حدود {{maxWords}} کلمه",

  "llm.prediction.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.repeatPenalty/subTitle": "چقدر تکرار همان توکن نامطلوب شود",
  "llm.prediction.repeatPenalty/info": "از اسناد راهنمای llama.cpp: \"کمک می‌کند تا از تولید متن تکراری یا یکنواخت توسط مدل جلوگیری شود.\n\nمقدار بالاتر (مثلاً 1.5) تکرارها را شدیدتر جریمه می‌کند، در حالی که مقدار پایین‌تر (مثلاً 0.9) آسان‌گیرتر خواهد بود.\" • مقدار پیش‌فرض <{{dynamicValue}}> است",

  "llm.prediction.minPSampling/title": "نمونه‌برداری Min P",
  "llm.prediction.minPSampling/subTitle": "حداقل احتمال پایه برای انتخاب یک توکن برای خروجی",
  "llm.prediction.minPSampling/info": "از اسناد راهنمای llama.cpp:\n\nحداقل احتمال برای در نظر گرفتن یک توکن، نسبت به احتمال محتمل‌ترین توکن. باید در بازه [0، 1] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",

  "llm.prediction.topPSampling/title": "نمونه‌برداری Top P",
  "llm.prediction.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های بعدی ممکن. مشابه دما عمل می‌کند",
  "llm.prediction.topPSampling/info": "از اسناد راهنمای llama.cpp:\n\nنمونه‌برداری top-p، که به عنوان نمونه‌برداری هسته نیز شناخته می‌شود، روشی دیگر برای تولید متن است که توکن بعدی را از زیرمجموعه‌ای از توکن‌ها انتخاب می‌کند که مجموع احتمالات آنها حداقل p است.\n\nاین روش تعادلی بین تنوع و کیفیت ایجاد می‌کند و احتمالات توکن‌ها و تعداد توکن‌های نمونه‌برداری شده را در نظر می‌گیرد.\n\nمقدار بالاتر برای top-p (مثلاً 0.95) منجر به متن متنوع‌تر می‌شود، در حالی که مقدار پایین‌تر (مثلاً 0.5) متن محافظه‌کارانه‌تر و متمرکزتری تولید می‌کند. باید در بازه (0، 1] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",

  "llm.prediction.stopStrings/title": "رشته‌های توقف",
  "llm.prediction.stopStrings/subTitle": "رشته‌هایی که باید مدل را از تولید توکن‌های بیشتر متوقف کنند",
  "llm.prediction.stopStrings/info": "رشته‌های خاصی که در صورت مشاهده، مدل را از تولید توکن‌های بیشتر متوقف می‌کنند",
  "llm.prediction.stopStrings/placeholder": "رشته را وارد کنید و ⏎ را فشار دهید",

  "llm.prediction.contextOverflowPolicy/title": "سرریز مکالمه",
  "llm.prediction.contextOverflowPolicy/subTitle": "نحوه رفتار مدل هنگامی که مکالمه از ظرفیت پردازش آن فراتر می‌رود",
  "llm.prediction.contextOverflowPolicy/info": "تصمیم بگیرید وقتی مکالمه از اندازه حافظه کاری مدل ('context') فراتر می‌رود چه اتفاقی بیفتد",

  "customInputs.contextOverflowPolicy.stopAtLimit": "توقف در محدودیت",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "وقتی مدل پر شد، تولید را متوقف می‌کند",
  "customInputs.contextOverflowPolicy.truncateMiddle": "حذف از وسط",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "پیام‌های میانی مکالمه را حذف می‌کند تا برای موارد جدید جا باز شود. مدل همچنان ابتدای مکالمه را به یاد خواهد داشت",
  "customInputs.contextOverflowPolicy.rollingWindow": "پنجره غلتان",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "مدل همیشه چند پیام آخر را دریافت می‌کند، اما ممکن است ابتدای مکالمه را فراموش کند",

  "llm.prediction.llama.frequencyPenalty/title": "جریمه فرکانس",
  "llm.prediction.llama.presencePenalty/title": "جریمه حضور",
  "llm.prediction.llama.tailFreeSampling/title": "نمونه‌برداری بدون دنباله",
  "llm.prediction.llama.locallyTypicalSampling/title": "نمونه‌برداری محلی معمول",

  "llm.prediction.onnx.topKSampling/title": "نمونه‌برداری Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "توکن بعدی را به یکی از k توکن با بیشترین احتمال محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topKSampling/info": "از اسناد راهنمای ONNX:\n\nتعداد توکن‌های واژگان با بیشترین احتمال که باید برای فیلتر top-k نگه داشته شوند\n\n• این فیلتر غیرفعال است",

  "llm.prediction.onnx.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.onnx.repeatPenalty/subTitle": "چقدر تکرار همان توکن نامطلوب شود",
  "llm.prediction.onnx.repeatPenalty/info": "مقدار بالاتر تکرار همان توکن را بیشتر جریمه می‌کند",

  "llm.prediction.onnx.topPSampling/title": "نمونه‌برداری Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های بعدی ممکن. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topPSampling/info": "از اسناد راهنمای ONNX:\n\nفقط توکن‌های با بیشترین احتمال با احتمال TopP یا بالاتر برای تولید نگه داشته می‌شوند.\n\n• این فیلتر غیرفعال است",

  "llm.prediction.seed/title": "عدد تصادفی",
  "llm.prediction.structured/title": "خروجی ساختاریافته",
  "llm.prediction.structured/info": "خروجی ساختاریافته",
  "llm.prediction.promptTemplate/title": "قالب پیام",
  "llm.prediction.promptTemplate/subTitle": "قالبی که در آن پیام‌های مکالمه به مدل ارسال می‌شوند. تغییر این قالب می‌تواند باعث رفتار غیرمنتظره شود - مطمئن شوید که می‌دانید چه کار می‌کنید!",

  "llm.load.contextLength/title": "طول متن",
  "llm.load.contextLength/subTitle": "بیشترین تعداد توکن‌هایی که مدل می‌تواند در یک پیام پردازش کند. برای روش‌های دیگر کنترل این محدودیت به 'پارامترهای پیش‌بینی' در زیر 'سرریز مکالمه' مراجعه کنید",
  "llm.load.contextLength/info": "تعیین می‌کند حداکثر چند توکن می‌تواند همزمان توسط مدل پردازش شود، که بر میزان متنی که مدل می‌تواند در حین پردازش به خاطر بسپارد تأثیر می‌گذارد",

  "llm.load.seed/title": "عدد تصادفی",
  "llm.load.seed/subTitle": "عدد تصادفی برای تولید اعداد تصادفی. -1 به معنای عدد تصادفی است",
  "llm.load.seed/info": "عدد تصادفی: تنظیم عدد تصادفی برای تولید اعداد تصادفی جهت اطمینان از تکرارپذیری نتایج",

  "llm.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "llm.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی که باید همزمان پردازش شوند. افزایش این عدد عملکرد را بهبود می‌بخشد اما مصرف حافظه را نیز افزایش می‌دهد",
  "llm.load.llama.evalBatchSize/info": "تعداد توکن‌هایی که در یک دسته در طول ارزیابی پردازش می‌شوند را تنظیم می‌کند، که بر سرعت و مصرف حافظه تأثیر می‌گذارد",

  "llm.load.llama.ropeFrequencyBase/title": "فرکانس پایه RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای جاسازی موقعیت چرخشی (RoPE). افزایش این عدد می‌تواند عملکرد بهتری در طول‌های متن بالا ایجاد کند",
  "llm.load.llama.ropeFrequencyBase/info": "[پیشرفته] فرکانس پایه را برای جاسازی موقعیت چرخشی تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",

  "llm.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "طول متن در این ضریب ضرب می‌شود تا متن مؤثر با استفاده از RoPE گسترش یابد",
  "llm.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس فرکانس را برای جاسازی موقعیت چرخشی تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",

  "llm.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های گسسته مدل که باید برای شتاب‌دهی GPU روی GPU محاسبه شوند",
  "llm.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید روی GPU محاسبه شوند را تنظیم می‌کند.",

  "llm.load.llama.flashAttention/title": "توجه سریع",
  "llm.load.llama.flashAttention/subTitle": "مصرف حافظه و زمان تولید را در برخی مدل‌ها کاهش می‌دهد",
  "llm.load.llama.flashAttention/info": "مکانیسم‌های توجه را برای پردازش سریع‌تر و کارآمدتر تسریع می‌کند",

  "llm.load.llama.keepModelInMemory/title": "نگهداری مدل در حافظه",
  "llm.load.llama.keepModelInMemory/subTitle": "حافظه سیستم را برای مدل رزرو می‌کند، حتی وقتی به GPU منتقل شده است. عملکرد را بهبود می‌بخشد اما به RAM سیستم بیشتری نیاز دارد",
  "llm.load.llama.keepModelInMemory/info": "از تعویض مدل به دیسک جلوگیری می‌کند، دسترسی سریع‌تر را به قیمت مصرف بیشتر RAM تضمین می‌کند",

  "llm.load.llama.useFp16ForKVCache/title": "استفاده از FP16 برای حافظه نهان KV",
  "llm.load.llama.useFp16ForKVCache/info": "با ذخیره حافظه نهان با دقت نیم (FP16)، مصرف حافظه را کاهش می‌دهد",

  "llm.load.llama.tryMmap/title": "تلاش برای mmap()",
  "llm.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن آن می‌تواند عملکرد را بهبود بخشد وقتی مدل از RAM سیستم موجود بزرگتر است",
  "llm.load.llama.tryMmap/info": "فایل‌های مدل را مستقیماً از دیسک به حافظه بارگذاری می‌کند",

  "embedding.load.contextLength/title": "طول متن",
  "embedding.load.contextLength/subTitle": "بیشترین تعداد توکن‌هایی که مدل می‌تواند در یک پیام پردازش کند. برای روش‌های دیگر کنترل این محدودیت به 'پارامترهای پیش‌بینی' در زیر 'سرریز مکالمه' مراجعه کنید",
  "embedding.load.contextLength/info": "تعیین می‌کند حداکثر چند توکن می‌تواند همزمان توسط مدل پردازش شود، که بر میزان متنی که مدل می‌تواند در حین پردازش به خاطر بسپارد تأثیر می‌گذارد",

  "embedding.load.llama.ropeFrequencyBase/title": "فرکانس پایه RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای جاسازی موقعیت چرخشی (RoPE). افزایش این عدد می‌تواند عملکرد بهتری در طول‌های متن بالا ایجاد کند",
  "embedding.load.llama.ropeFrequencyBase/info": "[پیشرفته] فرکانس پایه را برای جاسازی موقعیت چرخشی تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",

  "embedding.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "embedding.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی که باید همزمان پردازش شوند. افزایش این عدد عملکرد را بهبود می‌بخشد اما مصرف حافظه را نیز افزایش می‌دهد",
  "embedding.load.llama.evalBatchSize/info": "تعداد توکن‌هایی که در یک دسته در طول ارزیابی پردازش می‌شوند را تنظیم می‌کند، که بر سرعت و مصرف حافظه تأثیر می‌گذارد",

  "embedding.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "طول متن در این ضریب ضرب می‌شود تا متن مؤثر با استفاده از RoPE گسترش یابد",
  "embedding.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس فرکانس را برای جاسازی موقعیت چرخشی تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",

  "embedding.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های گسسته مدل که باید برای شتاب‌دهی GPU روی GPU محاسبه شوند",
  "embedding.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید روی GPU محاسبه شوند را تنظیم می‌کند.",

  "embedding.load.llama.keepModelInMemory/title": "نگهداری مدل در حافظه",
  "embedding.load.llama.keepModelInMemory/subTitle": "حافظه سیستم را برای مدل رزرو می‌کند، حتی وقتی به GPU منتقل شده است. عملکرد را بهبود می‌بخشد اما به RAM سیستم بیشتری نیاز دارد",
  "embedding.load.llama.keepModelInMemory/info": "از تعویض مدل به دیسک جلوگیری می‌کند، دسترسی سریع‌تر را به قیمت مصرف بیشتر RAM تضمین می‌کند",

  "embedding.load.llama.tryMmap/title": "تلاش برای mmap()",
  "embedding.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن آن می‌تواند عملکرد را بهبود بخشد وقتی مدل از RAM سیستم موجود بزرگتر است",
  "embedding.load.llama.tryMmap/info": "فایل‌های مدل را مستقیماً از دیسک به حافظه بارگذاری می‌کند",

  "embedding.load.seed/title": "عدد تصادفی",
  "embedding.load.seed/subTitle": "عدد تصادفی برای تولید اعداد تصادفی. -1 به معنای عدد تصادفی است",
  "embedding.load.seed/info": "عدد تصادفی: تنظیم عدد تصادفی برای تولید اعداد تصادفی. -1 به معنای عدد تصادفی است",

  "presetTooltip": {
    "included/title": "Preset Values",
    "included/description": "The following fields will be applied",
    "included/empty": "No fields of this preset apply in this context.",
    "included/conflict": "You will be asked to choose whether to apply this value",
    "separateLoad/title": "Load-time Configuration",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "to apply to",
    "separateLoad/description.3": ".",
    "excluded/title": "May not apply",
    "excluded/description": "The following fields are included in the preset but does not apply in the current context.",
    "legacy/title": "Legacy Preset",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Template",
      "jinja/error": "Failed to parse Jinja template: {{error}}",
      "jinja/empty": "Please enter a Jinja template above.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Before System",
      "manual.subfield.beforeSystem/placeholder": "Enter System prefix...",
      "manual.subfield.afterSystem/label": "After System",
      "manual.subfield.afterSystem/placeholder": "Enter System suffix...",
      "manual.subfield.beforeUser/label": "Before User",
      "manual.subfield.beforeUser/placeholder": "Enter User prefix...",
      "manual.subfield.afterUser/label": "After User",
      "manual.subfield.afterUser/placeholder": "Enter User suffix...",
      "manual.subfield.beforeAssistant/label": "Before Assistant",
      "manual.subfield.beforeAssistant/placeholder": "Enter Assistant prefix...",
      "manual.subfield.afterAssistant/label": "After Assistant",
      "manual.subfield.afterAssistant/placeholder": "Enter Assistant suffix...",
      "stopStrings/label": "Additional Stop Strings",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "This is the maximum number of tokens the model was trained to handle. Click to set the context to this value",
      "maxValueTextStart": "Model supports up to",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "While a model may support up to a certain number of tokens, performance may deteriorate if your machine's resources cannot handle the load - use caution when increasing this value"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Stop at Limit",
      "stopAtLimitSub": "Stop generating once the model's memory gets full",
      "truncateMiddle": "Truncate Middle",
      "truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
      "rollingWindow": "Rolling Window",
      "rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    }
  },
  "saveConflictResolution": {
    "title": "Choose which values to include in the Preset",
    "description": "Pick and choose which values to keep",
    "instructions": "Click on a value to include it",
    "userValues": "Previous Value",
    "presetValues": "New Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "applyConflictResolution": {
    "title": "Which values to keep?",
    "description": "You have uncommitted changes which overlap with the incoming Preset",
    "instructions": "Click on a value to keep it",
    "userValues": "Current Value",
    "presetValues": "Incoming Preset Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "empty": "<Empty>",
  "presets": {
    "title": "Preset",
    "commitChanges": "Commit Changes",
    "commitChanges/description": "Commit your changes to the preset.",
    "commitChanges.manual": "New fields detected. You will be able to choose which changes to include in the preset.",
    "commitChanges.manual.hold.0": "Hold",
    "commitChanges.manual.hold.1": "to choose which changes to commit to the preset.",
    "commitChanges.saveAll.hold.0": "Hold",
    "commitChanges.saveAll.hold.1": "to save all changes.",
    "commitChanges.saveInPreset.hold.0": "Hold",
    "commitChanges.saveInPreset.hold.1": "to only save changes to fields that are already included in the preset.",
    "commitChanges/error": "Failed to commit changes to the preset.",
    "commitChanges.manual/description": "Choose which changes to include in the preset.",
    "saveAs": "Save As New...",
    "presetNamePlaceholder": "Enter a name for the preset...",
    "cannotCommitChangesLegacy": "This is a legacy preset and cannot be modified. You can create a copy by using \"Save As New...\".",
    "cannotCommitChangesNoChanges": "No changes to commit.",
    "emptyNoUnsaved": "Select a Preset...",
    "emptyWithUnsaved": "Unsaved Preset",
    "saveEmptyWithUnsaved": "Save Preset As...",
    "saveConfirm": "Save",
    "saveCancel": "Cancel",
    "saving": "Saving...",
    "save/error": "Failed to save preset.",
    "deselect": "Deselect Preset",
    "deselect/error": "Failed to deselect preset.",
    "select/error": "Failed to select preset.",
    "delete/error": "Failed to delete preset.",
    "discardChanges": "Discard Unsaved",
    "discardChanges/info": "Discard all uncommitted changes and restore the preset to its original state",
    "newEmptyPreset": "Create new empty preset...",
    "contextMenuSelect": "Select Preset",
    "contextMenuDelete": "Delete"
  },

  "flashAttentionWarning": "Flash Attention is an experimental feature that may cause issues with some models. If you encounter problems, try disabling it.",

  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}
