{
  "noInstanceSelected": "هیچ نمونه مدلی انتخاب نشده است",
  "resetToDefault": "بازنشانی",
  "showAdvancedSettings": "نمایش تنظیمات پیشرفته",
  "showAll": "همه",
  "basicSettings": "پایه",
  "configSubtitle": "بارگذاری یا ذخیره پیش‌تنظیمات و آزمایش با پارامترهای مدل",
  "inferenceParameters/title": "پارامترهای پیش‌بینی",
  "inferenceParameters/info": "آزمایش با پارامترهایی که بر پیش‌بینی تأثیر می‌گذارند.",
  "generalParameters/title": "عمومی",
  "samplingParameters/title": "نمونه‌برداری",
  "basicTab": "پایه",
  "advancedTab": "پیشرفته",
  "advancedTab/title": "🧪 پیکربندی پیشرفته",
  "advancedTab/expandAll": "گسترش همه",
  "advancedTab/overridesTitle": "بازنویسی‌های پیکربندی",
  "advancedTab/noConfigsText": "شما هیچ تغییر ذخیره نشده‌ای ندارید - مقادیر بالا را ویرایش کنید تا بازنویسی‌ها را اینجا ببینید.",
  "loadInstanceFirst": "یک مدل بارگذاری کنید تا پارامترهای قابل پیکربندی را مشاهده کنید",
  "noListedConfigs": "هیچ پارامتر قابل پیکربندی وجود ندارد",
  "generationParameters/info": "آزمایش با پارامترهای پایه که بر تولید متن تأثیر می‌گذارند.",
  "loadParameters/title": "پارامترهای بارگذاری",
  "loadParameters/description": "تنظیمات برای کنترل نحوه مقداردهی اولیه و بارگذاری مدل در حافظه.",
  "loadParameters/reload": "بارگذاری مجدد برای اعمال تغییرات",
  "loadParameters/reload/error": "بارگذاری مجدد مدل ناموفق بود",
  "discardChanges": "لغو تغییرات",
  "loadModelToSeeOptions": "یک مدل بارگذاری کنید تا گزینه‌ها را ببینید",
  "schematicsError.title": "طرح‌های پیکربندی شامل خطاهایی در فیلدهای زیر است:",
  "manifestSections": {
    "structuredOutput/title": "خروجی ساختاریافته",
    "speculativeDecoding/title": "رمزگشایی حدسی",
    "sampling/title": "نمونه‌برداری",
    "settings/title": "تنظیمات",
    "toolUse/title": "استفاده از ابزار",
    "promptTemplate/title": "قالب دستورالعمل"
  },

  "llm.prediction.systemPrompt/title": "دستورالعمل سیستم",
  "llm.prediction.systemPrompt/description": "از این فیلد برای ارائه دستورالعمل‌های پس‌زمینه به مدل استفاده کنید، مانند مجموعه‌ای از قوانین، محدودیت‌ها یا الزامات کلی.",
  "llm.prediction.systemPrompt/subTitle": "دستورالعمل‌های هوش مصنوعی",
  "llm.prediction.temperature/title": "دما",
  "llm.prediction.temperature/subTitle": "میزان تصادفی بودن. 0 هر بار نتیجه یکسانی خواهد داشت، در حالی که مقادیر بالاتر خلاقیت و تنوع را افزایش می‌دهند",
  "llm.prediction.temperature/info": "از مستندات راهنمای llama.cpp: \"مقدار پیش‌فرض <{{dynamicValue}}> است که تعادل بین تصادفی بودن و قطعیت را فراهم می‌کند. در حالت افراطی، دمای 0 همیشه محتمل‌ترین توکن بعدی را انتخاب می‌کند که منجر به خروجی‌های یکسان در هر اجرا می‌شود\"",
  "llm.prediction.llama.sampling/title": "نمونه‌برداری",
  "llm.prediction.topKSampling/title": "نمونه‌برداری Top K",
  "llm.prediction.topKSampling/subTitle": "توکن بعدی را به یکی از k توکن محتمل‌ترین محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.topKSampling/info": "از مستندات راهنمای llama.cpp:\n\nنمونه‌برداری Top-k روشی برای تولید متن است که توکن بعدی را فقط از k توکن محتمل‌ترین پیش‌بینی شده توسط مدل انتخاب می‌کند.\n\nاین به کاهش خطر تولید توکن‌های کم‌احتمال یا بی‌معنی کمک می‌کند، اما ممکن است تنوع خروجی را نیز محدود کند.\n\nمقدار بالاتر برای top-k (مثلاً 100) توکن‌های بیشتری را در نظر می‌گیرد و منجر به متن متنوع‌تر می‌شود، در حالی که مقدار پایین‌تر (مثلاً 10) بر توکن‌های محتمل‌ترین تمرکز می‌کند و متن محافظه‌کارانه‌تری تولید می‌کند.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",
  "llm.prediction.llama.cpuThreads/title": "نخ‌های CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "تعداد نخ‌های CPU برای استفاده در زمان استنتاج",
  "llm.prediction.llama.cpuThreads/info": "تعداد نخ‌هایی که در زمان محاسبه استفاده می‌شوند. افزایش تعداد نخ‌ها همیشه با عملکرد بهتر همبستگی ندارد. مقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.prediction.maxPredictedTokens/title": "محدودیت طول پاسخ",
  "llm.prediction.maxPredictedTokens/subTitle": "اختیاری: طول پاسخ هوش مصنوعی را محدود کنید",
  "llm.prediction.maxPredictedTokens/info": "حداکثر طول پاسخ چت‌بات را کنترل کنید. برای تنظیم محدودیت در حداکثر طول پاسخ روشن کنید، یا خاموش کنید تا چت‌بات تصمیم بگیرد چه زمانی متوقف شود.",
  "llm.prediction.maxPredictedTokens/inputLabel": "حداکثر طول پاسخ (توکن)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "حدود {{maxWords}} کلمه",
  "llm.prediction.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.repeatPenalty/subTitle": "میزان جلوگیری از تکرار همان توکن",
  "llm.prediction.repeatPenalty/info": "از مستندات راهنمای llama.cpp: \"به جلوگیری از تولید متن تکراری یا یکنواخت توسط مدل کمک می‌کند.\n\nمقدار بالاتر (مثلاً 1.5) تکرارها را شدیدتر جریمه می‌کند، در حالی که مقدار پایین‌تر (مثلاً 0.9) ملایم‌تر خواهد بود.\" • مقدار پیش‌فرض <{{dynamicValue}}> است",
  "llm.prediction.minPSampling/title": "نمونه‌برداری Min P",
  "llm.prediction.minPSampling/subTitle": "حداقل احتمال پایه برای انتخاب یک توکن برای خروجی",
  "llm.prediction.minPSampling/info": "از مستندات راهنمای llama.cpp:\n\nحداقل احتمال برای در نظر گرفتن یک توکن، نسبت به احتمال محتمل‌ترین توکن. باید در [0, 1] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",
  "llm.prediction.topPSampling/title": "نمونه‌برداری Top P",
  "llm.prediction.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های بعدی ممکن. مشابه دما عمل می‌کند",
  "llm.prediction.topPSampling/info": "از مستندات راهنمای llama.cpp:\n\nنمونه‌برداری Top-p، که به عنوان نمونه‌برداری هسته‌ای نیز شناخته می‌شود، روش دیگری برای تولید متن است که توکن بعدی را از زیرمجموعه‌ای از توکن‌ها که در مجموع احتمال تجمعی حداقل p دارند انتخاب می‌کند.\n\nاین روش با در نظر گرفتن هم احتمالات توکن‌ها و هم تعداد توکن‌هایی که باید نمونه‌برداری شوند، تعادلی بین تنوع و کیفیت فراهم می‌کند.\n\nمقدار بالاتر برای top-p (مثلاً 0.95) منجر به متن متنوع‌تر می‌شود، در حالی که مقدار پایین‌تر (مثلاً 0.5) متن متمرکزتر و محافظه‌کارانه‌تری تولید می‌کند. باید در (0, 1] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",
  "llm.prediction.stopStrings/title": "رشته‌های توقف",
  "llm.prediction.stopStrings/subTitle": "رشته‌هایی که باید مدل را از تولید توکن‌های بیشتر متوقف کنند",
  "llm.prediction.stopStrings/info": "رشته‌های خاصی که هنگام مواجهه با آن‌ها، مدل از تولید توکن‌های بیشتر متوقف می‌شود",
  "llm.prediction.stopStrings/placeholder": "یک رشته وارد کنید و ⏎ را فشار دهید",
  "llm.prediction.contextOverflowPolicy/title": "سرریز متن",
  "llm.prediction.contextOverflowPolicy/subTitle": "نحوه رفتار مدل زمانی که گفتگو بیش از حد بزرگ می‌شود",
  "llm.prediction.contextOverflowPolicy/info": "تصمیم بگیرید که وقتی گفتگو از اندازه حافظه کاری مدل ('متن') فراتر می‌رود چه کاری انجام شود",
  "llm.prediction.llama.frequencyPenalty/title": "جریمه فرکانس",
  "llm.prediction.llama.presencePenalty/title": "جریمه حضور",
  "llm.prediction.llama.tailFreeSampling/title": "نمونه‌برداری Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "نمونه‌برداری معمولی محلی",
  "llm.prediction.llama.xtcProbability/title": "احتمال نمونه‌برداری XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "نمونه‌بردار XTC (حذف انتخاب‌های برتر) فقط با این احتمال برای هر توکن تولید شده فعال می‌شود. نمونه‌برداری XTC می‌تواند خلاقیت را افزایش دهد و کلیشه‌ها را کاهش دهد",
  "llm.prediction.llama.xtcProbability/info": "نمونه‌برداری XTC (حذف انتخاب‌های برتر) فقط با این احتمال، برای هر توکن تولید شده فعال می‌شود. نمونه‌برداری XTC معمولاً خلاقیت را افزایش می‌دهد و کلیشه‌ها را کاهش می‌دهد",
  "llm.prediction.llama.xtcThreshold/title": "آستانه نمونه‌برداری XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "آستانه XTC (حذف انتخاب‌های برتر). با احتمال `xtc-probability`، به دنبال توکن‌هایی با احتمالات بین `xtc-threshold` و 0.5 بگردید و همه این توکن‌ها را به جز کم‌احتمال‌ترین آن‌ها حذف کنید",
  "llm.prediction.llama.xtcThreshold/info": "آستانه XTC (حذف انتخاب‌های برتر). با احتمال `xtc-probability`، به دنبال توکن‌هایی با احتمالات بین `xtc-threshold` و 0.5 بگردید و همه این توکن‌ها را به جز کم‌احتمال‌ترین آن‌ها حذف کنید",
  "llm.prediction.mlx.topKSampling/title": "نمونه‌برداری Top K",
  "llm.prediction.mlx.topKSampling/subTitle": "توکن بعدی را به یکی از k توکن محتمل‌ترین محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.mlx.topKSampling/info": "توکن بعدی را به یکی از k توکن محتمل‌ترین محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topKSampling/title": "نمونه‌برداری Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "توکن بعدی را به یکی از k توکن محتمل‌ترین محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topKSampling/info": "از مستندات ONNX:\n\nتعداد توکن‌های واژگان با بالاترین احتمال برای نگهداری در فیلتر top-k\n\n• این فیلتر به طور پیش‌فرض خاموش است",
  "llm.prediction.onnx.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.onnx.repeatPenalty/subTitle": "میزان جلوگیری از تکرار همان توکن",
  "llm.prediction.onnx.repeatPenalty/info": "مقدار بالاتر مدل را از تکرار خود باز می‌دارد",
  "llm.prediction.onnx.topPSampling/title": "نمونه‌برداری Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های بعدی ممکن. مشابه دما عمل می‌کند",
  "llm.prediction.onnx.topPSampling/info": "از مستندات ONNX:\n\nفقط توکن‌های محتمل‌ترین با احتمالاتی که به TopP یا بالاتر می‌رسند برای تولید نگهداری می‌شوند\n\n• این فیلتر به طور پیش‌فرض خاموش است",
  "llm.prediction.seed/title": "دانه",
  "llm.prediction.structured/title": "خروجی ساختاریافته",
  "llm.prediction.structured/info": "خروجی ساختاریافته",
  "llm.prediction.structured/description": "پیشرفته: می‌توانید یک [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) برای اعمال یک فرمت خروجی خاص از مدل ارائه دهید. برای اطلاعات بیشتر [مستندات](https://lmstudio.ai/docs/advanced/structured-output) را بخوانید",
  "llm.prediction.tools/title": "استفاده از ابزار",
  "llm.prediction.tools/description": "پیشرفته: می‌توانید لیستی سازگار با JSON از ابزارها برای درخواست فراخوانی‌ها توسط مدل ارائه دهید. برای اطلاعات بیشتر [مستندات](https://lmstudio.ai/docs/advanced/tool-use) را بخوانید",
  "llm.prediction.tools/serverPageDescriptionAddon": "این را از طریق بدنه درخواست به عنوان `tools` هنگام استفاده از API سرور ارسال کنید",
  "llm.prediction.promptTemplate/title": "قالب دستورالعمل",
  "llm.prediction.promptTemplate/subTitle": "فرمتی که پیام‌ها در چت به مدل ارسال می‌شوند. تغییر این ممکن است رفتار غیرمنتظره‌ای ایجاد کند - مطمئن شوید که می‌دانید چه می‌کنید!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "توکن‌های پیش‌نویس برای تولید",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "تعداد توکن‌هایی که باید با مدل پیش‌نویس برای هر توکن مدل اصلی تولید شوند. نقطه شیرین محاسبه در مقابل پاداش را پیدا کنید",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "آستانه احتمال پیش‌نویس",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "تا زمانی که احتمال یک توکن زیر این آستانه نرود به پیش‌نویس ادامه دهید. مقادیر بالاتر معمولاً به معنای ریسک کمتر، پاداش کمتر است",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "حداقل اندازه پیش‌نویس",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "پیش‌نویس‌های کوچکتر از این توسط مدل اصلی نادیده گرفته می‌شوند. مقادیر بالاتر معمولاً به معنای ریسک کمتر، پاداش کمتر است",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "حداکثر اندازه پیش‌نویس",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "حداکثر تعداد توکن مجاز در یک پیش‌نویس. سقف اگر همه احتمالات توکن > آستانه باشند. مقادیر پایین‌تر معمولاً به معنای ریسک کمتر، پاداش کمتر است",
  "llm.prediction.speculativeDecoding.draftModel/title": "مدل پیش‌نویس",
  "llm.prediction.reasoning.parsing/title": "تجزیه بخش استدلال",
  "llm.prediction.reasoning.parsing/subTitle": "نحوه تجزیه بخش‌های استدلال در خروجی مدل",

  "llm.load.contextLength/title": "طول متن",
  "llm.load.contextLength/subTitle": "حداکثر تعداد توکن‌هایی که مدل می‌تواند در یک دستورالعمل به آن‌ها توجه کند. برای راه‌های بیشتر برای مدیریت این، گزینه‌های سرریز گفتگو را در \"پارامترهای استنتاج\" ببینید",
  "llm.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل می‌تواند همزمان در نظر بگیرد را مشخص می‌کند، که بر میزان متنی که در طول پردازش حفظ می‌کند تأثیر می‌گذارد",
  "llm.load.contextLength/warning": "تنظیم مقدار بالا برای طول متن می‌تواند تأثیر قابل توجهی بر استفاده از حافظه داشته باشد",
  "llm.load.seed/title": "دانه",
  "llm.load.seed/subTitle": "دانه برای مولد اعداد تصادفی استفاده شده در تولید متن. -1 تصادفی است",
  "llm.load.seed/info": "دانه تصادفی: دانه را برای تولید اعداد تصادفی تنظیم می‌کند تا نتایج قابل تکرار را تضمین کند",

  "llm.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "llm.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی برای پردازش در یک زمان. افزایش این مقدار عملکرد را به هزینه استفاده از حافظه افزایش می‌دهد",
  "llm.load.llama.evalBatchSize/info": "تعداد مثال‌هایی که در یک دسته در طول ارزیابی با هم پردازش می‌شوند را تنظیم می‌کند، که بر سرعت و استفاده از حافظه تأثیر می‌گذارد",
  "llm.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای جاسازی‌های موقعیتی چرخشی (RoPE). افزایش این مقدار ممکن است عملکرد بهتری در طول‌های متن بالا را فعال کند",
  "llm.load.llama.ropeFrequencyBase/info": "[پیشرفته] فرکانس پایه را برای رمزگذاری موقعیتی چرخشی تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",
  "llm.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "طول متن با این فاکتور مقیاس می‌شود تا متن مؤثر با استفاده از RoPE گسترش یابد",
  "llm.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس فرکانس را برای رمزگذاری موقعیتی چرخشی تغییر می‌دهد تا دقت رمزگذاری موقعیتی را کنترل کند",
  "llm.load.llama.acceleration.offloadRatio/title": "تخلیه GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل مجزا برای محاسبه روی GPU برای شتاب GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU تخلیه شوند را تنظیم کنید.",
  "llm.load.llama.flashAttention/title": "توجه فلش",
  "llm.load.llama.flashAttention/subTitle": "استفاده از حافظه و زمان تولید را در برخی مدل‌ها کاهش می‌دهد",
  "llm.load.llama.flashAttention/info": "مکانیزم‌های توجه را برای پردازش سریع‌تر و کارآمدتر تسریع می‌کند",
  "llm.load.numExperts/title": "تعداد متخصصان",
  "llm.load.numExperts/subTitle": "تعداد متخصصانی که باید در مدل استفاده شوند",
  "llm.load.numExperts/info": "تعداد متخصصانی که باید در مدل استفاده شوند",
  "llm.load.llama.keepModelInMemory/title": "نگهداری مدل در حافظه",
  "llm.load.llama.keepModelInMemory/subTitle": "حتی در صورت تخلیه به GPU، حافظه سیستم را برای مدل رزرو کنید. عملکرد را بهبود می‌بخشد اما به RAM سیستم بیشتری نیاز دارد",
  "llm.load.llama.keepModelInMemory/info": "از تخلیه مدل به دیسک جلوگیری می‌کند و دسترسی سریع‌تر را به هزینه استفاده بیشتر از RAM تضمین می‌کند",
  "llm.load.llama.useFp16ForKVCache/title": "استفاده از FP16 برای حافظه نهان KV",
  "llm.load.llama.useFp16ForKVCache/info": "با ذخیره حافظه نهان در نیم‌دقت (FP16) استفاده از حافظه را کاهش می‌دهد",
  "llm.load.llama.tryMmap/title": "تلاش برای mmap()",
  "llm.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن این ممکن است عملکرد را زمانی که مدل بزرگتر از RAM سیستم موجود است بهبود بخشد",
  "llm.load.llama.tryMmap/info": "فایل‌های مدل را مستقیماً از دیسک به حافظه بارگذاری می‌کند",
  "llm.load.llama.cpuThreadPoolSize/title": "اندازه استخر نخ CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "تعداد نخ‌های CPU برای تخصیص به استخر نخ استفاده شده برای محاسبه مدل",
  "llm.load.llama.cpuThreadPoolSize/info": "تعداد نخ‌های CPU برای تخصیص به استخر نخ استفاده شده برای محاسبه مدل. افزایش تعداد نخ‌ها همیشه با عملکرد بهتر همبستگی ندارد. مقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.load.llama.kCacheQuantizationType/title": "نوع کوانتیزاسیون حافظه نهان K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "مقادیر پایین‌تر استفاده از حافظه را کاهش می‌دهند اما ممکن است کیفیت را کاهش دهند. تأثیر به طور قابل توجهی بین مدل‌ها متفاوت است.",
  "llm.load.llama.vCacheQuantizationType/title": "نوع کوانتیزاسیون حافظه نهان V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "مقادیر پایین‌تر استفاده از حافظه را کاهش می‌دهند اما ممکن است کیفیت را کاهش دهند. تأثیر به طور قابل توجهی بین مدل‌ها متفاوت است.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ اگر توجه فلش فعال نیست، باید این مقدار را غیرفعال کنید",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "فقط زمانی می‌تواند روشن شود که توجه فلش فعال باشد",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ هنگام استفاده از F32 باید توجه فلش را غیرفعال کنید",
  "llm.load.mlx.kvCacheBits/title": "کوانتیزاسیون حافظه نهان KV",
  "llm.load.mlx.kvCacheBits/subTitle": "تعداد بیت‌هایی که حافظه نهان KV باید به آن کوانتیزه شود",
  "llm.load.mlx.kvCacheBits/info": "تعداد بیت‌هایی که حافظه نهان KV باید به آن کوانتیزه شود",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "تنظیم طول متن هنگام استفاده از کوانتیزاسیون حافظه نهان KV نادیده گرفته می‌شود",
  "llm.load.mlx.kvCacheGroupSize/title": "کوانتیزاسیون حافظه نهان KV: اندازه گروه",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "اندازه گروه در طول عملیات کوانتیزاسیون برای حافظه نهان KV. اندازه گروه بالاتر استفاده از حافظه را کاهش می‌دهد اما ممکن است کیفیت را کاهش دهد",
  "llm.load.mlx.kvCacheGroupSize/info": "تعداد بیت‌هایی که حافظه نهان KV باید به آن کوانتیزه شود",
  "llm.load.mlx.kvCacheQuantizationStart/title": "کوانتیزاسیون حافظه نهان KV: شروع کوانتیزاسیون وقتی متن از این طول عبور می‌کند",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "آستانه طول متن برای شروع کوانتیزاسیون حافظه نهان KV",
  "llm.load.mlx.kvCacheQuantizationStart/info": "آستانه طول متن برای شروع کوانتیزاسیون حافظه نهان KV",
  "llm.load.mlx.kvCacheQuantization/title": "کوانتیزاسیون حافظه نهان KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "حافظه نهان KV مدل را کوانتیزه کنید. این ممکن است منجر به تولید سریع‌تر و استفاده کمتر از حافظه شود،\nبه هزینه کیفیت خروجی مدل.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "بیت‌های کوانتیزاسیون حافظه نهان KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "تعداد بیت‌هایی که حافظه نهان KV باید به آن کوانتیزه شود",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "بیت‌ها",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "استراتژی اندازه گروه",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "دقت",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "متوازن",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "سریع",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "پیشرفته: پیکربندی 'اندازه گروه matmul کوانتیزه شده'\n\n• دقت = اندازه گروه 32\n• متوازن = اندازه گروه 64\n• سریع = اندازه گروه 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "شروع کوانتیزاسیون وقتی متن به این طول می‌رسد",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "وقتی متن به این تعداد توکن می‌رسد،\nشروع به کوانتیزاسیون حافظه نهان KV کنید",

  "embedding.load.contextLength/title": "طول متن",
  "embedding.load.contextLength/subTitle": "حداکثر تعداد توکن‌هایی که مدل می‌تواند در یک دستورالعمل به آن‌ها توجه کند. برای راه‌های بیشتر برای مدیریت این، گزینه‌های سرریز گفتگو را در \"پارامترهای استنتاج\" ببینید",
  "embedding.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل می‌تواند همزمان در نظر بگیرد را مشخص می‌کند، که بر میزان متنی که در طول پردازش حفظ می‌کند تأثیر می‌گذارد",
  "embedding.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای جاسازی‌های موقعیتی چرخشی (RoPE). افزایش این مقدار ممکن است عملکرد بهتری در طول‌های متن بالا را فعال کند",
  "embedding.load.llama.ropeFrequencyBase/info": "[پیشرفته] فرکانس پایه را برای رمزگذاری موقعیتی چرخشی تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیتی تأثیر می‌گذارد",
  "embedding.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "embedding.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی برای پردازش در یک زمان. افزایش این مقدار عملکرد را به هزینه استفاده از حافظه افزایش می‌دهد",
  "embedding.load.llama.evalBatchSize/info": "تعداد توکن‌هایی که در یک دسته در طول ارزیابی با هم پردازش می‌شوند را تنظیم می‌کند",
  "embedding.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "طول متن با این فاکتور مقیاس می‌شود تا متن مؤثر با استفاده از RoPE گسترش یابد",
  "embedding.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس فرکانس را برای رمزگذاری موقعیتی چرخشی تغییر می‌دهد تا دقت رمزگذاری موقعیتی را کنترل کند",
  "embedding.load.llama.acceleration.offloadRatio/title": "تخلیه GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل مجزا برای محاسبه روی GPU برای شتاب GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU تخلیه شوند را تنظیم کنید.",
  "embedding.load.llama.keepModelInMemory/title": "نگهداری مدل در حافظه",
  "embedding.load.llama.keepModelInMemory/subTitle": "حتی در صورت تخلیه به GPU، حافظه سیستم را برای مدل رزرو کنید. عملکرد را بهبود می‌بخشد اما به RAM سیستم بیشتری نیاز دارد",
  "embedding.load.llama.keepModelInMemory/info": "از تخلیه مدل به دیسک جلوگیری می‌کند و دسترسی سریع‌تر را به هزینه استفاده بیشتر از RAM تضمین می‌کند",
  "embedding.load.llama.tryMmap/title": "تلاش برای mmap()",
  "embedding.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن این ممکن است عملکرد را زمانی که مدل بزرگتر از RAM سیستم موجود است بهبود بخشد"
}
