{
  "noInstanceSelected": "هیچ نمونه مدلی انتخاب نشده است",
  "resetToDefault": "بازنشانی",
  "showAdvancedSettings": "نمایش تنظیمات پیشرفته",
  "showAll": "همه",
  "basicSettings": "پایه‌ای",
  "configSubtitle": "تنظیمات را بارگذاری یا ذخیره کنید و با تغییر پارامترهای مدل آزمایش کنید",
  "inferenceParameters/title": "پارامترهای پیش‌بینی",
  "inferenceParameters/info": "با پارامترهایی که بر پیش‌بینی تاثیر می‌گذارند، آزمایش کنید.",
  "generalParameters/title": "عمومی",
  "samplingParameters/title": "نمونه‌گیری",
  "basicTab": "پایه‌ای",
  "advancedTab": "پیشرفته",
  "advancedTab/title": "🧪 تنظیمات پیشرفته",
  "advancedTab/expandAll": "باز کردن همه",
  "advancedTab/overridesTitle": "تغییرات تنظیمات",
  "advancedTab/noConfigsText": "هیچ تغییری ذخیره نشده - مقادیر بالا را ویرایش کنید تا تغییرات اینجا نمایش داده شوند.",
  "loadInstanceFirst": "مدلی بارگذاری کنید تا پارامترهای قابل تنظیم نمایش داده شوند",
  "noListedConfigs": "هیچ پارامتر قابل تنظیمی وجود ندارد",
  "generationParameters/info": "با پارامترهای پایه‌ای که بر تولید متن تاثیر دارند، آزمایش کنید.",
  "loadParameters/title": "بارگذاری پارامترها",
  "loadParameters/description": "تنظیماتی برای کنترل نحوه بارگذاری و مقداردهی اولیه مدل در حافظه.",
  "loadParameters/reload": "برای اعمال تغییرات مجدد بارگذاری کنید",
  "discardChanges": "لغو تغییرات",
  "loadModelToSeeOptions": "مدلی بارگذاری کنید تا گزینه‌ها نمایش داده شوند",
  "llm.prediction.systemPrompt/title": "پیام سیستمی",
  "llm.prediction.systemPrompt/description": "از این فیلد برای ارائه دستورالعمل‌های پس‌زمینه به مدل، مانند قوانین، محدودیت‌ها یا الزامات کلی استفاده کنید.",
  "llm.prediction.systemPrompt/subTitle": "راهنمایی‌ها برای هوش مصنوعی",
  "llm.prediction.temperature/title": "دما",
  "llm.prediction.temperature/subTitle": "میزان تصادفی بودن ورودی. مقدار ۰ همیشه نتیجه یکسان تولید می‌کند، در حالی که مقادیر بالاتر خلاقیت و تنوع را افزایش می‌دهند",
  "llm.prediction.temperature/info": "از مستندات llama.cpp: \"مقدار پیش‌فرض <{{dynamicValue}}> است که بین تصادفی بودن و تعیین‌پذیری تعادل ایجاد می‌کند. در حد نهایی، دمای ۰ همیشه محتمل‌ترین توکن بعدی را انتخاب می‌کند، که به خروجی‌های یکسان در هر اجرا منجر می‌شود\"",
  "llm.prediction.llama.sampling/title": "نمونه‌گیری",
  "llm.prediction.topKSampling/title": "نمونه‌گیری Top K",
  "llm.prediction.topKSampling/subTitle": "توکن بعدی را به یکی از k توکن‌های با احتمال بالا محدود می‌کند. مشابه دما عمل می‌کند",
  "llm.prediction.topKSampling/info": "از مستندات llama.cpp:\n\nنمونه‌گیری Top K روشی برای تولید متن است که فقط توکن بعدی را از میان k توکن‌های محتمل‌تر انتخاب می‌کند.\n\nاین روش خطر تولید توکن‌های کم‌احتمال یا غیرمنطقی را کاهش می‌دهد، اما ممکن است تنوع خروجی را نیز محدود کند.\n\nمقدار بالاتر برای k (مثلاً ۱۰۰) توکن‌های بیشتری را در نظر می‌گیرد و به متن متنوع‌تر منجر می‌شود، در حالی که مقدار پایین‌تر (مثلاً ۱۰) بر محتمل‌ترین توکن‌ها تمرکز دارد و متنی محافظه‌کارانه‌تر تولید می‌کند.\n\n• مقدار پیش‌فرض <{{dynamicValue}}> است",
  "llm.prediction.llama.cpuThreads/title": "رشته‌های CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "تعداد رشته‌های CPU که در طول پیش‌بینی استفاده می‌شوند",
  "llm.prediction.llama.cpuThreads/info": "تعداد رشته‌هایی که در طول محاسبات استفاده می‌شوند. افزایش تعداد رشته‌ها همیشه با عملکرد بهتر همبسته نیست. مقدار پیش‌فرض <{{dynamicValue}}> است.",
  "llm.prediction.maxPredictedTokens/title": "محدودیت طول پاسخ",
  "llm.prediction.maxPredictedTokens/subTitle": "اختیاری: طول پاسخ هوش مصنوعی را محدود کنید",
  "llm.prediction.maxPredictedTokens/info": "حداکثر طول پاسخ چت‌بات را کنترل کنید. برای محدود کردن حداکثر طول پاسخ روشن کنید، یا خاموش کنید تا چت‌بات خودش تصمیم بگیرد کجا متوقف شود.",
  "llm.prediction.maxPredictedTokens/inputLabel": "حداکثر طول پاسخ (توکن‌ها)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "حدود {{maxWords}} کلمه",
  "llm.prediction.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.repeatPenalty/subTitle": "چقدر تکرار همان توکن را کاهش دهیم",
  "llm.prediction.repeatPenalty/info": "از مستندات llama.cpp: \"به جلوگیری از تولید متن تکراری یا یکنواخت کمک می‌کند.\n\nمقدار بالاتر (مثلاً ۱.۵) تکرارها را به شدت جریمه می‌کند، در حالی که مقدار پایین‌تر (مثلاً ۰.۹) آسان‌گیرتر است.\" • مقدار پیش‌فرض <{{dynamicValue}}> است""llm.prediction.minPSampling/title": "حداقل نمونه‌برداری P",
  "llm.prediction.minPSampling/subTitle": "حداقل احتمال پایه برای انتخاب یک توکن جهت خروجی",
  "llm.prediction.minPSampling/info": "از مستندات کمک llama.cpp:\n\nحداقل احتمالی که برای یک توکن در نظر گرفته می‌شود، نسبت به احتمال توکن با بیشترین احتمال. باید در بازه [0, 1] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}>",
  "llm.prediction.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های ممکن بعدی. مشابه با دما عمل می‌کند",
  "llm.prediction.topPSampling/info": "از مستندات کمک llama.cpp:\n\nنمونه‌برداری Top-p، که به نام نمونه‌برداری هسته نیز شناخته می‌شود، یک روش دیگر برای تولید متن است که توکن بعدی را از یک زیرمجموعه از توکن‌ها انتخاب می‌کند که احتمال تجمعی آن‌ها حداقل p باشد.\n\nاین روش تعادلی بین تنوع و کیفیت ایجاد می‌کند، با در نظر گرفتن همزمان احتمال توکن‌ها و تعداد توکن‌هایی که باید از آن‌ها نمونه‌برداری شود.\n\nیک مقدار بالاتر برای top-p (مثلاً 0.95) باعث تولید متن متنوع‌تری خواهد شد، در حالی که مقدار پایین‌تر (مثلاً 0.5) متن متمرکزتر و محتاطانه‌تری ایجاد می‌کند. باید در بازه (0, 1] باشد.\n\n• مقدار پیش‌فرض <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "رشته‌های توقف",
  "llm.prediction.stopStrings/subTitle": "رشته‌هایی که باید مدل را از تولید توکن‌های بیشتر متوقف کنند",
  "llm.prediction.stopStrings/info": "رشته‌های خاصی که هنگام برخورد با آن‌ها مدل از تولید توکن‌های بیشتر متوقف می‌شود",
  "llm.prediction.stopStrings/placeholder": "یک رشته وارد کنید و کلید ⏎ را بزنید",
  "llm.prediction.contextOverflowPolicy/title": "اضافه بار زمینه",
  "llm.prediction.contextOverflowPolicy/subTitle": "چگونه مدل باید رفتار کند زمانی که مکالمه بیش از حد برای پردازش آن بزرگ می‌شود",
  "llm.prediction.contextOverflowPolicy/info": "تصمیم بگیرید که چه کاری باید انجام دهید وقتی که مکالمه از اندازه حافظه کاری مدل ('زمینه') فراتر رود",
  "llm.prediction.llama.frequencyPenalty/title": "جریمه فرکانس",
  "llm.prediction.llama.presencePenalty/title": "جریمه حضور",
  "llm.prediction.llama.tailFreeSampling/title": "نمونه‌برداری بدون دم",
  "llm.prediction.llama.locallyTypicalSampling/title": "نمونه‌برداری معمولی محلی",
  "llm.prediction.onnx.topKSampling/title": "نمونه‌برداری Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "محدود کردن توکن بعدی به یکی از k توکن با بالاترین احتمال. مشابه با دما عمل می‌کند",
  "llm.prediction.onnx.topKSampling/info": "از مستندات ONNX:\n\nتعداد توکن‌های واژگان با بالاترین احتمال که برای فیلتر کردن top-k نگه‌داشته می‌شوند\n\n• این فیلتر به‌طور پیش‌فرض غیرفعال است",
  "llm.prediction.onnx.repeatPenalty/title": "جریمه تکرار",
  "llm.prediction.onnx.repeatPenalty/subTitle": "چقدر باید از تکرار همان توکن جلوگیری شود",
  "llm.prediction.onnx.repeatPenalty/info": "یک مقدار بالاتر از تکرار جلوگیری می‌کند و مدل را از تکرار خود منصرف می‌کند",
  "llm.prediction.onnx.topPSampling/title": "نمونه‌برداری Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "حداقل احتمال تجمعی برای توکن‌های ممکن بعدی. مشابه با دما عمل می‌کند",
  "llm.prediction.onnx.topPSampling/info": "از مستندات ONNX:\n\nفقط توکن‌های با بیشترین احتمال که احتمالات آن‌ها به TopP یا بیشتر جمع می‌شود، برای تولید نگه‌داشته می‌شوند\n\n• این فیلتر به‌طور پیش‌فرض غیرفعال است",
  "llm.prediction.seed/title": "بذر",
  "llm.prediction.structured/title": "خروجی ساختاریافته",
  "llm.prediction.structured/info": "خروجی ساختاریافته",
  "llm.prediction.structured/description": "پیشرفته: شما می‌توانید یک اسکیما JSON فراهم کنید تا فرمت خاصی از خروجی را از مدل تحمیل کنید. برای اطلاعات بیشتر به [مستندات](https://lmstudio.ai/docs/advanced/structured-output) مراجعه کنید",
  "llm.prediction.promptTemplate/title": "الگوی درخواست",
  "llm.prediction.promptTemplate/subTitle": "فرمت پیام‌هایی که در چت به مدل ارسال می‌شود. تغییر این ممکن است رفتار غیرمنتظره‌ای ایجاد کند - مطمئن شوید که می‌دانید چه کاری انجام می‌دهید!""llm.load.contextLength/title": "طول زمینه",
  "llm.load.contextLength/subTitle": "حداکثر تعداد توکن‌هایی که مدل می‌تواند در یک درخواست بررسی کند. برای روش‌های دیگر مدیریت این موضوع، گزینه‌های \"پارامترهای استنتاج\" را ببینید",
  "llm.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل می‌تواند به طور همزمان بررسی کند، که بر میزان زمینه‌ای که در هنگام پردازش حفظ می‌کند تأثیر می‌گذارد",
  "llm.load.contextLength/warning": "تنظیم مقدار بالا برای طول زمینه می‌تواند تأثیر زیادی بر مصرف حافظه بگذارد",
  "llm.load.seed/title": "بذر",
  "llm.load.seed/subTitle": "بذری برای تولید اعداد تصادفی در تولید متن. -1 تصادفی است",
  "llm.load.seed/info": "بذر تصادفی: بذر تولید اعداد تصادفی را تنظیم می‌کند تا نتایج قابل تکرار تضمین شود",
  "llm.load.llama.evalBatchSize/title": "اندازه بچ ارزیابی",
  "llm.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی برای پردازش به طور همزمان. افزایش این تعداد باعث بهبود عملکرد با هزینه مصرف بیشتر حافظه می‌شود",
  "llm.load.llama.evalBatchSize/info": "تعداد نمونه‌هایی که در یک بچ به طور همزمان پردازش می‌شوند، که بر سرعت و مصرف حافظه تأثیر می‌گذارد",
  "llm.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای رمزگذاری موقعیت چرخشی (RoPE). افزایش این مقدار ممکن است عملکرد بهتری در طول زمینه‌های بالا فراهم کند",
  "llm.load.llama.ropeFrequencyBase/info": "[پیشرفته] فرکانس پایه برای رمزگذاری موقعیت چرخشی را تنظیم می‌کند که بر نحوه گنجاندن اطلاعات موقعیتی تأثیر می‌گذارد",
  "llm.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "طول زمینه با این فاکتور مقیاس می‌شود تا از RoPE برای گسترش زمینه مؤثر استفاده شود",
  "llm.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس فرکانس برای رمزگذاری موقعیت چرخشی را تغییر می‌دهد تا ریزجزئیات رمزگذاری موقعیت را کنترل کند",
  "llm.load.llama.acceleration.offloadRatio/title": "انتقال به GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل که باید برای تسریع با GPU محاسبه شوند",
  "llm.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU منتقل شوند را تنظیم کنید.",
  "llm.load.llama.flashAttention/title": "توجه فلش",
  "llm.load.llama.flashAttention/subTitle": "استفاده از حافظه و زمان تولید را در برخی مدل‌ها کاهش می‌دهد",
  "llm.load.llama.flashAttention/info": "مکانیزم‌های توجه را برای پردازش سریع‌تر و کارآمدتر تسریع می‌کند",
  "llm.load.numExperts/title": "تعداد متخصص‌ها",
  "llm.load.numExperts/subTitle": "تعداد متخصص‌هایی که باید در مدل استفاده شوند",
  "llm.load.numExperts/info": "تعداد متخصص‌هایی که باید در مدل استفاده شوند",
  "llm.load.llama.keepModelInMemory/title": "نگه داشتن مدل در حافظه",
  "llm.load.llama.keepModelInMemory/subTitle": "حافظه سیستم را برای مدل رزرو می‌کند، حتی زمانی که به GPU منتقل می‌شود. عملکرد را بهبود می‌بخشد اما نیاز به رم بیشتری دارد",
  "llm.load.llama.keepModelInMemory/info": "از انتقال مدل به دیسک جلوگیری می‌کند و دسترسی سریع‌تری را تضمین می‌کند، که به هزینه مصرف بیشتر رم است",
  "llm.load.llama.useFp16ForKVCache/title": "استفاده از FP16 برای کش KV",
  "llm.load.llama.useFp16ForKVCache/info": "مصرف حافظه را با ذخیره کش در دقت نیمه (FP16) کاهش می‌دهد",
  "llm.load.llama.tryMmap/title": "آزمایش mmap()",
  "llm.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن این گزینه می‌تواند عملکرد را زمانی که مدل از حافظه سیستم بیشتر است، بهبود بخشد",
  "llm.load.llama.tryMmap/info": "مدل را به طور مستقیم از دیسک به حافظه بارگذاری می‌کند""embedding.load.contextLength/title": "طول زمینه",
  "embedding.load.contextLength/subTitle": "حداکثر تعداد توکن‌هایی که مدل می‌تواند در یک درخواست پردازش کند. برای مشاهده گزینه‌های اضافی در مورد \"پارامترهای استنتاج\" به گزینه‌های Overflow گفتگو مراجعه کنید.",
  "embedding.load.contextLength/info": "حداکثر تعداد توکن‌هایی که مدل می‌تواند به طور همزمان در نظر بگیرد را مشخص می‌کند، که بر میزان زمینه‌ای که در طول پردازش نگه می‌دارد تأثیر می‌گذارد.",
  "embedding.load.llama.ropeFrequencyBase/title": "پایه فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "فرکانس پایه سفارشی برای رمزگذاری موقعیت چرخشی (RoPE). افزایش این مقدار ممکن است عملکرد بهتری در طول زمینه‌های طولانی‌تر ارائه دهد.",
  "embedding.load.llama.ropeFrequencyBase/info": "[پیشرفته] فرکانس پایه برای رمزگذاری موقعیت چرخشی را تنظیم می‌کند، که بر نحوه جاسازی اطلاعات موقعیت تأثیر می‌گذارد.",
  "embedding.load.llama.evalBatchSize/title": "اندازه دسته ارزیابی",
  "embedding.load.llama.evalBatchSize/subTitle": "تعداد توکن‌های ورودی که به طور همزمان پردازش می‌شوند. افزایش این مقدار عملکرد را بهبود می‌بخشد ولی مصرف حافظه را افزایش می‌دهد.",
  "embedding.load.llama.evalBatchSize/info": "تعداد توکن‌هایی که در یک دسته برای ارزیابی پردازش می‌شوند را تنظیم می‌کند.",
  "embedding.load.llama.ropeFrequencyScale/title": "مقیاس فرکانس RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "طول زمینه بر اساس این عامل مقیاس می‌شود تا زمینه مؤثر را با استفاده از RoPE گسترش دهد.",
  "embedding.load.llama.ropeFrequencyScale/info": "[پیشرفته] مقیاس‌گذاری فرکانس برای رمزگذاری موقعیت چرخشی را تغییر می‌دهد تا دقت رمزگذاری موقعیت را کنترل کند.",
  "embedding.load.llama.acceleration.offloadRatio/title": "بارگذاری GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "تعداد لایه‌های مدل که برای شتاب‌دهی GPU محاسبه می‌شوند.",
  "embedding.load.llama.acceleration.offloadRatio/info": "تعداد لایه‌هایی که باید به GPU منتقل شوند را تنظیم می‌کند.",
  "embedding.load.llama.keepModelInMemory/title": "حفظ مدل در حافظه",
  "embedding.load.llama.keepModelInMemory/subTitle": "حافظه سیستم را برای مدل رزرو می‌کند، حتی زمانی که به GPU منتقل شود. این کار عملکرد را بهبود می‌بخشد ولی به حافظه RAM بیشتری نیاز دارد.",
  "embedding.load.llama.keepModelInMemory/info": "از جابجایی مدل به دیسک جلوگیری می‌کند و دسترسی سریع‌تری را فراهم می‌آورد که با هزینه مصرف بیشتر RAM همراه است.",
  "embedding.load.llama.tryMmap/title": "استفاده از mmap()",
  "embedding.load.llama.tryMmap/subTitle": "زمان بارگذاری مدل را بهبود می‌بخشد. غیرفعال کردن این گزینه ممکن است عملکرد را زمانی که مدل بزرگ‌تر از RAM سیستم موجود است، بهبود دهد.",
  "embedding.load.llama.tryMmap/info": "مدل‌ها را مستقیماً از دیسک به حافظه بارگذاری می‌کند.",
  "embedding.load.seed/title": "بذر",
  "embedding.load.seed/subTitle": "بذری برای تولید اعداد تصادفی که در تولید متن استفاده می‌شود. -1 تصادفی است.",
  "embedding.load.seed/info": "بذر تصادفی: بذر تولید اعداد تصادفی را تنظیم می‌کند تا نتایج قابل تکرار را تضمین کند.",
  "presetTooltip": {
      "included/title": "مقادیر پیش‌فرض",
      "included/description": "فیلدهای زیر اعمال خواهند شد",
      "included/empty": "هیچ یک از فیلدهای این پیش‌فرض در این زمینه قابل اعمال نیستند.",
      "included/conflict": "از شما خواسته خواهد شد که انتخاب کنید آیا این مقدار اعمال شود یا خیر.",
      "separateLoad/title": "پیکربندی زمان بارگذاری",
      "separateLoad/description.1": "پیش‌فرض همچنین شامل پیکربندی زمان بارگذاری است. پیکربندی زمان بارگذاری برای مدل در سطح وسیع است و برای تأثیرگذاری نیاز به بارگذاری مجدد مدل دارد. نگه دارید",
      "separateLoad/description.2": "برای اعمال به",
      "separateLoad/description.3": ".",
      "excluded/title": "ممکن است اعمال نشود",
      "excluded/description": "فیلدهای زیر در پیش‌فرض گنجانده شده‌اند اما در زمینه فعلی قابل اعمال نیستند.",
      "legacy/title": "پیش‌فرض قدیمی",
      "legacy/description": "این پیش‌فرض یک پیش‌فرض قدیمی است. این پیش‌فرض شامل فیلدهایی است که یا به طور خودکار مدیریت می‌شوند، یا دیگر قابل اعمال نیستند."
  }
  "customInputs": {
      "string": {
          "emptyParagraph": "<خالی>"
      },
      "checkboxNumeric": {
          "off": "خاموش"
      },
      "stringArray": {
          "empty": "<خالی>"
      },
      "llmPromptTemplate": {
          "type": "نوع",
          "types.jinja/label": "الگو (Jinja)",
          "jinja.bosToken/label": "توکن BOS",
          "jinja.eosToken/label": "توکن EOS",
          "jinja.template/label": "الگو",
          "jinja/error": "عدم توانایی در تجزیه الگوی Jinja: {{error}}",
          "jinja/empty": "لطفاً یک الگوی Jinja وارد کنید.",
          "jinja/unlikelyToWork": "الگوی Jinja که وارد کرده‌اید احتمالاً کار نخواهد کرد زیرا به متغیر \"messages\" اشاره نمی‌کند. لطفاً دوباره بررسی کنید که آیا الگوی درستی وارد کرده‌اید.",
          "types.manual/label": "دستی",
          "manual.subfield.beforeSystem/label": "قبل از سیستم",
          "manual.subfield.beforeSystem/placeholder": "پیشوند سیستم را وارد کنید...",
          "manual.subfield.afterSystem/label": "بعد از سیستم",
          "manual.subfield.afterSystem/placeholder": "پسوند سیستم را وارد کنید...",
          "manual.subfield.beforeUser/label": "قبل از کاربر",
          "manual.subfield.beforeUser/placeholder": "پیشوند کاربر را وارد کنید...",
          "manual.subfield.afterUser/label": "بعد از کاربر",
          "manual.subfield.afterUser/placeholder": "پسوند کاربر را وارد کنید...",
          "manual.subfield.beforeAssistant/label": "قبل از دستیار",
          "manual.subfield.beforeAssistant/placeholder": "پیشوند دستیار را وارد کنید...",
          "manual.subfield.afterAssistant/label": "بعد از دستیار",
          "manual.subfield.afterAssistant/placeholder": "پسوند دستیار را وارد کنید...",
          "stopStrings/label": "رشته‌های توقف اضافی",
          "stopStrings/subTitle": "رشته‌های توقف خاص الگو که علاوه بر رشته‌های توقف مشخص شده توسط کاربر استفاده خواهند شد."
      },
      "contextLength": {
          "maxValueTooltip": "این حداکثر تعداد توکن‌هایی است که مدل برای پردازش آموزش دیده است. برای تنظیم زمینه به این مقدار کلیک کنید",
          "maxValueTextStart": "مدل تا",
          "maxValueTextEnd": "توکن را پشتیبانی می‌کند",
          "tooltipHint": "در حالی که یک مدل ممکن است از تعداد معینی توکن پشتیبانی کند، عملکرد ممکن است در صورتی که منابع سیستم شما نتواند بار را تحمل کند کاهش یابد - در افزایش این مقدار احتیاط کنید"
      },
      "contextOverflowPolicy": {
          "stopAtLimit": "متوقف شدن در حد",
          "stopAtLimitSub": "تولید متوقف می‌شود زمانی که حافظه مدل پر شود",
          "truncateMiddle": "برش وسط",
          "truncateMiddleSub": "پیام‌ها از وسط مکالمه حذف می‌شوند تا جا برای پیام‌های جدیدتر باز شود. مدل همچنان ابتدای مکالمه را به یاد خواهد داشت",
          "rollingWindow": "پنجره متحرک",
          "rollingWindowSub": "مدل همیشه جدیدترین پیام‌ها را دریافت خواهد کرد اما ممکن است ابتدای مکالمه را فراموش کند"
      },
      "llamaAccelerationOffloadRatio": {
          "max": "حداکثر",
          "off": "خاموش"
      }
  }
  "saveConflictResolution": {
      "title": "انتخاب کنید که کدام مقادیر در پیش‌تنظیم گنجانده شوند",
      "description": "انتخاب کنید که کدام مقادیر حفظ شوند",
      "instructions": "روی یک مقدار کلیک کنید تا آن را اضافه کنید",
      "userValues": "مقدار قبلی",
      "presetValues": "مقدار جدید",
      "confirm": "تأیید",
      "cancel": "لغو"
  },
  "applyConflictResolution": {
      "title": "کدام مقادیر را نگه داریم؟",
      "description": "شما تغییراتی دارید که با پیش‌تنظیم وارد شده تداخل دارند",
      "instructions": "روی یک مقدار کلیک کنید تا آن را نگه دارید",
      "userValues": "مقدار فعلی",
      "presetValues": "مقدار پیش‌تنظیم وارد شده",
      "confirm": "تأیید",
      "cancel": "لغو"
  },
  "empty": "<خالی>",
  "presets": {
      "title": "پیش‌تنظیم",
      "commitChanges": "ثبت تغییرات",
      "commitChanges/description": "تغییرات خود را در پیش‌تنظیم ثبت کنید.",
      "commitChanges.manual": "فیلدهای جدید شناسایی شده‌اند. شما قادر خواهید بود انتخاب کنید که کدام تغییرات در پیش‌تنظیم گنجانده شوند.",
      "commitChanges.manual.hold.0": "نگه دارید",
      "commitChanges.manual.hold.1": "برای انتخاب تغییراتی که باید در پیش‌تنظیم ثبت شوند.",
      "commitChanges.saveAll.hold.0": "نگه دارید",
      "commitChanges.saveAll.hold.1": "برای ذخیره همه تغییرات.",
      "commitChanges.saveInPreset.hold.0": "نگه دارید",
      "commitChanges.saveInPreset.hold.1": "برای ذخیره تغییرات فقط در فیلدهایی که قبلاً در پیش‌تنظیم گنجانده شده‌اند.",
      "commitChanges/error": "ثبت تغییرات در پیش‌تنظیم با خطا مواجه شد.",
      "commitChanges.manual/description": "انتخاب کنید که کدام تغییرات در پیش‌تنظیم گنجانده شوند.",
      "saveAs": "ذخیره به عنوان جدید...",
      "presetNamePlaceholder": "نامی برای پیش‌تنظیم وارد کنید...",
      "cannotCommitChangesLegacy": "این پیش‌تنظیم قدیمی است و قابل تغییر نیست. شما می‌توانید یک کپی با استفاده از \"ذخیره به عنوان جدید...\" ایجاد کنید.",
      "cannotCommitChangesNoChanges": "تغییری برای ثبت وجود ندارد.",
      "emptyNoUnsaved": "یک پیش‌تنظیم انتخاب کنید...",
      "emptyWithUnsaved": "پیش‌تنظیم ذخیره‌نشده",
      "saveEmptyWithUnsaved": "ذخیره پیش‌تنظیم به عنوان...",
      "saveConfirm": "ذخیره",
      "saveCancel": "لغو",
      "saving": "در حال ذخیره‌سازی...",
      "save/error": "ذخیره پیش‌تنظیم با خطا مواجه شد.",
      "deselect": "لغو انتخاب پیش‌تنظیم",
      "deselect/error": "لغو انتخاب پیش‌تنظیم با خطا مواجه شد.",
      "select/error": "انتخاب پیش‌تنظیم با خطا مواجه شد.",
      "delete/error": "حذف پیش‌تنظیم با خطا مواجه شد.",
      "discardChanges": "رد کردن تغییرات ذخیره‌نشده",
      "discardChanges/info": "تمام تغییرات ثبت‌نشده را رد کرده و پیش‌تنظیم را به حالت اولیه خود بازگردانید",
      "newEmptyPreset": "ایجاد پیش‌تنظیم خالی جدید...",
      "contextMenuSelect": "انتخاب پیش‌تنظیم",
      "contextMenuDelete": "حذف"
  },
  "flashAttentionWarning": "Flash Attention یک ویژگی آزمایشی است که ممکن است باعث مشکلاتی در برخی مدل‌ها شود. اگر با مشکلی مواجه شدید، سعی کنید آن را غیرفعال کنید.",
  "seedUncheckedHint": "دانه تصادفی",
  "ropeFrequencyBaseUncheckedHint": "خودکار",
  "ropeFrequencyScaleUncheckedHint": "خودکار"
}