{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部",
  "basicSettings": "基础",
  "configSubtitle": "加载或保存预设并尝试模型参数覆盖",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "尝试影响预测的参数。",
  "generalParameters/title": "通用",
  "samplingParameters/title": "采样",
  "basicTab": "基础",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开所有",
  "advancedTab/overridesTitle": "配置覆盖",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上方值以在此处查看覆盖。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "尝试影响文本生成的基础参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "控制模型初始化和加载到内存的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "loadParameters/reload/error": "重新加载模型失败",
  "discardChanges": "放弃更改",
  "loadModelToSeeOptions": "加载模型以查看选项",
  "schematicsError.title": "配置结构在以下字段存在错误：",
  "manifestSections": {
    "structuredOutput/title": "结构化输出",
    "speculativeDecoding/title": "投机解码",
    "sampling/title": "采样",
    "settings/title": "设置",
    "toolUse/title": "工具调用",
    "promptTemplate/title": "提示词模板",
    "customFields/title": "自定义字段"
  },

  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段向模型提供背景指令,如一套规则、约束或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.systemPrompt/openEditor": "编辑器",
  "llm.prediction.systemPrompt/closeEditor": "关闭编辑器",
  "llm.prediction.systemPrompt/openedEditor": "在编辑器中打开...",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "引入多少随机性。0 将始终产生相同的结果,而较高值将增加创造性和变化。",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档：\"默认值为 <{{dynamicValue}}>，它在随机性和确定性之间提供了平衡。极端情况下，温度为 0 会始终选择最可能的下一个token，导致每次运行的输出相同\"",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.topKSampling/title": "Top K 采样",
  "llm.prediction.topKSampling/subTitle": "将下一个token限制为模型预测的前 k 个最可能的token。作用类似于温度",
  "llm.prediction.topKSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-k 采样是一种仅从模型预测的前 k 个最可能的token中选择下一个token的文本生成方法。\n\n它有助于减少生成低概率或无意义token的风险，但也可能限制输出的多样性。\n\n更高的 top-k 值（例如，100）将考虑更多token，从而生成更多样化的文本，而较低的值（例如，10）将专注于最可能的token，生成更保守的文本。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期间使用的 CPU 线程数",
  "llm.prediction.llama.cpuThreads/info": "计算期间要使用的线程数。增加线程数并不总是与更好的性能相关联。默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选地限制 AI 响应的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人的响应最大长度。开启以设置响应的最大长度限制,或关闭以让聊天机器人决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度（token）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "约 {{maxWords}} 词",
  "llm.prediction.repeatPenalty/title": "重复惩罚",
  "llm.prediction.repeatPenalty/subTitle": "多大程度上避免重复相同的token",
  "llm.prediction.repeatPenalty/info": "来自 llama.cpp 帮助文档：\"有助于防止模型生成重复或单调的文本。\n\n更高的值(例如,1.5)将更强烈地惩罚重复,而更低的值(例如,0.9)将更为宽容。\" • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "最小 P 采样",
  "llm.prediction.minPSampling/subTitle": "token被选为输出的最低基本概率",
  "llm.prediction.minPSampling/info": "来自 llama.cpp 帮助文档：\n\n相对于最可能token的概率，token被视为考虑的最低概率。必须在 [0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P 采样",
  "llm.prediction.topPSampling/subTitle": "可能的下一个token的最小累积概率。作用类似于温度",
  "llm.prediction.topPSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-p 采样，也称为核心采样，是另一种文本生成方法，从累积概率至少为 p 的token子集中选择下一个token。\n\n这种方法通过同时考虑token的概率和要从中采样的token数量，在多样性和质量之间提供了平衡。\n\n更高的 top-p 值（例如，0.95）将导致更多样化的文本，而较低的值（例如，0.5）将生成更集中和保守的文本。必须在 (0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应该停止模型生成更多token的字符串",
  "llm.prediction.stopStrings/info": "遇到特定字符串时将停止模型生成更多token",
  "llm.prediction.stopStrings/placeholder": "输入一个字符串并按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话超出模型处理能力时,模型应该如何表现",
  "llm.prediction.contextOverflowPolicy/info": "决定当对话超过模型的工作内存('上下文')大小时该怎么做",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "尾部自由采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.llama.xtcProbability/title": "XTC 采样概率",
  "llm.prediction.llama.xtcProbability/subTitle": "XTC（排除顶选）采样器将在每个生成token时以该概率激活。XTC 采样有助于提升创造力，减少陈词滥调",
  "llm.prediction.llama.xtcProbability/info": "XTC（排除顶选）采样将以该概率在每个token生成时激活。XTC 采样通常可以提升创造力并减少陈词滥调",
  "llm.prediction.llama.xtcThreshold/title": "XTC 采样阈值",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除顶选）阈值。在 `xtc-probability` 概率下，查找概率介于 `xtc-threshold` 和 0.5 之间的token，并仅保留其中概率最低的一个",
  "llm.prediction.llama.xtcThreshold/info": "XTC（排除顶选）阈值。在 `xtc-probability` 概率下，查找概率介于 `xtc-threshold` 和 0.5 之间的所有token，仅保留概率最低的一个，其余全部移除",
  "llm.prediction.mlx.topKSampling/title": "Top K 采样",
  "llm.prediction.mlx.topKSampling/subTitle": "将下一个token限制为概率最高的前 k 个token。作用类似于温度",
  "llm.prediction.mlx.topKSampling/info": "仅从概率最高的前 k 个token中选择下一个token，作用类似于温度",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个token限制为前 k 个最可能的token。作用类似于温度",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档：\n\n保留最高概率词汇表token的数量以进行 top-k 过滤\n\n• 默认情况下此过滤器关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "多大程度上避免重复相同的token",
  "llm.prediction.onnx.repeatPenalty/info": "更高的值阻止模型重复自身",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一个token的最小累积概率。作用类似于温度",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档：\n\n仅保留累积概率达到或超过 TopP 的最可能token用于生成\n\n• 默认情况下此过滤器关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.structured/description": "高级：您可以提供[JSON Schema](https://json-schema.org/learn/miscellaneous-examples)来强制执行模型中的特定输出格式。阅读[留档](https://lmstudio.ai/docs/advanced/structured-output)了解更多",
  "llm.prediction.tools/title": "工具调用",
  "llm.prediction.tools/description": "高级功能：你可以提供 JSON 格式的工具列表，模型可请求调用这些工具。详情请查阅[文档](https://lmstudio.ai/docs/advanced/tool-use)",
  "llm.prediction.tools/serverPageDescriptionAddon": "通过服务端 API 调用时，请将其作为 `tools` 字段传入请求体",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate/subTitle": "聊天中消息发送给模型的格式。更改此设置可能会引入意外行为 - 确保您知道自己在做什么！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "草稿生成token数",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "每生成一个主模型token，草稿模型生成的token数量。平衡计算量与收益，选择合适的数值",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿概率阈值",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "仅当token概率高于该阈值时才继续草稿。值越高风险越低，收益也越低",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿长度",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "草稿长度低于该值将被主模型忽略。值越高风险越低，收益也越低",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿长度",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "草稿中允许的最大token数。如果所有token概率都高于阈值，则为上限。值越低风险越低，收益也越低",
  "llm.prediction.speculativeDecoding.draftModel/title": "草稿模型",
  "llm.prediction.reasoning.parsing/title": "推理过程解析方式",
  "llm.prediction.reasoning.parsing/subTitle": "控制模型输出中推理过程的解析方式",

  "llm.load.mainGpu/title": "主 GPU",
  "llm.load.mainGpu/subTitle": "用于模型计算的 GPU 优先级",
  "llm.load.mainGpu/placeholder": "选择主 GPU...",
  "llm.load.splitStrategy/title": "拆分策略",
  "llm.load.splitStrategy/subTitle": "如何跨 GPU 拆分模型计算",
  "llm.load.splitStrategy/placeholder": "选择拆分策略...",
  "llm.load.offloadKVCacheToGpu/title": "将 KV 缓存卸载到 GPU 内存",
  "llm.load.offloadKVCacheToGpu/subTitle": "将 KV 缓存卸载到 GPU 内存。这可以提高性能但需要更多 GPU 内存",
  "load.gpuStrictVramCap/title": "限制模型卸载至专用 GPU 内存",
  "load.gpuStrictVramCap.customSubTitleOff": "关闭：若专用 GPU 内存已满，允许将模型权重卸载至共享内存",
  "load.gpuStrictVramCap.customSubTitleOn": "开启：系统将限制模型权重的卸载仅限于专用 GPU 内存及 RAM 。上下文仍可能使用共享内存",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "模型的卸载仅限于专用 GPU 内存。实际卸载的层数可能会有所不同",
  "load.allGpusDisabledWarning": "所有 GPU 目前均被禁用。请启用至少一个以进行卸载",

  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型可以一次性关注的token最大数量。请参阅“推理参数”下的“对话溢出”选项以获取更多管理方式",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大token数量，影响其处理过程中保留的上下文量",
  "llm.load.contextLength/warning": "设置较高的上下文长度值会对内存使用产生显著影响",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机",
  "llm.load.seed/info": "随机种子：设置随机数生成的种子以确保可重复的结果",

  "llm.load.llama.evalBatchSize/title": "评估批处理大小",
  "llm.load.llama.evalBatchSize/subTitle": "每次处理的输入token数量。增加此值会提高性能，但会增加内存使用量",
  "llm.load.llama.evalBatchSize/info": "设置评估期间一起处理的示例数量,影响速度和内存使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 频率基",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入(RoPE)的自定义基频。增加此值可能在高上下文长度下提高性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频,影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 频率比例",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放,以使用 RoPE 扩展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放,以控制位置编码的粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的离散模型层数",
  "llm.load.llama.acceleration.offloadRatio/info": "设置卸载到 GPU 的层数。",
  "llm.load.llama.flashAttention/title": "快速注意力",
  "llm.load.llama.flashAttention/subTitle": "降低某些模型的内存使用量和生成时间",
  "llm.load.llama.flashAttention/info": "加速注意力机制,实现更快、更高效的处理",
  "llm.load.numExperts/title": "专家数量",
  "llm.load.numExperts/subTitle": "模型中使用的专家数量",
  "llm.load.numExperts/info": "模型中使用的专家数量",
  "llm.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使模型卸载到 GPU 也预留系统内存。提高性能但需要更多的系统 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型交换到磁盘,确保更快的访问,但以更高的 RAM 使用率为代价",
  "llm.load.llama.useFp16ForKVCache/title": "使用 FP16 用于 KV 缓存",
  "llm.load.llama.useFp16ForKVCache/info": "通过以半精度(FP16)存储缓存来减少内存使用",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/subTitle": "提高模型的加载时间。禁用此功能可能在模型大于可用系统 RAM 时提高性能",
  "llm.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU 线程池大小",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "为模型计算分配的 CPU 线程池线程数",
  "llm.load.llama.cpuThreadPoolSize/info": "分配用于模型计算的 CPU 线程池线程数量。线程数增加未必总能带来更佳性能。默认值为 <{{dynamicValue}}>。",
  "llm.load.llama.kCacheQuantizationType/title": "K 缓存量化类型",
  "llm.load.llama.kCacheQuantizationType/subTitle": "较低的量化类型可降低内存占用，但可能影响模型质量，不同模型效果差异大。",
  "llm.load.llama.vCacheQuantizationType/title": "V 缓存量化类型",
  "llm.load.llama.vCacheQuantizationType/subTitle": "较低的量化类型可降低内存占用，但可能影响模型质量，不同模型效果差异大。",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 如未启用Flash Attention，请务必关闭该选项",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "仅在启用Flash Attention时可用",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 使用 F32 时请禁用Flash Attention",
  "llm.load.mlx.kvCacheBits/title": "KV 缓存量化位数",
  "llm.load.mlx.kvCacheBits/subTitle": "KV 缓存量化使用的位数",
  "llm.load.mlx.kvCacheBits/info": "设置 KV 缓存需要量化成的位数",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "启用 KV 缓存量化时，上下文长度设置将被忽略",
  "llm.load.mlx.kvCacheGroupSize/title": "KV 缓存量化分组大小",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "量化操作时分组的大小，组越大内存占用越低，但模型质量可能下降",
  "llm.load.mlx.kvCacheGroupSize/info": "KV 缓存量化时使用的分组位数",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV 缓存量化：开始量化的上下文长度",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "达到此上下文长度后开始对 KV 缓存进行量化",
  "llm.load.mlx.kvCacheQuantizationStart/info": "达到此上下文长度后开始对 KV 缓存进行量化",
  "llm.load.mlx.kvCacheQuantization/title": "KV 缓存量化",
  "llm.load.mlx.kvCacheQuantization/subTitle": "对模型的 KV 缓存进行量化，可加快生成速度并降低内存占用，但可能影响输出质量。",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV 缓存量化位数",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "KV 缓存量化所用的位数",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "位数",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "分组策略",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "高精度",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "均衡",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "极速",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "高级：量化乘法的分组大小配置\n\n• 高精度 = 分组 32\n• 均衡 = 分组 64\n• 极速 = 分组 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "达到此上下文长度后开始量化",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "当上下文长度达到该值时，开始对 KV 缓存进行量化",

  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型可以一次性关注的token最大数量。请参阅“推理参数”下的“对话溢出”选项以获取更多管理方式",
  "embedding.load.contextLength/info": "指定模型一次可以考虑的最大token数量，影响其处理过程中保留的上下文量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 频率基",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入(RoPE)的自定义基频。增加此值可能在高上下文长度下提高性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频,影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批处理大小",
  "embedding.load.llama.evalBatchSize/subTitle": "每次处理的输入token数量。增加此值会提高性能，但会增加内存使用量",
  "embedding.load.llama.evalBatchSize/info": "设置评估期间一起处理的token数量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 频率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放,以使用 RoPE 扩展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放,以控制位置编码的粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的离散模型层数",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置卸载到 GPU 的层数。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使模型卸载到 GPU 也预留系统内存。提高性能但需要更多的系统 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型交换到磁盘,确保更快的访问,但以更高的 RAM 使用率为代价",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "提高模型的加载时间。禁用此功能可能在模型大于可用系统 RAM 时提高性能",
  "embedding.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机种子",

  "embedding.load.seed/info": "随机种子：设置随机数生成的种子以确保可重复的结果",

  "presetTooltip": {
    "included/title": "预设值",
    "included/description": "以下字段将会被应用",
    "included/empty": "在此上下文中,此预设没有适用的字段。",
    "included/conflict": "您将被要求选择是否应用此值",
    "separateLoad/title": "加载时配置",
    "separateLoad/description.1": "预设还包含以下加载时配置。加载时配置是全模型范围的,并且需要重新加载模型才能生效。按住",
    "separateLoad/description.2": "应用到",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不适用",
    "excluded/description": "以下字段包含在预设中,但在当前上下文中不适用。",
    "legacy/title": "旧版预设",
    "legacy/description": "这是一个旧版预设。它包括以下字段,这些字段现在要么自动处理,要么不再适用。",
    "button/publish": "发布到 Hub",
    "button/pushUpdate": "推送更改到 Hub",
    "button/noChangesToPush": "没有可推送的更改",
    "button/export": "导出",
    "hubLabel": "来自 {{user}} 的 Hub 预设",
    "ownHubLabel": "您的 Hub 预设"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "关闭"
    },
    "llamaCacheQuantizationType": {
      "off": "关闭"
    },
    "mlxKvCacheBits": {
      "off": "关闭"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "类型",
      "types.jinja/label": "模板 (Jinja)",
      "jinja.bosToken/label": "开始token (BOS Token)",
      "jinja.eosToken/label": "结束token (EOS Token)",
      "jinja.template/label": "模板",
      "jinja/error": "解析 Jinja 模板失败: {{error}}",
      "jinja/empty": "请在上方输入一个 Jinja 模板。",
      "jinja/unlikelyToWork": "您提供的 Jinja 模板很可能无法正常工作,因为它没有引用变量 \"messages\"。请检查您输入的模板是否正确。",
      "types.manual/label": "手动",
      "manual.subfield.beforeSystem/label": "系统前缀",
      "manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
      "manual.subfield.afterSystem/label": "系统后缀",
      "manual.subfield.afterSystem/placeholder": "输入系统后缀...",
      "manual.subfield.beforeUser/label": "用户前缀",
      "manual.subfield.beforeUser/placeholder": "输入用户前缀...",
      "manual.subfield.afterUser/label": "用户后缀",
      "manual.subfield.afterUser/placeholder": "输入用户后缀...",
      "manual.subfield.beforeAssistant/label": "助手前缀",
      "manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
      "manual.subfield.afterAssistant/label": "助手后缀",
      "manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
      "stopStrings/label": "额外停止字符串",
      "stopStrings/subTitle": "除了用户指定的停止字符串之外,还将使用特定于模板的停止字符串。"
    },
    "contextLength": {
      "maxValueTooltip": "这是模型训练所能处理的最大token数量。点击以将上下文设置为此值",
      "maxValueTextStart": "模型支持最多",
      "maxValueTextEnd": "个token",
      "tooltipHint": "尽管模型可能支持一定数量的token，但如果您的机器资源无法处理负载，性能可能会下降 - 增加此值时请谨慎"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "到达限制时停止",
      "stopAtLimitSub": "一旦模型的内存满载即停止生成",
      "truncateMiddle": "截断中间",
      "truncateMiddleSub": "从对话中间移除消息以为新消息腾出空间。模型仍然会记住对话的开头",
      "rollingWindow": "滚动窗口",
      "rollingWindowSub": "模型将始终接收最近的几条消息,但可能会忘记对话的开头"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "关闭"
    },
    "gpuSplitStrategy": {
      "evenly": "均匀分配",
      "favorMainGpu": "优先主 GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "了解工作原理",
      "placeholder": "选择兼容的草稿模型",
      "noCompatible": "当前模型选择下未找到兼容的草稿模型",
      "stillLoading": "正在识别兼容的草稿模型...",
      "notCompatible": "所选草稿模型（<draft/>）与当前模型选择（<current/>）不兼容。",
      "off": "关闭",
      "loadModelToSeeOptions": "加载模型 <keyboard-shortcut /> 以查看兼容选项",
      "compatibleWithNumberOfModels": "推荐用于至少 {{dynamicValue}} 个模型",
      "recommendedForSomeModels": "推荐用于部分模型",
      "recommendedForLlamaModels": "推荐用于 Llama 模型",
      "recommendedForQwenModels": "推荐用于 Qwen 模型",
      "onboardingModal": {
        "introducing": "新功能介绍",
        "speculativeDecoding": "投机解码",
        "firstStepBody": "<custom-span>llama.cpp</custom-span> 和 <custom-span>MLX</custom-span> 模型推理加速",
        "secondStepTitle": "投机解码能够加速推理",
        "secondStepBody": "投机解码是一种让两个模型协作的技术：\n - 一个规模较大的“主”模型\n - 一个较小的“草稿”模型\n\n生成过程中，草稿模型会快速提出token，由主模型进行验证。验证的过程比实际生成更快。\n**通常，主模型与草稿模型的体积差距越大，加速效果越明显。**\n\n为了保证质量，主模型只会接受与自身结果一致的token，从而实现大模型的响应质量与更快的推理速度。两个模型必须使用相同的词表。",
        "draftModelRecommendationsTitle": "草稿模型推荐",
        "basedOnCurrentModels": "基于您当前的模型",
        "close": "关闭",
        "next": "下一步",
        "done": "完成"
      },
      "speculativeDecodingLoadModelToSeeOptions": "请先加载模型 <model-badge /> ",
      "errorEngineNotSupported": "投机解码需引擎 {{engineName}} 至少版本 {{minVersion}}。请更新引擎（<key/>）并重新加载模型以使用此功能。",
      "errorEngineNotSupported/noKey": "投机解码需引擎 {{engineName}} 至少版本 {{minVersion}}。请更新引擎并重新加载模型以使用此功能。"
    },
    "llmReasoningParsing": {
      "startString/label": "起始字符串",
      "startString/placeholder": "请输入起始字符串...",
      "endString/label": "结束字符串",
      "endString/placeholder": "请输入结束字符串..."
    }
  },
  "saveConflictResolution": {
    "title": "选择要包含在预设中的值",
    "description": "挑选并选择要保留的值",
    "instructions": "点击一个值以包含它",
    "userValues": "先前的值",
    "presetValues": "新值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "applyConflictResolution": {
    "title": "保留哪些值？",
    "description": "您有未提交的更改与即将应用的预设有重叠",
    "instructions": "点击一个值以保留它",
    "userValues": "当前值",
    "presetValues": "即将应用的预设值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "empty": "<空>",
  "noModelSelected": "未选择模型",
  "apiIdentifier.label": "API 标识符",
  "apiIdentifier.hint": "可选，为此模型提供一个标识符。该标识符将在 API 请求中使用。留空则使用默认标识符。",
  "idleTTL.label": "空闲时自动卸载",
  "idleTTL.hint": "如设置，模型在空闲指定时间后将自动卸载。",
  "idleTTL.mins": "分钟",

  "presets": {
    "title": "预设",
    "commitChanges": "提交更改",
    "commitChanges/description": "将您的更改提交给预设。",
    "commitChanges.manual": "检测到新的字段。您将能够选择要包含在预设中的更改。",
    "commitChanges.manual.hold.0": "按住",
    "commitChanges.manual.hold.1": "选择要提交给预设的更改。",
    "commitChanges.saveAll.hold.0": "按住",
    "commitChanges.saveAll.hold.1": "保存所有更改。",
    "commitChanges.saveInPreset.hold.0": "按住",
    "commitChanges.saveInPreset.hold.1": "仅保存已经包含在预设中的字段的更改。",
    "commitChanges/error": "未能将更改提交给预设。",
    "commitChanges.manual/description": "选择要包含在预设中的更改。",
    "saveAs": "另存为新预设...",
    "presetNamePlaceholder": "为预设输入一个名称...",
    "cannotCommitChangesLegacy": "这是一个旧版预设,无法修改。您可以使用“另存为新预设...”创建一个副本。",
    "cannotCommitChangesNoChanges": "没有更改可以提交。",
    "emptyNoUnsaved": "选择一个预设...",
    "emptyWithUnsaved": "未保存的预设",
    "saveEmptyWithUnsaved": "保存预设为...",
    "saveConfirm": "保存",
    "saveCancel": "取消",
    "saving": "正在保存...",
    "save/error": "未能保存预设。",
    "deselect": "取消选择预设",
    "deselect/error": "取消选择预设失败。",
    "select/error": "选择预设失败。",
    "delete/error": "删除预设失败。",
    "discardChanges": "丢弃未保存的更改",
    "discardChanges/info": "丢弃所有未提交的更改并恢复预设至原始状态",
    "newEmptyPreset": "创建新的空预设...",
    "importPreset": "导入",
    "contextMenuCopyIdentifier": "复制预设标识符",
    "contextMenuSelect": "选择预设",
    "contextMenuDelete": "删除",
    "contextMenuShare": "发布中...",
    "contextMenuOpenInHub": "在 Hub 上查看",
    "contextMenuPullFromHub": "拉取最新版本",
    "contextMenuPushChanges": "推送更改到 Hub",
    "contextMenuPushingChanges": "正在推送...",
    "contextMenuPushedChanges": "更改已推送",
    "contextMenuExport": "导出文件",
    "contextMenuRevealInExplorer": "在文件资源管理器中显示",
    "contextMenuRevealInFinder": "在 Finder 中显示",
    "share": {
      "title": "发布预设",
      "action": "分享你的预设，让他人下载、点赞和fork",
      "presetOwnerLabel": "所有者",
      "uploadAs": "你的预设将以 {{name}} 创建",
      "presetNameLabel": "预设名称",
      "descriptionLabel": "描述（可选）",
      "loading": "正在发布...",
      "success": "预设已成功发布",
      "presetIsLive": "<preset-name /> 已在 Hub 上发布！",
      "close": "关闭",
      "confirmViewOnWeb": "在网页上查看",
      "confirmCopy": "复制链接",
      "confirmCopied": "已复制！",
      "pushedToHub": "你的预设已推送到 Hub",
      "descriptionPlaceholder": "请输入描述...",
      "willBePublic": "发布你的预设将使其公开",
      "willBePrivate": "仅您可见",
      "willBeOrgVisible": "组织内成员均可见",
      "publicSubtitle": "你的预设现在为 <custom-bold>公开</custom-bold>。其他人可以在 lmstudio.ai 下载和 fork 它",
      "privateUsageReached": "私有预设的数量已达上限",
      "continueInBrowser": "在浏览器继续",
      "confirmShareButton": "发布",
      "error": "预设发布失败",
      "createFreeAccount": "请在 Hub 创建免费账号以发布预设"
    },
    "update": {
      "title": "推送更改到 Hub",
      "title/success": "预设已成功更新",
      "subtitle": "修改 <custom-preset-name /> 并推送到 Hub",
      "descriptionLabel": "描述",
      "descriptionPlaceholder": "请输入描述...",
      "loading": "正在推送...",
      "cancel": "取消",
      "createFreeAccount": "请在 Hub 创建免费账号以发布预设",
      "error": "推送更新失败",
      "confirmUpdateButton": "推送"
    },
    "resolve": {
      "title": "解决冲突...",
      "tooltip": "打开窗口以解决与 Hub 版本的差异"
    },
    "loginToManage": {
      "title": "登录以管理..."
    },
    "downloadFromHub": {
      "title": "下载",
      "downloading": "下载中...",
      "success": "下载完成！",
      "error": "下载失败"
    },
    "push": {
      "title": "推送更改",
      "pushing": "推送中...",
      "success": "推送成功！",
      "tooltip": "将本地更改推送到 Hub 上托管的远程版本",
      "error": "推送失败"
    },
    "saveAsNewModal": {
      "title": "哎呀！在 Hub 上未找到预设",
      "confirmSaveAsNewDescription": "您是否希望将此预设作为新版本发布？",
      "confirmButton": "作为新版本发布"
    },
    "pull": {
      "title": "拉取最新版本",
      "error": "拉取失败",
      "contextMenuErrorMessage": "拉取失败",
      "success": "已拉取",
      "pulling": "拉取中...",
      "upToDate": "已是最新版本！",
      "unsavedChangesModal": {
        "title": "你有未保存的更改。",
        "bodyContent": "从远程拉取的内容将覆盖您的未保存更改，是否继续？",
        "confirmButton": "覆盖未保存的更改"
      }
    },
    "import": {
      "title": "从文件导入预设",
      "dragPrompt": "拖拽预设 JSON 文件或<custom-link>从电脑选择</custom-link>",
      "remove": "移除",
      "cancel": "取消",
      "importPreset_zero": "导入预设",
      "importPreset_one": "导入预设",
      "importPreset_other": "导入 {{count}} 个预设",
      "selectDialog": {
        "title": "选择预设文件（.json 或者 .tar.gz）",
        "button": "导入"
      },
      "error": "导入预设失败",
      "resultsModal": {
        "titleSuccessSection_one": "成功导入 1 个预设",
        "titleSuccessSection_other": "成功导入 {{count}} 个预设",
        "titleFailSection_zero": "",
        "titleFailSection_one": "（{{count}} 个失败）",
        "titleFailSection_other": "（{{count}} 个失败）",
        "titleAllFailed": "预设导入失败",
        "importMore": "继续导入",
        "close": "完成",
        "successBadge": "成功",
        "alreadyExistsBadge": "预设已存在",
        "errorBadge": "错误",
        "invalidFileBadge": "无效文件",
        "otherErrorBadge": "导入预设失败",
        "errorViewDetailsButton": "查看详情",
        "seeError": "查看错误",
        "noName": "无预设名称",
        "useInChat": "在聊天中使用"
      },
      "importFromUrl": {
        "button": "从 URL 导入...",
        "title": "从 URL 导入",
        "back": "从文件导入...",
        "action": "请在下方粘贴你要导入的 LM Studio Hub 预设链接",
        "invalidUrl": "无效的 URL，请确保输入的是有效的 LM Studio Hub 预设链接。",
        "tip": "你也可以在 LM Studio Hub 直接点击 {{buttonName}} 按钮安装该预设",
        "confirm": "导入",
        "cancel": "取消",
        "loading": "正在导入...",
        "error": "下载预设失败。"
      }
    },
    "download": {
      "title": "从 LM Studio Hub 拉取 <preset-name />",
      "subtitle": "保存 <custom-name /> 到你的预设。保存后你可以在应用中使用此预设",
      "button": "拉取",
      "button/loading": "正在拉取...",
      "cancel": "取消",
      "error": "下载预设失败。"
    },
    "inclusiveness": {
      "speculativeDecoding": "包含在预设中"
    }
  },

  "flashAttentionWarning": "Flash Attention 是一项实验性功能,可能会导致某些模型出现问题。如果您遇到问题,请尝试禁用它。",
  "llamaKvCacheQuantizationWarning": "KV 缓存量化是一项实验性功能，可能会导致某些模型出现问题。V 缓存量化必须启用 Flash Attention。如果遇到问题，请将默认值重置为\"F16\"。",

  "seedUncheckedHint": "随机种子",
  "ropeFrequencyBaseUncheckedHint": "自动",
  "ropeFrequencyScaleUncheckedHint": "自动",

  "hardware": {
    "environmentVariables": "环境变量",
    "environmentVariables.info": "如果不确定,请保留默认值",
    "environmentVariables.reset": "重置为默认值",
    
    "gpus.information": "配置检测到的图形处理单元 (GPU）",
    "gpuSettings": {
      "editMaxCapacity": "编辑最大容量",
      "hideEditMaxCapacity": "隐藏最大容量编辑",
      "allOffWarning": "所有 GPU 均已关闭或禁用，请确保分配了至少一个 GPU 以加载模型",
      "split": {
        "title": "分配策略",
        "placeholder": "选择 GPU 内存分配方式",
        "options": {
          "generalDescription": "配置模型将如何加载到您的 GPU 上",
          "evenly": {
            "title": "均匀分配",
            "description": "在多个 GPU 之间均匀分配内存"
          },
          "priorityOrder": {
            "title": "按顺序填充",
            "description": "先在第一个 GPU 上分配内存,然后依次分配到后续 GPU"
          },
          "custom": {
            "title": "自定义",
            "description": "分配内存",
            "maxAllocation": "最大分配"
          }
        }
      },
      "deviceId.info": "此设备的唯一标识符",
      "changesOnlyAffectNewlyLoadedModels": "更改仅影响新加载的模型",
      "toggleGpu": "启用/禁用 GPU"
    }
  },

  "load.gpuSplitConfig/title": "GPU 分配配置",
  "envVars/title": "设置环境变量",
  "envVars": {
    "select": {
      "placeholder": "选择环境变量...",
      "noOptions": "无更多可用选项",
      "filter": {
        "placeholder": "过滤搜索结果",
        "resultsFound_zero": "未找到结果",
        "resultsFound_one": "找到 1 个结果",
        "resultsFound_other": "找到 {{count}} 个结果"
      }
    },
    "inputValue": {
      "placeholder": "输入值"
    },
    "values": {
      "title": "当前值"
    }
  }
}
