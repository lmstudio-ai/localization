{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部",
  "basicSettings": "基本",
  "configSubtitle": "加载或保存预设，并尝试模型参数覆盖",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "尝试影响预测的参数。",
  "generalParameters/title": "常规",
  "samplingParameters/title": "采样",
  "basicTab": "基础",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "全部展开",
  "advancedTab/overridesTitle": "配置覆盖",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上方值以在此处查看覆盖。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "尝试影响文本生成的基础参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "控制模型初始化和加载到内存的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "discardChanges": "放弃更改",
  "loadModelToSeeOptions": "加载模型以查看选项",
  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段向模型提供背景指令，例如一组规则、约束或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "引入随机性的程度。0 将每次都产生相同的结果，而更高的值将增加创造力和变化。",
  "llm.prediction.temperature/info": "从 llama.cpp 帮助文档：\"默认值为 <{{dynamicValue}}>，它在随机性和确定性之间提供了平衡。极端情况下，温度为 0 将总是选择最可能的下一个词元，导致每次运行的输出都相同\"",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.topKSampling/title": "Top K 采样",
  "llm.prediction.topKSampling/subTitle": "将下一个词元限制为最有可能的前 k 个词元之一。其作用类似于温度",
  "llm.prediction.topKSampling/info": "从 llama.cpp 帮助文档：\n\nTop-k 采样是一种文本生成方法，它仅从模型预测的最有可能的前 k 个词元中选择下一个词元。\n\n它有助于减少生成低概率或无意义词元的风险，但也可能限制输出的多样性。\n\n较高的 top-k 值（例如 100）将考虑更多的词元，从而产生更多样化的文本，而较低的值（例如 10）将集中在最可能的词元上，生成更保守的文本。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期间使用的 CPU 线程数",
  "llm.prediction.llama.cpuThreads/info": "计算期间使用的线程数。增加线程数并不总是与更好的性能相关联。默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选地限制 AI 响应的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人的响应最大长度。开启以设置响应的最大长度限制，或者关闭让聊天机器人决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度（词元）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "大约 {{maxWords}} 个词",
  "llm.prediction.repeatPenalty/title": "重复惩罚",
  "llm.prediction.repeatPenalty/subTitle": "多少程度上阻止重复相同的词元",
  "llm.prediction.repeatPenalty/info": "从 llama.cpp 帮助文档：\"帮助防止模型生成重复或单调的文本。\n\n较高的值（例如 1.5）将更强烈地惩罚重复，而较低的值（例如 0.9）将更加宽容。\" • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "最小 P 采样",
  "llm.prediction.minPSampling/subTitle": "词元被选中输出的最小基础概率",
  "llm.prediction.minPSampling/info": "从 llama.cpp 帮助文档：\n\n相对于最可能词元的概率，词元被考虑的最小概率。必须在 [0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P 采样",
  "llm.prediction.topPSampling/subTitle": "可能的下一个词元的最小累积概率。其作用类似于温度",
  "llm.prediction.topPSampling/info": "从 llama.cpp 帮助文档：\n\nTop-p 采样，也称为核采样，是另一种文本生成方法，它从累积概率至少为 p 的词元子集中选择下一个词元。\n\n这种方法通过同时考虑词元的概率和采样词元的数量，在多样性和质量之间提供了平衡。\n\n较高的 top-p 值（例如 0.95）将导致更多样化的文本，而较低的值（例如 0.5）将生成更聚焦和保守的文本。必须在 (0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应该停止模型生成更多词元的字符串",
  "llm.prediction.stopStrings/info": "遇到特定字符串时将停止模型生成更多词元",
  "llm.prediction.stopStrings/placeholder": "输入字符串并按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话变得太大以至于模型无法处理时的行为",
  "llm.prediction.contextOverflowPolicy/info": "决定当对话超出模型工作内存（'上下文'）大小时该怎么办",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "尾部自由采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个词元限制为最有可能的前 k 个词元之一。其作用类似于温度",
  "llm.prediction.onnx.topKSampling/info": "从 ONNX 文档：\n\n保留最高概率词汇表词元的数量用于 top-k 过滤\n\n• 此过滤器默认关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "多少程度上阻止重复相同的词元",
  "llm.prediction.onnx.repeatPenalty/info": "较高的值会阻止模型重复自身",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一个词元的最小累积概率。其作用类似于温度",
  "llm.prediction.onnx.topPSampling/info": "从 ONNX 文档：\n\n仅保留累积概率达到 TopP 或更高的最可能词元用于生成\n\n• 此过滤器默认关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate/subTitle": "聊天中消息发送给模型的格式。更改此设置可能会引入意外行为 - 确保您知道自己在做什么！",

  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型可以一次性关注的最大词元数。有关更多管理此设置的方法，请参见“推理参数”下的“对话溢出选项”",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大词元数，影响其在处理过程中保留的上下文量",
  "llm.load.contextLength/warning": "设置较高的上下文长度值可能会显著影响内存使用",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机",
  "llm.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",

  "llm.load.llama.evalBatchSize/title": "评估批处理大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次处理的输入词元数量。增加此值会提高性能，但以增加内存使用为代价",
  "llm.load.llama.evalBatchSize/info": "设置评估期间一起处理的例子数量，影响速度和内存使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 频率基底",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入（RoPE）的自定义基频。增加此值可能在高上下文长度下实现更好的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 频率缩放",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放，以使用 RoPE 扩展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码的粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的离散模型层的数量",
  "llm.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "llm.load.llama.flashAttention/title": "快速注意力",
  "llm.load.llama.flashAttention/subTitle": "在某些模型上减少内存使用和生成时间",
  "llm.load.llama.flashAttention/info": "加速注意力机制以实现更快更高效的处理",
  "llm.load.numExperts/title": "专家数量",
  "llm.load.numExperts/subTitle": "模型中使用的专家数量",
  "llm.load.numExperts/info": "模型中使用的专家数量",
  "llm.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使模型卸载到 GPU 也要预留系统内存。提高性能但需要更多的系统 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问，但以更高的 RAM 使用为代价",
  "llm.load.llama.useFp16ForKVCache/title": "使用 FP16 存储 KV 缓存",
  "llm.load.llama.useFp16ForKVCache/info": "通过以半精度（FP16）存储缓存来减少内存使用",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/subTitle": "改善模型加载时间。当模型大于可用系统 RAM 时禁用此功能可能会提高性能",
  "llm.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",

  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型一次可以关注的最大词元数。有关更多管理此设置的方法，请参见“推理参数”下的“对话溢出选项”",
  "embedding.load.contextLength/info": "指定模型一次可以考虑的最大词元数，影响其在处理过程中保留的上下文量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 频率基底",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入（RoPE）的自定义基频。增加此值可能在高上下文长度下实现更好的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批处理大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次处理的输入词元数量。增加此值会提高性能，但以增加内存使用为代价",
  "embedding.load.llama.evalBatchSize/info": "设置评估期间一起处理的词元数量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 频率缩放",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放，以使用 RoPE 扩展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码的粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的离散模型层的数量",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使模型卸载到 GPU 也要预留系统内存。提高性能但需要更多的系统 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问，但以更高的 RAM 使用为代价",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "改善模型加载时间。当模型大于可用系统 RAM 时禁用此功能可能会提高性能",
  "embedding.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 表示随机种子",

  "embedding.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",

  "presetTooltip": {
    "included/title": "预设值",
    "included/description": "以下字段将被应用",
    "included/empty": "在此上下文中，此预设的任何字段都不适用。",
    "included/conflict": "您将被询问是否应用此值",
    "separateLoad/title": "加载时配置",
    "separateLoad/description.1": "预设还包括以下加载时配置。加载时配置是模型范围内的，需要重新加载模型才能生效。按住",
    "separateLoad/description.2": "以应用到",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不适用",
    "excluded/description": "以下字段包含在预设中但在当前上下文中不适用",
    "legacy/title": "旧版预设",
    "legacy/description": "此预设是一个旧版预设。它包括以下字段，这些字段现在要么自动处理，要么不再适用。"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "关闭"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "类型",
      "types.jinja/label": "模板 (Jinja)",
      "jinja.bosToken/label": "开始标记 (BOS Token)",
      "jinja.eosToken/label": "结束标记 (EOS Token)",
      "jinja.template/label": "模板",
      "jinja/error": "解析 Jinja 模板失败: {{error}}",
      "types.manual/label": "手动",
      "manual.subfield.beforeSystem/label": "系统前缀",
      "manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
      "manual.subfield.afterSystem/label": "系统后缀",
      "manual.subfield.afterSystem/placeholder": "输入系统后缀...",
      "manual.subfield.beforeUser/label": "用户前缀",
      "manual.subfield.beforeUser/placeholder": "输入用户前缀...",
      "manual.subfield.afterUser/label": "用户后缀",
      "manual.subfield.afterUser/placeholder": "输入用户后缀...",
      "manual.subfield.beforeAssistant/label": "助手前缀",
      "manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
      "manual.subfield.afterAssistant/label": "助手后缀",
      "manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
      "stopStrings/label": "额外停止字符串",
      "stopStrings/subTitle": "除了用户指定的停止字符串之外，还将使用特定于模板的停止字符串。"
    },
    "contextLength": {
      "maxValueTooltip": "这是模型训练时能够处理的最大词元数量。点击以将上下文设置为此值",
      "maxValueTextStart": "模型支持最多",
      "maxValueTextEnd": "个词元",
      "tooltipHint": "虽然模型可能支持多达一定数量的词元，但如果您的机器资源无法处理负载，则性能可能会下降 - 增加此值时请谨慎"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "到达限制时停止",
      "stopAtLimitSub": "一旦模型的内存满了就停止生成",
      "truncateMiddle": "截断中间",
      "truncateMiddleSub": "删除对话中间的消息以为新消息腾出空间。模型仍然会记住对话的开头",
      "rollingWindow": "滚动窗口",
      "rollingWindowSub": "模型总是获取最近的几条消息，但可能会忘记对话的开头"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "关闭"
    }
  },
  "saveConflictResolution": {
    "title": "选择要包含在预设中的值",
    "description": "挑选并选择要保留的值",
    "instructions": "点击一个值以包含它",
    "userValues": "之前的值",
    "presetValues": "新的值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "applyConflictResolution": {
    "title": "保留哪些值？",
    "description": "您有未提交的更改与传入的预设重叠",
    "instructions": "点击一个值以保留它",
    "userValues": "当前值",
    "presetValues": "传入预设值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "empty": "<空>",
  "presets": {
    "title": "预设",
    "commitChanges": "提交更改",
    "commitChanges/description": "提交对预设的更改。",
    "commitChanges.manual": "检测到新的字段。您可以选择要包含在预设中的更改。",
    "commitChanges.manual.hold.0": "按住",
    "commitChanges.manual.hold.1": "以选择要提交给预设的更改。",
    "commitChanges.saveAll.hold.0": "按住",
    "commitChanges.saveAll.hold.1": "以保存所有更改。",
    "commitChanges.saveInPreset.hold.0": "按住",
    "commitChanges.saveInPreset.hold.1": "以仅保存已包含在预设中的字段的更改。",
    "commitChanges/error": "提交预设更改失败。",
    "commitChanges.manual/description": "选择要包含在预设中的更改。",
    "saveAs": "另存为新预设...",
    "presetNamePlaceholder": "输入预设名称...",
    "cannotCommitChangesLegacy": "这是一个旧版预设，不能被修改。您可以通过使用“另存为新预设...”来创建副本。",
    "cannotCommitChangesNoChanges": "没有更改需要提交。",
    "emptyNoUnsaved": "选择预设...",
    "emptyWithUnsaved": "未保存的预设",
    "saveEmptyWithUnsaved": "另存为预设...",
    "saveConfirm": "保存",
    "saveCancel": "取消",
    "saving": "正在保存...",
    "save/error": "保存预设失败。",
    "deselect": "取消选择预设",
    "deselect/error": "取消选择预设失败。",
    "select/error": "选择预设失败。",
    "delete/error": "删除预设失败。",
    "discardChanges": "放弃未保存的更改",
    "discardChanges/info": "放弃所有未提交的更改，并将预设恢复到原始状态",
    "newEmptyPreset": "创建新的空预设...",
    "contextMenuSelect": "选择预设",
    "contextMenuDelete": "删除"
  },

  "flashAttentionWarning": "快速注意力是一个实验性功能，可能会导致某些模型出现问题。如果遇到问题，请尝试禁用它。",

  "seedUncheckedHint": "随机种子",
  "ropeFrequencyBaseUncheckedHint": "自动",
  "ropeFrequencyScaleUncheckedHint": "自动"
}
