{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部",
  "basicSettings": "基础",
  "configSubtitle": "加载或保存预设并尝试覆盖模型参数",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "尝试影响预测的参数。",
  "generalParameters/title": "常规",
  "samplingParameters/title": "采样",
  "basicTab": "基础",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "全部展开",
  "advancedTab/overridesTitle": "配置覆盖",
  "advancedTab/noConfigsText": "您没有任何未保存的更改 - 编辑上方值以在此处查看覆盖。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "尝试影响文本生成的基本参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "控制模型初始化和加载到内存中的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "loadParameters/reload/error": "无法重新加载模型",
  "discardChanges": "放弃更改",
  "loadModelToSeeOptions": "加载模型以查看选项",
  "schematicsError.title": "配置架构在以下字段中包含错误：",
  "manifestSections": {
    "structuredOutput/title": "结构化输出",
    "speculativeDecoding/title": "推测解码",
    "sampling/title": "采样",
    "settings/title": "设置",
    "toolUse/title": "工具使用",
    "promptTemplate/title": "提示模板"
  },

  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段为模型提供背景说明，例如一组规则、约束或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI指南",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "引入多少随机性。0将每次产生相同的结果，而较高的值将增加创造性和差异性",
  "llm.prediction.temperature/info": "来自llama.cpp帮助文档：\"默认值是 ，它在随机性和确定性之间提供了平衡。极端情况下，温度为0将始终选择最可能的下一个令牌，导致每次运行的输出相同\"",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.topKSampling/title": "Top K 采样",
  "llm.prediction.topKSampling/subTitle": "将下一个令牌限制为前k个最可能的令牌之一。作用类似于温度",
  "llm.prediction.topKSampling/info": "来自llama.cpp帮助文档：\n\nTop-k采样是一种文本生成方法，仅从模型预测的前k个最可能的令牌中选择下一个令牌。\n\n它有助于减少生成低概率或无意义令牌的风险，但也可能限制输出的多样性。\n\ntop-k的较高值（例如100）将考虑更多的令牌并导致更多样化的文本，而较低的值（例如10）将关注最可能的令牌并生成更保守的文本。\n\n• 默认值是 ",
  "llm.prediction.llama.cpuThreads/title": "CPU线程数",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期间使用的CPU线程数",
  "llm.prediction.llama.cpuThreads/info": "计算期间使用的线程数。增加线程数并不总是与更好的性能相关。默认值是 。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选地限制AI响应的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人的响应最大长度。打开以设置响应的最大长度限制，或关闭以让聊天机器人决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度（令牌）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "大约 {{maxWords}} 字",
  "llm.prediction.repeatPenalty/title": "重复惩罚",
  "llm.prediction.repeatPenalty/subTitle": "对重复同一令牌的抑制程度",
  "llm.prediction.repeatPenalty/info": "来自llama.cpp帮助文档：\"有助于防止模型生成重复或单调的文本。\n\n较高的值（例如1.5）将更强烈地惩罚重复，而较低的值（例如0.9）将更加宽容。\" • 默认值是 ",
  "llm.prediction.minPSampling/title": "Min P 采样",
  "llm.prediction.minPSampling/subTitle": "选择输出令牌所需的最低基本概率",
  "llm.prediction.minPSampling/info": "来自llama.cpp帮助文档：\n\n相对于最可能令牌的概率，考虑令牌的最低概率。必须在[0,1]范围内。\n\n• 默认值是 ",
  "llm.prediction.topPSampling/title": "Top P 采样",
  "llm.prediction.topPSampling/subTitle": "可能的下一个令牌的最小累积概率。作用类似于温度",
  "llm.prediction.topPSampling/info": "来自llama.cpp帮助文档：\n\nTop-p采样，也称为核采样，是另一种文本生成方法，从累积概率至少为p的子集令牌中选择下一个令牌。\n\n该方法通过同时考虑令牌的概率和要采样的令牌数量，在多样性和质量之间提供平衡。\n\ntop-p的较高值（例如0.95）将导致更多样化的文本，而较低的值（例如0.5）将生成更专注和保守的文本。必须在(0,1]范围内。\n\n• 默认值是 ",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应停止模型生成更多令牌的字符串",
  "llm.prediction.stopStrings/info": "当遇到特定字符串时将停止模型生成更多令牌",
  "llm.prediction.stopStrings/placeholder": "输入字符串并按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话变得过大以至于模型无法处理时，模型应如何表现",
  "llm.prediction.contextOverflowPolicy/info": "决定当对话超过模型工作内存（'上下文'）大小时应采取的措施",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "尾部自由采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.llama.xtcProbability/title": "XTC 采样概率",
  "llm.prediction.llama.xtcProbability/subTitle": "每个生成的令牌激活XTC（排除顶级选择）采样的概率。XTC采样可以提高创造力并减少陈词滥调",
  "llm.prediction.llama.xtcProbability/info": "每个生成的令牌激活XTC（排除顶级选择）采样的概率。XTC采样通常提高创造力并减少陈词滥调",
  "llm.prediction.llama.xtcThreshold/title": "XTC 采样阈值",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除顶级选择）阈值。有`xtc-probability`的机会，搜索概率在`xtc-threshold`和0.5之间的令牌，并移除所有这样的令牌，除了概率最低的一个",
  "llm.prediction.llama.xtcThreshold/info": "XTC（排除顶级选择）阈值。有`xtc-probability`的机会，搜索概率在`xtc-threshold`和0.5之间的令牌，并移除所有这样的令牌，除了概率最低的一个",
  "llm.prediction.mlx.topKSampling/title": "Top K 采样",
  "llm.prediction.mlx.topKSampling/subTitle": "将下一个令牌限制为前k个最可能的令牌之一。作用类似于温度",
  "llm.prediction.mlx.topKSampling/info": "将下一个令牌限制为前k个最可能的令牌之一。作用类似于温度",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个令牌限制为前k个最可能的令牌之一。作用类似于温度",
  "llm.prediction.onnx.topKSampling/info": "来自ONNX文档：\n\n用于top-k过滤的最高概率词汇表令牌的数量\n\n• 此过滤器默认关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "对重复同一令牌的抑制程度",
  "llm.prediction.onnx.repeatPenalty/info": "较高的值会阻止模型重复自身",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一个令牌的最小累积概率。作用类似于温度",
  "llm.prediction.onnx.topPSampling/info": "来自ONNX文档：\n\n只有累积概率至少为TopP的最高概率令牌才会保留用于生成\n\n• 此过滤器默认关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.structured/description": "高级：您可以提供一个[JSON Schema](https://json-schema.org/learn/miscellaneous-examples)以强制模型输出特定格式。阅读[文档](https://lmstudio.ai/docs/advanced/structured-output)了解更多",
  "llm.prediction.tools/title": "工具使用",
  "llm.prediction.tools/description": "高级：您可以提供一个符合JSON的工具列表供模型请求调用。阅读[文档](https://lmstudio.ai/docs/advanced/tool-use)了解更多",
  "llm.prediction.tools/serverPageDescriptionAddon": "通过请求体作为`tools`传递，当使用服务器API时",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate/subTitle": "聊天消息发送到模型的格式。更改此内容可能会引入意外行为 - 确保您知道自己在做什么！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "生成的草稿令牌数",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "主模型每个令牌生成的草稿模型令牌数。找到计算与奖励之间的最佳点",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿概率截止",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "继续起草直到令牌概率低于此阈值。较高的值通常意味着风险较低，回报较低",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿大小",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "小于这个大小的草稿将被主模型忽略。较高的值通常意味着风险较低，回报较低",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿大小",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "草稿中允许的最大令牌数。如果所有令牌概率都大于截止值，则为上限。较低的值通常意味着风险较低，回报较低",
  "llm.prediction.speculativeDecoding.draftModel/title": "草稿模型",
  "llm.prediction.reasoning.parsing/title": "推理部分解析",
  "llm.prediction.reasoning.parsing/subTitle": "如何解析模型输出中的推理部分",

  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型在一个提示中可以关注的最大令牌数。有关管理此功能的更多信息，请参阅“推理参数”下的对话溢出选项",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大令牌数，影响其在处理过程中的上下文保留能力",
  "llm.load.contextLength/warning": "为上下文长度设置高值会显著影响内存使用",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1为随机",
  "llm.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",

  "llm.load.llama.evalBatchSize/title": "评估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次处理的输入令牌数。增加此值会提高性能但会增加内存使用",
  "llm.load.llama.evalBatchSize/info": "设置评估期间一批处理的示例数量，影响速度和内存使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE频率基",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入（RoPE）的自定义基频。增加此值可能在高上下文长度下实现更好的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE频率比例",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文长度通过此因子缩放以使用RoPE扩展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率比例以控制位置编码粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用于GPU加速的离散模型层数",
  "llm.load.llama.acceleration.offloadRatio/info": "设置要卸载到GPU的层数。",
  "llm.load.llama.flashAttention/title": "快速注意力",
  "llm.load.llama.flashAttention/subTitle": "在某些模型上减少内存使用和生成时间",
  "llm.load.llama.flashAttention/info": "加速注意力机制以实现更快和更高效的处理",
  "llm.load.numExperts/title": "专家数量",
  "llm.load.numExperts/subTitle": "模型中使用的专家数量",
  "llm.load.numExperts/info": "模型中使用的专家数量",
  "llm.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使卸载到GPU，也要为模型保留系统内存。提高性能但需要更多系统RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型被换出到磁盘，确保更快访问，但代价是更高的RAM使用",
  "llm.load.llama.useFp16ForKVCache/title": "使用FP16用于KV缓存",
  "llm.load.llama.useFp16ForKVCache/info": "通过半精度（FP16）存储缓存来减少内存使用",
  "llm.load.llama.tryMmap/title": "尝试mmap()",
  "llm.load.llama.tryMmap/subTitle": "改进模型的加载时间。禁用此功能可能在模型大于可用系统RAM时提高性能",
  "llm.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU线程池大小",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "分配给用于模型计算的线程池的CPU线程数",
  "llm.load.llama.cpuThreadPoolSize/info": "分配给用于模型计算的线程池的CPU线程数。增加线程数并不总是与更好的性能相关。默认值是 。",
  "llm.load.llama.kCacheQuantizationType/title": "K缓存量化类型",
  "llm.load.llama.kCacheQuantizationType/subTitle": "较低的值减少内存使用但可能降低质量。效果因模型而异。",
  "llm.load.llama.vCacheQuantizationType/title": "V缓存量化类型",
  "llm.load.llama.vCacheQuantizationType/subTitle": "较低的值减少内存使用但可能降低质量。效果因模型而异。",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 如果未启用闪存注意力，必须禁用此值",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "只有在启用闪存注意力时才能开启",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 使用F32时必须禁用闪存注意力",
  "llm.load.mlx.kvCacheBits/title": "KV缓存量化",
  "llm.load.mlx.kvCacheBits/subTitle": "KV缓存应量化的比特数",
  "llm.load.mlx.kvCacheBits/info": "KV缓存应量化的比特数",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "使用KV缓存量化时忽略上下文长度设置",
  "llm.load.mlx.kvCacheGroupSize/title": "KV缓存量化：组大小",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "KV缓存量化的组大小。较高的组大小减少内存使用但可能降低质量",
  "llm.load.mlx.kvCacheGroupSize/info": "KV缓存应量化的比特数",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV缓存量化：ctx越过此长度时开始量化",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "开始量化KV缓存的上下文长度阈值",
  "llm.load.mlx.kvCacheQuantizationStart/info": "开始量化KV缓存的上下文长度阈值",
  "llm.load.mlx.kvCacheQuantization/title": "KV缓存量化",
  "llm.load.mlx.kvCacheQuantization/subTitle": "量化模型的KV缓存。这可能导致更快的生成和更低的内存占用，\n但会牺牲模型输出的质量。",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV缓存量化比特",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "量化KV缓存的比特数",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "比特",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "组大小策略",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "准确",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "平衡",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "快速",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "高级：量化'matmul组大小'配置\n\n• 准确 = 组大小32\n• 平衡 = 组大小64\n• 快速 = 组大小128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "ctx达到此长度时开始量化",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "当上下文达到此数量的令牌时，\n开始量化KV缓存",

  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型在一个提示中可以关注的最大令牌数。有关管理此功能的更多信息，请参阅“推理参数”下的对话溢出选项",
  "embedding.load.contextLength/info": "指定模型一次可以考虑的最大令牌数，影响其在处理过程中的上下文保留能力",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE频率基",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入（RoPE）的自定义基频。增加此值可能在高上下文长度下实现更好的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次处理的输入令牌数。增加此值会提高性能但会增加内存使用",
  "embedding.load.llama.evalBatchSize/info": "设置评估期间一批处理的令牌数量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE频率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文长度通过此因子缩放以使用RoPE扩展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率比例以控制位置编码粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用于GPU加速的离散模型层数",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置要卸载到GPU的层数。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使卸载到GPU，也要为模型保留系统内存。提高性能但需要更多系统RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被换出到磁盘，确保更快访问，但代价是更高的RAM使用",
  "embedding.load.llama.tryMmap/title": "尝试mmap()",
  "embedding.load.llama.tryMmap/subTitle": "改进模型的加载时间。禁用此功能可能在模型大于可用系统RAM时提高性能",
  "embedding.load.llama.tryMmap/info": "直接从磁盘加载模型文件到内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1为随机种子",

  "embedding.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",

  "presetTooltip": {
    "included/title": "预设值",
    "included/description": "将应用以下字段",
    "included/empty": "此上下文中不适用任何预设字段。",
    "included/conflict": "将询问您是否应用此值",
    "separateLoad/title": "加载时配置",
    "separateLoad/description.1": "预设还包括以下加载时配置。加载时配置是模型范围的，需要重新加载模型才能生效。按住",
    "separateLoad/description.2": "以应用于",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不适用",
    "excluded/description": "以下字段包含在预设中，但在当前上下文中不适用。",
    "legacy/title": "旧版预设",
    "legacy/description": "这是一个旧版预设。它包括以下字段，这些字段现在自动处理或不再适用。",
    "button/publish": "发布到Hub",
    "button/pushUpdate": "推送更改到Hub",
    "button/export": "导出"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": " "
    },
    "checkboxNumeric": {
      "off": "关闭"
    },
    "llamaCacheQuantizationType": {
      "off": "关闭"
    },
    "mlxKvCacheBits": {
      "off": "关闭"
    },
    "stringArray": {
      "empty": " "
    },
    "llmPromptTemplate": {
      "type": "类型",
      "types.jinja/label": "模板 (Jinja)",
      "jinja.bosToken/label": "BOS令牌",
      "jinja.eosToken/label": "EOS令牌",
      "jinja.template/label": "模板",
      "jinja/error": "无法解析Jinja模板：{{error}}",
      "jinja/empty": "请在上面输入Jinja模板。",
      "jinja/unlikelyToWork": "您在上面提供的Jinja模板不太可能工作，因为它没有引用变量“messages”。请仔细检查是否输入了正确的模板。",
      "types.manual/label": "手动",
      "manual.subfield.beforeSystem/label": "系统之前",
      "manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
      "manual.subfield.afterSystem/label": "系统之后",
      "manual.subfield.afterSystem/placeholder": "输入系统后缀...",
      "manual.subfield.beforeUser/label": "用户之前",
      "manual.subfield.beforeUser/placeholder": "输入用户前缀...",
      "manual.subfield.afterUser/label": "用户之后",
      "manual.subfield.afterUser/placeholder": "输入用户后缀...",
      "manual.subfield.beforeAssistant/label": "助手之前",
      "manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
      "manual.subfield.afterAssistant/label": "助手之后",
      "manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
      "stopStrings/label": "附加停止字符串",
      "stopStrings/subTitle": "模板特定的停止字符串，将与用户指定的停止字符串一起使用。"
    },
    "contextLength": {
      "maxValueTooltip": "这是模型训练所能处理的最大令牌数。点击以将上下文设置为此值",
      "maxValueTextStart": "模型支持最多",
      "maxValueTextEnd": "令牌",
      "tooltipHint": "虽然模型可能支持多达一定数量的令牌，但如果您的机器资源无法承受负载，性能可能会下降 - 增加此值时请谨慎"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "到达限制时停止",
      "stopAtLimitSub": "一旦模型的内存满了就停止生成",
      "truncateMiddle": "中间截断",
      "truncateMiddleSub": "从中部删除消息以为较新的消息腾出空间。模型仍会记住对话的开头",
      "rollingWindow": "滚动窗口",
      "rollingWindowSub": "模型将始终获取最近的几条消息，但可能会忘记对话的开头"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "关闭"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "均匀",
      "favorMainGpu": "偏向主GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "阅读其工作原理",
      "placeholder": "选择兼容的草稿模型",
      "noCompatible": "未找到与当前模型选择兼容的草稿模型",
      "stillLoading": "正在识别兼容的草稿模型...",
      "notCompatible": "所选草稿模型（ ）与当前模型选择（ ）不兼容。",
      "off": "关闭",
      "loadModelToSeeOptions": "加载模型   以查看兼容选项",
      "compatibleWithNumberOfModels": "推荐用于至少{{dynamicValue}}个您的模型",
      "recommendedForSomeModels": "推荐用于某些模型",
      "recommendedForLlamaModels": "推荐用于Llama模型",
      "recommendedForQwenModels": "推荐用于Qwen模型",
      "onboardingModal": {
        "introducing": "介绍",
        "speculativeDecoding": "推测解码",
        "firstStepBody": "适用于 llama.cpp 和 MLX 模型的推理加速",
        "secondStepTitle": "推测解码的推理加速",
        "secondStepBody": "推测解码是一种涉及两个模型协作的技术：\n - 较大的“主”模型\n - 较小的“草稿”模型\n\n在生成过程中，草稿模型快速提出令牌供较大的主模型验证。验证令牌比实际生成它们快得多，这是速度提升的来源。**通常，主模型和草稿模型之间的尺寸差异越大，加速越明显**。\n\n为了保持质量，主模型只接受与其自身生成一致的令牌，从而在较快的推理速度下保持较大模型的响应质量。两个模型必须共享相同的词汇表。",
        "draftModelRecommendationsTitle": "草稿模型建议",
        "basedOnCurrentModels": "基于您当前的模型",
        "close": "关闭",
        "next": "下一步",
        "done": "完成"
      },
      "speculativeDecodingLoadModelToSeeOptions": "请先加载模型   ",
      "errorEngineNotSupported": "推测解码需要至少版本{{minVersion}}的引擎{{engineName}}。请更新引擎（ ）并重新加载模型以使用此功能。",
      "errorEngineNotSupported/noKey": "推测解码需要至少版本{{minVersion}}的引擎{{engineName}}。请更新引擎并重新加载模型以使用此功能。"
    },
    "llmReasoningParsing": {
      "startString/label": "起始字符串",
      "startString/placeholder": "输入起始字符串...",
      "endString/label": "结束字符串",
      "endString/placeholder": "输入结束字符串..."
    }
  },
  "saveConflictResolution": {
    "title": "选择要包含在预设中的值",
    "description": "选择要保留的值",
    "instructions": "点击一个值以包含它",
    "userValues": "先前值",
    "presetValues": "新值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "applyConflictResolution": {
    "title": "保留哪些值？",
    "description": "您有未提交的更改，与即将应用的预设重叠",
    "instructions": "点击一个值以保留它",
    "userValues": "当前值",
    "presetValues": "即将到来的预设值",
    "confirm": "确认",
    "cancel": "取消"
  },
  "empty": " ",
  "noModelSelected": "未选择模型",
  "apiIdentifier.label": "API标识符",
  "apiIdentifier.hint": "可选地为此模型提供一个标识符。这将在API请求中使用。留空以使用默认标识符。",
  "idleTTL.label": "空闲自动卸载 (TTL)",
  "idleTTL.hint": "如果设置，模型将在空闲指定时间后自动卸载。",
  "idleTTL.mins": "分钟",

  "presets": {
    "title": "预设",
    "commitChanges": "提交更改",
    "commitChanges/description": "将您的更改提交到预设。",
    "commitChanges.manual": "检测到新字段。您将能够选择要包含在预设中的更改。",
    "commitChanges.manual.hold.0": "按住",
    "commitChanges.manual.hold.1": "以选择要提交到预设的更改。",
    "commitChanges.saveAll.hold.0": "按住",
    "commitChanges.saveAll.hold.1": "以保存所有更改。",
    "commitChanges.saveInPreset.hold.0": "按住",
    "commitChanges.saveInPreset.hold.1": "以仅保存已包含在预设中的字段的更改。",
    "commitChanges/error": "无法提交更改到预设。",
    "commitChanges.manual/description": "选择要包含在预设中的更改。",
    "saveAs": "另存为新...",
    "presetNamePlaceholder": "输入预设名称...",
    "cannotCommitChangesLegacy": "这是一个旧版预设，无法修改。您可以通过使用“另存为新...”创建副本。",
    "cannotCommitChangesNoChanges": "没有要提交的更改。",
    "emptyNoUnsaved": "选择预设...",
    "emptyWithUnsaved": "未保存预设",
    "saveEmptyWithUnsaved": "保存预设为...",
    "saveConfirm": "保存",
    "saveCancel": "取消",
    "saving": "保存中...",
    "save/error": "无法保存预设。",
    "deselect": "取消选择预设",
    "deselect/error": "无法取消选择预设。",
    "select/error": "无法选择预设。",
    "delete/error": "无法删除预设。",
    "discardChanges": "放弃未保存",
    "discardChanges/info": "放弃所有未提交的更改并将预设恢复到原始状态",
    "newEmptyPreset": "+ 新建预设",
    "importPreset": "导入",
    "contextMenuSelect": "应用预设",
    "contextMenuDelete": "删除...",
    "contextMenuShare": "发布...",
    "contextMenuOpenInHub": "在Hub上查看",
    "contextMenuPushChanges": "推送更改到Hub",
    "contextMenuPushingChanges": "推送中...",
    "contextMenuPushedChanges": "更改已推送",
    "contextMenuExport": "导出文件",
    "contextMenuRevealInExplorer": "在文件资源管理器中显示",
    "contextMenuRevealInFinder": "在Finder中显示",
    "share": {
      "title": "发布预设",
      "action": "分享您的预设，供他人下载、点赞和派生",
      "presetOwnerLabel": "拥有者",
      "uploadAs": "您的预设将作为{{name}}创建",
      "presetNameLabel": "预设名称",
      "descriptionLabel": "描述（可选）",
      "loading": "发布中...",
      "success": "预设成功推送",
      "presetIsLive": "  现已在Hub上上线！",
      "close": "关闭",
      "confirmViewOnWeb": "在网页上查看",
      "confirmCopy": "复制URL",
      "confirmCopied": "已复制！",
      "pushedToHub":  "您的预设已推送到Hub",
      "descriptionPlaceholder": "输入描述...",
      "willBePublic": "发布您的预设将使其公开",
      "publicSubtitle": "您的预设是 公开 的。其他人可以在lmstudio.ai上下载和派生它",
      "confirmShareButton": "发布",
      "error": "无法发布预设",
      "createFreeAccount": "在Hub上创建免费账户以发布预设"
    },
    "update": {
      "title": "推送更改到Hub",
      "title/success": "预设成功更新",
      "subtitle": "对   进行更改并将其推送到Hub",
      "descriptionLabel": "描述",
      "descriptionPlaceholder": "输入描述...",
      "loading": "推送中...",
      "cancel": "取消",
      "createFreeAccount": "在Hub上创建免费账户以发布预设",
      "error": "推送更新失败",
      "confirmUpdateButton": "推送"
    },
    "import": {
      "title": "从文件导入预设",
      "dragPrompt": "拖放预设JSON文件或 从计算机中选择 ",
      "remove": "移除",
      "cancel": "取消",
      "importPreset_zero": "导入预设",
      "importPreset_one": "导入预设",
      "importPreset_other": "导入 {{ count}} 个预设",
      "selectDialog": {
        "title": "选择预设文件 (.json)",
        "button": "导入"
      },
      "error": "无法导入预设",
      "resultsModal": {
        "titleSuccessSection_one": "成功导入1个预设",
        "titleSuccessSection_other": "成功导入{{count}}个预设",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} 失败)",
        "titleFailSection_other": "({{count}} 失败)",
        "titleAllFailed": "无法导入预设",
        "importMore": "导入更多",
        "close": "完成",
        "successBadge": "成功",
        "alreadyExistsBadge": "预设已存在",
        "errorBadge": "错误",
        "invalidFileBadge": "无效文件",
        "otherErrorBadge": "无法导入预设",
        "errorViewDetailsButton": "查看详情",
        "seeError": "查看错误",
        "noName": "无预设名称",
        "useInChat": "在聊天中使用"
      },
      "importFromUrl": {
        "button": "从URL导入...",
        "title": "从URL导入",
        "back": "从文件导入...",
        "action": "在下方粘贴要导入的预设的LM Studio Hub URL",
        "invalidUrl": "无效URL。请确保您粘贴的是正确的LM Studio Hub URL。",
        "tip": "您可以直接使用LM Studio Hub中的{{buttonName}}按钮安装预设",
        "confirm": "导入",
        "cancel": "取消",
        "loading": "正在导入...",
        "error": "无法下载预设。"
      }
    },
    "download": {
      "title": "从LM Studio Hub拉取<preset-name />",
      "subtitle": "将<custom-name />保存到您的预设中。这样做后，您可以在应用中使用此预设",
      "button": "拉取",
      "button/loading": "正在拉取...",
      "cancel": "取消",
      "error": "无法下载预设。"
    },
    "inclusiveness": {
      "speculativeDecoding": "包含在预设中"
    }
  },
  
  "flashAttentionWarning": "Flash Attention 是一项实验性功能，可能会导致某些模型出现问题。如果遇到问题，请尝试禁用它。",
  "llamaKvCacheQuantizationWarning": "KV缓存量化是一项实验性功能，可能会导致某些模型出现问题。启用V缓存量化时必须启用Flash Attention。如果遇到问题，请重置为默认的“F16”。",

  "seedUncheckedHint": "随机种子",
  "ropeFrequencyBaseUncheckedHint": "自动",
  "ropeFrequencyScaleUncheckedHint": "自动",

  "hardware": {
    "advancedGpuSettings": "高级GPU设置",
    "advancedGpuSettings.info": "如果不确定，请将这些值保留为默认值",
    "advancedGpuSettings.reset": "重置为默认值",
    "environmentVariables": {
      "title": "环境变量",
      "description": "模型生命周期内的活动环境变量。",
      "key.placeholder": "选择变量...",
      "value.placeholder": "值"
    },
    "mainGpu": {
      "title": "主GPU",
      "description": "优先用于模型计算的GPU。",
      "placeholder": "选择主GPU..."
    },
    "splitStrategy": {
      "title": "拆分策略",
      "description": "如何在多个GPU之间拆分模型计算。",
      "placeholder": "选择拆分策略..."
    }
  }
}
