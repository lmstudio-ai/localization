{
  "noInstanceSelected": "Nessuna istanza di modello selezionata",
  "resetToDefault": "Ripristina",
  "showAdvancedSettings": "Mostra impostazioni avanzate",
  "showAll": "Mostra tutto",
  "basicSettings": "Base",
  "configSubtitle": "Carica o salva preset e sperimenta con la modifica dei parametri del modello",
  "inferenceParameters/title": "Parametri di Predizione",
  "inferenceParameters/info": "Sperimenta con i parametri che influenzano la predizione.",
  "generalParameters/title": "Generale",
  "samplingParameters/title": "Campionamento",
  "basicTab": "Base",
  "advancedTab": "Avanzato",
  "advancedTab/title": "Impostazioni di Esecuzione",
  "advancedTab/expandAll": "Espandi tutto",
  "advancedTab/overridesTitle": "Sostituzioni Attive",
  "advancedTab/noConfigsText": "Non ci sono modifiche non salvate - modifica i valori sopra per vedere le sostituzioni qui.",
  "loadInstanceFirst": "Carica un modello per vedere i parametri configurabili",
  "noListedConfigs": "Non ci sono parametri configurabili",
  "generationParameters/info": "Sperimenta con parametri di base che influenzano la generazione del testo.",
  "loadParameters/title": "Carica Parametri",
  "loadParameters/description": "Cambiare questi parametri richiede di ricaricare il modello",
  "loadParameters/reload": "Ricarica per applicare le modifiche dei parametri di caricamento",
  "discardChanges": "Scarta modifiche",
  "llm.prediction.systemPrompt/title": "Messaggio di Sistema",
  "llm.prediction.systemPrompt/description": "Usa questo campo per fornire istruzioni di base al modello, come un insieme di regole, restrizioni o requisiti generali. Questo campo è anche conosciuto come il 'messaggio di sistema'.",
  "llm.prediction.systemPrompt/subTitle": "Linee guida per l'IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quanta casualità introdurre. Un valore di 0 darà lo stesso risultato ogni volta, mentre valori più alti aumenteranno la creatività e l'incertezza.",
  "llm.prediction.temperature/info": "Dai documenti di aiuto di llama.cpp: 'Il valore predefinito è <{{dynamicValue}}>, che fornisce un equilibrio tra casualità e determinismo. All'estremo, una temperatura di 0 sceglierà sempre il prossimo token più probabile, portando a risultati identici in ogni esecuzione'",
  "llm.prediction.topKSampling/title": "Campionamento Top K",
  "llm.prediction.topKSampling/subTitle": "Limita il prossimo token a uno dei migliori k token più probabili. Funziona in modo simile alla temperatura.",
  "llm.prediction.topKSampling/info": "Dai documenti di aiuto di llama.cpp:\n\nIl campionamento top-k è un metodo di generazione di testo che seleziona il prossimo token solo dai k token più probabili predetti dal modello.\n\nAiuta a ridurre il rischio di generare token di bassa probabilità o senza senso, ma può anche limitare la diversità dell'output.\n\nUn valore più alto per top-k (ad esempio, 100) considererà più token e genererà un testo più diversificato, mentre un valore più basso (ad esempio, 10) si concentrerà sui token più probabili e genererà un testo più conservativo.\n\n• Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Thread CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Numero di thread del processore da utilizzare durante l'inferenza",
  "llm.prediction.llama.cpuThreads/info": "Il numero di thread da utilizzare durante il calcolo. Aumentare il numero di thread non sempre si traduce in un miglior rendimento. Il valore predefinito è <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limita Lunghezza Risposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Opzionalmente stabilire un limite sulla lunghezza della risposta dell'AI.",
  "llm.prediction.maxPredictedTokens/info": "Controlla la lunghezza massima della risposta del chatbot. Attivalo per impostare un limite alla lunghezza massima di una risposta, o disattivalo per permettere al chatbot di decidere quando fermarsi.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Lunghezza massima della risposta (token)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Circa {{maxWords}} parole",
  "llm.prediction.repeatPenalty/title": "Penalità per Ripetizione",
  "llm.prediction.repeatPenalty/subTitle": "Quanto disincoraggiare la ripetizione dello stesso token.",
  "llm.prediction.repeatPenalty/info": "Dai documenti di aiuto di llama.cpp: 'Aiuta a prevenire che il modello generi testo ripetitivo o monotono.\n\nUn valore più alto (ad esempio, 1.5) penalizzerà le ripetizioni più fortemente, mentre un valore più basso (ad esempio, 0.9) sarà più indulgente.' • Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Campionamento Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilità minima di base per un token essere selezionato per l'output",
  "llm.prediction.minPSampling/info": "Dai documenti di aiuto di llama.cpp:\n\nLa probabilità minima per cui un token viene considerato, relativa alla probabilità del token più probabile. Deve essere in [0, 1].\n\n• Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Campionamento Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilità minima cumulativa per i possibili prossimi token. Funziona in modo simile alla temperatura.",
  "llm.prediction.topPSampling/info": "Dai documenti di aiuto di llama.cpp:\n\nIl campionamento top-p, noto anche come campionamento del nucleo, è un altro metodo di generazione di testo che seleziona il prossimo token da un sottoinsieme di token che insieme hanno una probabilità cumulativa di almeno p.\n\nQuesto metodo fornisce un equilibrio tra diversità e qualità considerando sia le probabilità dei token che il numero di token da campionare.\n\nUn valore più alto per top-p (ad esempio, 0.95) genererà un testo più diversificato, mentre un valore più basso (ad esempio, 0.5) genererà un testo più mirato e conservativo. Deve essere in (0, 1].\n\n• Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stringhe di Arresto",
  "llm.prediction.stopStrings/subTitle": "Strings that should stop the model from generating more tokens",
  "llm.prediction.stopStrings/info": "Stringhe specifiche che, se trovate, interromperanno il modello nella generazione di ulteriori token",
  "llm.prediction.stopStrings/placeholder": "Inserisci una stringa e premi ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Gestione del Sovraccarico della Conversazione",
  "llm.prediction.contextOverflowPolicy/subTitle": "Come il modello dovrebbe comportarsi quando la conversazione diventa troppo grande da gestire per esso.",
  "llm.prediction.contextOverflowPolicy/info": "Decidi cosa fare quando la conversazione supera la dimensione della memoria di lavoro del modello ('contesto')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Fermati al Limite",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Smetti di generare una volta che la memoria del modello è piena",
  "customInputs.contextOverflowPolicy.truncateMiddle": "Tronca il Centro",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Rimuove i messaggi centrali della conversazione per fare spazio ai più recenti. Il modello ricorderà ancora l'inizio della conversazione",
  "customInputs.contextOverflowPolicy.rollingWindow": "Finestra Mobile",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "Il modello otterrà sempre gli ultimi messaggi, ma potrebbe dimenticare l'inizio della conversazione",
  "llm.prediction.llama.frequencyPenalty/title": "Penalità per Frequenza",
  "llm.prediction.llama.frequencyPenalty/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.presencePenalty/title": "Penalità per Presenza",
  "llm.prediction.llama.presencePenalty/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.tailFreeSampling/title": "Campionamento Senza Coda",
  "llm.prediction.llama.tailFreeSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.locallyTypicalSampling/title": "Campionamento Tipico Locale",
  "llm.prediction.llama.locallyTypicalSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.onnx.topKSampling/title": "Campionamento Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita il prossimo token a uno dei migliori k token più probabili. Funziona in modo simile alla temperatura.",
  "llm.prediction.onnx.topKSampling/info": "Dalla documentazione di ONNX:\n\nNumero di token di vocabolario con maggiore probabilità da mantenere per il filtro top-k\n\n• Questo filtro è disattivato per impostazione predefinita",
  "llm.prediction.onnx.repeatPenalty/title": "Penalità per Ripetizione",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Quanto disincoraggiare la ripetizione dello stesso token.",
  "llm.prediction.onnx.repeatPenalty/info": "Un valore più alto scoraggia il modello dal ripetersi",
  "llm.prediction.onnx.topPSampling/title": "Campionamento Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilità minima cumulativa per i possibili prossimi token. Funziona in modo simile alla temperatura.",
  "llm.prediction.onnx.topPSampling/info": "Dalla documentazione di ONNX:\n\nVengono mantenuti solo i token più probabili con probabilità che sommano TopP o più per la generazione\n\n• Questo filtro è disattivato per impostazione predefinita",
  "llm.prediction.seed/title": "Seme",
  "llm.prediction.seed/subTitle": "Il sottotitolo va qui",
  "llm.prediction.structured/title": "Output Strutturato",
  "llm.prediction.structured/subTitle": "Il sottotitolo va qui",
  "llm.prediction.structured/info": "Output Strutturato",
  "llm.load.contextLength/title": "Lunghezza del Contesto",
  "llm.load.contextLength/subTitle": "Il massimo numero di token che il modello può tenere presente in una sola richiesta. Vedi le opzioni di overflow della conversazione sotto \"Parametri di inferenza\" per altre vie di gestione.",
  "llm.load.contextLength/info": "Specifica il numero massimo di token che il modello può considerare contemporaneamente, influenzando quanto contesto viene mantenuto durante l'elaborazione",
  "llm.load.seed/title": "Seme",
  "llm.load.seed/subTitle": "La semeizzazione del generatore numerico casuale utilizzato nella generazione testuale. -1 è casuale.",
  "llm.load.seed/info": "Seme Casuale: Imposta il seme per la generazione di numeri casuali per assicurare risultati riproducibili",
  "llm.load.llama.evalBatchSize/title": "Dimensione del Lotto di Valutazione",
  "llm.load.llama.evalBatchSize/subTitle": "Numero di token di ingresso da elaborare contemporaneamente. Aumentando questo valore si aumenta prestazioni al costo di utilizzo della memoria.",
  "llm.load.llama.evalBatchSize/info": "Imposta il numero di esempi elaborati insieme in un lotto durante la valutazione, influenzando la velocità e l'uso di memoria",
  "llm.load.llama.ropeFrequencyBase/title": "Frequenza Base RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanzato] Regola la frequenza base per la Codifica Posizionale Rotatoria, influenzando come viene incorporata l'informazione posizionale",
  "llm.load.llama.ropeFrequencyScale/title": "Scala di Frequenza RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "La lunghezza del contesto viene scalata con questo fattore per estendere il contesto effettivo con RoPE.",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanzato] Modifica la scala di frequenza per la Codifica Posizionale Rotatoria per controllare la granularità della codifica posizionale",
  "llm.load.llama.acceleration.offloadRatio/title": "Scaricamento su GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Numero di strati del modello discreti da calcolare su GPU per l'accelerazione GPU.",
  "llm.load.llama.acceleration.offloadRatio/info": "Imposta la proporzione di calcolo da scaricare su GPU. Impostalo su spento per disattivare lo scaricamento su GPU, o su automatico affinché il modello decida.",
  "llm.load.llama.flashAttention/title": "Attenzione Flash",
  "llm.load.llama.flashAttention/subTitle": "Riduce l'utilizzo di memoria e il tempo di generazione per alcuni modelli.",
  "llm.load.llama.flashAttention/info": "Accelera i meccanismi di attenzione per un'elaborazione più rapida ed efficiente",
  "llm.load.llama.keepModelInMemory/title": "Mantieni il Modello in Memoria",
  "llm.load.llama.keepModelInMemory/subTitle": "Riserva la memoria del sistema per il modello, anche quando viene scaricato sulla GPU. Migliora prestazioni ma richiede più RAM del sistema.",
  "llm.load.llama.keepModelInMemory/info": "Evita che il modello venga scambiato su disco, assicurando un accesso più rapido a scapito di un maggiore uso di RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usa FP16 per la Cache KV",
  "llm.load.llama.useFp16ForKVCache/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.useFp16ForKVCache/info": "Riduce l'uso di memoria memorizzando la cache in precisione ridotta (FP16)",
  "llm.load.llama.tryMmap/title": "Prova mmap()",
  "llm.load.llama.tryMmap/subTitle": "Migliora il tempo di caricamento del modello. Disattivare questo parametrarlo può migliorare prestazioni quando il modello è più grande rispetto alla memoria RAM del sistema.",
  "llm.load.llama.tryMmap/info": "Carica i file del modello direttamente dal disco alla memoria"
}
