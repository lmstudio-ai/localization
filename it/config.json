{
  "noInstanceSelected": "Nessuna istanza del modello selezionata",
  "resetToDefault": "Ripristina",
  "showAdvancedSettings": "Mostra impostazioni avanzate",
  "showAll": "Tutti",
  "basicSettings": "Base",
  "configSubtitle": "Carica o salva preimpostazioni e sperimenta con le sovrascritture dei parametri del modello",
  "inferenceParameters/title": "Parametri di Predizione",
  "inferenceParameters/info": "Sperimenta con i parametri che influenzano la predizione.",
  "generalParameters/title": "Generali",
  "samplingParameters/title": "Campionamento",
  "basicTab": "Base",
  "advancedTab": "Avanzate",
  "advancedTab/title": "üß™ Configurazione Avanzata",
  "advancedTab/expandAll": "Espandi tutto",
  "advancedTab/overridesTitle": "Sovrascritture Configurazione",
  "advancedTab/noConfigsText": "Non hai modifiche non salvate - modifica i valori sopra per vedere le sovrascritture qui.",
  "loadInstanceFirst": "Carica un modello per visualizzare i parametri configurabili",
  "noListedConfigs": "Nessun parametro configurabile",
  "generationParameters/info": "Sperimenta con i parametri di base che influenzano la generazione del testo.",
  "loadParameters/title": "Parametri di Caricamento",
  "loadParameters/description": "Impostazioni per controllare il modo in cui il modello viene inizializzato e caricato in memoria.",
  "loadParameters/reload": "Ricarica per applicare le modifiche",
  "loadParameters/reload/error": "Impossibile ricaricare il modello",
  "discardChanges": "Annulla modifiche",
  "loadModelToSeeOptions": "Carica un modello per vedere le opzioni",
  "schematicsError.title": "Gli schemi di configurazione contengono errori nei seguenti campi:",
  "manifestSections": {
    "structuredOutput/title": "Output Strutturato",
    "speculativeDecoding/title": "Decodifica Speculativa",
    "sampling/title": "Campionamento",
    "settings/title": "Impostazioni",
    "toolUse/title": "Uso degli Strumenti",
    "promptTemplate/title": "Template del Prompt"
  },
  "llm.prediction.systemPrompt/title": "Prompt di Sistema",
  "llm.prediction.systemPrompt/description": "Usa questo campo per fornire istruzioni di base al modello, come un insieme di regole, vincoli o requisiti generali.",
  "llm.prediction.systemPrompt/subTitle": "Linee guida per l'IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quanta casualit√† introdurre. 0 produrr√† lo stesso risultato ogni volta, mentre valori pi√π alti aumenteranno la creativit√† e la varianza",
  "llm.prediction.temperature/info": "Dalla documentazione di llama.cpp: \"Il valore predefinito √® <{{dynamicValue}}>, che fornisce un equilibrio tra casualit√† e determinismo. All'estremo, una temperatura di 0 sceglier√† sempre il token successivo pi√π probabile, portando a output identici in ogni esecuzione\"",
  "llm.prediction.llama.sampling/title": "Campionamento",
  "llm.prediction.topKSampling/title": "Campionamento Top-K",
  "llm.prediction.topKSampling/subTitle": "Limita il token successivo a uno dei k token pi√π probabili. Agisce in modo simile alla temperatura",
  "llm.prediction.topKSampling/info": "Dalla documentazione di llama.cpp:\n\nIl campionamento Top-k √® un metodo di generazione del testo che seleziona il token successivo solo tra i k token pi√π probabili previsti dal modello.\n\nAiuta a ridurre il rischio di generare token a bassa probabilit√† o senza senso, ma pu√≤ anche limitare la diversit√† dell'output.\n\nUn valore pi√π alto per top-k (es. 100) considerer√† pi√π token e porter√† a un testo pi√π vario, mentre un valore pi√π basso (es. 10) si concentrer√† sui token pi√π probabili e generer√† un testo pi√π conservativo.\n\n‚Ä¢ Il valore predefinito √® <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Thread CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Numero di thread CPU da utilizzare durante l'inferenza",
  "llm.prediction.llama.cpuThreads/info": "Il numero di thread da utilizzare durante il calcolo. Aumentare il numero di thread non sempre corrisponde a prestazioni migliori. Il valore predefinito √® <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limita Lunghezza Risposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Limita opzionalmente la lunghezza della risposta dell'IA",
  "llm.prediction.maxPredictedTokens/info": "Controlla la lunghezza massima della risposta del chatbot. Attiva per impostare un limite alla lunghezza massima di una risposta, o disattiva per lasciare che il chatbot decida quando fermarsi.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Lunghezza massima della risposta (token)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Circa {{maxWords}} parole",
  "llm.prediction.repeatPenalty/title": "Penalit√† per Ripetizione",
  "llm.prediction.repeatPenalty/subTitle": "Quanto scoraggiare la ripetizione dello stesso token",
  "llm.prediction.repeatPenalty/info": "Dalla documentazione di llama.cpp: \"Aiuta a prevenire che il modello generi testo ripetitivo o monotono.\n\nUn valore pi√π alto (es. 1.5) penalizzer√† pi√π fortemente le ripetizioni, mentre un valore pi√π basso (es. 0.9) sar√† pi√π indulgente.\" ‚Ä¢ Il valore predefinito √® <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Campionamento Min-P",
  "llm.prediction.minPSampling/subTitle": "Probabilit√† minima di base affinch√© un token venga selezionato per l'output",
  "llm.prediction.minPSampling/info": "Dalla documentazione di llama.cpp:\n\nLa probabilit√† minima affinch√© un token sia considerato, relativa alla probabilit√† del token pi√π probabile. Deve essere in [0, 1].\n\n‚Ä¢ Il valore predefinito √® <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Campionamento Top-P",
  "llm.prediction.topPSampling/subTitle": "Probabilit√† cumulativa minima per i possibili token successivi. Agisce in modo simile alla temperatura",
  "llm.prediction.topPSampling/info": "Dalla documentazione di llama.cpp:\n\nIl campionamento Top-p, noto anche come campionamento a nucleo, √® un altro metodo di generazione del testo che seleziona il token successivo da un sottoinsieme di token che insieme hanno una probabilit√† cumulativa di almeno p.\n\nQuesto metodo fornisce un equilibrio tra diversit√† e qualit√† considerando sia le probabilit√† dei token sia il numero di token da campionare.\n\nUn valore pi√π alto per top-p (es. 0.95) porter√† a un testo pi√π vario, mentre un valore pi√π basso (es. 0.5) generer√† un testo pi√π focalizzato e conservativo. Deve essere in (0, 1].\n\n‚Ä¢ Il valore predefinito √® <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stringhe di Interruzione",
  "llm.prediction.stopStrings/subTitle": "Stringhe che dovrebbero fermare il modello dal generare altri token",
  "llm.prediction.stopStrings/info": "Stringhe specifiche che, quando incontrate, fermeranno il modello dal generare altri token",
  "llm.prediction.stopStrings/placeholder": "Inserisci una stringa e premi ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Overflow del Contesto",
  "llm.prediction.contextOverflowPolicy/subTitle": "Come il modello dovrebbe comportarsi quando la conversazione diventa troppo grande per essere gestita",
  "llm.prediction.contextOverflowPolicy/info": "Decidi cosa fare quando la conversazione supera la dimensione della memoria di lavoro del modello ('contesto')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalit√† di Frequenza",
  "llm.prediction.llama.presencePenalty/title": "Penalit√† di Presenza",
  "llm.prediction.llama.tailFreeSampling/title": "Campionamento senza Coda (Tail-Free)",
  "llm.prediction.llama.locallyTypicalSampling/title": "Campionamento Localmente Tipico",
  "llm.prediction.llama.xtcProbability/title": "Probabilit√† di Campionamento XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "Il campionatore XTC (Exclude Top Choices) sar√† attivato solo con questa probabilit√† per ogni token generato. Il campionamento XTC pu√≤ aumentare la creativit√† e ridurre i clich√©",
  "llm.prediction.llama.xtcProbability/info": "Il campionamento XTC (Exclude Top Choices) sar√† attivato solo con questa probabilit√†, per ogni token generato. Il campionamento XTC di solito aumenta la creativit√† e riduce i clich√©",
  "llm.prediction.llama.xtcThreshold/title": "Soglia di Campionamento XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Soglia XTC (Exclude Top Choices). Con una probabilit√† di `xtc-probability`, cerca token con probabilit√† tra `xtc-threshold` e 0.5, e rimuove tutti questi token tranne quello meno probabile",
  "llm.prediction.llama.xtcThreshold/info": "Soglia XTC (Exclude Top Choices). Con una probabilit√† di `xtc-probability`, cerca token con probabilit√† tra `xtc-threshold` e 0.5, e rimuove tutti questi token tranne quello meno probabile",
  "llm.prediction.mlx.topKSampling/title": "Campionamento Top-K",
  "llm.prediction.mlx.topKSampling/subTitle": "Limita il token successivo a uno dei k token pi√π probabili. Agisce in modo simile alla temperatura",
  "llm.prediction.mlx.topKSampling/info": "Limita il token successivo a uno dei k token pi√π probabili. Agisce in modo simile alla temperatura",
  "llm.prediction.onnx.topKSampling/title": "Campionamento Top-K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita il token successivo a uno dei k token pi√π probabili. Agisce in modo simile alla temperatura",
  "llm.prediction.onnx.topKSampling/info": "Dalla documentazione ONNX:\n\nNumero di token del vocabolario con la probabilit√† pi√π alta da mantenere per il filtraggio top-k\n\n‚Ä¢ Questo filtro √® disattivato per impostazione predefinita",
  "llm.prediction.onnx.repeatPenalty/title": "Penalit√† per Ripetizione",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Quanto scoraggiare la ripetizione dello stesso token",
  "llm.prediction.onnx.repeatPenalty/info": "Un valore pi√π alto scoraggia il modello dal ripetersi",
  "llm.prediction.onnx.topPSampling/title": "Campionamento Top-P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilit√† cumulativa minima per i possibili token successivi. Agisce in modo simile alla temperatura",
  "llm.prediction.onnx.topPSampling/info": "Dalla documentazione ONNX:\n\nSolo i token pi√π probabili con probabilit√† che sommano a TopP o superiore vengono mantenuti per la generazione\n\n‚Ä¢ Questo filtro √® disattivato per impostazione predefinita",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Output Strutturato",
  "llm.prediction.structured/info": "Output Strutturato",
  "llm.prediction.structured/description": "Avanzato: puoi fornire uno [Schema JSON](https://json-schema.org/learn/miscellaneous-examples) per forzare un particolare formato di output dal modello. Leggi la [documentazione](https://lmstudio.ai/docs/advanced/structured-output) per saperne di pi√π",
  "llm.prediction.tools/title": "Uso degli Strumenti",
  "llm.prediction.tools/description": "Avanzato: puoi fornire un elenco di strumenti conformi a JSON che il modello pu√≤ richiedere di chiamare. Leggi la [documentazione](https://lmstudio.ai/docs/advanced/tool-use) per saperne di pi√π",
  "llm.prediction.tools/serverPageDescriptionAddon": "Passalo attraverso il corpo della richiesta come `tools` quando usi l'API del server",
  "llm.prediction.promptTemplate/title": "Template del Prompt",
  "llm.prediction.promptTemplate/subTitle": "Il formato in cui i messaggi della chat vengono inviati al modello. Modificare questo pu√≤ introdurre comportamenti inaspettati - assicurati di sapere cosa stai facendo!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Token Bozza da Generare",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "Il numero di token da generare con il modello bozza per ogni token del modello principale. Trova il punto di equilibrio tra calcolo e ricompensa",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Soglia di Probabilit√† per la Bozza",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continua a creare la bozza finch√© la probabilit√† di un token non scende al di sotto di questa soglia. Valori pi√π alti generalmente significano minor rischio, minor ricompensa",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Dimensione Minima Bozza",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Le bozze pi√π piccole di questa dimensione saranno ignorate dal modello principale. Valori pi√π alti generalmente significano minor rischio, minor ricompensa",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Dimensione Massima Bozza",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Numero massimo di token consentiti in una bozza. Limite massimo se tutte le probabilit√† dei token sono > della soglia. Valori pi√π bassi generalmente significano minor rischio, minor ricompensa",
  "llm.prediction.speculativeDecoding.draftModel/title": "Modello Bozza",
  "llm.prediction.reasoning.parsing/title": "Parsing Sezione di Ragionamento",
  "llm.prediction.reasoning.parsing/subTitle": "Come analizzare le sezioni di ragionamento nell'output del modello",
  "llm.load.contextLength/title": "Lunghezza del Contesto",
  "llm.load.contextLength/subTitle": "Il numero massimo di token a cui il modello pu√≤ prestare attenzione in un singolo prompt. Vedi le opzioni di Overflow della Conversazione sotto \"Parametri di Inferenza\" per altri modi di gestire questo",
  "llm.load.contextLength/info": "Specifica il numero massimo di token che il modello pu√≤ considerare contemporaneamente, influenzando quanto contesto mantiene durante l'elaborazione",
  "llm.load.contextLength/warning": "Impostare un valore alto per la lunghezza del contesto pu√≤ avere un impatto significativo sull'uso della memoria",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "Il seed per il generatore di numeri casuali usato nella generazione del testo. -1 √® casuale",
  "llm.load.seed/info": "Seed Casuale: Imposta il seed per la generazione di numeri casuali per garantire risultati riproducibili",
  "llm.load.llama.evalBatchSize/title": "Dimensione Batch di Valutazione",
  "llm.load.llama.evalBatchSize/subTitle": "Numero di token di input da elaborare alla volta. Aumentare questo valore aumenta le prestazioni a scapito dell'uso della memoria",
  "llm.load.llama.evalBatchSize/info": "Imposta il numero di esempi elaborati insieme in un unico batch durante la valutazione, influenzando velocit√† e uso della memoria",
  "llm.load.llama.ropeFrequencyBase/title": "Frequenza Base RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frequenza base personalizzata per gli embedding posizionali rotanti (RoPE). Aumentare questo valore pu√≤ consentire prestazioni migliori a lunghezze di contesto elevate",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanzato] Regola la frequenza base per la Codifica Posizionale Rotante, influenzando come vengono incorporate le informazioni posizionali",
  "llm.load.llama.ropeFrequencyScale/title": "Scala di Frequenza RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "La lunghezza del contesto √® scalata da questo fattore per estendere il contesto effettivo usando RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanzato] Modifica la scala della frequenza per la Codifica Posizionale Rotante per controllare la granularit√† della codifica posizionale",
  "llm.load.llama.acceleration.offloadRatio/title": "Offload GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Numero di layer discreti del modello da calcolare sulla GPU per l'accelerazione GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Imposta il numero di layer da scaricare sulla GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Diminuisce l'uso della memoria e il tempo di generazione su alcuni modelli",
  "llm.load.llama.flashAttention/info": "Accelera i meccanismi di attenzione per un'elaborazione pi√π veloce ed efficiente",
  "llm.load.numExperts/title": "Numero di Esperti",
  "llm.load.numExperts/subTitle": "Numero di esperti da utilizzare nel modello",
  "llm.load.numExperts/info": "Il numero di esperti da utilizzare nel modello",
  "llm.load.llama.keepModelInMemory/title": "Mantieni Modello in Memoria",
  "llm.load.llama.keepModelInMemory/subTitle": "Riserva memoria di sistema per il modello, anche quando scaricato sulla GPU. Migliora le prestazioni ma richiede pi√π RAM di sistema",
  "llm.load.llama.keepModelInMemory/info": "Impedisce che il modello venga scambiato su disco, garantendo un accesso pi√π rapido al costo di un maggiore utilizzo di RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usa FP16 per la Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Riduce l'uso della memoria memorizzando la cache a mezza precisione (FP16)",
  "llm.load.llama.tryMmap/title": "Prova mmap()",
  "llm.load.llama.tryMmap/subTitle": "Migliora il tempo di caricamento del modello. Disabilitarlo pu√≤ migliorare le prestazioni quando il modello √® pi√π grande della RAM di sistema disponibile",
  "llm.load.llama.tryMmap/info": "Carica i file del modello direttamente dal disco alla memoria",
  "llm.load.llama.cpuThreadPoolSize/title": "Dimensione Pool di Thread CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Numero di thread CPU da allocare al pool di thread utilizzato per il calcolo del modello",
  "llm.load.llama.cpuThreadPoolSize/info": "Il numero di thread CPU da allocare al pool di thread utilizzato per il calcolo del modello. Aumentare il numero di thread non sempre corrisponde a prestazioni migliori. Il valore predefinito √® <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Tipo di Quantizzazione Cache K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Valori pi√π bassi riducono l'uso della memoria ma possono diminuire la qualit√†. L'effetto varia significativamente tra i modelli.",
  "llm.load.llama.vCacheQuantizationType/title": "Tipo di Quantizzazione Cache V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Valori pi√π bassi riducono l'uso della memoria ma possono diminuire la qualit√†. L'effetto varia significativamente tra i modelli.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Devi disabilitare questo valore se Flash Attention non √® abilitato",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Pu√≤ essere attivato solo quando Flash Attention √® abilitato",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Devi disabilitare Flash Attention quando usi F32",
  "llm.load.mlx.kvCacheBits/title": "Quantizzazione Cache KV",
  "llm.load.mlx.kvCacheBits/subTitle": "Numero di bit a cui la cache KV deve essere quantizzata",
  "llm.load.mlx.kvCacheBits/info": "Numero di bit a cui la cache KV deve essere quantizzata",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "L'impostazione Lunghezza Contesto √® ignorata quando si usa la Quantizzazione Cache KV",
  "llm.load.mlx.kvCacheGroupSize/title": "Quantizzazione Cache KV: Dimensione Gruppo",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Dimensione del gruppo durante l'operazione di quantizzazione per la cache KV. Una dimensione di gruppo maggiore riduce l'uso della memoria ma pu√≤ diminuire la qualit√†",
  "llm.load.mlx.kvCacheGroupSize/info": "Numero di bit a cui la cache KV deve essere quantizzata",
  "llm.load.mlx.kvCacheQuantizationStart/title": "Quantizzazione Cache KV: Inizia a quantizzare quando il contesto supera questa lunghezza",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Soglia di lunghezza del contesto per iniziare a quantizzare la cache KV",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Soglia di lunghezza del contesto per iniziare a quantizzare la cache KV",
  "llm.load.mlx.kvCacheQuantization/title": "Quantizzazione Cache KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantizza la cache KV del modello. Ci√≤ pu√≤ comportare una generazione pi√π rapida e un minore ingombro di memoria, a scapito della qualit√† dell'output del modello.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "Bit di quantizzazione cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Numero di bit a cui quantizzare la cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bit",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Strategia dimensione gruppo",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Precisione",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Bilanciato",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Veloce",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Avanzato: Configurazione 'dimensione gruppo matmul' quantizzata\n\n‚Ä¢ Precisione = dimensione gruppo 32\n‚Ä¢ Bilanciato = dimensione gruppo 64\n‚Ä¢ Veloce = dimensione gruppo 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Inizia a quantizzare quando il ctx raggiunge questa lunghezza",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Quando il contesto raggiunge questa quantit√† di token,\ninizia a quantizzare la cache KV",
  "embedding.load.contextLength/title": "Lunghezza del Contesto",
  "embedding.load.contextLength/subTitle": "Il numero massimo di token a cui il modello pu√≤ prestare attenzione in un singolo prompt. Vedi le opzioni di Overflow della Conversazione sotto \"Parametri di Inferenza\" per altri modi di gestire questo",
  "embedding.load.contextLength/info": "Specifica il numero massimo di token che il modello pu√≤ considerare contemporaneamente, influenzando quanto contesto mantiene durante l'elaborazione",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequenza Base RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frequenza base personalizzata per gli embedding posizionali rotanti (RoPE). Aumentare questo valore pu√≤ consentire prestazioni migliori a lunghezze di contesto elevate",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avanzato] Regola la frequenza base per la Codifica Posizionale Rotante, influenzando come vengono incorporate le informazioni posizionali",
  "embedding.load.llama.evalBatchSize/title": "Dimensione Batch di Valutazione",
  "embedding.load.llama.evalBatchSize/subTitle": "Numero di token di input da elaborare alla volta. Aumentare questo valore aumenta le prestazioni a scapito dell'uso della memoria",
  "embedding.load.llama.evalBatchSize/info": "Imposta il numero di token elaborati insieme in un unico batch durante la valutazione",
  "embedding.load.llama.ropeFrequencyScale/title": "Scala di Frequenza RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "La lunghezza del contesto √® scalata da questo fattore per estendere il contesto effettivo usando RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avanzato] Modifica la scala della frequenza per la Codifica Posizionale Rotante per controllare la granularit√† della codifica posizionale",
  "embedding.load.llama.acceleration.offloadRatio/title": "Offload GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Numero di layer discreti del modello da calcolare sulla GPU per l'accelerazione GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Imposta il numero di layer da scaricare sulla GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Mantieni Modello in Memoria",
  "embedding.load.llama.keepModelInMemory/subTitle": "Riserva memoria di sistema per il modello, anche quando scaricato sulla GPU. Migliora le prestazioni ma richiede pi√π RAM di sistema",
  "embedding.load.llama.keepModelInMemory/info": "Impedisce che il modello venga scambiato su disco, garantendo un accesso pi√π rapido al costo di un maggiore utilizzo di RAM",
  "embedding.load.llama.tryMmap/title": "Prova mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Migliora il tempo di caricamento del modello. Disabilitarlo pu√≤ migliorare le prestazioni quando il modello √® pi√π grande della RAM di sistema disponibile",
  "embedding.load.llama.tryMmap/info": "Carica i file del modello direttamente dal disco alla memoria",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "Il seed per il generatore di numeri casuali usato nella generazione del testo. -1 √® seed casuale",
  "embedding.load.seed/info": "Seed Casuale: Imposta il seed per la generazione di numeri casuali per garantire risultati riproducibili",
  "presetTooltip": {
    "included/title": "Valori Preimpostati",
    "included/description": "Saranno applicati i seguenti campi",
    "included/empty": "Nessun campo di questa preimpostazione si applica in questo contesto.",
    "included/conflict": "Ti verr√† chiesto di scegliere se applicare questo valore",
    "separateLoad/title": "Configurazione al Caricamento",
    "separateLoad/description.1": "La preimpostazione include anche la seguente configurazione al caricamento. La configurazione al caricamento √® a livello di modello e richiede il ricaricamento del modello per avere effetto. Tieni premuto",
    "separateLoad/description.2": "per applicare a",
    "separateLoad/description.3": ".",
    "excluded/title": "Potrebbe non applicarsi",
    "excluded/description": "I seguenti campi sono inclusi nella preimpostazione ma non si applicano nel contesto attuale.",
    "legacy/title": "Preimpostazione Legacy",
    "legacy/description": "Questa √® una preimpostazione legacy. Include i seguenti campi che ora sono gestiti automaticamente o non sono pi√π applicabili.",
    "button/publish": "Pubblica sull'Hub",
    "button/pushUpdate": "Invia Modifiche all'Hub",
    "button/export": "Esporta"
  },
  "customInputs": {
    "string": {
      "emptyParagraph": "<Vuoto>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "llamaCacheQuantizationType": {
      "off": "OFF"
    },
    "mlxKvCacheBits": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Vuoto>"
    },
    "llmPromptTemplate": {
      "type": "Tipo",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Template",
      "jinja/error": "Impossibile analizzare il template Jinja: {{error}}",
      "jinja/empty": "Inserisci un template Jinja sopra.",
      "jinja/unlikelyToWork": "Il template Jinja che hai fornito sopra √® improbabile che funzioni poich√© non fa riferimento alla variabile \"messages\". Per favore, controlla di nuovo se hai inserito un template corretto.",
      "types.manual/label": "Manuale",
      "manual.subfield.beforeSystem/label": "Prima del Sistema",
      "manual.subfield.beforeSystem/placeholder": "Inserisci prefisso Sistema...",
      "manual.subfield.afterSystem/label": "Dopo il Sistema",
      "manual.subfield.afterSystem/placeholder": "Inserisci suffisso Sistema...",
      "manual.subfield.beforeUser/label": "Prima dell'Utente",
      "manual.subfield.beforeUser/placeholder": "Inserisci prefisso Utente...",
      "manual.subfield.afterUser/label": "Dopo l'Utente",
      "manual.subfield.afterUser/placeholder": "Inserisci suffisso Utente...",
      "manual.subfield.beforeAssistant/label": "Prima dell'Assistente",
      "manual.subfield.beforeAssistant/placeholder": "Inserisci prefisso Assistente...",
      "manual.subfield.afterAssistant/label": "Dopo l'Assistente",
      "manual.subfield.afterAssistant/placeholder": "Inserisci suffisso Assistente...",
      "stopStrings/label": "Stringhe di Interruzione Aggiuntive",
      "stopStrings/subTitle": "Stringhe di interruzione specifiche del template che verranno utilizzate in aggiunta a quelle specificate dall'utente."
    },
    "contextLength": {
      "maxValueTooltip": "Questo √® il numero massimo di token che il modello √® stato addestrato a gestire. Clicca per impostare il contesto a questo valore",
      "maxValueTextStart": "Il modello supporta fino a",
      "maxValueTextEnd": "token",
      "tooltipHint": "Anche se un modello pu√≤ supportare un certo numero di token, le prestazioni potrebbero deteriorarsi se le risorse della tua macchina non possono gestire il carico - usa cautela nell'aumentare questo valore"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Ferma al Limite",
      "stopAtLimitSub": "Smetti di generare una volta che la memoria del modello si riempie",
      "truncateMiddle": "Tronca al Centro",
      "truncateMiddleSub": "Rimuove i messaggi dal centro della conversazione per fare spazio a quelli pi√π recenti. Il modello ricorder√† comunque l'inizio della conversazione",
      "rollingWindow": "Finestra Mobile",
      "rollingWindowSub": "Il modello ricever√† sempre i messaggi pi√π recenti ma potrebbe dimenticare l'inizio della conversazione"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Uniformemente",
      "favorMainGpu": "Favorisci GPU Principale"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Leggi come funziona",
      "placeholder": "Seleziona un modello bozza compatibile",
      "noCompatible": "Nessun modello bozza compatibile trovato per la tua selezione di modello attuale",
      "stillLoading": "Identificazione dei modelli bozza compatibili in corso...",
      "notCompatible": "Il modello bozza selezionato (<draft/>) non √® compatibile con la selezione del modello attuale (<current/>).",
      "off": "OFF",
      "loadModelToSeeOptions": "Carica modello <keyboard-shortcut /> per vedere le opzioni compatibili",
      "compatibleWithNumberOfModels": "Raccomandato per almeno {{dynamicValue}} dei tuoi modelli",
      "recommendedForSomeModels": "Raccomandato per alcuni modelli",
      "recommendedForLlamaModels": "Raccomandato per i modelli Llama",
      "recommendedForQwenModels": "Raccomandato per i modelli Qwen",
      "onboardingModal": {
        "introducing": "Presentazione di",
        "speculativeDecoding": "Decodifica Speculativa",
        "firstStepBody": "Aumento della velocit√† di inferenza per modelli <custom-span>llama.cpp</custom-span> e <custom-span>MLX</custom-span>",
        "secondStepTitle": "Aumento della Velocit√† di Inferenza con Decodifica Speculativa",
        "secondStepBody": "La Decodifica Speculativa √® una tecnica che coinvolge la collaborazione di due modelli:\n - Un modello \"principale\" pi√π grande\n - Un modello \"bozza\" pi√π piccolo\n\nDurante la generazione, il modello bozza propone rapidamente dei token che il modello principale pi√π grande deve verificare. Verificare i token √® un processo molto pi√π veloce che generarli, ed √® questa la fonte dei guadagni di velocit√†. **Generalmente, maggiore √® la differenza di dimensioni tra il modello principale e il modello bozza, maggiore sar√† l'aumento di velocit√†**.\n\nPer mantenere la qualit√†, il modello principale accetta solo i token che si allineano con ci√≤ che avrebbe generato autonomamente, consentendo la qualit√† di risposta del modello pi√π grande a velocit√† di inferenza maggiori. Entrambi i modelli devono condividere lo stesso vocabolario.",
        "draftModelRecommendationsTitle": "Raccomandazioni per il modello bozza",
        "basedOnCurrentModels": "Basato sui tuoi modelli attuali",
        "close": "Chiudi",
        "next": "Avanti",
        "done": "Fatto"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Per favore, carica prima un modello <model-badge /> ",
      "errorEngineNotSupported": "La decodifica speculativa richiede almeno la versione {{minVersion}} del motore {{engineName}}. Per favore, aggiorna il motore (<key/>) e ricarica il modello per usare questa funzione.",
      "errorEngineNotSupported/noKey": "La decodifica speculativa richiede almeno la versione {{minVersion}} del motore {{engineName}}. Per favore, aggiorna il motore e ricarica il modello per usare questa funzione."
    },
    "llmReasoningParsing": {
      "startString/label": "Stringa di Inizio",
      "startString/placeholder": "Inserisci la stringa di inizio...",
      "endString/label": "Stringa di Fine",
      "endString/placeholder": "Inserisci la stringa di fine..."
    }
  },
  "saveConflictResolution": {
    "title": "Scegli quali valori includere nella Preimpostazione",
    "description": "Scegli quali valori mantenere",
    "instructions": "Clicca su un valore per includerlo",
    "userValues": "Valore Precedente",
    "presetValues": "Nuovo Valore",
    "confirm": "Conferma",
    "cancel": "Annulla"
  },
  "applyConflictResolution": {
    "title": "Quali valori mantenere?",
    "description": "Hai modifiche non salvate che si sovrappongono con la Preimpostazione in arrivo",
    "instructions": "Clicca su un valore per mantenerlo",
    "userValues": "Valore Attuale",
    "presetValues": "Valore Preimpostazione in Arrivo",
    "confirm": "Conferma",
    "cancel": "Annulla"
  },
  "empty": "<Vuoto>",
  "noModelSelected": "Nessun modello selezionato",
  "apiIdentifier.label": "Identificatore API",
  "apiIdentifier.hint": "Fornisci opzionalmente un identificatore per questo modello. Sar√† usato nelle richieste API. Lascia vuoto per usare l'identificatore predefinito.",
  "idleTTL.label": "Scarica Automaticamente se Inattivo (TTL)",
  "idleTTL.hint": "Se impostato, il modello sar√† scaricato automaticamente dopo essere stato inattivo per il tempo specificato.",
  "idleTTL.mins": "min",
  "presets": {
    "title": "Preimpostazione",
    "commitChanges": "Salva Modifiche",
    "commitChanges/description": "Salva le tue modifiche nella preimpostazione.",
    "commitChanges.manual": "Rilevati nuovi campi. Potrai scegliere quali modifiche includere nella preimpostazione.",
    "commitChanges.manual.hold.0": "Tieni premuto",
    "commitChanges.manual.hold.1": "per scegliere quali modifiche salvare nella preimpostazione.",
    "commitChanges.saveAll.hold.0": "Tieni premuto",
    "commitChanges.saveAll.hold.1": "per salvare tutte le modifiche.",
    "commitChanges.saveInPreset.hold.0": "Tieni premuto",
    "commitChanges.saveInPreset.hold.1": "per salvare solo le modifiche ai campi gi√† inclusi nella preimpostazione.",
    "commitChanges/error": "Impossibile salvare le modifiche alla preimpostazione.",
    "commitChanges.manual/description": "Scegli quali modifiche includere nella preimpostazione.",
    "saveAs": "Salva Come Nuovo...",
    "presetNamePlaceholder": "Inserisci un nome per la preimpostazione...",
    "cannotCommitChangesLegacy": "Questa √® una preimpostazione legacy e non pu√≤ essere modificata. Puoi creare una copia usando \"Salva Come Nuovo...\".",
    "cannotCommitChangesNoChanges": "Nessuna modifica da salvare.",
    "emptyNoUnsaved": "Seleziona una Preimpostazione...",
    "emptyWithUnsaved": "Preimpostazione non Salvata",
    "saveEmptyWithUnsaved": "Salva Preimpostazione Come...",
    "saveConfirm": "Salva",
    "saveCancel": "Annulla",
    "saving": "Salvataggio in corso...",
    "save/error": "Impossibile salvare la preimpostazione.",
    "deselect": "Deseleziona Preimpostazione",
    "deselect/error": "Impossibile deselezionare la preimpostazione.",
    "select/error": "Impossibile selezionare la preimpostazione.",
    "delete/error": "Impossibile eliminare la preimpostazione.",
    "discardChanges": "Annulla Modifiche non Salvate",
    "discardChanges/info": "Annulla tutte le modifiche non salvate e ripristina lo stato originale della preimpostazione",
    "newEmptyPreset": "+ Nuova Preimpostazione",
    "importPreset": "Importa",
    "contextMenuSelect": "Applica Preimpostazione",
    "contextMenuDelete": "Elimina...",
    "contextMenuShare": "Pubblica...",
    "contextMenuOpenInHub": "Visualizza sull'Hub",
    "contextMenuPushChanges": "Invia modifiche all'Hub",
    "contextMenuPushingChanges": "Invio modifiche in corso...",
    "contextMenuPushedChanges": "Modifiche inviate",
    "contextMenuExport": "Esporta File",
    "contextMenuRevealInExplorer": "Mostra in Esplora File",
    "contextMenuRevealInFinder": "Mostra nel Finder",
    "share": {
      "title": "Pubblica Preimpostazione",
      "action": "Condividi la tua preimpostazione affinch√© altri possano scaricarla, metterle 'mi piace' e farne un fork",
      "presetOwnerLabel": "Proprietario",
      "uploadAs": "La tua preimpostazione sar√† creata come {{name}}",
      "presetNameLabel": "Nome Preimpostazione",
      "descriptionLabel": "Descrizione (opzionale)",
      "loading": "Pubblicazione in corso...",
      "success": "Preimpostazione Inviata con Successo",
      "presetIsLive": "<preset-name /> √® ora online sull'Hub!",
      "close": "Chiudi",
      "confirmViewOnWeb": "Visualizza sul web",
      "confirmCopy": "Copia URL",
      "confirmCopied": "Copiato!",
      "pushedToHub": "La tua preimpostazione √® stata inviata all'Hub",
      "descriptionPlaceholder": "Inserisci una descrizione...",
      "willBePublic": "La pubblicazione della tua preimpostazione la render√† pubblica",
      "publicSubtitle": "La tua preimpostazione √® <custom-bold>Pubblica</custom-bold>. Altri possono scaricarla e farne un fork su lmstudio.ai",
      "confirmShareButton": "Pubblica",
      "error": "Impossibile pubblicare la preimpostazione",
      "createFreeAccount": "Crea un account gratuito sull'Hub per pubblicare preimpostazioni"
    },
    "update": {
      "title": "Invia Modifiche all'Hub",
      "title/success": "Preimpostazione Aggiornata con Successo",
      "subtitle": "Apporta modifiche a <custom-preset-name /> e inviale all'Hub",
      "descriptionLabel": "Descrizione",
      "descriptionPlaceholder": "Inserisci una descrizione...",
      "loading": "Invio in corso...",
      "cancel": "Annulla",
      "createFreeAccount": "Crea un account gratuito sull'Hub per pubblicare preimpostazioni",
      "error": "Impossibile inviare l'aggiornamento",
      "confirmUpdateButton": "Invia"
    },
    "import": {
      "title": "Importa una Preimpostazione da File",
      "dragPrompt": "Trascina e rilascia file JSON di preimpostazione o <custom-link>seleziona dal tuo computer</custom-link>",
      "remove": "Rimuovi",
      "cancel": "Annulla",
      "importPreset_zero": "Importa Preimpostazione",
      "importPreset_one": "Importa Preimpostazione",
      "importPreset_other": "Importa {{count}} Preimpostazioni",
      "selectDialog": {
        "title": "Seleziona File Preimpostazione (.json)",
        "button": "Importa"
      },
      "error": "Impossibile importare la preimpostazione",
      "resultsModal": {
        "titleSuccessSection_one": "Importata 1 preimpostazione con successo",
        "titleSuccessSection_other": "Importate {{count}} preimpostazioni con successo",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} fallita)",
        "titleFailSection_other": "({{count}} fallite)",
        "titleAllFailed": "Impossibile importare le preimpostazioni",
        "importMore": "Importa Altre",
        "close": "Fatto",
        "successBadge": "Successo",
        "alreadyExistsBadge": "La preimpostazione esiste gi√†",
        "errorBadge": "Errore",
        "invalidFileBadge": "File non valido",
        "otherErrorBadge": "Impossibile importare la preimpostazione",
        "errorViewDetailsButton": "Vedi Dettagli",
        "seeError": "Vedi Errore",
        "noName": "Nessun nome preimpostazione",
        "useInChat": "Usa nella Chat"
      },
      "importFromUrl": {
        "button": "Importa da URL...",
        "title": "Importa da URL",
        "back": "Importa da File...",
        "action": "Incolla l'URL dell'Hub di LM Studio della preimpostazione che vuoi importare qui sotto",
        "invalidUrl": "URL non valido. Assicurati di incollare un URL corretto dell'Hub di LM Studio.",
        "tip": "Puoi installare la preimpostazione direttamente con il pulsante {{buttonName}} nell'Hub di LM Studio",
        "confirm": "Importa",
        "cancel": "Annulla",
        "loading": "Importazione in corso...",
        "error": "Impossibile scaricare la preimpostazione."
      }
    },
    "download": {
      "title": "Scarica <preset-name /> dall'Hub di LM Studio",
      "subtitle": "Salva <custom-name /> nelle tue preimpostazioni. Facendo ci√≤ potrai usare questa preimpostazione nell'app",
      "button": "Scarica",
      "button/loading": "Scaricamento in corso...",
      "cancel": "Annulla",
      "error": "Impossibile scaricare la preimpostazione."
    },
    "inclusiveness": {
      "speculativeDecoding": "Includi nella Preimpostazione"
    }
  },
  "flashAttentionWarning": "Flash Attention √® una funzionalit√† sperimentale che potrebbe causare problemi con alcuni modelli. Se incontri problemi, prova a disabilitarla.",
  "llamaKvCacheQuantizationWarning": "La Quantizzazione della Cache KV √® una funzionalit√† sperimentale che potrebbe causare problemi con alcuni modelli. Flash Attention deve essere abilitato per la quantizzazione della cache V. Se incontri problemi, ripristina il valore predefinito \"F16\".",
  "seedUncheckedHint": "Seed Casuale",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",
  "hardware": {
    "advancedGpuSettings": "Impostazioni GPU Avanzate",
    "advancedGpuSettings.info": "Se non sei sicuro, lascia questi valori predefiniti",
    "advancedGpuSettings.reset": "Ripristina predefiniti",
    "environmentVariables": {
      "title": "Variabili d'Ambiente",
      "description": "Variabili d'ambiente attive durante il ciclo di vita del modello.",
      "key.placeholder": "Seleziona var...",
      "value.placeholder": "Valore"
    },
    "mainGpu": {
      "title": "GPU Principale",
      "description": "La GPU da prioritizzare per il calcolo del modello.",
      "placeholder": "Seleziona GPU principale..."
    },
    "splitStrategy": {
      "title": "Strategia di Divisione",
      "description": "Come dividere il calcolo del modello tra le GPU.",
      "placeholder": "Seleziona strategia di divisione..."
    }
  }
}