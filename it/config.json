{
  "noInstanceSelected": "Nessuna istanza di modello selezionata",
  "resetToDefault": "Ripristina",
  "showAdvancedSettings": "Mostra impostazioni avanzate",
  "showAll": "Mostra tutto",
  "basicSettings": "Base",
  "configSubtitle": "Carica o salva preset e sperimenta con la modifica dei parametri del modello",
  "inferenceParameters/title": "Parametri di Predizione",
  "inferenceParameters/info": "Sperimenta con i parametri che influenzano la predizione.",
  "generalParameters/title": "Generale",
  "samplingParameters/title": "Campionamento",
  "basicTab": "Base",
  "advancedTab": "Avanzato",
  "advancedTab/title": "Impostazioni di Esecuzione",
  "advancedTab/expandAll": "Espandi tutto",
  "advancedTab/overridesTitle": "Sostituzioni Attive",
  "advancedTab/noConfigsText": "Non ci sono modifiche non salvate - modifica i valori sopra per vedere le sostituzioni qui.",
  "loadInstanceFirst": "Carica un modello per vedere i parametri configurabili",
  "noListedConfigs": "Non ci sono parametri configurabili",
  "generationParameters/info": "Sperimenta con parametri di base che influenzano la generazione del testo.",
  "loadParameters/title": "Carica Parametri",
  "loadParameters/description": "Cambiare questi parametri richiede di ricaricare il modello",
  "loadParameters/reload": "Ricarica per applicare le modifiche dei parametri di caricamento",
  "discardChanges": "Scarta modifiche",
  "llm.prediction.systemPrompt/title": "Messaggio di Sistema",
  "llm.prediction.systemPrompt/description": "Usa questo campo per fornire istruzioni di base al modello, come un insieme di regole, restrizioni o requisiti generali. Questo campo è anche conosciuto come il 'messaggio di sistema'.",
  "llm.prediction.systemPrompt/subTitle": "Linee guida per l'IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Il sottotitolo va qui",
  "llm.prediction.temperature/info": "Dai documenti di aiuto di llama.cpp: 'Il valore predefinito è <{{dynamicValue}}>, che fornisce un equilibrio tra casualità e determinismo. All'estremo, una temperatura di 0 sceglierà sempre il prossimo token più probabile, portando a risultati identici in ogni esecuzione'",
  "llm.prediction.topKSampling/title": "Campionamento Top K",
  "llm.prediction.topKSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.topKSampling/info": "Dai documenti di aiuto di llama.cpp:\n\nIl campionamento top-k è un metodo di generazione di testo che seleziona il prossimo token solo dai k token più probabili predetti dal modello.\n\nAiuta a ridurre il rischio di generare token di bassa probabilità o senza senso, ma può anche limitare la diversità dell'output.\n\nUn valore più alto per top-k (ad esempio, 100) considererà più token e genererà un testo più diversificato, mentre un valore più basso (ad esempio, 10) si concentrerà sui token più probabili e genererà un testo più conservativo.\n\n• Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Thread CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.cpuThreads/info": "Il numero di thread da utilizzare durante il calcolo. Aumentare il numero di thread non sempre si traduce in un miglior rendimento. Il valore predefinito è <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limita Lunghezza Risposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Il sottotitolo va qui",
  "llm.prediction.maxPredictedTokens/info": "Controlla la lunghezza massima della risposta del chatbot. Attivalo per impostare un limite alla lunghezza massima di una risposta, o disattivalo per permettere al chatbot di decidere quando fermarsi.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Lunghezza massima della risposta (token)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Circa {{maxWords}} parole",
  "llm.prediction.repeatPenalty/title": "Penalità per Ripetizione",
  "llm.prediction.repeatPenalty/subTitle": "Il sottotitolo va qui",
  "llm.prediction.repeatPenalty/info": "Dai documenti di aiuto di llama.cpp: 'Aiuta a prevenire che il modello generi testo ripetitivo o monotono.\n\nUn valore più alto (ad esempio, 1.5) penalizzerà le ripetizioni più fortemente, mentre un valore più basso (ad esempio, 0.9) sarà più indulgente.' • Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Campionamento Min P",
  "llm.prediction.minPSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.minPSampling/info": "Dai documenti di aiuto di llama.cpp:\n\nLa probabilità minima per cui un token viene considerato, relativa alla probabilità del token più probabile. Deve essere in [0, 1].\n\n• Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Campionamento Top P",
  "llm.prediction.topPSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.topPSampling/info": "Dai documenti di aiuto di llama.cpp:\n\nIl campionamento top-p, noto anche come campionamento del nucleo, è un altro metodo di generazione di testo che seleziona il prossimo token da un sottoinsieme di token che insieme hanno una probabilità cumulativa di almeno p.\n\nQuesto metodo fornisce un equilibrio tra diversità e qualità considerando sia le probabilità dei token che il numero di token da campionare.\n\nUn valore più alto per top-p (ad esempio, 0.95) genererà un testo più diversificato, mentre un valore più basso (ad esempio, 0.5) genererà un testo più mirato e conservativo. Deve essere in (0, 1].\n\n• Il valore predefinito è <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stringhe di Arresto",
  "llm.prediction.stopStrings/subTitle": "Il sottotitolo va qui",
  "llm.prediction.stopStrings/info": "Stringhe specifiche che, se trovate, interromperanno il modello nella generazione di ulteriori token",
  "llm.prediction.stopStrings/placeholder": "Inserisci una stringa e premi ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Gestione del Sovraccarico della Conversazione",
  "llm.prediction.contextOverflowPolicy/subTitle": "Il sottotitolo va qui",
  "llm.prediction.contextOverflowPolicy/info": "Decidi cosa fare quando la conversazione supera la dimensione della memoria di lavoro del modello ('contesto')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Fermati al Limite",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Smetti di generare una volta che la memoria del modello è piena",
  "customInputs.contextOverflowPolicy.truncateMiddle": "Tronca il Centro",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Rimuove i messaggi centrali della conversazione per fare spazio ai più recenti. Il modello ricorderà ancora l'inizio della conversazione",
  "customInputs.contextOverflowPolicy.rollingWindow": "Finestra Mobile",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "Il modello otterrà sempre gli ultimi messaggi, ma potrebbe dimenticare l'inizio della conversazione",
  "llm.prediction.llama.frequencyPenalty/title": "Penalità per Frequenza",
  "llm.prediction.llama.frequencyPenalty/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.presencePenalty/title": "Penalità per Presenza",
  "llm.prediction.llama.presencePenalty/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.tailFreeSampling/title": "Campionamento Senza Coda",
  "llm.prediction.llama.tailFreeSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.llama.locallyTypicalSampling/title": "Campionamento Tipico Locale",
  "llm.prediction.llama.locallyTypicalSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.onnx.topKSampling/title": "Campionamento Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.onnx.topKSampling/info": "Dalla documentazione di ONNX:\n\nNumero di token di vocabolario con maggiore probabilità da mantenere per il filtro top-k\n\n• Questo filtro è disattivato per impostazione predefinita",
  "llm.prediction.onnx.repeatPenalty/title": "Penalità per Ripetizione",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Il sottotitolo va qui",
  "llm.prediction.onnx.repeatPenalty/info": "Un valore più alto scoraggia il modello dal ripetersi",
  "llm.prediction.onnx.topPSampling/title": "Campionamento Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Il sottotitolo va qui",
  "llm.prediction.onnx.topPSampling/info": "Dalla documentazione di ONNX:\n\nVengono mantenuti solo i token più probabili con probabilità che sommano TopP o più per la generazione\n\n• Questo filtro è disattivato per impostazione predefinita",
  "llm.prediction.seed/title": "Seme",
  "llm.prediction.seed/subTitle": "Il sottotitolo va qui",
  "llm.prediction.structured/title": "Output Strutturato",
  "llm.prediction.structured/subTitle": "Il sottotitolo va qui",
  "llm.prediction.structured/info": "Output Strutturato",
  "llm.load.contextLength/title": "Lunghezza del Contesto",
  "llm.load.contextLength/subTitle": "Il sottotitolo va qui",
  "llm.load.contextLength/info": "Specifica il numero massimo di token che il modello può considerare contemporaneamente, influenzando quanto contesto viene mantenuto durante l'elaborazione",
  "llm.load.seed/title": "Seme",
  "llm.load.seed/subTitle": "Il sottotitolo va qui",
  "llm.load.seed/info": "Seme Casuale: Imposta il seme per la generazione di numeri casuali per assicurare risultati riproducibili",
  "llm.load.llama.evalBatchSize/title": "Dimensione del Lotto di Valutazione",
  "llm.load.llama.evalBatchSize/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.evalBatchSize/info": "Imposta il numero di esempi elaborati insieme in un lotto durante la valutazione, influenzando la velocità e l'uso di memoria",
  "llm.load.llama.ropeFrequencyBase/title": "Frequenza Base RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanzato] Regola la frequenza base per la Codifica Posizionale Rotatoria, influenzando come viene incorporata l'informazione posizionale",
  "llm.load.llama.ropeFrequencyScale/title": "Scala di Frequenza RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanzato] Modifica la scala di frequenza per la Codifica Posizionale Rotatoria per controllare la granularità della codifica posizionale",
  "llm.load.llama.acceleration.offloadRatio/title": "Scaricamento su GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.acceleration.offloadRatio/info": "Imposta la proporzione di calcolo da scaricare su GPU. Impostalo su spento per disattivare lo scaricamento su GPU, o su automatico affinché il modello decida.",
  "llm.load.llama.flashAttention/title": "Attenzione Flash",
  "llm.load.llama.flashAttention/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.flashAttention/info": "Accelera i meccanismi di attenzione per un'elaborazione più rapida ed efficiente",
  "llm.load.llama.keepModelInMemory/title": "Mantieni il Modello in Memoria",
  "llm.load.llama.keepModelInMemory/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.keepModelInMemory/info": "Evita che il modello venga scambiato su disco, assicurando un accesso più rapido a scapito di un maggiore uso di RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usa FP16 per la Cache KV",
  "llm.load.llama.useFp16ForKVCache/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.useFp16ForKVCache/info": "Riduce l'uso di memoria memorizzando la cache in precisione ridotta (FP16)",
  "llm.load.llama.tryMmap/title": "Prova mmap()",
  "llm.load.llama.tryMmap/subTitle": "Il sottotitolo va qui",
  "llm.load.llama.tryMmap/info": "Carica i file del modello direttamente dal disco alla memoria"
}