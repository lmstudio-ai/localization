{
  "noInstanceSelected": "Nie wybrano instancji modelu",
  "resetToDefault": "Resetuj",
  "showAdvancedSettings": "Pokaż ustawienia zaawansowane",
  "showAll": "Wszystkie",
  "basicSettings": "Podstawowe",
  "configSubtitle": "Wczytaj lub zapisz ustawienia i eksperymentuj z parametrami modelu",
  "inferenceParameters/title": "Parametry predykcji",
  "inferenceParameters/info": "Eksperymentuj z parametrami wpływającymi na predykcję.",
  "generalParameters/title": "Ogólne",
  "samplingParameters/title": "Próbkowanie",
  "basicTab": "Podstawowe",
  "advancedTab": "Zaawansowane",
  "advancedTab/title": "🧪 Konfiguracja zaawansowana",
  "advancedTab/expandAll": "Rozwiń wszystkie",
  "advancedTab/overridesTitle": "Nadpisania konfiguracji",
  "advancedTab/noConfigsText": "Nie masz niezapisanych zmian - edytuj wartości powyżej, aby zobaczyć nadpisania tutaj.",
  "loadInstanceFirst": "Wczytaj model, aby zobaczyć konfigurowalne parametry",
  "noListedConfigs": "Brak konfigurowalnych parametrów",
  "generationParameters/info": "Eksperymentuj z podstawowymi parametrami wpływającymi na generowanie tekstu.",
  "loadParameters/title": "Parametry wczytywania",
  "loadParameters/description": "Ustawienia kontrolujące sposób inicjalizacji i wczytywania modelu do pamięci.",
  "loadParameters/reload": "Przeładuj, aby zastosować zmiany",
  "discardChanges": "Odrzuć zmiany",
  "llm.prediction.systemPrompt/title": "Prompt systemowy",
  "llm.prediction.systemPrompt/description": "Użyj tego pola, aby dostarczyć modelowi instrukcje tła, takie jak zestaw reguł, ograniczeń lub ogólnych wymagań. To pole jest często określane jako \"prompt systemowy\".",
  "llm.prediction.systemPrompt/subTitle": "Wytyczne dla AI",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Ile losowości wprowadzić. 0 da ten sam wynik za każdym razem, podczas gdy wyższe wartości zwiększą kreatywność i zmienność",
  "llm.prediction.temperature/info": "Z dokumentacji pomocy llama.cpp: \"Domyślna wartość to <{{dynamicValue}}>, co zapewnia równowagę między losowością a determinizmem. W skrajnym przypadku temperatura 0 zawsze wybierze najbardziej prawdopodobny następny token,... prowadząc do identycznych wyników w każdym uruchomieniu\"",
  "llm.prediction.llama.sampling/title": "Próbkowanie",
  "llm.prediction.topKSampling/title": "Próbkowanie Top K",
  "llm.prediction.topKSampling/subTitle": "Ogranicza następny token do jednego z k najbardziej prawdopodobnych tokenów. Działa podobnie do temperatury",
  "llm.prediction.topKSampling/info": "Z dokumentacji pomocy llama.cpp:\n\nPróbkowanie top-k to metoda generowania tekstu, która wybiera następny token tylko spośród k najbardziej prawdopodobnych tokenów przewidzianych przez model.\n\nPomaga to zmniejszyć ryzyko generowania tokenów o niskim prawdopodobieństwie lub bezsensownych, ale może również ograniczyć różnorodność wyjścia.\n\nWyższa wartość dla top-k (np. 100) uwzględni więcej tokenów i doprowadzi do bardziej zróżnicowanego tekstu, podczas gdy niższa wartość (np. 10) skupi się na najbardziej prawdopodobnych tokenach i wygeneruje bardziej konserwatywny tekst.\n\n• Domyślna wartość to <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Wątki CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Liczba wątków CPU do użycia podczas wnioskowania",
  "llm.prediction.llama.cpuThreads/info": "Liczba wątków do użycia podczas obliczeń. Zwiększenie liczby wątków nie zawsze koreluje z lepszą wydajnością. Domyślnie jest to <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Ogranicz długość odpowiedzi",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcjonalnie ogranicz długość odpowiedzi AI",
  "llm.prediction.maxPredictedTokens/info": "Kontroluj maksymalną długość odpowiedzi chatbota. Włącz, aby ustawić limit maksymalnej długości odpowiedzi, lub wyłącz, aby pozwolić chatbotowi zdecydować, kiedy zakończyć.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maksymalna długość odpowiedzi (tokeny)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Około {{maxWords}} słów",
  "llm.prediction.repeatPenalty/title": "Kara za powtórzenia",
  "llm.prediction.repeatPenalty/subTitle": "Jak bardzo zniechęcać do powtarzania tego samego tokena",
  "llm.prediction.repeatPenalty/info": "Z dokumentacji pomocy llama.cpp:... \"Pomaga zapobiegać generowaniu przez model powtarzalnego lub monotonnego tekstu.\n\nWyższa wartość (np. 1,5) będzie bardziej surowo karać powtórzenia, podczas gdy niższa wartość (np. 0,9) będzie bardziej pobłażliwa.\" • Domyślna wartość to <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Próbkowanie Min P",
  "llm.prediction.minPSampling/subTitle": "Minimalne prawdopodobieństwo bazowe dla tokena, aby został wybrany do wyjścia",
  "llm.prediction.minPSampling/info": "Z dokumentacji pomocy llama.cpp:\n\nMinimalne prawdopodobieństwo dla tokena do rozważenia, w stosunku do prawdopodobieństwa najbardziej prawdopodobnego tokena. Musi być w [0, 1].\n\n• Domyślna wartość to <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Próbkowanie Top P",
  "llm.prediction.topPSampling/subTitle": "Minimalne skumulowane prawdopodobieństwo dla możliwych następnych tokenów. Działa podobnie do temperatury",
  "llm.prediction.topPSampling/info": "Z dokumentacji pomocy llama.cpp:\n\nPróbkowanie top-p, znane również jako próbkowanie jądrowe, to inna metoda generowania tekstu, która wybiera następny token z podzbioru tokenów, które razem mają skumulowane prawdopodobieństwo co najmniej p.\n\nTa metoda zapewnia równowagę między różnorodnością a jakością, biorąc pod uwagę zarówno prawdopodobieństwa tokenów, jak i liczbę tokenów do próbkowania.\n\nWyższa wartość dla top-p (np. 0,95) doprowadzi do bardziej zróżnicowanego tekstu, podczas gdy niższa wartość (np. 0,5) wygeneruje bardziej skoncentrowany i konserwatywny tekst. Musi być w (0, 1].\n\n• Domyślna wartość to <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Ciągi zatrzymujące",
  "llm.prediction.stopStrings/subTitle": "Ciągi, które powinny zatrzymać model przed generowaniem kolejnych tokenów",
  "llm.prediction.stopStrings/info": "Konkretne ciągi, które po napotkaniu zatrzymają model przed generowaniem kolejnych tokenów",
  "llm.prediction.stopStrings/placeholder": "Wprowadź ciąg i naciśnij ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Przepełnienie konwersacji",
  "llm.prediction.contextOverflowPolicy/subTitle": "Jak model powinien zachować się, gdy konwersacja staje się zbyt duża do obsłużenia",
  "llm.prediction.contextOverflowPolicy/info": "Zdecyduj, co zrobić, gdy konwersacja przekroczy rozmiar pamięci roboczej modelu ('kontekst')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Zatrzymaj na limicie",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Zatrzymaj generowanie, gdy pamięć modelu się zapełni",
  "customInputs.contextOverflowPolicy.truncateMiddle": "Przytnij środek",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Usuwa wiadomości ze środka konwersacji, aby zrobić miejsce na nowsze. Model nadal będzie pamiętał początek konwersacji",
  "customInputs.contextOverflowPolicy.rollingWindow": "Okno przesuwne",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "Model zawsze otrzyma kilka najnowszych wiadomości, ale może zapomnieć początek konwersacji",
  "llm.prediction.llama.frequencyPenalty/title": "Kara za częstotliwość",
  "llm.prediction.llama.presencePenalty/title": "Kara za obecność",
  "llm.prediction.llama.tailFreeSampling/title": "Próbkowanie bez ogona",
  "llm.prediction.llama.locallyTypicalSampling/title": "Próbkowanie lokalnie typowe",
  "llm.prediction.onnx.topKSampling/title": "Próbkowanie Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Ogranicza następny token do jednego z k najbardziej prawdopodobnych tokenów. Działa podobnie do temperatury",
  "llm.prediction.onnx.topKSampling/info": "Z dokumentacji ONNX:\n\nLiczba tokenów słownikowych o najwyższym prawdopodobieństwie do zachowania dla filtrowania top-k\n\n• Ten filtr jest domyślnie wyłączony",
  "llm.prediction.onnx.repeatPenalty/title": "Kara za powtórzenia",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Jak bardzo zniechęcać do powtarzania tego samego tokena",
  "llm.prediction.onnx.repeatPenalty/info": "Wyższa wartość zniechęca model do powtarzania się",
  "llm.prediction.onnx.topPSampling/title": "Próbkowanie Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimalne skumulowane prawdopodobieństwo dla możliwych następnych tokenów. Działa podobnie do temperatury",
  "llm.prediction.onnx.topPSampling/info": "Z dokumentacji ONNX:\n\nTylko najbardziej prawdopodobne tokeny z prawdopodobieństwami, które sumują się do TopP lub wyższych, są zachowywane do generowania\n\n• Ten filtr jest domyślnie wyłączony",
  "llm.prediction.seed/title": "Ziarno",
  "llm.prediction.structured/title": "Wyjście strukturalne",
  "llm.prediction.structured/info": "Wyjście strukturalne",
  "llm.prediction.promptTemplate/title": "Szablon promptu",
  "llm.prediction.promptTemplate/subTitle": "Format, w jakim wiadomości w czacie są wysyłane do modelu. Zmiana tego może wprowadzić nieoczekiwane zachowanie - upewnij się, że wiesz, co robisz!",
  "customInputs.llmPromptTemplate.types.jinja/label": "Jinja",
  "customInputs.llmPromptTemplate.jinja/error": "Nie udało się przetworzyć szablonu Jinja: {{error}}",
  "customInputs.llmPromptTemplate.types.manual/label": "Ręczny",
  "customInputs.llmPromptTemplate.manual.subfield.beforeSystem/label": "Przed systemem",
  "customInputs.llmPromptTemplate.manual.subfield.beforeSystem/placeholder": "Wprowadź prefiks systemu...",
  "customInputs.llmPromptTemplate.manual.subfield.afterSystem/label": "Po systemie",
  "customInputs.llmPromptTemplate.manual.subfield.afterSystem/placeholder": "Wprowadź sufiks systemu...",
  "customInputs.llmPromptTemplate.manual.subfield.beforeUser/label": "Przed użytkownikiem",
  "customInputs.llmPromptTemplate.manual.subfield.beforeUser/placeholder": "Wprowadź prefiks użytkownika...",
  "customInputs.llmPromptTemplate.manual.subfield.afterUser/label": "Po użytkowniku",
  "customInputs.llmPromptTemplate.manual.subfield.afterUser/placeholder": "Wprowadź sufiks użytkownika...",
  "customInputs.llmPromptTemplate.manual.subfield.beforeAssistant/label": "Przed asystentem",
  "customInputs.llmPromptTemplate.manual.subfield.beforeAssistant/placeholder": "Wprowadź prefiks asystenta...",
  "customInputs.llmPromptTemplate.manual.subfield.afterAssistant/label": "Po asystencie",
  "customInputs.llmPromptTemplate.manual.subfield.afterAssistant/placeholder": "Wprowadź suffix asystenta...",
  "customInputs.llmPromptTemplate.stopStrings/label": "Dodatkowe ciągi zatrzymujące",
  "customInputs.llmPromptTemplate.stopStrings/subTitle": "Specyficzne dla szablonu ciągi zatrzymujące, które będą używane oprócz ciągów zatrzymujących określonych przez użytkownika.",
  "llm.load.contextLength/title": "Długość kontekstu",
  "llm.load.contextLength/subTitle": "Maksymalna liczba tokenów, którą model może uwzględnić w jednym prompcie. Zobacz opcje Przepełnienia konwersacji w sekcji \"Parametry wnioskowania\", aby uzyskać więcej sposobów zarządzania tym",
  "llm.load.contextLength/info": "Określa maksymalną liczbę tokenów, które model może rozważyć jednocześnie, wpływając na to, ile kontekstu zachowuje podczas przetwarzania",
  "llm.load.seed/title": "Ziarno",
  "llm.load.seed/subTitle": "Ziarno dla generatora liczb losowych używanego w generowaniu tekstu. -1 oznacza losowe ziarno",
  "llm.load.seed/info": "Ziarno losowe: Ustawia ziarno dla generowania liczb losowych, aby zapewnić powtarzalne wyniki",
  "llm.load.llama.evalBatchSize/title": "Rozmiar partii ewaluacji",
  "llm.load.llama.evalBatchSize/subTitle": "Liczba tokenów wejściowych do przetworzenia jednocześnie. Zwiększenie tego zwiększa wydajność kosztem zużycia pamięci",
  "llm.load.llama.evalBatchSize/info": "Ustawia liczbę przykładów przetwarzanych razem w jednej partii podczas ewaluacji, wpływając na szybkość i zużycie pamięci",
  "llm.load.llama.ropeFrequencyBase/title": "Baza częstotliwości RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Niestandardowa częstotliwość bazowa dla obrotowych osadzań pozycyjnych (RoPE). Zwiększenie tego może umożliwić lepszą wydajność przy długich kontekstach",
  "llm.load.llama.ropeFrequencyBase/info": "[Zaawansowane] Dostosowuje częstotliwość bazową dla Rotacyjnego Kodowania Pozycyjnego, wpływając na sposób osadzania informacji pozycyjnych",
  "llm.load.llama.ropeFrequencyScale/title": "Skala częstotliwości RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Długość kontekstu jest skalowana przez ten czynnik, aby rozszerzyć efektywny kontekst przy użyciu RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Zaawansowane] Modyfikuje skalowanie częstotliwości dla Rotacyjnego Kodowania Pozycyjnego, aby kontrolować ziarnistość kodowania pozycyjnego",
  "llm.load.llama.acceleration.offloadRatio/title": "Offload GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Liczba dyskretnych warstw modelu do obliczenia na GPU dla akceleracji GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Ustaw liczbę warstw do przeniesienia na GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Zmniejsza zużycie pamięci i czas generowania w niektórych modelach",
  "llm.load.llama.flashAttention/info": "Przyspiesza mechanizmy uwagi dla szybszego i bardziej efektywnego przetwarzania",
  "llm.load.llama.keepModelInMemory/title": "Utrzymuj model w pamięci",
  "llm.load.llama.keepModelInMemory/subTitle": "Rezerwuj pamięć systemową dla modelu, nawet gdy jest przeniesiony na GPU. Poprawia wydajność, ale wymaga więcej pamięci RAM systemu",
  "llm.load.llama.keepModelInMemory/info": "Zapobiega wymianie modelu na dysk, zapewniając szybszy dostęp kosztem większego zużycia pamięci RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Użyj FP16 dla pamięci podręcznej KV",
  "llm.load.llama.useFp16ForKVCache/info": "Zmniejsza zużycie pamięci poprzez przechowywanie pamięci podręcznej w połowie precyzji (FP16)",
  "llm.load.llama.tryMmap/title": "Spróbuj mmap()",
  "llm.load.llama.tryMmap/subTitle": "Poprawia czas ładowania modelu. Wyłączenie tego może poprawić wydajność, gdy model jest większy niż dostępna pamięć RAM systemu",
  "llm.load.llama.tryMmap/info": "Ładuj pliki modelu bezpośrednio z dysku do pamięci",
  "embedding.load.contextLength/title": "Długość kontekstu",
  "embedding.load.contextLength/subTitle": "Maksymalna liczba tokenów, którą model może uwzględnić w jednym prompcie. Zobacz opcje Przepełnienia konwersacji w sekcji \"Parametry wnioskowania\", aby uzyskać więcej sposobów zarządzania tym",
  "embedding.load.contextLength/info": "Określa maksymalną liczbę tokenów, które model może rozważyć jednocześnie, wpływając na to, ile kontekstu zachowuje podczas przetwarzania",
  "embedding.load.llama.ropeFrequencyBase/title": "Baza częstotliwości RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Niestandardowa częstotliwość bazowa dla obrotowych osadzań pozycyjnych (RoPE). Zwiększenie tego może umożliwić lepszą wydajność przy długich kontekstach",
  "embedding.load.llama.ropeFrequencyBase/info": "[Zaawansowane] Dostosowuje częstotliwość bazową dla Rotacyjnego Kodowania Pozycyjnego, wpływając na sposób osadzania informacji pozycyjnych",
  "embedding.load.llama.evalBatchSize/title": "Rozmiar partii ewaluacji",
  "embedding.load.llama.evalBatchSize/subTitle": "Liczba tokenów wejściowych do przetworzenia jednocześnie. Zwiększenie tego zwiększa wydajność kosztem zużycia pamięci",
  "embedding.load.llama.evalBatchSize/info": "Ustawia liczbę tokenów przetwarzanych razem w jednej partii podczas ewaluacji",
  "embedding.load.llama.ropeFrequencyScale/title": "Skala częstotliwości RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Długość kontekstu jest skalowana przez ten czynnik, aby rozszerzyć efektywny kontekst przy użyciu RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Zaawansowane] Modyfikuje skalowanie częstotliwości dla Rotacyjnego Kodowania Pozycyjnego, aby kontrolować ziarnistość kodowania pozycyjnego",
  "embedding.load.llama.acceleration.offloadRatio/title": "Offload GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Liczba dyskretnych warstw modelu do obliczenia na GPU dla akceleracji GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Ustaw liczbę warstw do przeniesienia na GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Utrzymuj model w pamięci",
  "embedding.load.llama.keepModelInMemory/subTitle": "Rezerwuj pamięć systemową dla modelu, nawet gdy jest przeniesiony na GPU. Poprawia wydajność, ale wymaga więcej pamięci RAM systemu",
  "embedding.load.llama.keepModelInMemory/info": "Zapobiega wymianie modelu na dysk, zapewniając szybszy dostęp kosztem większego zużycia pamięci RAM",
  "embedding.load.llama.tryMmap/title": "Spróbuj mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Poprawia czas ładowania modelu. Wyłączenie tego może poprawić wydajność, gdy model jest większy niż dostępna pamięć RAM systemu",
  "embedding.load.llama.tryMmap/info": "Ładuj pliki modelu bezpośrednio z dysku do pamięci",
  "embedding.load.seed/title": "Ziarno",
  "embedding.load.seed/subTitle": "Ziarno dla generatora liczb losowych używanego w generowaniu tekstu. -1 oznacza losowe ziarno",
  "embedding.load.seed/info": "Ziarno losowe: Ustawia ziarno dla generowania liczb losowych, aby zapewnić powtarzalne wyniki"
}
