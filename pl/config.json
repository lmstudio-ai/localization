{
  "noInstanceSelected": "Nie wybrano instancji modelu",
  "resetToDefault": "Resetuj",
  "showAdvancedSettings": "PokaÅ¼ ustawienia zaawansowane",
  "showAll": "Wszystkie",
  "basicSettings": "Podstawowe",
  "configSubtitle": "Wczytaj lub zapisz ustawienia i eksperymentuj z parametrami modelu",
  "inferenceParameters/title": "Parametry predykcji",
  "inferenceParameters/info": "Eksperymentuj z parametrami wpÅ‚ywajÄ…cymi na predykcjÄ™.",
  "generalParameters/title": "OgÃ³lne",
  "samplingParameters/title": "PrÃ³bkowanie",
  "basicTab": "Podstawowe",
  "advancedTab": "Zaawansowane",
  "advancedTab/title": "ğŸ§ª Konfiguracja zaawansowana",
  "advancedTab/expandAll": "RozwiÅ„ wszystkie",
  "advancedTab/overridesTitle": "Nadpisania konfiguracji",
  "advancedTab/noConfigsText": "Nie masz niezapisanych zmian - edytuj wartoÅ›ci powyÅ¼ej, aby zobaczyÄ‡ nadpisania tutaj.",
  "loadInstanceFirst": "Wczytaj model, aby zobaczyÄ‡ konfigurowalne parametry",
  "noListedConfigs": "Brak konfigurowalnych parametrÃ³w",
  "generationParameters/info": "Eksperymentuj z podstawowymi parametrami wpÅ‚ywajÄ…cymi na generowanie tekstu.",
  "loadParameters/title": "Parametry wczytywania",
  "loadParameters/description": "Ustawienia kontrolujÄ…ce sposÃ³b inicjalizacji i wczytywania modelu do pamiÄ™ci.",
  "loadParameters/reload": "PrzeÅ‚aduj, aby zastosowaÄ‡ zmiany",
  "discardChanges": "OdrzuÄ‡ zmiany",
  "llm.prediction.systemPrompt/title": "Prompt systemowy",
  "llm.prediction.systemPrompt/description": "UÅ¼yj tego pola, aby dostarczyÄ‡ modelowi instrukcje tÅ‚a, takie jak zestaw reguÅ‚, ograniczeÅ„ lub ogÃ³lnych wymagaÅ„. To pole jest czÄ™sto okreÅ›lane jako \"prompt systemowy\".",
  "llm.prediction.systemPrompt/subTitle": "Wytyczne dla AI",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Ile losowoÅ›ci wprowadziÄ‡. 0 da ten sam wynik za kaÅ¼dym razem, podczas gdy wyÅ¼sze wartoÅ›ci zwiÄ™kszÄ… kreatywnoÅ›Ä‡ i zmiennoÅ›Ä‡",
  "llm.prediction.temperature/info": "Z dokumentacji pomocy llama.cpp: \"DomyÅ›lna wartoÅ›Ä‡ to <{{dynamicValue}}>, co zapewnia rÃ³wnowagÄ™ miÄ™dzy losowoÅ›ciÄ… a determinizmem. W skrajnym przypadku temperatura 0 zawsze wybierze najbardziej prawdopodobny nastÄ™pny token,... prowadzÄ…c do identycznych wynikÃ³w w kaÅ¼dym uruchomieniu\"",
  "llm.prediction.llama.sampling/title": "PrÃ³bkowanie",
  "llm.prediction.topKSampling/title": "PrÃ³bkowanie Top K",
  "llm.prediction.topKSampling/subTitle": "Ogranicza nastÄ™pny token do jednego z k najbardziej prawdopodobnych tokenÃ³w. DziaÅ‚a podobnie do temperatury",
  "llm.prediction.topKSampling/info": "Z dokumentacji pomocy llama.cpp:\n\nPrÃ³bkowanie top-k to metoda generowania tekstu, ktÃ³ra wybiera nastÄ™pny token tylko spoÅ›rÃ³d k najbardziej prawdopodobnych tokenÃ³w przewidzianych przez model.\n\nPomaga to zmniejszyÄ‡ ryzyko generowania tokenÃ³w o niskim prawdopodobieÅ„stwie lub bezsensownych, ale moÅ¼e rÃ³wnieÅ¼ ograniczyÄ‡ rÃ³Å¼norodnoÅ›Ä‡ wyjÅ›cia.\n\nWyÅ¼sza wartoÅ›Ä‡ dla top-k (np. 100) uwzglÄ™dni wiÄ™cej tokenÃ³w i doprowadzi do bardziej zrÃ³Å¼nicowanego tekstu, podczas gdy niÅ¼sza wartoÅ›Ä‡ (np. 10) skupi siÄ™ na najbardziej prawdopodobnych tokenach i wygeneruje bardziej konserwatywny tekst.\n\nâ€¢ DomyÅ›lna wartoÅ›Ä‡ to <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "WÄ…tki CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Liczba wÄ…tkÃ³w CPU do uÅ¼ycia podczas wnioskowania",
  "llm.prediction.llama.cpuThreads/info": "Liczba wÄ…tkÃ³w do uÅ¼ycia podczas obliczeÅ„. ZwiÄ™kszenie liczby wÄ…tkÃ³w nie zawsze koreluje z lepszÄ… wydajnoÅ›ciÄ…. DomyÅ›lnie jest to <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Ogranicz dÅ‚ugoÅ›Ä‡ odpowiedzi",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcjonalnie ogranicz dÅ‚ugoÅ›Ä‡ odpowiedzi AI",
  "llm.prediction.maxPredictedTokens/info": "Kontroluj maksymalnÄ… dÅ‚ugoÅ›Ä‡ odpowiedzi chatbota. WÅ‚Ä…cz, aby ustawiÄ‡ limit maksymalnej dÅ‚ugoÅ›ci odpowiedzi, lub wyÅ‚Ä…cz, aby pozwoliÄ‡ chatbotowi zdecydowaÄ‡, kiedy zakoÅ„czyÄ‡.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maksymalna dÅ‚ugoÅ›Ä‡ odpowiedzi (tokeny)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "OkoÅ‚o {{maxWords}} sÅ‚Ã³w",
  "llm.prediction.repeatPenalty/title": "Kara za powtÃ³rzenia",
  "llm.prediction.repeatPenalty/subTitle": "Jak bardzo zniechÄ™caÄ‡ do powtarzania tego samego tokena",
  "llm.prediction.repeatPenalty/info": "Z dokumentacji pomocy llama.cpp:... \"Pomaga zapobiegaÄ‡ generowaniu przez model powtarzalnego lub monotonnego tekstu.\n\nWyÅ¼sza wartoÅ›Ä‡ (np. 1,5) bÄ™dzie bardziej surowo karaÄ‡ powtÃ³rzenia, podczas gdy niÅ¼sza wartoÅ›Ä‡ (np. 0,9) bÄ™dzie bardziej pobÅ‚aÅ¼liwa.\" â€¢ DomyÅ›lna wartoÅ›Ä‡ to <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "PrÃ³bkowanie Min P",
  "llm.prediction.minPSampling/subTitle": "Minimalne prawdopodobieÅ„stwo bazowe dla tokena, aby zostaÅ‚ wybrany do wyjÅ›cia",
  "llm.prediction.minPSampling/info": "Z dokumentacji pomocy llama.cpp:\n\nMinimalne prawdopodobieÅ„stwo dla tokena do rozwaÅ¼enia, w stosunku do prawdopodobieÅ„stwa najbardziej prawdopodobnego tokena. Musi byÄ‡ w [0, 1].\n\nâ€¢ DomyÅ›lna wartoÅ›Ä‡ to <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "PrÃ³bkowanie Top P",
  "llm.prediction.topPSampling/subTitle": "Minimalne skumulowane prawdopodobieÅ„stwo dla moÅ¼liwych nastÄ™pnych tokenÃ³w. DziaÅ‚a podobnie do temperatury",
  "llm.prediction.topPSampling/info": "Z dokumentacji pomocy llama.cpp:\n\nPrÃ³bkowanie top-p, znane rÃ³wnieÅ¼ jako prÃ³bkowanie jÄ…drowe, to inna metoda generowania tekstu, ktÃ³ra wybiera nastÄ™pny token z podzbioru tokenÃ³w, ktÃ³re razem majÄ… skumulowane prawdopodobieÅ„stwo co najmniej p.\n\nTa metoda zapewnia rÃ³wnowagÄ™ miÄ™dzy rÃ³Å¼norodnoÅ›ciÄ… a jakoÅ›ciÄ…, biorÄ…c pod uwagÄ™ zarÃ³wno prawdopodobieÅ„stwa tokenÃ³w, jak i liczbÄ™ tokenÃ³w do prÃ³bkowania.\n\nWyÅ¼sza wartoÅ›Ä‡ dla top-p (np. 0,95) doprowadzi do bardziej zrÃ³Å¼nicowanego tekstu, podczas gdy niÅ¼sza wartoÅ›Ä‡ (np. 0,5) wygeneruje bardziej skoncentrowany i konserwatywny tekst. Musi byÄ‡ w (0, 1].\n\nâ€¢ DomyÅ›lna wartoÅ›Ä‡ to <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "CiÄ…gi zatrzymujÄ…ce",
  "llm.prediction.stopStrings/subTitle": "CiÄ…gi, ktÃ³re powinny zatrzymaÄ‡ model przed generowaniem kolejnych tokenÃ³w",
  "llm.prediction.stopStrings/info": "Konkretne ciÄ…gi, ktÃ³re po napotkaniu zatrzymajÄ… model przed generowaniem kolejnych tokenÃ³w",
  "llm.prediction.stopStrings/placeholder": "WprowadÅº ciÄ…g i naciÅ›nij â",
  "llm.prediction.contextOverflowPolicy/title": "PrzepeÅ‚nienie konwersacji",
  "llm.prediction.contextOverflowPolicy/subTitle": "Jak model powinien zachowaÄ‡ siÄ™, gdy konwersacja staje siÄ™ zbyt duÅ¼a do obsÅ‚uÅ¼enia",
  "llm.prediction.contextOverflowPolicy/info": "Zdecyduj, co zrobiÄ‡, gdy konwersacja przekroczy rozmiar pamiÄ™ci roboczej modelu ('kontekst')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Zatrzymaj na limicie",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Zatrzymaj generowanie, gdy pamiÄ™Ä‡ modelu siÄ™ zapeÅ‚ni",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Przytnij Å›rodek",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Usuwa wiadomoÅ›ci ze Å›rodka konwersacji, aby zrobiÄ‡ miejsce na nowsze. Model nadal bÄ™dzie pamiÄ™taÅ‚ poczÄ…tek konwersacji",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Okno przesuwne",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Model zawsze otrzyma kilka najnowszych wiadomoÅ›ci, ale moÅ¼e zapomnieÄ‡ poczÄ…tek konwersacji",
  "llm.prediction.llama.frequencyPenalty/title": "Kara za czÄ™stotliwoÅ›Ä‡",
  "llm.prediction.llama.presencePenalty/title": "Kara za obecnoÅ›Ä‡",
  "llm.prediction.llama.tailFreeSampling/title": "PrÃ³bkowanie bez ogona",
  "llm.prediction.llama.locallyTypicalSampling/title": "PrÃ³bkowanie lokalnie typowe",
  "llm.prediction.onnx.topKSampling/title": "PrÃ³bkowanie Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Ogranicza nastÄ™pny token do jednego z k najbardziej prawdopodobnych tokenÃ³w. DziaÅ‚a podobnie do temperatury",
  "llm.prediction.onnx.topKSampling/info": "Z dokumentacji ONNX:\n\nLiczba tokenÃ³w sÅ‚ownikowych o najwyÅ¼szym prawdopodobieÅ„stwie do zachowania dla filtrowania top-k\n\nâ€¢ Ten filtr jest domyÅ›lnie wyÅ‚Ä…czony",
  "llm.prediction.onnx.repeatPenalty/title": "Kara za powtÃ³rzenia",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Jak bardzo zniechÄ™caÄ‡ do powtarzania tego samego tokena",
  "llm.prediction.onnx.repeatPenalty/info": "WyÅ¼sza wartoÅ›Ä‡ zniechÄ™ca model do powtarzania siÄ™",
  "llm.prediction.onnx.topPSampling/title": "PrÃ³bkowanie Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimalne skumulowane prawdopodobieÅ„stwo dla moÅ¼liwych nastÄ™pnych tokenÃ³w. DziaÅ‚a podobnie do temperatury",
  "llm.prediction.onnx.topPSampling/info": "Z dokumentacji ONNX:\n\nTylko najbardziej prawdopodobne tokeny z prawdopodobieÅ„stwami, ktÃ³re sumujÄ… siÄ™ do TopP lub wyÅ¼szych, sÄ… zachowywane do generowania\n\nâ€¢ Ten filtr jest domyÅ›lnie wyÅ‚Ä…czony",
  "llm.prediction.seed/title": "Ziarno",
  "llm.prediction.structured/title": "WyjÅ›cie strukturalne",
  "llm.prediction.structured/info": "WyjÅ›cie strukturalne",
  "llm.prediction.promptTemplate/title": "Szablon promptu",
  "llm.prediction.promptTemplate/subTitle": "Format, w jakim wiadomoÅ›ci w czacie sÄ… wysyÅ‚ane do modelu. Zmiana tego moÅ¼e wprowadziÄ‡ nieoczekiwane zachowanie - upewnij siÄ™, Å¼e wiesz, co robisz!",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Nie udaÅ‚o siÄ™ przetworzyÄ‡ szablonu Jinja: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "RÄ™czny",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Przed systemem",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "WprowadÅº prefiks systemu...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Po systemie",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "WprowadÅº sufiks systemu...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Przed uÅ¼ytkownikiem",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "WprowadÅº prefiks uÅ¼ytkownika...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "Po uÅ¼ytkowniku",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "WprowadÅº sufiks uÅ¼ytkownika...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Przed asystentem",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "WprowadÅº prefiks asystenta...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Po asystencie",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "WprowadÅº suffix asystenta...",
  "llm.prediction.promptTemplate.stopStrings/label": "Dodatkowe ciÄ…gi zatrzymujÄ…ce",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "Specyficzne dla szablonu ciÄ…gi zatrzymujÄ…ce, ktÃ³re bÄ™dÄ… uÅ¼ywane oprÃ³cz ciÄ…gÃ³w zatrzymujÄ…cych okreÅ›lonych przez uÅ¼ytkownika.",
  "llm.load.contextLength/title": "DÅ‚ugoÅ›Ä‡ kontekstu",
  "llm.load.contextLength/subTitle": "Maksymalna liczba tokenÃ³w, ktÃ³rÄ… model moÅ¼e uwzglÄ™dniÄ‡ w jednym prompcie. Zobacz opcje PrzepeÅ‚nienia konwersacji w sekcji \"Parametry wnioskowania\", aby uzyskaÄ‡ wiÄ™cej sposobÃ³w zarzÄ…dzania tym",
  "llm.load.contextLength/info": "OkreÅ›la maksymalnÄ… liczbÄ™ tokenÃ³w, ktÃ³re model moÅ¼e rozwaÅ¼yÄ‡ jednoczeÅ›nie, wpÅ‚ywajÄ…c na to, ile kontekstu zachowuje podczas przetwarzania",
  "llm.load.seed/title": "Ziarno",
  "llm.load.seed/subTitle": "Ziarno dla generatora liczb losowych uÅ¼ywanego w generowaniu tekstu. -1 oznacza losowe ziarno",
  "llm.load.seed/info": "Ziarno losowe: Ustawia ziarno dla generowania liczb losowych, aby zapewniÄ‡ powtarzalne wyniki",
  "llm.load.llama.evalBatchSize/title": "Rozmiar partii ewaluacji",
  "llm.load.llama.evalBatchSize/subTitle": "Liczba tokenÃ³w wejÅ›ciowych do przetworzenia jednoczeÅ›nie. ZwiÄ™kszenie tego zwiÄ™ksza wydajnoÅ›Ä‡ kosztem zuÅ¼ycia pamiÄ™ci",
  "llm.load.llama.evalBatchSize/info": "Ustawia liczbÄ™ przykÅ‚adÃ³w przetwarzanych razem w jednej partii podczas ewaluacji, wpÅ‚ywajÄ…c na szybkoÅ›Ä‡ i zuÅ¼ycie pamiÄ™ci",
  "llm.load.llama.ropeFrequencyBase/title": "Baza czÄ™stotliwoÅ›ci RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Niestandardowa czÄ™stotliwoÅ›Ä‡ bazowa dla obrotowych osadzaÅ„ pozycyjnych (RoPE). ZwiÄ™kszenie tego moÅ¼e umoÅ¼liwiÄ‡ lepszÄ… wydajnoÅ›Ä‡ przy dÅ‚ugich kontekstach",
  "llm.load.llama.ropeFrequencyBase/info": "[Zaawansowane] Dostosowuje czÄ™stotliwoÅ›Ä‡ bazowÄ… dla Rotacyjnego Kodowania Pozycyjnego, wpÅ‚ywajÄ…c na sposÃ³b osadzania informacji pozycyjnych",
  "llm.load.llama.ropeFrequencyScale/title": "Skala czÄ™stotliwoÅ›ci RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "DÅ‚ugoÅ›Ä‡ kontekstu jest skalowana przez ten czynnik, aby rozszerzyÄ‡ efektywny kontekst przy uÅ¼yciu RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Zaawansowane] Modyfikuje skalowanie czÄ™stotliwoÅ›ci dla Rotacyjnego Kodowania Pozycyjnego, aby kontrolowaÄ‡ ziarnistoÅ›Ä‡ kodowania pozycyjnego",
  "llm.load.llama.acceleration.offloadRatio/title": "Offload GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Liczba dyskretnych warstw modelu do obliczenia na GPU dla akceleracji GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Ustaw liczbÄ™ warstw do przeniesienia na GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Zmniejsza zuÅ¼ycie pamiÄ™ci i czas generowania w niektÃ³rych modelach",
  "llm.load.llama.flashAttention/info": "Przyspiesza mechanizmy uwagi dla szybszego i bardziej efektywnego przetwarzania",
  "llm.load.llama.keepModelInMemory/title": "Utrzymuj model w pamiÄ™ci",
  "llm.load.llama.keepModelInMemory/subTitle": "Rezerwuj pamiÄ™Ä‡ systemowÄ… dla modelu, nawet gdy jest przeniesiony na GPU. Poprawia wydajnoÅ›Ä‡, ale wymaga wiÄ™cej pamiÄ™ci RAM systemu",
  "llm.load.llama.keepModelInMemory/info": "Zapobiega wymianie modelu na dysk, zapewniajÄ…c szybszy dostÄ™p kosztem wiÄ™kszego zuÅ¼ycia pamiÄ™ci RAM",
  "llm.load.llama.useFp16ForKVCache/title": "UÅ¼yj FP16 dla pamiÄ™ci podrÄ™cznej KV",
  "llm.load.llama.useFp16ForKVCache/info": "Zmniejsza zuÅ¼ycie pamiÄ™ci poprzez przechowywanie pamiÄ™ci podrÄ™cznej w poÅ‚owie precyzji (FP16)",
  "llm.load.llama.tryMmap/title": "SprÃ³buj mmap()",
  "llm.load.llama.tryMmap/subTitle": "Poprawia czas Å‚adowania modelu. WyÅ‚Ä…czenie tego moÅ¼e poprawiÄ‡ wydajnoÅ›Ä‡, gdy model jest wiÄ™kszy niÅ¼ dostÄ™pna pamiÄ™Ä‡ RAM systemu",
  "llm.load.llama.tryMmap/info": "Åaduj pliki modelu bezpoÅ›rednio z dysku do pamiÄ™ci",
  "embedding.load.contextLength/title": "DÅ‚ugoÅ›Ä‡ kontekstu",
  "embedding.load.contextLength/subTitle": "Maksymalna liczba tokenÃ³w, ktÃ³rÄ… model moÅ¼e uwzglÄ™dniÄ‡ w jednym prompcie. Zobacz opcje PrzepeÅ‚nienia konwersacji w sekcji \"Parametry wnioskowania\", aby uzyskaÄ‡ wiÄ™cej sposobÃ³w zarzÄ…dzania tym",
  "embedding.load.contextLength/info": "OkreÅ›la maksymalnÄ… liczbÄ™ tokenÃ³w, ktÃ³re model moÅ¼e rozwaÅ¼yÄ‡ jednoczeÅ›nie, wpÅ‚ywajÄ…c na to, ile kontekstu zachowuje podczas przetwarzania",
  "embedding.load.llama.ropeFrequencyBase/title": "Baza czÄ™stotliwoÅ›ci RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Niestandardowa czÄ™stotliwoÅ›Ä‡ bazowa dla obrotowych osadzaÅ„ pozycyjnych (RoPE). ZwiÄ™kszenie tego moÅ¼e umoÅ¼liwiÄ‡ lepszÄ… wydajnoÅ›Ä‡ przy dÅ‚ugich kontekstach",
  "embedding.load.llama.ropeFrequencyBase/info": "[Zaawansowane] Dostosowuje czÄ™stotliwoÅ›Ä‡ bazowÄ… dla Rotacyjnego Kodowania Pozycyjnego, wpÅ‚ywajÄ…c na sposÃ³b osadzania informacji pozycyjnych",
  "embedding.load.llama.evalBatchSize/title": "Rozmiar partii ewaluacji",
  "embedding.load.llama.evalBatchSize/subTitle": "Liczba tokenÃ³w wejÅ›ciowych do przetworzenia jednoczeÅ›nie. ZwiÄ™kszenie tego zwiÄ™ksza wydajnoÅ›Ä‡ kosztem zuÅ¼ycia pamiÄ™ci",
  "embedding.load.llama.evalBatchSize/info": "Ustawia liczbÄ™ tokenÃ³w przetwarzanych razem w jednej partii podczas ewaluacji",
  "embedding.load.llama.ropeFrequencyScale/title": "Skala czÄ™stotliwoÅ›ci RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "DÅ‚ugoÅ›Ä‡ kontekstu jest skalowana przez ten czynnik, aby rozszerzyÄ‡ efektywny kontekst przy uÅ¼yciu RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Zaawansowane] Modyfikuje skalowanie czÄ™stotliwoÅ›ci dla Rotacyjnego Kodowania Pozycyjnego, aby kontrolowaÄ‡ ziarnistoÅ›Ä‡ kodowania pozycyjnego",
  "embedding.load.llama.acceleration.offloadRatio/title": "Offload GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Liczba dyskretnych warstw modelu do obliczenia na GPU dla akceleracji GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Ustaw liczbÄ™ warstw do przeniesienia na GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Utrzymuj model w pamiÄ™ci",
  "embedding.load.llama.keepModelInMemory/subTitle": "Rezerwuj pamiÄ™Ä‡ systemowÄ… dla modelu, nawet gdy jest przeniesiony na GPU. Poprawia wydajnoÅ›Ä‡, ale wymaga wiÄ™cej pamiÄ™ci RAM systemu",
  "embedding.load.llama.keepModelInMemory/info": "Zapobiega wymianie modelu na dysk, zapewniajÄ…c szybszy dostÄ™p kosztem wiÄ™kszego zuÅ¼ycia pamiÄ™ci RAM",
  "embedding.load.llama.tryMmap/title": "SprÃ³buj mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Poprawia czas Å‚adowania modelu. WyÅ‚Ä…czenie tego moÅ¼e poprawiÄ‡ wydajnoÅ›Ä‡, gdy model jest wiÄ™kszy niÅ¼ dostÄ™pna pamiÄ™Ä‡ RAM systemu",
  "embedding.load.llama.tryMmap/info": "Åaduj pliki modelu bezpoÅ›rednio z dysku do pamiÄ™ci",
  "embedding.load.seed/title": "Ziarno",
  "embedding.load.seed/subTitle": "Ziarno dla generatora liczb losowych uÅ¼ywanego w generowaniu tekstu. -1 oznacza losowe ziarno",
  "embedding.load.seed/info": "Ziarno losowe: Ustawia ziarno dla generowania liczb losowych, aby zapewniÄ‡ powtarzalne wyniki"
}
