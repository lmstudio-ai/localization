{
  "noInstanceSelected": "モデルインスタンスが選択されていません",
  "resetToDefault": "リセット",
  "showAdvancedSettings": "詳細設定を表示",
  "showAll": "全て",
  "basicSettings": "基本設定",
  "configSubtitle": "プリセットの読み込みや保存、モデルパラメータの上書き設定を試してみましょう",
  "inferenceParameters/title": "予測パラメータ",
  "inferenceParameters/info": "予測に影響を与えるパラメータを試すことができます。",
  "generalParameters/title": "一般設定",
  "samplingParameters/title": "サンプリング設定",
  "basicTab": "基本",
  "advancedTab": "詳細",
  "advancedTab/title": "🧪 詳細設定",
  "advancedTab/expandAll": "全て展開",
  "advancedTab/overridesTitle": "設定の上書き",
  "advancedTab/noConfigsText": "未保存の変更はありません - 上記の値を編集して上書き設定をここに表示します。",
  "loadInstanceFirst": "モデルを読み込んで設定可能なパラメータを表示してください",
  "noListedConfigs": "設定可能なパラメータがありません",
  "generationParameters/info": "テキスト生成に影響を与える基本パラメータを試してみましょう。",
  "loadParameters/title": "読み込みパラメータ",
  "loadParameters/description": "モデルがメモリに読み込まれる際の設定を制御します。",
  "loadParameters/reload": "変更を適用するには再読み込み",
  "loadParameters/reload/error": "モデルの再読み込みに失敗しました",
  "discardChanges": "変更を破棄",
  "loadModelToSeeOptions": "オプションを表示するにはモデルを読み込んでください",
  "schematicsError.title": "以下の項目にエラーがあります:",
  "manifestSections": {
    "structuredOutput/title": "構造化出力",
    "speculativeDecoding/title": "投機的デコーディング",
    "sampling/title": "サンプリング",
    "settings/title": "設定",
    "toolUse/title": "Tool Use",
    "promptTemplate/title": "プロンプトテンプレート",
    "customFields/title": "カスタム項目"
  },

  "llm.prediction.systemPrompt/title": "システムプロンプト",
  "llm.prediction.systemPrompt/description": "モデルに対して背景となる指示やルール、制約、全体的な要件を提供するためのフィールドです。このフィールドは「システムプロンプト」とも呼ばれます。",
  "llm.prediction.systemPrompt/subTitle": "AIへのガイドライン",
  "llm.prediction.systemPrompt/openEditor": "エディタ",
  "llm.prediction.systemPrompt/closeEditor": "エディタを閉じる",
  "llm.prediction.systemPrompt/openedEditor": "エディタで開く...",
  "llm.prediction.systemPrompt/edit": "システムプロンプトを編集...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "インストラクションを追加...",
  "llm.prediction.systemPrompt/addInstructions": "インストラクションを追加",
  "llm.prediction.temperature/title": "temperature",
  "llm.prediction.temperature/subTitle": "ランダム性をどれくらい加えるか。0にすると毎回同じ結果になり、高い値にすると創造性と多様性が増加します。",
  "llm.prediction.temperature/info": "llama.cpp のドキュメントによると、デフォルト値は <{{dynamicValue}}> で、ランダム性と決定論のバランスを取ります。極端な場合、temperatureが0だと常に最も可能性の高いトークンが選ばれ、毎回同じ結果になります。",
  "llm.prediction.llama.sampling/title": "サンプリング",
  "llm.prediction.topKSampling/title": "Top-Kサンプリング",
  "llm.prediction.topKSampling/subTitle": "次のトークンを最も確率の高いK個のトークンから選択します。temperatureと似たように機能します。",
  "llm.prediction.topKSampling/info": "llama.cppのドキュメントによると、Top-Kサンプリングは、モデルが予測した最も可能性の高いK個のトークンの中から次のトークンを選択するテキスト生成法です。これにより、低確率や意味不明なトークンが生成されるリスクが減りますが、多様性も制限される可能性があります。Kの値が大きい（例:100）と、多様なテキストが生成され、小さい（例:10）と、より保守的なテキストが生成されます。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.llama.cpuThreads/title": "CPUスレッド数",
  "llm.prediction.llama.cpuThreads/subTitle": "推論時に使用するCPUスレッド数",
  "llm.prediction.llama.cpuThreads/info": "計算中に使用するスレッド数です。スレッド数を増やすと必ずしも性能が向上するわけではありません。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.maxPredictedTokens/title": "レスポンスの長さを制限",
  "llm.prediction.maxPredictedTokens/subTitle": "AIのレスポンスの長さをオプションで制限できます。",
  "llm.prediction.maxPredictedTokens/info": "チャットボットのレスポンスの最大長を制御します。オンにするとレスポンスの最大長を設定できます。オフにするとチャットボットが停止するタイミングを決定します。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大レスポンス長（トークン数）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 語",
  "llm.prediction.repeatPenalty/title": "繰り返しペナルティ",
  "llm.prediction.repeatPenalty/subTitle": "同じトークンを繰り返すことをどの程度抑制するか",
  "llm.prediction.repeatPenalty/info": "llama.cppのドキュメントによると、この設定はモデルが繰り返しや単調なテキストを生成するのを防ぎます。値が高い（例:1.5）ほど、繰り返しに対するペナルティが強くなり、値が低い（例:0.9）ほど寛容になります。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.minPSampling/title": "最小Pサンプリング",
  "llm.prediction.minPSampling/subTitle": "トークンが出力されるための最小ベース確率",
  "llm.prediction.minPSampling/info": "llama.cppのドキュメントによると、次のトークンを選択するための最小確率は、最も可能性の高いトークンの確率に基づきます。値は[0, 1]の範囲内で設定できます。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.topPSampling/title": "Top-Pサンプリング",
  "llm.prediction.topPSampling/subTitle": "次のトークンの可能性のある最小累積確率。temperatureと似たように機能します。",
  "llm.prediction.topPSampling/info": "llama.cppのドキュメントによると、Top-Pサンプリングは、可能性のあるトークンの累積確率がP以上になるようにトークンを選択します。この手法は、多様性と品質のバランスをとるため、トークンの確率とトークン数の両方を考慮します。Top-Pの値が高い（例:0.95）と多様なテキストが生成され、値が低い（例:0.5）とより保守的なテキストが生成されます。値は (0, 1] の範囲内で設定できます。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.stopStrings/title": "停止文字列",
  "llm.prediction.stopStrings/subTitle": "モデルがトークンの生成を停止するべき文字列",
  "llm.prediction.stopStrings/info": "特定の文字列が検出された場合に、モデルがトークンの生成を停止します。",
  "llm.prediction.stopStrings/placeholder": "文字列を入力し、⏎キーを押してください",
  "llm.prediction.contextOverflowPolicy/title": "会話のオーバーフロー",
  "llm.prediction.contextOverflowPolicy/subTitle": "会話がモデルの処理可能なサイズを超えた場合の動作",
  "llm.prediction.contextOverflowPolicy/info": "会話がモデルのメモリ容量（コンテキスト）を超えた場合にどうするかを設定します。",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency ペナルティ",
  "llm.prediction.llama.presencePenalty/title": "Presence ペナルティ",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Freeサンプリング",
  "llm.prediction.llama.locallyTypicalSampling/title": "ローカルTypicalサンプリング",
  "llm.prediction.mlx.topKSampling/title": "Top-Kサンプリング",
  "llm.prediction.mlx.topKSampling/subTitle": "次のトークンを最も確率の高いK個のトークンに制限します。temperatureと同様の機能を持ちます。",
  "llm.prediction.mlx.topKSampling/info": "次のトークンを最も確率の高いK個のトークンに制限します。temperatureと同様の機能を持ちます。",
  "llm.prediction.onnx.topKSampling/title": "Top-Kサンプリング",
  "llm.prediction.onnx.topKSampling/subTitle": "次のトークンを最も確率の高いK個のトークンに制限します。temperatureと同様の機能を持ちます。",
  "llm.prediction.onnx.topKSampling/info": "ONNXのドキュメントによると、最も確率の高いボキャブラリトークンの上位K個をTop-Kフィルタリングのために保持します。デフォルトではこのフィルタはオフです。",
  "llm.prediction.onnx.repeatPenalty/title": "繰り返しペナルティ",
  "llm.prediction.onnx.repeatPenalty/subTitle": "同じトークンを繰り返すことをどの程度抑制するか",
  "llm.prediction.onnx.repeatPenalty/info": "モデルが繰り返しを行わないようにするための設定です。値が高いほど抑制されます。",
  "llm.prediction.onnx.topPSampling/title": "Top-Pサンプリング",
  "llm.prediction.onnx.topPSampling/subTitle": "次のトークンの可能性のある最小累積確率。temperatureと似たように機能します。",
  "llm.prediction.onnx.topPSampling/info": "ONNXのドキュメントによると、確率がTop-Pまたはそれ以上になる最も可能性の高いトークンだけが保持されます。デフォルトではこのフィルタはオフです。",
  "llm.prediction.seed/title": "シード値",
  "llm.prediction.structured/title": "構造化出力",
  "llm.prediction.structured/info": "構造化された出力を生成します。",
  "llm.prediction.structured/description": "上級者向け: モデルから特定の出力形式を強制するためにJSONスキーマを指定できます。詳細については[ドキュメント](https://lmstudio.ai/docs/advanced/structured-output)をご覧ください。",
  "llm.prediction.promptTemplate/title": "プロンプトテンプレート",
  "llm.prediction.promptTemplate/subTitle": "チャットでメッセージがモデルに送信されるフォーマットです。これを変更すると予期しない動作が発生する可能性があります。変更前に必ず確認してください。",

  "llm.load.mainGpu/title": "メインGPU",
  "llm.load.mainGpu/subTitle": "モデルの計算に優先して使用するGPU",
  "llm.load.mainGpu/placeholder": "メインGPUを選択...",
  "llm.load.splitStrategy/title": "分割戦略",
  "llm.load.splitStrategy/subTitle": "複数のGPU間でモデル計算を分割する方法",
  "llm.load.splitStrategy/placeholder": "分割戦略を選択...",
  "llm.load.offloadKVCacheToGpu/title": "KVキャッシュをGPUメモリにオフロード",
  "llm.load.offloadKVCacheToGpu/subTitle": "KVキャッシュをGPUメモリにオフロードします。パフォーマンスは向上しますが、より多くのGPUメモリが必要となります。",
  "load.gpuStrictVramCap/title": "モデルのオフロードをGPU専用メモリに制限",
  "load.gpuStrictVramCap.customSubTitleOff": "OFF: GPU専用メモリを上限まで使用した場合、モデルの重みを共有メモリへオフロードします。",
  "load.gpuStrictVramCap.customSubTitleOn": "ON: モデルの重みはGPU専用メモリとRAMのみへオフロードされます。コンテキストは共有メモリを使用する場合があります。",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "モデルのオフロードはGPU専用メモリに制限されます。実際にオフロードされるレイヤー数は異なる場合があります。",
  "load.allGpusDisabledWarning": "全GPUが無効です。オフロードする場合は、少なくとも1つを有効にしてください。",

  "llm.load.contextLength/title": "コンテキスト長",
  "llm.load.contextLength/subTitle": "1つのプロンプトでモデルが処理できるトークンの最大数。コンテキストのオーバーフローオプションは「予測パラメータ」セクションにあります。",
  "llm.load.contextLength/info": "モデルが一度に処理できるトークンの最大数を指定します。これにより、処理中に保持されるコンテキスト量が影響を受けます。",
  "llm.load.contextLength/warning": "コンテキスト長の値を高く設定すると、メモリ使用量が大幅に増加する可能性があります。",
  "llm.load.seed/title": "シード値",
  "llm.load.seed/subTitle": "テキスト生成に使用される乱数生成器のシード値。-1でランダムになります。",
  "llm.load.seed/info": "乱数シード: 乱数生成のシード値を設定して結果を再現可能にします。",

  "llm.load.llama.evalBatchSize/title": "評価バッチサイズ",
  "llm.load.llama.evalBatchSize/subTitle": "一度に処理する入力トークンの数。これを増やすとパフォーマンスが向上しますが、メモリ使用量も増加します。",
  "llm.load.llama.evalBatchSize/info": "評価時に一度に処理される例の数を設定し、速度とメモリ使用量に影響を与えます。",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE基本周波数",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Rotary Positional Embeddings（RoPE）の基本周波数をカスタマイズします。これを増やすと、長いコンテキストでのパフォーマンスが向上する場合があります。",
  "llm.load.llama.ropeFrequencyBase/info": "[高度] 回転位置エンコーディングの基本周波数を調整し、位置情報の埋め込み方法に影響を与えます。",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE周波数スケール",
  "llm.load.llama.ropeFrequencyScale/subTitle": "RoPEを使用して、コンテキスト長をこの倍率でスケールします。",
  "llm.load.llama.ropeFrequencyScale/info": "[高度] 回転位置エンコーディングの周波数スケーリングを調整し、位置エンコーディングの粒度を制御します。",
  "llm.load.llama.acceleration.offloadRatio/title": "GPUオフロード",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "GPUアクセラレーションのためにGPUで計算するモデル層の数",
  "llm.load.llama.acceleration.offloadRatio/info": "GPUにオフロードする層の数を設定します。",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "一部のモデルでメモリ使用量と生成時間を削減します。",
  "llm.load.llama.flashAttention/info": "注意メカニズムを高速化し、より効率的に処理します。",
  "llm.load.numExperts/title": "エキスパートの数",
  "llm.load.numExperts/subTitle": "モデルで使用するエキスパートの数",
  "llm.load.numExperts/info": "モデルで使用するエキスパートの数を設定します。",
  "llm.load.llama.keepModelInMemory/title": "モデルをメモリに保持",
  "llm.load.llama.keepModelInMemory/subTitle": "モデルがGPUにオフロードされていても、システムメモリにモデルを保持します。これによりパフォーマンスが向上しますが、システムRAMを多く消費します。",
  "llm.load.llama.keepModelInMemory/info": "モデルがディスクにスワップアウトされるのを防ぎ、より高速なアクセスを可能にしますが、RAMの使用量が増加します。",
  "llm.load.llama.useFp16ForKVCache/title": "KVキャッシュにFP16を使用",
  "llm.load.llama.useFp16ForKVCache/info": "キャッシュを半精度（FP16）で保存することで、メモリ使用量を削減します。",
  "llm.load.llama.tryMmap/title": "mmap()を試す",
  "llm.load.llama.tryMmap/subTitle": "モデルのロード時間を短縮します。モデルがシステムRAMを超える場合は、これを無効にするとパフォーマンスが向上することがあります。",
  "llm.load.llama.tryMmap/info": "モデルファイルをディスクからメモリに直接読み込みます。",

  "embedding.load.contextLength/title": "コンテキスト長",
  "embedding.load.contextLength/subTitle": "1つのプロンプトでモデルが処理できるトークンの最大数。「予測パラメータ」セクションでコンテキストのオーバーフローオプションを参照してください。",
  "embedding.load.contextLength/info": "モデルが一度に処理できるトークンの最大数を指定し、処理中に保持されるコンテキストの量に影響します。",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE基本周波数",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Rotary Positional Embeddings（RoPE）の基本周波数をカスタマイズします。これを増やすと、長いコンテキストでのパフォーマンスが向上する場合があります。",
  "embedding.load.llama.ropeFrequencyBase/info": "[高度] RoPE（Rotary Positional Embeddings）の基本周波数を調整し、位置情報のEmbeddingに影響を与えます。",
  "embedding.load.llama.evalBatchSize/title": "評価バッチサイズ",
  "embedding.load.llama.evalBatchSize/subTitle": "一度に処理する入力トークンの数。これを増やすとパフォーマンスが向上しますが、メモリ使用量も増加します。",
  "embedding.load.llama.evalBatchSize/info": "評価時に一度に処理されるトークンの数を設定し、速度とメモリ使用量に影響を与えます。",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE周波数スケール",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "RoPEを使用して、コンテキスト長をこの倍率でスケールします。",
  "embedding.load.llama.ropeFrequencyScale/info": "[高度] RoPE（Rotary Positional Embeddings）の周波数スケーリングを調整して、位置情報のエンコーディングを細かく制御します。",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPUオフロード",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "GPUアクセラレーションのためにGPUで処理するモデル層の数",
  "embedding.load.llama.acceleration.offloadRatio/info": "GPUにオフロードする層の数を設定します。",
  "embedding.load.llama.keepModelInMemory/title": "モデルをメモリに保持",
  "embedding.load.llama.keepModelInMemory/subTitle": "モデルがGPUにオフロードされていても、システムメモリにモデルを保持します。これによりパフォーマンスが向上しますが、システムRAMを多く消費します。",
  "embedding.load.llama.keepModelInMemory/info": "モデルがストレージにスワップアウトされるのを防ぎ、より高速なアクセスを可能にしますが、RAMの使用量が増加します。",
  "embedding.load.llama.tryMmap/title": "mmap()を試す",
  "embedding.load.llama.tryMmap/subTitle": "モデルの読み込み時間を短縮します。モデルのサイズがシステムRAMの容量を超える場合は、これを無効にするとパフォーマンスが向上することがあります。",
  "embedding.load.llama.tryMmap/info": "モデルファイルをディスクからメモリに直接読み込みます。",
  "embedding.load.seed/title": "シード値",
  "embedding.load.seed/subTitle": "テキスト生成に使用される乱数生成器のシード値。-1でランダムになります。",

  "embedding.load.seed/info": "ランダムシード値: 乱数生成のシード値を設定して、再現可能な結果を得られるようにします。",

  "presetTooltip": {
    "included/title": "適用される項目",
    "included/description": "次のフィールドが適用されます",
    "included/empty": "このコンテキストで適用されるフィールドはありません。",
    "included/conflict": "この値を適用するかどうか、このあと選択を求められます",
    "separateLoad/title": "ロード時の設定",
    "separateLoad/description.1": "プリセットには以下のロード時設定も含まれています。ロード時の設定はモデル全体に適用され、効果を発揮するにはモデルの再読み込みが必要です。ホールド",
    "separateLoad/description.2": "して適用するには",
    "separateLoad/description.3": "を押してください。",
    "excluded/title": "適用されない項目",
    "excluded/description": "次のフィールドはプリセットに含まれていますが、現在のコンテキストでは適用されません。",
    "legacy/title": "レガシープリセット",
    "legacy/description": "このプリセットはレガシープリセットです。次のフィールドは現在自動的に処理されるか、もはや適用されません。"
  },
  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "オフ"
    },
    "llamaCacheQuantizationType": {
      "off": "オフ"
    },
    "mlxKvCacheBits": {
      "off": "オフ"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "タイプ",
      "types.jinja/label": "Jinja",
      "jinja.bosToken/label": "BOSトークン",
      "jinja.eosToken/label": "EOSトークン",
      "jinja.template/label": "テンプレート",
      "jinja/error": "Jinjaテンプレートの解析に失敗しました: {{error}}",
      "jinja/empty": "上部にJinjaテンプレートを入力してください。",
      "jinja/unlikelyToWork": "入力されたJinjaテンプレートは、変数\"messages\"を参照していないため、正しく動作しない可能性があります。正しいテンプレートが入力されているかもう一度確認してください。",
      "types.manual/label": "マニュアル",
      "manual.subfield.beforeSystem/label": "システムの前",
      "manual.subfield.beforeSystem/placeholder": "システムのプレフィックスを入力...",
      "manual.subfield.afterSystem/label": "システムの後",
      "manual.subfield.afterSystem/placeholder": "システムのサフィックスを入力...",
      "manual.subfield.beforeUser/label": "ユーザーの前",
      "manual.subfield.beforeUser/placeholder": "ユーザーのプレフィックスを入力...",
      "manual.subfield.afterUser/label": "ユーザーの後",
      "manual.subfield.afterUser/placeholder": "ユーザーのサフィックスを入力...",
      "manual.subfield.beforeAssistant/label": "アシスタントの前",
      "manual.subfield.beforeAssistant/placeholder": "アシスタントのプレフィックスを入力...",
      "manual.subfield.afterAssistant/label": "アシスタントの後",
      "manual.subfield.afterAssistant/placeholder": "アシスタントのサフィックスを入力...",
      "stopStrings/label": "追加の停止文字列",
      "stopStrings/subTitle": "テンプレート固有の停止文字列で、ユーザー指定の停止文字列に加えて使用されます。"
    },
    "contextLength": {
      "maxValueTooltip": "これはモデルが処理できる最大のトークン数です。この値に設定するにはクリックしてください。",
      "maxValueTextStart": "モデルは最大で",
      "maxValueTextEnd": "トークンをサポートします",
      "tooltipHint": "モデルが一定数のトークンをサポートしていても、マシンのリソースが処理負荷に対して低い場合はパフォーマンスが低下する可能性があります。値を増やす際は注意してください。"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "上限で停止",
      "stopAtLimitSub": "モデルのメモリがいっぱいになったら生成を停止",
      "truncateMiddle": "中間を切り捨て",
      "truncateMiddleSub": "会話の中間部分を削除して新しいメッセージにスペースを作ります。モデルは会話の最初の部分を覚えています。",
      "rollingWindow": "ローリングウィンドウ",
      "rollingWindowSub": "モデルは常に最新のメッセージを数件取得しますが、会話の最初の部分を忘れる可能性があります。"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "オフ"
    },
    "gpuSplitStrategy": {
      "evenly": "均等",
      "favorMainGpu": "メインGPUを優先"
    }
  },
  "saveConflictResolution": {
    "title": "プリセットに含める値を選択してください",
    "description": "保持する値を選択してください",
    "instructions": "値をクリックして含めます",
    "userValues": "以前の値",
    "presetValues": "新しい値",
    "confirm": "確認",
    "cancel": "キャンセル"
  },
  "applyConflictResolution": {
    "title": "どの値を保持しますか？",
    "description": "未適用の変更が受信したプリセットと重複しています",
    "instructions": "保持する値をクリックしてください",
    "userValues": "現在の値",
    "presetValues": "受信したプリセットの値",
    "confirm": "確認",
    "cancel": "キャンセル"
  },
  "empty": "<空>",
  "noModelSelected": "モデルが選択されていません",
  "apiIdentifier.label": "API識別子",
  "apiIdentifier.hint": "必要な場合は、このモデルの識別子を入力してください。APIリクエストで使用されます。デフォルトの識別子を使用する場合は、空白のままにしてください。",
  "idleTTL.label": "アイドル時の自動アンロード (TTL)",
  "idleTTL.hint": "設定した場合、未使用で指定された時間が経過すると、モデルは自動的にアンロードされます。",
  "idleTTL.mins": "分",

  "presets": {
    "title": "プリセット",
    "commitChanges": "変更を適用",
    "commitChanges/description": "変更をプリセットに適用します。",
    "commitChanges.manual": "新しいフィールドが検出されました。どの変更をプリセットに含めるか選択できます。",
    "commitChanges.manual.hold.0": "長押し",
    "commitChanges.manual.hold.1": "してプリセットに適用する変更を選択してください。",
    "commitChanges.saveAll.hold.0": "長押し",
    "commitChanges.saveAll.hold.1": "してすべての変更を保存します。",
    "commitChanges.saveInPreset.hold.0": "長押し",
    "commitChanges.saveInPreset.hold.1": "してプリセットにすでに含まれているフィールドの変更だけを保存します。",
    "commitChanges/error": "プリセットへの変更の適用に失敗しました。",
    "commitChanges.manual/description": "プリセットに含める変更を選択してください。",
    "saveAs": "新しいプリセットとして保存...",
    "presetNamePlaceholder": "プリセット名を入力してください...",
    "cannotCommitChangesLegacy": "これはレガシープリセットのため、変更はできません。「新しいプリセットとして保存」を使用してコピーを作成できます。",
    "cannotCommitChangesNoChanges": "適用する変更がありません。",
    "emptyNoUnsaved": "プリセットを選択...",
    "emptyWithUnsaved": "未保存のプリセット",
    "saveEmptyWithUnsaved": "プリセットを名前を付けて保存...",
    "saveConfirm": "保存",
    "saveCancel": "キャンセル",
    "saving": "保存中...",
    "save/error": "プリセットの保存に失敗しました。",
    "deselect": "プリセットの選択を解除",
    "deselect/error": "プリセットの選択解除に失敗しました。",
    "select/error": "プリセットの選択に失敗しました。",
    "delete/error": "プリセットの削除に失敗しました。",
    "discardChanges": "未保存の変更を破棄",
    "discardChanges/info": "未保存の全ての変更を破棄し、プリセットを元の状態に戻します。",
    "newEmptyPreset": "新しい空のプリセットを作成...",
    "contextMenuSelect": "プリセットを選択",
    "contextMenuDelete": "削除"
  },
  "flashAttentionWarning": "Flash Attention は実験的な機能であり、特定のモデルで問題を引き起こす可能性があります。問題が発生した場合は無効にしてみてください。",
  "seedUncheckedHint": "ランダムシード",
  "ropeFrequencyBaseUncheckedHint": "自動",
  "ropeFrequencyScaleUncheckedHint": "自動"
}
