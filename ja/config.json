{
  "noInstanceSelected": "モデルが選択されていません",
  "resetToDefault": "リセット",
  "showAdvancedSettings": "詳細設定を表示",
  "showAll": "すべて",
  "basicSettings": "基本",
  "configSubtitle": "プリセットの読み込みまたは保存、モデルパラメータのオーバーライドを試す",
  "inferenceParameters/title": "予測パラメータ",
  "inferenceParameters/info": "予測に影響を与えるパラメータを試す。",
  "generalParameters/title": "一般",
  "samplingParameters/title": "サンプリング",
  "basicTab": "基本",
  "advancedTab": "詳細",
  "advancedTab/title": "🧪 詳細設定",
  "advancedTab/expandAll": "すべて展開",
  "advancedTab/overridesTitle": "設定のオーバーライド",
  "advancedTab/noConfigsText": "未保存の変更はありません - 上の値を編集してここでオーバーライドを表示します。",
  "loadInstanceFirst": "モデルを読み込んで、設定可能なパラメータを表示します",
  "noListedConfigs": "設定可能なパラメータはありません",
  "generationParameters/info": "テキスト生成に影響を与える基本的なパラメータを試す。",
  "loadParameters/title": "パラメータの読み込み",
  "loadParameters/description": "モデルの初期化とメモリへの読み込み方法を制御する設定。",
  "loadParameters/reload": "変更を適用するには再読み込み",
  "loadParameters/reload/error": "モデルの再読み込みに失敗しました",
  "discardChanges": "変更を破棄",
  "loadModelToSeeOptions": "オプションを表示するにはモデルを読み込んでください",
  "schematicsError.title": "設定スキーマに以下のフィールドにエラーがあります:",
  "manifestSections": {
    "structuredOutput/title": "構造化出力",
    "speculativeDecoding/title": "推測デコーディング",
    "sampling/title": "サンプリング",
    "settings/title": "設定",
    "toolUse/title": "ツールの使用",
    "promptTemplate/title": "プロンプトテンプレート",
    "customFields/title": "カスタムフィールド"
  },

  "llm.prediction.systemPrompt/title": "システムプロンプト",
  "llm.prediction.systemPrompt/description": "このフィールドには、モデルに対する背景指示（ルール、制約、一般的な要件など）を記載してください。",
  "llm.prediction.systemPrompt/subTitle": "AI のガイドライン",
  "llm.prediction.systemPrompt/openEditor": "エディター",
  "llm.prediction.systemPrompt/closeEditor": "エディターを閉じる",
  "llm.prediction.systemPrompt/openedEditor": "エディターで開いています...",
  "llm.prediction.systemPrompt/edit": "システムプロンプトを編集...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "指示を追加...",
  "llm.prediction.systemPrompt/addInstructions": "指示を追加",
  "llm.prediction.temperature/title": "Temperature",
  "llm.prediction.temperature/subTitle": "ランダム性の度合い。0は毎回同じ結果になり、高い値では創造性とばらつきが増加します",
  "llm.prediction.temperature/info": "llama.cpp ヘルプドキュメントより：「デフォルト値は <{{dynamicValue}}> で、ランダム性と決定性のバランスを提供します。極端な場合、temperature が 0 だと常に最も可能性の高い次のトークンを選択し、毎回同じ出力になります」",
  "llm.prediction.llama.sampling/title": "サンプリング",
  "llm.prediction.topKSampling/title": "Top K サンプリング",
  "llm.prediction.topKSampling/subTitle": "次のトークンを上位k個の最も確率の高いトークンのいずれかに制限します。temperatureと同様の動作をします",
  "llm.prediction.topKSampling/info": "llama.cpp ヘルプドキュメントより：\n\nTop-kサンプリングは、モデルによって予測された上位k個の最も可能性の高いトークンのみから次のトークンを選択するテキスト生成方法です。\n\n確率の低いまたは無意味なトークンを生成するリスクを減らすのに役立ちますが、出力の多様性を制限する可能性もあります。\n\ntop-kの値が高いほど（例：100）、より多くのトークンが考慮され、より多様なテキストが生成され、値が低いほど（例：10）、最も確率の高いトークンに焦点を当て、より保守的なテキストが生成されます。\n\n• デフォルト値は <{{dynamicValue}}> です",
  "llm.prediction.llama.cpuThreads/title": "CPUスレッド",
  "llm.prediction.llama.cpuThreads/subTitle": "推論中に使用するCPUスレッドの数",
  "llm.prediction.llama.cpuThreads/info": "計算中に使用するスレッド数。スレッド数を増やしても必ずしもパフォーマンスの向上に繋がるわけではありません。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.maxPredictedTokens/title": "応答長の制限",
  "llm.prediction.maxPredictedTokens/subTitle": "AIの応答の長さを制限します",
  "llm.prediction.maxPredictedTokens/info": "チャットボットの応答の最大長を制御します。オンにして応答の最大長を制限するか、オフにしてチャットボット自体に停止タイミングを決めさせます。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大応答長（トークン）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約{{maxWords}}語",
  "llm.prediction.repeatPenalty/title": "繰り返しペナルティ",
  "llm.prediction.repeatPenalty/subTitle": "同じトークンの繰り返しをどの程度抑制するか",
  "llm.prediction.repeatPenalty/info": "llama.cpp ヘルプドキュメントより：「モデルが繰り返しや単調なテキストを生成することを防ぐのに役立ちます。\n\n高い値（例：1.5）は繰り返しをより強くペナルティ化し、低い値（例：0.9）はより寛大になります。」 • デフォルト値は <{{dynamicValue}}> です",
  "llm.prediction.minPSampling/title": "Min P サンプリング",
  "llm.prediction.minPSampling/subTitle": "出力対象として選択されるトークンの最小ベース確率",
  "llm.prediction.minPSampling/info": "llama.cpp ヘルプドキュメントより：\n\n最も可能性の高いトークンの確率に対する相対的な、トークンが考慮される最小確率。[0, 1]の範囲である必要があります。\n\n• デフォルト値は <{{dynamicValue}}> です",
  "llm.prediction.topPSampling/title": "Top P サンプリング",
  "llm.prediction.topPSampling/subTitle": "次の可能なトークンの最小累積確率。temperatureと同様の動作をします",
  "llm.prediction.topPSampling/info": "llama.cpp ヘルプドキュメントより：\n\nNucleus サンプリングとも呼ばれる Top-p サンプリングは、累積確率が少なくとも p となるトークンのサブセットから次のトークンを選択するテキスト生成方法です。\n\nこの方法は、トークンの確率とサンプリング対象となるトークン数の両方を考慮することで、多様性と品質のバランスを提供します。\n\ntop-p の値が高い（例：0.95）場合はより多様なテキストが生成され、低い値（例：0.5）の場合はより集中的で保守的なテキストが生成されます。(0, 1]の範囲である必要があります。\n\n• デフォルト値は <{{dynamicValue}}> です",
  "llm.prediction.stopStrings/title": "停止文字列",
  "llm.prediction.stopStrings/subTitle": "モデルがトークン生成を停止する文字列",
  "llm.prediction.stopStrings/info": "これらの文字列が検出されたときにモデルがさらなるトークン生成を停止する特定の文字列",
  "llm.prediction.stopStrings/placeholder": "文字列を入力してから ⏎ を押してください",
  "llm.prediction.contextOverflowPolicy/title": "コンテキストオーバーフロー",
  "llm.prediction.contextOverflowPolicy/subTitle": "会話がモデルの処理能力を超えた場合の動作方法",
  "llm.prediction.contextOverflowPolicy/info": "会話がモデルの作業メモリ（'コンテキスト'）のサイズを超えた場合の対応を決定します",
  "llm.prediction.llama.frequencyPenalty/title": "頻度ペナルティ",
  "llm.prediction.llama.presencePenalty/title": "存在ペナルティ",
  "llm.prediction.llama.tailFreeSampling/title": "テイルフリーサンプリング",
  "llm.prediction.llama.locallyTypicalSampling/title": "局所的典型サンプリング",
  "llm.prediction.llama.xtcProbability/title": "XTCサンプリング確率",
  "llm.prediction.llama.xtcProbability/subTitle": "XTC（Exclude Top Choices）サンプラーが生成された各トークンでこの確率で有効化されます。XTCサンプリングは創造性を向上させ、陳腐な表現を減らします",
  "llm.prediction.llama.xtcProbability/info": "XTC（Exclude Top Choices）サンプリングは、生成された各トークンでこの確率でのみ有効化されます。XTCサンプリングは通常創造性を向上させ、陳腐な表現を減らします",
  "llm.prediction.llama.xtcThreshold/title": "XTCサンプリング閾値",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC（Exclude Top Choices）閾値。`xtc-probability`の確率で、`xtc-threshold`と0.5の間の確率を持つトークンを検索し、最も確率の低いもの以外のそのようなトークンをすべて除去します",
  "llm.prediction.llama.xtcThreshold/info": "XTC（Exclude Top Choices）閾値。`xtc-probability`の確率で、`xtc-threshold`と0.5の間の確率を持つトークンを検索し、最も確率の低いもの以外のそのようなトークンをすべて除去します",
  "llm.prediction.mlx.topKSampling/title": "Top K サンプリング",
  "llm.prediction.mlx.topKSampling/subTitle": "次のトークンを上位k個の最も確率の高いトークンのいずれかに制限します。temperatureと同様の動作をします",
  "llm.prediction.mlx.topKSampling/info": "次のトークンを上位k個の最も確率の高いトークンのいずれかに制限します。temperatureと同様の動作をします",
  "llm.prediction.onnx.topKSampling/title": "Top K サンプリング",
  "llm.prediction.onnx.topKSampling/subTitle": "次のトークンを上位k個の最も確率の高いトークンのいずれかに制限します。temperatureと同様の動作をします",
  "llm.prediction.onnx.topKSampling/info": "ONNX ドキュメントより：\n\ntop-k フィルタリング用に保持する最高確率語彙トークンの数\n\n• このフィルターはデフォルトでオフになっています",
  "llm.prediction.onnx.repeatPenalty/title": "繰り返しペナルティ",
  "llm.prediction.onnx.repeatPenalty/subTitle": "同じトークンの繰り返しをどの程度抑制するか",
  "llm.prediction.onnx.repeatPenalty/info": "高い値により、モデルが同じ内容を繰り返すことを抑制します",
  "llm.prediction.onnx.topPSampling/title": "Top P サンプリング",
  "llm.prediction.onnx.topPSampling/subTitle": "次の可能なトークンの最小累積確率。temperatureと同様の動作をします",
  "llm.prediction.onnx.topPSampling/info": "ONNX ドキュメントより：\n\nTopP 以上の確率を加算した最も確率の高いトークンのみが生成用に保持されます\n\n• このフィルターはデフォルトでオフになっています",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "構造化出力",
  "llm.prediction.structured/info": "構造化出力",
  "llm.prediction.structured/description": "高度：特定の出力形式をモデルに強制するために[JSON Schema](https://json-schema.org/learn/miscellaneous-examples)を提供できます。詳細は[ドキュメント](https://lmstudio.ai/docs/advanced/structured-output)をお読みください",
  "llm.prediction.tools/title": "ツールの使用",
  "llm.prediction.tools/description": "高度：モデルが呼び出しを要求するツールのJSONに準拠したリストを提供できます。詳細は[ドキュメント](https://lmstudio.ai/docs/advanced/tool-use)をお読みください",
  "llm.prediction.tools/serverPageDescriptionAddon": "サーバーAPIを使用する際は、リクエストボディで`tools`としてこれを渡してください",
  "llm.prediction.promptTemplate/title": "プロンプトテンプレート",
  "llm.prediction.promptTemplate/subTitle": "チャット内のメッセージがモデルに送信される形式。これを変更すると予期しない動作を引き起こす可能性があります - 何をしているか理解していることを確認してください！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "生成するドラフトトークン数",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "メインモデルのトークンごとにドラフトモデルで生成するトークン数。計算量と報酬の最適なバランスを見つけてください",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "ドラフティング確率カットオフ",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "トークンの確率がこの閾値を下回るまでドラフティングを続行します。高い値は一般的に低リスク・低報酬を意味します",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小ドラフトサイズ",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "これより小さいドラフトはメインモデルによって無視されます。高い値は一般的に低リスク・低報酬を意味します",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大ドラフトサイズ",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "ドラフトで許可されるトークンの最大数。すべてのトークン確率がカットオフを上回る場合の上限。低い値は一般的に低リスク・低報酬を意味します",
  "llm.prediction.speculativeDecoding.draftModel/title": "ドラフトモデル",
  "llm.prediction.reasoning.parsing/title": "推論セクション解析",
  "llm.prediction.reasoning.parsing/subTitle": "モデル出力の推論セクションの解析方法",

  "llm.load.mainGpu/title": "メインGPU",
  "llm.load.mainGpu/subTitle": "モデル計算で優先するGPU",
  "llm.load.mainGpu/placeholder": "メインGPUを選択...",
  "llm.load.splitStrategy/title": "分割戦略",
  "llm.load.splitStrategy/subTitle": "モデル計算をGPU間でどのように分割するか",
  "llm.load.splitStrategy/placeholder": "分割戦略を選択...",
  "llm.load.offloadKVCacheToGpu/title": "KVキャッシュをGPUメモリにオフロード",
  "llm.load.offloadKVCacheToGpu/subTitle": "KVキャッシュをGPUメモリにオフロードします。パフォーマンスが向上しますが、より多くのGPUメモリが必要です",
  "llm.load.numParallelSessions/title": "最大同時予測数",
  "llm.load.numParallelSessions/subTitle": "モデルが同時に実行できる予測の最大数。並行性が高いと個々の予測速度は低下する可能性がありますが、各予測の開始が速くなり、総スループットが向上します",
  "llm.load.useUnifiedKvCache/title": "統一KVキャッシュ",
  "llm.load.useUnifiedKvCache/subTitle": "同時予測が単一のKVキャッシュを共有するかどうかを制御し、メモリを節約します。無効にすると、各予測が完全なコンテキスト長を利用できますが、メモリ使用量が増加します",
  "load.gpuStrictVramCap/title": "モデルオフロードを専用GPUメモリに制限",
  "load.gpuStrictVramCap.customSubTitleOff": "オフ：専用GPUメモリが満杯の場合、モデル重みを共有メモリにオフロードすることを許可します",
  "load.gpuStrictVramCap.customSubTitleOn": "オン：システムはモデル重みのオフロードを専用GPUメモリとRAMのみに制限します。コンテキストは引き続き共有メモリを使用する可能性があります",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "モデルオフロードが専用GPUメモリに制限されました。実際のオフロードされるレイヤー数は異なる場合があります",
  "load.allGpusDisabledWarning": "現在、すべてのGPUが無効になっています。オフロードするには少なくとも1つを有効にしてください",

  "llm.load.contextLength/title": "コンテキスト長",
  "llm.load.contextLength/subTitle": "モデルが一つのプロンプトで処理できる最大トークン数。これを管理する方法については、「推論パラメータ」の下の会話オーバーフロー オプションを参照してください",
  "llm.load.contextLength/info": "モデルが一度に考慮できるトークンの最大数を指定し、処理中に保持するコンテキストの量に影響します",
  "llm.load.contextLength/warning": "コンテキスト長に高い値を設定すると、メモリ使用量に大きな影響を与える可能性があります",
  "llm.load.seed/title": "シード",
  "llm.load.seed/subTitle": "テキスト生成で使用される乱数ジェネレータのシード。-1はランダムです",
  "llm.load.seed/info": "ランダムシード：再現可能な結果を保証するために、乱数生成のシードを設定します",
  "llm.load.numCpuExpertLayersRatio/title": "モデルエキスパート重みをCPUに強制",
  "llm.load.numCpuExpertLayersRatio/subTitle": "MoEエキスパート重みをCPU RAMに強制するかどうか。VRAMを節約でき、部分的なGPUオフロードより高速になる場合があります。モデルがVRAMに完全に収まる場合は推奨されません。",
  "llm.load.numCpuExpertLayersRatio/info": "すべてのMoEエキスパートレイヤーをCPU RAMに配置するかどうかを指定します。注意レイヤーはGPUに残り、VRAMを節約しながら推論を比較的高速に保ちます。",

  "llm.load.llama.evalBatchSize/title": "評価バッチサイズ",
  "llm.load.llama.evalBatchSize/subTitle": "一度に処理する入力トークンの数。これを増やすとメモリ使用量は増えますがパフォーマンスが向上します",
  "llm.load.llama.evalBatchSize/info": "評価中に一つのバッチで一緒に処理される例の数を設定し、速度とメモリ使用量に影響します",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE周波数ベース",
  "llm.load.llama.ropeFrequencyBase/subTitle": "回転位置エンベディング（RoPE）のカスタムベース周波数。これを増やすと、高いコンテキスト長でのパフォーマンス向上が期待できます",
  "llm.load.llama.ropeFrequencyBase/info": "[高度] 回転位置エンコーディングのベース周波数を調整し、位置情報の埋め込み方法に影響します",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE周波数スケール",
  "llm.load.llama.ropeFrequencyScale/subTitle": "RoPEを使用して効果的なコンテキストを拡張するために、コンテキスト長がこの要素でスケールされます",
  "llm.load.llama.ropeFrequencyScale/info": "[高度] 回転位置エンコーディングの周波数スケーリングを変更し、位置エンコーディングの粒度を制御します",
  "llm.load.llama.acceleration.offloadRatio/title": "GPUオフロード",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "GPU高速化のためにGPUで計算する個別のモデルレイヤー数",
  "llm.load.llama.acceleration.offloadRatio/info": "GPUにオフロードするレイヤー数を設定します。",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "一部のモデルでメモリ使用量と生成時間を削減します",
  "llm.load.llama.flashAttention/info": "より高速で効率的な処理のためにAttentionメカニズムを加速します",
  "llm.load.numExperts/title": "エキスパート数",
  "llm.load.numExperts/subTitle": "モデルで使用するエキスパートの数",
  "llm.load.numExperts/info": "モデルで使用するエキスパートの数",
  "llm.load.llama.keepModelInMemory/title": "モデルをメモリに保持",
  "llm.load.llama.keepModelInMemory/subTitle": "GPUにオフロードした場合でも、モデル用にシステムメモリを予約します。パフォーマンスが向上しますが、より多くのシステムRAMが必要です",
  "llm.load.llama.keepModelInMemory/info": "モデルがディスクにスワップされることを防ぎ、RAMの使用量増加を犠牲により高速なアクセスを保証します",
  "llm.load.llama.useFp16ForKVCache/title": "KVキャッシュにFP16を使用",
  "llm.load.llama.useFp16ForKVCache/info": "キャッシュを半精度（FP16）で保存することでメモリ使用量を削減します",
  "llm.load.llama.tryMmap/title": "mmap() を試行",
  "llm.load.llama.tryMmap/subTitle": "モデルの読み込み時間が改善されます。利用可能なシステムRAMよりもモデルが大きい場合は、これを無効にするとパフォーマンスが向上する可能性があります",
  "llm.load.llama.tryMmap/info": "モデルファイルをディスクから直接メモリに読み込みます",
  "llm.load.llama.cpuThreadPoolSize/title": "CPUスレッドプールサイズ",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "モデル計算に使用するスレッドプールに割り当てるCPUスレッド数",
  "llm.load.llama.cpuThreadPoolSize/info": "モデル計算に使用するスレッドプールに割り当てるCPUスレッド数。スレッド数を増やしても必ずしもパフォーマンスの向上に繋がるわけではありません。デフォルト値は <{{dynamicValue}}> です。",
  "llm.load.llama.kCacheQuantizationType/title": "K キャッシュ量子化タイプ",
  "llm.load.llama.kCacheQuantizationType/subTitle": "低い値でメモリ使用量を削減しますが、品質が低下する可能性があります。効果はモデルによって大きく異なります。",
  "llm.load.llama.vCacheQuantizationType/title": "V キャッシュ量子化タイプ",
  "llm.load.llama.vCacheQuantizationType/subTitle": "低い値でメモリ使用量を削減しますが、品質が低下する可能性があります。効果はモデルによって大きく異なります。",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ Flash Attentionが有効でない場合は、この値を無効にする必要があります",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Flash Attentionが有効な場合にのみオンにできます",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ F32を使用する際はFlash Attentionを無効にする必要があります",
  "llm.load.mlx.kvCacheBits/title": "KVキャッシュ量子化",
  "llm.load.mlx.kvCacheBits/subTitle": "KVキャッシュを量子化するビット数",
  "llm.load.mlx.kvCacheBits/info": "KVキャッシュを量子化するビット数",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "KVキャッシュ量子化を使用すると、コンテキスト長設定は無視されます",
  "llm.load.mlx.kvCacheGroupSize/title": "KVキャッシュ量子化: グループサイズ",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "KVキャッシュの量子化操作中のグループサイズ。グループサイズが大きいほどメモリ使用量は削減されますが、品質が低下する可能性があります",
  "llm.load.mlx.kvCacheGroupSize/info": "KVキャッシュを量子化するビット数",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KVキャッシュ量子化: ctx がこの長さを超えたら量子化を開始",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "KVキャッシュの量子化を開始するコンテキスト長の閾値",
  "llm.load.mlx.kvCacheQuantizationStart/info": "KVキャッシュの量子化を開始するコンテキスト長の閾値",
  "llm.load.mlx.kvCacheQuantization/title": "KVキャッシュ量子化",
  "llm.load.mlx.kvCacheQuantization/subTitle": "モデルのKVキャッシュを量子化します。これにより生成の高速化とメモリ使用量の削減が可能ですが、\nモデル出力の品質を犠牲にします。",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KVキャッシュ量子化ビット",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "KVキャッシュを量子化するビット数",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "ビット",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "グループサイズ戦略",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "精度",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "バランス",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "高速",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "高度：量子化された「行列積グループサイズ」の構成\n\n• 精度 = グループサイズ 32\n• バランス = グループサイズ 64\n• 高速 = グループサイズ 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "ctx がこの長さに達したら量子化を開始",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "コンテキストがこのトークン数に達したら、\nKVキャッシュの量子化を開始する",

  "embedding.load.contextLength/title": "コンテキスト長",
  "embedding.load.contextLength/subTitle": "モデルが一つのプロンプトで処理できる最大トークン数。これを管理する方法については、「推論パラメータ」の下の会話オーバーフロー オプションを参照してください",
  "embedding.load.contextLength/info": "モデルが一度に考慮できるトークンの最大数を指定し、処理中に保持するコンテキストの量に影響します",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE周波数ベース",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "回転位置エンベディング（RoPE）のカスタムベース周波数。これを増やすと、高いコンテキスト長でのパフォーマンス向上が期待できます",
  "embedding.load.llama.ropeFrequencyBase/info": "[高度] 回転位置エンコーディングのベース周波数を調整し、位置情報の埋め込み方法に影響します",
  "embedding.load.llama.evalBatchSize/title": "評価バッチサイズ",
  "embedding.load.llama.evalBatchSize/subTitle": "一度に処理する入力トークンの数。これを増やすとメモリ使用量は増えますがパフォーマンスが向上します",
  "embedding.load.llama.evalBatchSize/info": "評価中に一つのバッチで一緒に処理されるトークンの数を設定します",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE周波数スケール",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "RoPEを使用して効果的なコンテキストを拡張するために、コンテキスト長がこの要素でスケールされます",
  "embedding.load.llama.ropeFrequencyScale/info": "[高度] 回転位置エンコーディングの周波数スケーリングを変更し、位置エンコーディングの粒度を制御します",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPUオフロード",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "GPU高速化のためにGPUで計算する個別のモデルレイヤー数",
  "embedding.load.llama.acceleration.offloadRatio/info": "GPUにオフロードするレイヤー数を設定します。",
  "embedding.load.llama.keepModelInMemory/title": "モデルをメモリに保持",
  "embedding.load.llama.keepModelInMemory/subTitle": "GPUにオフロードした場合でも、モデル用にシステムメモリを予約します。パフォーマンスが向上しますが、より多くのシステムRAMが必要です",
  "embedding.load.llama.keepModelInMemory/info": "モデルがディスクにスワップされることを防ぎ、RAMの使用量増加を犠牲により高速なアクセスを保証します",
  "embedding.load.llama.tryMmap/title": "mmap() を試行",
  "embedding.load.llama.tryMmap/subTitle": "モデルの読み込み時間が改善されます。利用可能なシステムRAMよりもモデルが大きい場合は、これを無効にするとパフォーマンスが向上する可能性があります",
  "embedding.load.llama.tryMmap/info": "モデルファイルをディスクから直接メモリに読み込みます",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "テキスト生成で使用される乱数ジェネレータのシード。-1はランダムシードです",

  "embedding.load.seed/info": "ランダムシード：再現可能な結果を保証するために、乱数生成のシードを設定します",

  "presetTooltip": {
    "included/title": "プリセット値",
    "included/description": "以下のフィールドが適用されます",
    "included/empty": "このプリセットのフィールドはこのコンテキストで適用されません。",
    "included/conflict": "この値を適用するかどうか選択を求められます",
    "separateLoad/title": "読み込み時設定",
    "separateLoad/description.1": "プリセットには以下の読み込み時設定も含まれています。読み込み時設定はモデル全体に適用され、効果を有効にするにはモデルの再読み込みが必要です。",
    "separateLoad/description.2": "を押しながらクリックして",
    "separateLoad/description.3": "に適用します。",
    "excluded/title": "適用されない可能性があります",
    "excluded/description": "以下のフィールドはプリセットに含まれていますが、現在のコンテキストでは適用されません。",
    "legacy/title": "レガシープリセット",
    "legacy/description": "これはレガシープリセットです。現在では自動的に処理されるか、もはや適用できない以下のフィールドが含まれています。",
    "button/publish": "Hubに公開",
    "button/pushUpdate": "変更をHubにプッシュ",
    "button/noChangesToPush": "プッシュする変更がありません",
    "button/export": "エクスポート",
    "hubLabel": "{{user}}によるHubからのプリセット",
    "ownHubLabel": "Hubにある自分のプリセット"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "オフ"
    },
    "llamaCacheQuantizationType": {
      "off": "オフ"
    },
    "mlxKvCacheBits": {
      "off": "オフ"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "タイプ",
      "types.jinja/label": "テンプレート（Jinja）",
      "jinja.bosToken/label": "BOSトークン",
      "jinja.eosToken/label": "EOSトークン",
      "jinja.template/label": "テンプレート",
      "jinja/error": "Jinjaテンプレートの解析に失敗しました：{{error}}",
      "jinja/empty": "上記にJinjaテンプレートを入力してください。",
      "jinja/unlikelyToWork": "上記に提供されたJinjaテンプレートは、変数「messages」を参照していないため機能しない可能性があります。正しいテンプレートを入力したかご確認ください。",
      "types.manual/label": "手動",
      "manual.subfield.beforeSystem/label": "システムの前",
      "manual.subfield.beforeSystem/placeholder": "システムプレフィックスを入力...",
      "manual.subfield.afterSystem/label": "システムの後",
      "manual.subfield.afterSystem/placeholder": "システムサフィックスを入力...",
      "manual.subfield.beforeUser/label": "ユーザーの前",
      "manual.subfield.beforeUser/placeholder": "ユーザープレフィックスを入力...",
      "manual.subfield.afterUser/label": "ユーザーの後",
      "manual.subfield.afterUser/placeholder": "ユーザーサフィックスを入力...",
      "manual.subfield.beforeAssistant/label": "アシスタントの前",
      "manual.subfield.beforeAssistant/placeholder": "アシスタントプレフィックスを入力...",
      "manual.subfield.afterAssistant/label": "アシスタントの後",
      "manual.subfield.afterAssistant/placeholder": "アシスタントサフィックスを入力...",
      "stopStrings/label": "追加停止文字列",
      "stopStrings/subTitle": "ユーザー指定の停止文字列に加えて使用される、テンプレート固有の停止文字列。"
    },
    "contextLength": {
      "maxValueTooltip": "これは、モデルが処理するために訓練された最大トークン数です。クリックしてコンテキストをこの値に設定します",
      "maxValueTextStart": "モデルは最大で",
      "maxValueTextEnd": "トークンをサポートします",
      "tooltipHint": "モデルは特定の数のトークンまでサポートしていても、マシンのリソースが負荷を処理できない場合はパフォーマンスが劣化する可能性があります - この値を増加させる際は注意してください"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "制限で停止",
      "stopAtLimitSub": "モデルのメモリが満杯になったら生成を停止します",
      "truncateMiddle": "中間を切り詰め",
      "truncateMiddleSub": "新しいメッセージのためのスペースを作るために、会話の中間部分からメッセージを削除します。モデルは会話の最初の部分は記憶しています",
      "rollingWindow": "ローリングウィンドウ",
      "rollingWindowSub": "モデルは常に最新の数メッセージを取得しますが、会話の最初の部分を忘れる可能性があります"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "オフ"
    },
    "gpuSplitStrategy": {
      "evenly": "均等に",
      "favorMainGpu": "メインGPUを優先"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "動作原理を読む",
      "placeholder": "互換性のあるドラフトモデルを選択",
      "noCompatible": "現在のモデル選択に対応する互換性のあるドラフトモデルが見つかりません",
      "stillLoading": "互換性のあるドラフトモデルを識別しています...",
      "notCompatible": "選択されたドラフトモデル（<draft/>）は現在のモデル選択（<current/>）と互換性がありません。",
      "off": "オフ",
      "loadModelToSeeOptions": "互換性のあるオプションを確認するにはモデルを読み込んでください <keyboard-shortcut />",
      "compatibleWithNumberOfModels": "あなたのモデルの少なくとも{{dynamicValue}}個に推奨されます",
      "recommendedForSomeModels": "いくつかのモデルに推奨されます",
      "recommendedForLlamaModels": "Llamaモデルに推奨されます",
      "recommendedForQwenModels": "Qwenモデルに推奨されます",
      "onboardingModal": {
        "introducing": "紹介",
        "speculativeDecoding": "推測デコーディング",
        "firstStepBody": "<custom-span>llama.cpp</custom-span>と<custom-span>MLX</custom-span>モデルの推論高速化",
        "secondStepTitle": "推測デコーディングによる推論高速化",
        "secondStepBody": "推測デコーディングは、2つのモデルの協調を含む技術です：\n - 大きな「メイン」モデル\n - 小さな「ドラフト」モデル\n\n生成中、ドラフトモデルは大きなメインモデルが検証するためのトークンを迅速に提案します。トークンの検証は、実際にそれらを生成するよりもはるかに高速なプロセスであり、これが高速化の源泉です。**一般的に、メインモデルとドラフトモデルのサイズ差が大きいほど、高速化も大きくなります**。\n\n品質を維持するため、メインモデルは自身が生成したであろうものと一致するトークンのみを受け入れ、より大きなモデルの応答品質をより高速な推論速度で実現します。両モデルは同じ語彙を共有する必要があります。",
        "draftModelRecommendationsTitle": "ドラフトモデルの推奨",
        "basedOnCurrentModels": "現在のモデルに基づく",
        "close": "閉じる",
        "next": "次へ",
        "done": "完了"
      },
      "speculativeDecodingLoadModelToSeeOptions": "最初にモデルを読み込んでください <model-badge /> ",
      "errorEngineNotSupported": "推測デコーディングにはエンジン{{engineName}}のバージョン{{minVersion}}以上が必要です。エンジンを更新し（<key/>）、この機能を使用するためにモデルを再読み込みしてください。",
      "errorEngineNotSupported/noKey": "推測デコーディングにはエンジン{{engineName}}のバージョン{{minVersion}}以上が必要です。エンジンを更新し、この機能を使用するためにモデルを再読み込みしてください。"
    },
    "llmReasoningParsing": {
      "startString/label": "開始文字列",
      "startString/placeholder": "開始文字列を入力...",
      "endString/label": "終了文字列",
      "endString/placeholder": "終了文字列を入力..."
    }
  },
  "saveConflictResolution": {
    "title": "プリセットに含める値を選択",
    "description": "保持する値を選択してください",
    "instructions": "値をクリックして含めてください",
    "userValues": "以前の値",
    "presetValues": "新しい値",
    "confirm": "確認",
    "cancel": "キャンセル"
  },
  "applyConflictResolution": {
    "title": "どの値を保持しますか？",
    "description": "受信プリセットと重複する未コミットの変更があります",
    "instructions": "保持したい値をクリックしてください",
    "userValues": "現在の値",
    "presetValues": "受信プリセット値",
    "confirm": "確認",
    "cancel": "キャンセル"
  },
  "empty": "<空>",
  "noModelSelected": "モデルが選択されていません",
  "apiIdentifier.label": "API識別子",
  "apiIdentifier.hint": "オプションで、このモデルの識別子を提供してください。これはAPIリクエストで使用されます。空白のままにするとデフォルトの識別子を使用します。",
  "idleTTL.label": "アイドル時自動アンロード（TTL）",
  "idleTTL.hint": "設定すると、モデルは指定された時間アイドル状態が続いた後に自動的にアンロードされます。",
  "idleTTL.mins": "分",

  "presets": {
    "title": "プリセット",
    "saveChanges": "保存",
    "saveChanges/description": "プリセットへの変更を保存します。",
    "saveChanges.manual": "新しいフィールドが検出されました。プリセットに含める変更を選択できます。",
    "saveChanges.manual.hold.0": "押しながら",
    "saveChanges.manual.hold.1": "をクリックして、プリセットに保存する変更を選択してください。",
    "saveChanges.saveAll.hold.0": "押しながら",
    "saveChanges.saveAll.hold.1": "をクリックして、すべての変更を保存してください。",
    "saveChanges.saveInPreset.hold.0": "押しながら",
    "saveChanges.saveInPreset.hold.1": "をクリックして、プリセットに既に含まれているフィールドのみの変更を保存してください。",
    "saveChanges/error": "プリセットへの変更の保存に失敗しました。",
    "saveChanges.manual/description": "プリセットに含める変更を選択してください。",
    "saveAs": "新規保存...",
    "presetNamePlaceholder": "プリセットの名前を入力...",
    "cannotCommitChangesLegacy": "これはレガシープリセットで変更できません。「新規保存...」を使用してコピーを作成できます。",
    "cannotSaveChangesNoChanges": "保存する変更がありません。",
    "emptyNoUnsaved": "プリセットを選択...",
    "emptyWithUnsaved": "未保存のプリセット",
    "saveEmptyWithUnsaved": "プリセットに名前を付けて保存...",
    "saveConfirm": "保存",
    "saveCancel": "キャンセル",
    "saving": "保存中...",
    "save/error": "プリセットの保存に失敗しました。",
    "deselect": "プリセットの選択を解除",
    "deselect/error": "プリセットの選択解除に失敗しました。",
    "select/error": "プリセットの選択に失敗しました。",
    "delete/error": "プリセットの削除に失敗しました。",
    "discardChanges": "未保存を破棄",
    "discardChanges/info": "すべての未保存の変更を破棄し、プリセットを元の状態に復元します",
    "newEmptyPreset": "+ 新しいプリセット",
    "importPreset": "インポート",
    "contextMenuCopyIdentifier": "プリセット識別子をコピー",
    "contextMenuSelect": "プリセットを適用",
    "contextMenuDelete": "削除...",
    "contextMenuShare": "公開...",
    "contextMenuOpenInHub": "Webで表示",
    "contextMenuPullFromHub": "最新を取得",
    "contextMenuPushChanges": "変更をHubにプッシュ",
    "contextMenuPushingChanges": "プッシュ中...",
    "contextMenuPushedChanges": "変更がプッシュされました",
    "contextMenuExport": "ファイルをエクスポート",
    "contextMenuRevealInExplorer": "エクスプローラーで表示",
    "contextMenuRevealInFinder": "Finderで表示",
    "share": {
      "title": "プリセットを公開",
      "action": "他の人がダウンロード、いいね、フォークできるようにプリセットを共有します",
      "presetOwnerLabel": "所有者",
      "uploadAs": "あなたのプリセットは{{name}}として作成されます",
      "presetNameLabel": "プリセット名",
      "descriptionLabel": "説明（オプション）",
      "loading": "公開中...",
      "success": "プリセットの公開が完了しました",
      "presetIsLive": "<preset-name />がHubで公開されました！",
      "close": "閉じる",
      "confirmViewOnWeb": "Webで表示",
      "confirmCopy": "URLをコピー",
      "confirmCopied": "コピー完了！",
      "pushedToHub": "プリセットがHubにプッシュされました",
      "descriptionPlaceholder": "説明を入力...",
      "willBePublic": "このプリセットは公開されます。インターネット上の誰でも見ることができます。",
      "willBePrivate": "このプリセットはあなたのみが表示できます",
      "willBeOrgVisible": "このプリセットは組織内のすべてのメンバーに表示されます。",
      "publicSubtitle": "あなたのプリセットは<custom-bold>公開</custom-bold>されています。他の人がlmstudio.aiでダウンロードやフォークができます",
      "privateUsageReached": "プライベートプリセット数の上限に達しました。",
      "continueInBrowser": "ブラウザで続行",
      "confirmShareButton": "公開",
      "error": "プリセットの公開に失敗しました",
      "createFreeAccount": "Hubで無料アカウントを作成してプリセットを公開しましょう"
    },
    "update": {
      "title": "変更をHubにプッシュ",
      "title/success": "プリセットの更新が完了しました",
      "subtitle": "<custom-preset-name />に変更を加えてHubにプッシュします",
      "descriptionLabel": "説明",
      "descriptionPlaceholder": "説明を入力...",
      "loading": "プッシュ中...",
      "cancel": "キャンセル",
      "createFreeAccount": "Hubで無料アカウントを作成してプリセットを公開しましょう",
      "error": "更新のプッシュに失敗しました",
      "confirmUpdateButton": "プッシュ"
    },
    "resolve": {
      "title": "競合を解決...",
      "tooltip": "Hubバージョンとの差分を解決するためのモーダルを開きます"
    },
    "loginToManage": {
      "title": "管理するにはログイン..."
    },
    "downloadFromHub": {
      "title": "ダウンロード",
      "downloading": "ダウンロード中...",
      "success": "ダウンロード完了！",
      "error": "ダウンロードに失敗しました"
    },
    "push": {
      "title": "変更をプッシュ",
      "pushing": "プッシュ中...",
      "success": "プッシュ完了",
      "tooltip": "ローカルの変更をHubでホストされているリモートバージョンにプッシュします",
      "error": "プッシュに失敗しました"
    },
    "saveAsNewModal": {
      "title": "おっと！Hubでプリセットが見つかりませんでした",
      "confirmSaveAsNewDescription": "このプリセットを新しいプリセットとして公開しますか？",
      "confirmButton": "新規として公開"
    },
    "pull": {
      "title": "最新を取得",
      "error": "取得に失敗しました",
      "contextMenuErrorMessage": "取得に失敗しました",
      "success": "取得完了",
      "pulling": "取得中...",
      "upToDate": "最新です！",
      "unsavedChangesModal": {
        "title": "未保存の変更があります。",
        "bodyContent": "リモートからの取得により、未保存の変更が上書きされます。続行しますか？",
        "confirmButton": "未保存の変更を上書き"
      }
    },
    "import": {
      "title": "ファイルからプリセットをインポート",
      "dragPrompt": "プリセットファイル（.tar.gz または preset.json）をドラッグ＆ドロップするか、<custom-link>コンピューターから選択</custom-link>してください",
      "remove": "削除",
      "cancel": "キャンセル",
      "importPreset_zero": "プリセットをインポート",
      "importPreset_one": "プリセットをインポート",
      "importPreset_other": "{{count}}個のプリセットをインポート",
      "selectDialog": {
        "title": "プリセットファイルを選択（preset.json または .tar.gz）",
        "button": "インポート"
      },
      "error": "プリセットのインポートに失敗しました",
      "resultsModal": {
        "titleSuccessSection_one": "1個のプリセットを正常にインポートしました",
        "titleSuccessSection_other": "{{count}}個のプリセットを正常にインポートしました",
        "titleFailSection_zero": "",
        "titleFailSection_one": "（{{count}}個が失敗）",
        "titleFailSection_other": "（{{count}}個が失敗）",
        "titleAllFailed": "プリセットのインポートに失敗しました",
        "importMore": "さらにインポート",
        "close": "完了",
        "successBadge": "成功",
        "alreadyExistsBadge": "プリセットは既に存在します",
        "errorBadge": "エラー",
        "invalidFileBadge": "無効なファイル",
        "otherErrorBadge": "プリセットのインポートに失敗しました",
        "errorViewDetailsButton": "詳細を表示",
        "seeError": "エラーを確認",
        "noName": "プリセット名なし",
        "useInChat": "チャットで使用"
      },
      "importFromUrl": {
        "button": "URLからインポート...",
        "title": "URLからインポート",
        "back": "ファイルからインポート...",
        "action": "インポートしたいプリセットのLM Studio Hub URLを以下に貼り付けてください",
        "invalidUrl": "無効なURLです。正しいLM Studio Hub URLを貼り付けていることを確認してください。",
        "tip": "LM Studio Hubの{{buttonName}}ボタンでプリセットを直接インストールできます",
        "confirm": "インポート",
        "cancel": "キャンセル",
        "loading": "インポート中...",
        "error": "プリセットのダウンロードに失敗しました。"
      }
    },
    "download": {
      "title": "LM Studio Hubから<preset-name />を取得",
      "subtitle": "<custom-name />をプリセットに保存します。これにより、このプリセットをアプリで使用できるようになります",
      "button": "取得",
      "button/loading": "取得中...",
      "cancel": "キャンセル",
      "error": "プリセットのダウンロードに失敗しました。"
    },
    "inclusiveness": {
      "speculativeDecoding": "プリセットに含める"
    }
  },

  "flashAttentionWarning": "Flash Attentionは一部のモデルで問題を引き起こす可能性のある実験的機能です。問題が発生した場合は、無効にしてみてください。",
  "llamaKvCacheQuantizationWarning": "KVキャッシュ量子化は一部のモデルで問題を引き起こす可能性のある実験的機能です。Vキャッシュ量子化にはFlash Attentionを有効にする必要があります。問題が発生した場合は、デフォルトの「F16」にリセットしてください。",

  "seedUncheckedHint": "ランダムシード",
  "ropeFrequencyBaseUncheckedHint": "自動",
  "ropeFrequencyScaleUncheckedHint": "自動",

  "hardware": {
    "environmentVariables": "環境変数",
    "environmentVariables.info": "不明な場合は、これらをデフォルト値のままにしてください",
    "environmentVariables.reset": "デフォルトにリセット",

    "gpus.information": "マシンで検出されたグラフィックス処理装置（GPU）を設定します",
    "gpuSettings": {
      "editMaxCapacity": "最大容量を編集",
      "hideEditMaxCapacity": "最大容量の編集を非表示",
      "allOffWarning": "すべてのGPUがオフまたは無効になっています。モデルの読み込みを有効にするために、いくつかのGPU割り当てがあることを確認してください",
      "split": {
        "title": "戦略",
        "placeholder": "GPUメモリ割り当てを選択",
        "options": {
          "generalDescription": "モデルがGPUにどのように読み込まれるかを設定します",
          "evenly": {
            "title": "均等に分割",
            "description": "GPU間でメモリを均等に割り当てます"
          },
          "priorityOrder": {
            "title": "優先順位",
            "description": "ドラッグして優先順位を変更します。システムは最初にリストされたGPUにより多くを割り当てようとします"
          },
          "custom": {
            "title": "カスタム",
            "description": "メモリを割り当て",
            "maxAllocation": "最大割り当て"
          }
        }
      },
      "deviceId.info": "このデバイスの一意識別子",
      "changesOnlyAffectNewlyLoadedModels": "変更は新たに読み込まれるモデルにのみ影響します",
      "toggleGpu": "GPUを有効/無効にする"
    }
  },

  "load.gpuSplitConfig/title": "GPU分割設定",
  "envVars/title": "環境変数を設定",
  "envVars": {
    "select": {
      "placeholder": "環境変数を選択...",
      "noOptions": "利用可能な項目がありません",
      "filter": {
        "placeholder": "検索結果をフィルタリング",
        "resultsFound_zero": "結果が見つかりません",
        "resultsFound_one": "1件の結果が見つかりました",
        "resultsFound_other": "{{count}}件の結果が見つかりました"
      }
    },
    "inputValue": {
      "placeholder": "値を入力してください"
    },
    "values": {
      "title": "現在の値"
    }
  }
}
