{
  "noInstanceSelected": "モデルインスタンスが選択されていません",
  "resetToDefault": "リセット",
  "showAdvancedSettings": "詳細設定を表示",
  "showAll": "全て",
  "basicSettings": "基本設定",
  "configSubtitle": "プリセットの読み込みや保存、モデルパラメータの上書き設定を試してみましょう",
  "inferenceParameters/title": "予測パラメータ",
  "inferenceParameters/info": "予測に影響を与えるパラメータを試すことができます。",
  "generalParameters/title": "一般設定",
  "samplingParameters/title": "サンプリング設定",
  "basicTab": "基本",
  "advancedTab": "詳細",
  "advancedTab/title": "🧪 詳細設定",
  "advancedTab/expandAll": "全て展開",
  "advancedTab/overridesTitle": "設定の上書き",
  "advancedTab/noConfigsText": "未保存の変更はありません - 上記の値を編集して上書き設定をここに表示します。",
  "loadInstanceFirst": "モデルを読み込んで設定可能なパラメータを表示してください",
  "noListedConfigs": "設定可能なパラメータがありません",
  "generationParameters/info": "テキスト生成に影響を与える基本パラメータを試してみましょう。",
  "loadParameters/title": "読み込みパラメータ",
  "loadParameters/description": "モデルがメモリに読み込まれる際の設定を制御します。",
  "loadParameters/reload": "変更を適用するには再読み込み",
  "discardChanges": "変更を破棄",
  "llm.prediction.systemPrompt/title": "システムプロンプト",
  "llm.prediction.systemPrompt/description": "モデルに対して背景となる指示やルール、制約、全体的な要件を提供するためのフィールドです。このフィールドは「システムプロンプト」とも呼ばれます。",
  "llm.prediction.systemPrompt/subTitle": "AIへのガイドライン",
  "llm.prediction.temperature/title": "temperature",
  "llm.prediction.temperature/subTitle": "ランダム性をどれくらい加えるか。0にすると毎回同じ結果になり、高い値にすると創造性と多様性が増加します。",
  "llm.prediction.temperature/info": "llama.cpp のドキュメントによると、デフォルト値は <{{dynamicValue}}> で、ランダム性と決定論のバランスを取ります。極端な場合、temperatureが0だと常に最も可能性の高いトークンが選ばれ、毎回同じ結果になります。",
  "llm.prediction.llama.sampling/title": "サンプリング",
  "llm.prediction.topKSampling/title": "Top-Kサンプリング",
  "llm.prediction.topKSampling/subTitle": "次のトークンを最も確率の高いK個のトークンから選択します。temperatureと似たように機能します。",
  "llm.prediction.topKSampling/info": "llama.cppのドキュメントによると、Top-Kサンプリングは、モデルが予測した最も可能性の高いK個のトークンの中から次のトークンを選択するテキスト生成法です。これにより、低確率や意味不明なトークンが生成されるリスクが減りますが、多様性も制限される可能性があります。Kの値が大きい（例:100）と、多様なテキストが生成され、小さい（例:10）と、より保守的なテキストが生成されます。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.llama.cpuThreads/title": "CPUスレッド数",
  "llm.prediction.llama.cpuThreads/subTitle": "推論時に使用するCPUスレッド数",
  "llm.prediction.llama.cpuThreads/info": "計算中に使用するスレッド数です。スレッド数を増やすと必ずしも性能が向上するわけではありません。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.maxPredictedTokens/title": "レスポンスの長さを制限",
  "llm.prediction.maxPredictedTokens/subTitle": "AIのレスポンスの長さをオプションで制限できます。",
  "llm.prediction.maxPredictedTokens/info": "チャットボットのレスポンスの最大長を制御します。オンにするとレスポンスの最大長を設定できます。オフにするとチャットボットが停止するタイミングを決定します。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大レスポンス長（トークン数）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 語",
  "llm.prediction.repeatPenalty/title": "繰り返しペナルティ",
  "llm.prediction.repeatPenalty/subTitle": "同じトークンを繰り返すことをどの程度抑制するか",
  "llm.prediction.repeatPenalty/info": "llama.cppのドキュメントによると、この設定はモデルが繰り返しや単調なテキストを生成するのを防ぎます。値が高い（例:1.5）ほど、繰り返しに対するペナルティが強くなり、値が低い（例:0.9）ほど寛容になります。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.minPSampling/title": "最小Pサンプリング",
  "llm.prediction.minPSampling/subTitle": "トークンが出力されるための最小ベース確率",
  "llm.prediction.minPSampling/info": "llama.cppのドキュメントによると、次のトークンを選択するための最小確率は、最も可能性の高いトークンの確率に基づきます。値は[0, 1]の範囲内で設定できます。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.topPSampling/title": "Top-Pサンプリング",
  "llm.prediction.topPSampling/subTitle": "次のトークンの可能性のある最小累積確率。temperatureと似たように機能します。",
  "llm.prediction.topPSampling/info": "llama.cppのドキュメントによると、Top-Pサンプリングは、可能性のあるトークンの累積確率がP以上になるようにトークンを選択します。この手法は、多様性と品質のバランスをとるため、トークンの確率とトークン数の両方を考慮します。Top-Pの値が高い（例:0.95）と多様なテキストが生成され、値が低い（例:0.5）とより保守的なテキストが生成されます。値は (0, 1] の範囲内で設定できます。デフォルト値は <{{dynamicValue}}> です。",
  "llm.prediction.stopStrings/title": "停止文字列",
  "llm.prediction.stopStrings/subTitle": "モデルがトークンの生成を停止するべき文字列",
  "llm.prediction.stopStrings/info": "特定の文字列が検出された場合に、モデルがトークンの生成を停止します。",
  "llm.prediction.stopStrings/placeholder": "文字列を入力し、⏎キーを押してください",
  "llm.prediction.contextOverflowPolicy/title": "会話のオーバーフロー",
  "llm.prediction.contextOverflowPolicy/subTitle": "会話がモデルの処理可能なサイズを超えた場合の動作",
  "llm.prediction.contextOverflowPolicy/info": "会話がモデルのメモリ容量（コンテキスト）を超えた場合にどうするかを設定します。",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency ペナルティ",
  "llm.prediction.llama.presencePenalty/title": "Presence ペナルティ",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Freeサンプリング",
  "llm.prediction.llama.locallyTypicalSampling/title": "ローカルTypicalサンプリング",
  "llm.prediction.onnx.topKSampling/title": "Top-Kサンプリング",
  "llm.prediction.onnx.topKSampling/subTitle": "次のトークンを最も確率の高いK個のトークンに制限します。temperatureと同様の機能を持ちます。",
  "llm.prediction.onnx.topKSampling/info": "ONNXのドキュメントによると、最も確率の高いボキャブラリトークンの上位K個をTop-Kフィルタリングのために保持します。デフォルトではこのフィルタはオフです。",
  "llm.prediction.onnx.repeatPenalty/title": "繰り返しペナルティ",
  "llm.prediction.onnx.repeatPenalty/subTitle": "同じトークンを繰り返すことをどの程度抑制するか",
  "llm.prediction.onnx.repeatPenalty/info": "モデルが繰り返しを行わないようにするための設定です。値が高いほど抑制されます。",
  "llm.prediction.onnx.topPSampling/title": "Top-Pサンプリング",
  "llm.prediction.onnx.topPSampling/subTitle": "次のトークンの可能性のある最小累積確率。temperatureと似たように機能します。",
  "llm.prediction.onnx.topPSampling/info": "ONNXのドキュメントによると、確率がTop-Pまたはそれ以上になる最も可能性の高いトークンだけが保持されます。デフォルトではこのフィルタはオフです。",
  "llm.prediction.seed/title": "シード値",
  "llm.prediction.structured/title": "構造化出力",
  "llm.prediction.structured/info": "構造化された出力を生成します。",
  "llm.prediction.promptTemplate/title": "プロンプトテンプレート",
  "llm.prediction.promptTemplate/subTitle": "チャットでメッセージがモデルに送信されるフォーマットです。これを変更すると予期しない動作が発生する可能性があります。変更前に必ず確認してください。",

  "llm.load.contextLength/title": "コンテキスト長",
  "llm.load.contextLength/subTitle": "1つのプロンプトでモデルが処理できるトークンの最大数。コンテキストのオーバーフローオプションは「予測パラメータ」セクションにあります。",
  "llm.load.contextLength/info": "モデルが一度に処理できるトークンの最大数を指定します。これにより、処理中に保持されるコンテキスト量が影響を受けます。",
  "llm.load.contextLength/warning": "コンテキスト長の値を高く設定すると、メモリ使用量が大幅に増加する可能性があります。",
  "llm.load.seed/title": "シード値",
  "llm.load.seed/subTitle": "テキスト生成に使用される乱数生成器のシード値。-1でランダムになります。",
  "llm.load.seed/info": "乱数シード: 乱数生成のシード値を設定して結果を再現可能にします。",

  "llm.load.llama.evalBatchSize/title": "評価バッチサイズ",
  "llm.load.llama.evalBatchSize/subTitle": "一度に処理する入力トークンの数。これを増やすとパフォーマンスが向上しますが、メモリ使用量も増加します。",
  "llm.load.llama.evalBatchSize/info": "評価時に一度に処理される例の数を設定し、速度とメモリ使用量に影響を与えます。",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE基本周波数",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Rotary Positional Embeddings（RoPE）の基本周波数をカスタマイズします。これを増やすと、長いコンテキストでのパフォーマンスが向上する場合があります。",
  "llm.load.llama.ropeFrequencyBase/info": "[高度] 回転位置エンコーディングの基本周波数を調整し、位置情報の埋め込み方法に影響を与えます。",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE周波数スケール",
  "llm.load.llama.ropeFrequencyScale/subTitle": "RoPEを使用して、コンテキスト長をこの倍率でスケールします。",
  "llm.load.llama.ropeFrequencyScale/info": "[高度] 回転位置エンコーディングの周波数スケーリングを調整し、位置エンコーディングの粒度を制御します。",
  "llm.load.llama.acceleration.offloadRatio/title": "GPUオフロード",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "GPUアクセラレーションのためにGPUで計算するモデル層の数",
  "llm.load.llama.acceleration.offloadRatio/info": "GPUにオフロードする層の数を設定します。",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "一部のモデルでメモリ使用量と生成時間を削減します。",
  "llm.load.llama.flashAttention/info": "注意メカニズムを高速化し、より効率的に処理します。",
  "llm.load.numExperts/title": "エキスパートの数",
  "llm.load.numExperts/subTitle": "モデルで使用するエキスパートの数",
  "llm.load.numExperts/info": "モデルで使用するエキスパートの数を設定します。",
  "llm.load.llama.keepModelInMemory/title": "モデルをメモリに保持",
  "llm.load.llama.keepModelInMemory/subTitle": "モデルがGPUにオフロードされていても、システムメモリにモデルを保持します。これによりパフォーマンスが向上しますが、システムRAMを多く消費します。",
  "llm.load.llama.keepModelInMemory/info": "モデルがディスクにスワップアウトされるのを防ぎ、より高速なアクセスを可能にしますが、RAMの使用量が増加します。",
  "llm.load.llama.useFp16ForKVCache/title": "KVキャッシュにFP16を使用",
  "llm.load.llama.useFp16ForKVCache/info": "キャッシュを半精度（FP16）で保存することで、メモリ使用量を削減します。",
  "llm.load.llama.tryMmap/title": "mmap()を試す",
  "llm.load.llama.tryMmap/subTitle": "モデルのロード時間を短縮します。モデルがシステムRAMを超える場合は、これを無効にするとパフォーマンスが向上することがあります。",
  "llm.load.llama.tryMmap/info": "モデルファイルをディスクからメモリに直接読み込みます。",

  "embedding.load.contextLength/title": "コンテキスト長",
  "embedding.load.contextLength/subTitle": "1つのプロンプトでモデルが処理できるトークンの最大数。「予測パラメータ」セクションでコンテキストのオーバーフローオプションを参照してください。",
  "embedding.load.contextLength/info": "モデルが一度に処理できるトークンの最大数を指定し、処理中に保持されるコンテキストの量に影響します。",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE基本周波数",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Rotary Positional Embeddings（RoPE）の基本周波数をカスタマイズします。これを増やすと、長いコンテキストでのパフォーマンスが向上する場合があります。",
  "embedding.load.llama.ropeFrequencyBase/info": "[高度] RoPE（Rotary Positional Embeddings）の基本周波数を調整し、位置情報のEmbeddingに影響を与えます。",
  "embedding.load.llama.evalBatchSize/title": "評価バッチサイズ",
  "embedding.load.llama.evalBatchSize/subTitle": "一度に処理する入力トークンの数。これを増やすとパフォーマンスが向上しますが、メモリ使用量も増加します。",
  "embedding.load.llama.evalBatchSize/info": "評価時に一度に処理されるトークンの数を設定し、速度とメモリ使用量に影響を与えます。",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE周波数スケール",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "RoPEを使用して、コンテキスト長をこの倍率でスケールします。",
  "embedding.load.llama.ropeFrequencyScale/info": "[高度] RoPE（Rotary Positional Embeddings）の周波数スケーリングを調整して、位置情報のエンコーディングを細かく制御します。",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPUオフロード",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "GPUアクセラレーションのためにGPUで処理するモデル層の数",
  "embedding.load.llama.acceleration.offloadRatio/info": "GPUにオフロードする層の数を設定します。",
  "embedding.load.llama.keepModelInMemory/title": "モデルをメモリに保持",
  "embedding.load.llama.keepModelInMemory/subTitle": "モデルがGPUにオフロードされていても、システムメモリにモデルを保持します。これによりパフォーマンスが向上しますが、システムRAMを多く消費します。",
  "embedding.load.llama.keepModelInMemory/info": "モデルがストレージにスワップアウトされるのを防ぎ、より高速なアクセスを可能にしますが、RAMの使用量が増加します。",
  "embedding.load.llama.tryMmap/title": "mmap()を試す",
  "embedding.load.llama.tryMmap/subTitle": "モデルの読み込み時間を短縮します。モデルのサイズがシステムRAMの容量を超える場合は、これを無効にするとパフォーマンスが向上することがあります。",
  "embedding.load.llama.tryMmap/info": "モデルファイルをディスクからメモリに直接読み込みます。",
  "embedding.load.seed/title": "シード値",
  "embedding.load.seed/subTitle": "テキスト生成に使用される乱数生成器のシード値。-1でランダムになります。",

  "embedding.load.seed/info": "ランダムシード値: 乱数生成のシード値を設定して、再現可能な結果を得られるようにします。",

  "presetTooltip": {
    "included/title": "適用される項目",
    "included/description": "次のフィールドが適用されます",
    "included/empty": "このコンテキストで適用されるフィールドはありません。",
    "separateLoad/title": "ロード時の設定",
    "separateLoad/description.1": "プリセットには以下のロード時設定も含まれています。ロード時の設定はモデル全体に適用され、効果を発揮するにはモデルの再読み込みが必要です。ホールド",
    "separateLoad/description.2": "して適用するには",
    "separateLoad/description.3": "を押してください。",
    "excluded/title": "適用されない項目",
    "excluded/description": "次のフィールドはプリセットに含まれていますが、現在のコンテキストでは適用されません。",
    "legacy/title": "レガシープリセット",
    "legacy/description": "このプリセットはレガシープリセットです。次のフィールドは現在自動的に処理されるか、もはや適用されません。"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空>"
    },
    "checkboxNumeric": {
      "off": "オフ"
    },
    "stringArray": {
      "empty": "<空>"
    },
    "llmPromptTemplate": {
      "type": "タイプ",
      "types.jinja/label": "Jinja",
      "jinja.bosToken/label": "BOSトークン",
      "jinja.eosToken/label": "EOSトークン",
      "jinja.template/label": "テンプレート",
      "jinja.error": "Jinjaテンプレートの解析に失敗しました: {{error}}",
      "types.manual/label": "マニュアル",
      "manual.subfield.beforeSystem/label": "システムの前",
      "manual.subfield.beforeSystem/placeholder": "システムのプレフィックスを入力...",
      "manual.subfield.afterSystem/label": "システムの後",
      "manual.subfield.afterSystem/placeholder": "システムのサフィックスを入力...",
      "manual.subfield.beforeUser/label": "ユーザーの前",
      "manual.subfield.beforeUser/placeholder": "ユーザーのプレフィックスを入力...",
      "manual.subfield.afterUser/label": "ユーザーの後",
      "manual.subfield.afterUser/placeholder": "ユーザーのサフィックスを入力...",
      "manual.subfield.beforeAssistant/label": "アシスタントの前",
      "manual.subfield.beforeAssistant/placeholder": "アシスタントのプレフィックスを入力...",
      "manual.subfield.afterAssistant/label": "アシスタントの後",
      "manual.subfield.afterAssistant/placeholder": "アシスタントのサフィックスを入力...",
      "stopStrings/label": "追加の停止文字列",
      "stopStrings/subTitle": "テンプレート固有の停止文字列で、ユーザー指定の停止文字列に加えて使用されます。"
    },
    "contextLength": {
      "maxValueTooltip": "これはモデルが処理できる最大のトークン数です。この値に設定するにはクリックしてください。",
      "maxValueTextStart": "モデルは最大で",
      "maxValueTextEnd": "トークンをサポートします",
      "tooltipHint": "モデルが一定数のトークンをサポートしていても、マシンのリソースが処理負荷に対して低い場合はパフォーマンスが低下する可能性があります。値を増やす際は注意してください。"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "上限で停止",
      "stopAtLimitSub": "モデルのメモリがいっぱいになったら生成を停止",
      "truncateMiddle": "中間を切り捨て",
      "truncateMiddleSub": "会話の中間部分を削除して新しいメッセージにスペースを作ります。モデルは会話の最初の部分を覚えています。",
      "rollingWindow": "ローリングウィンドウ",
      "rollingWindowSub": "モデルは常に最新のメッセージを数件取得しますが、会話の最初の部分を忘れる可能性があります。"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大",
      "off": "オフ"
    }
  },
  "conflictResolution": {
    "title": "競合",
    "description": "保存されていない設定と適用しようとしているプリセットの間に競合があります。どのように解決しますか？",
    "userValues": "あなたの変更",
    "presetValues": "受信中の変更",
    "confirm": "確認",
    "cancel": "キャンセル"
  },
  "empty": "<空>",

  "flashAttentionWarning": "Flash Attention は実験的な機能であり、特定のモデルで問題を引き起こす可能性があります。問題が発生した場合は無効にしてみてください。"
}
