{
  "noInstanceSelected": "Keine Modellinstanz ausgew√§hlt",
  "resetToDefault": "Zur√ºcksetzen",
  "showAdvancedSettings": "Erweiterte Einstellungen anzeigen",
  "showAll": "Alle",
  "basicSettings": "Grundlegend",
  "configSubtitle": "Laden oder speichern Sie Voreinstellungen und experimentieren Sie mit √úberschreibungen von Modellparametern",
  "inferenceParameters/title": "Vorhersageparameter",
  "inferenceParameters/info": "Experimentieren Sie mit Parametern, die die Vorhersage beeinflussen.",
  "generalParameters/title": "Allgemein",
  "samplingParameters/title": "Sampling",
  "basicTab": "Grundlegend",
  "advancedTab": "Erweitert",
  "advancedTab/title": "üß™ Erweiterte Konfiguration",
  "advancedTab/expandAll": "Alle erweitern",
  "advancedTab/overridesTitle": "Konfigurations√ºberschreibungen",
  "advancedTab/noConfigsText": "Sie haben keine ungespeicherten √Ñnderungen - bearbeiten Sie die Werte oben, um √úberschreibungen hier anzuzeigen.",
  "loadInstanceFirst": "Laden Sie ein Modell, um konfigurierbare Parameter anzuzeigen",
  "noListedConfigs": "Keine konfigurierbaren Parameter",
  "generationParameters/info": "Experimentieren Sie mit grundlegenden Parametern, die die Textgenerierung beeinflussen.",
  "loadParameters/title": "Ladeparameter",
  "loadParameters/description": "Einstellungen zur Steuerung der Initialisierung und des Ladens des Modells in den Speicher.",
  "loadParameters/reload": "Neu laden, um √Ñnderungen anzuwenden",
  "loadParameters/reload/error": "Modell konnte nicht neu geladen werden",
  "discardChanges": "√Ñnderungen verwerfen",
  "loadModelToSeeOptions": "Laden Sie ein Modell, um Optionen anzuzeigen",
  "schematicsError.title": "Die Konfigurationsschemata enthalten Fehler in den folgenden Feldern:",
  "manifestSections": {
    "structuredOutput/title": "Strukturierte Ausgabe",
    "speculativeDecoding/title": "Spekulative Dekodierung",
    "sampling/title": "Sampling",
    "settings/title": "Einstellungen",
    "toolUse/title": "Toolverwendung",
    "promptTemplate/title": "Prompt-Vorlage",
    "customFields/title": "Benutzerdefinierte Felder"
  },

  "llm.prediction.systemPrompt/title": "System-Prompt",
  "llm.prediction.systemPrompt/description": "Verwenden Sie dieses Feld, um dem Modell Hintergrundanweisungen zu geben, z. B. eine Reihe von Regeln, Einschr√§nkungen oder allgemeinen Anforderungen.",
  "llm.prediction.systemPrompt/subTitle": "Richtlinien f√ºr die KI",
  "llm.prediction.systemPrompt/openEditor": "Editor",
  "llm.prediction.systemPrompt/closeEditor": "Editor schlie√üen",
  "llm.prediction.systemPrompt/openedEditor": "Im Editor ge√∂ffnet...",
  "llm.prediction.systemPrompt/edit": "System-Prompt bearbeiten...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "Instruktionen hinzuf√ºgen...",
  "llm.prediction.systemPrompt/addInstructions": "Instruktionen hinzuf√ºgen",
  "llm.prediction.temperature/title": "Temperatur",
  "llm.prediction.temperature/subTitle": "Wie viel Zuf√§lligkeit eingef√ºhrt werden soll. 0 ergibt jedes Mal das gleiche Ergebnis, w√§hrend h√∂here Werte Kreativit√§t und Varianz erh√∂hen",
  "llm.prediction.temperature/info": "Aus den llama.cpp-Hilfedokumenten: \"Der Standardwert ist <{{dynamicValue}}>, was ein Gleichgewicht zwischen Zuf√§lligkeit und Determinismus bietet. Im Extremfall w√§hlt eine Temperatur von 0 immer das wahrscheinlichste n√§chste Token aus, was zu identischen Ausgaben bei jedem Durchlauf f√ºhrt\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top-K-Sampling",
  "llm.prediction.topKSampling/subTitle": "Beschr√§nkt das n√§chste Token auf eines der k wahrscheinlichsten Tokens. Wirkt √§hnlich wie die Temperatur",
  "llm.prediction.topKSampling/info": "Aus den llama.cpp-Hilfedokumenten:\n\nTop-k-Sampling ist eine Textgenerierungsmethode, die das n√§chste Token nur aus den k wahrscheinlichsten Tokens ausw√§hlt, die vom Modell vorhergesagt werden.\n\nEs hilft, das Risiko der Generierung von Tokens mit geringer Wahrscheinlichkeit oder unsinnigen Tokens zu reduzieren, kann aber auch die Vielfalt der Ausgabe einschr√§nken.\n\nEin h√∂herer Wert f√ºr Top-k (z. B. 100) ber√ºcksichtigt mehr Tokens und f√ºhrt zu vielf√§ltigerem Text, w√§hrend ein niedrigerer Wert (z. B. 10) sich auf die wahrscheinlichsten Tokens konzentriert und konservativeren Text generiert.\n\n‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU-Threads",
  "llm.prediction.llama.cpuThreads/subTitle": "Anzahl der CPU-Threads, die w√§hrend der Inferenz verwendet werden sollen",
  "llm.prediction.llama.cpuThreads/info": "Die Anzahl der Threads, die w√§hrend der Berechnung verwendet werden sollen. Eine Erh√∂hung der Anzahl der Threads korreliert nicht immer mit einer besseren Leistung. Der Standardwert ist <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Antwortl√§nge begrenzen",
  "llm.prediction.maxPredictedTokens/subTitle": "Optional die L√§nge der KI-Antwort begrenzen",
  "llm.prediction.maxPredictedTokens/info": "Steuern Sie die maximale L√§nge der Chatbot-Antwort. Aktivieren Sie diese Option, um eine Grenze f√ºr die maximale L√§nge einer Antwort festzulegen, oder deaktivieren Sie sie, damit der Chatbot entscheiden kann, wann er aufh√∂ren soll.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximale Antwortl√§nge (Tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Ungef√§hr {{maxWords}} W√∂rter",
  "llm.prediction.repeatPenalty/title": "Wiederholungsstrafe",
  "llm.prediction.repeatPenalty/subTitle": "Wie stark die Wiederholung desselben Tokens unterbunden werden soll",
  "llm.prediction.repeatPenalty/info": "Aus den llama.cpp-Hilfedokumenten: \"Hilft zu verhindern, dass das Modell repetitiven oder monotonen Text generiert.\n\nEin h√∂herer Wert (z. B. 1,5) bestraft Wiederholungen st√§rker, w√§hrend ein niedrigerer Wert (z. B. 0,9) nachsichtiger ist.\" ‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min-P-Sampling",
  "llm.prediction.minPSampling/subTitle": "Minimale Basiswahrscheinlichkeit f√ºr die Auswahl eines Tokens f√ºr die Ausgabe",
  "llm.prediction.minPSampling/info": "Aus den llama.cpp-Hilfedokumenten:\n\nDie minimale Wahrscheinlichkeit, mit der ein Token ber√ºcksichtigt wird, relativ zur Wahrscheinlichkeit des wahrscheinlichsten Tokens. Muss im Bereich [0, 1] liegen.\n\n‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top-P-Sampling",
  "llm.prediction.topPSampling/subTitle": "Minimale kumulative Wahrscheinlichkeit f√ºr die m√∂glichen n√§chsten Tokens. Wirkt √§hnlich wie die Temperatur",
  "llm.prediction.topPSampling/info": "Aus den llama.cpp-Hilfedokumenten:\n\nTop-p-Sampling, auch bekannt als Nucleus-Sampling, ist eine weitere Textgenerierungsmethode, die das n√§chste Token aus einer Teilmenge von Tokens ausw√§hlt, die zusammen eine kumulative Wahrscheinlichkeit von mindestens p haben.\n\nDiese Methode bietet ein Gleichgewicht zwischen Vielfalt und Qualit√§t, indem sowohl die Wahrscheinlichkeiten von Tokens als auch die Anzahl der zu samplenden Tokens ber√ºcksichtigt werden.\n\nEin h√∂herer Wert f√ºr Top-p (z. B. 0,95) f√ºhrt zu vielf√§ltigerem Text, w√§hrend ein niedrigerer Wert (z. B. 0,5) fokussierteren und konservativeren Text generiert. Muss im Bereich (0, 1] liegen.\n\n‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stopp-Strings",
  "llm.prediction.stopStrings/subTitle": "Strings, die das Modell daran hindern sollen, weitere Tokens zu generieren",
  "llm.prediction.stopStrings/info": "Spezifische Strings, die das Modell daran hindern, weitere Tokens zu generieren, wenn sie auftreten",
  "llm.prediction.stopStrings/placeholder": "Geben Sie einen String ein und dr√ºcken Sie ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Kontext√ºberlauf",
  "llm.prediction.contextOverflowPolicy/subTitle": "Wie sich das Modell verhalten soll, wenn die Konversation zu gro√ü wird, um sie zu verarbeiten",
  "llm.prediction.contextOverflowPolicy/info": "Entscheiden Sie, was zu tun ist, wenn die Konversation die Gr√∂√üe des Arbeitsspeichers des Modells ('Kontext') √ºberschreitet",
  "llm.prediction.llama.frequencyPenalty/title": "Frequenzstrafe",
  "llm.prediction.llama.presencePenalty/title": "Pr√§senzstrafe",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free-Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Lokal typisches Sampling",
  "llm.prediction.llama.xtcProbability/title": "XTC-Sampling-Wahrscheinlichkeit",
  "llm.prediction.llama.xtcProbability/subTitle": "Der XTC-Sampler (Exclude Top Choices) wird nur mit dieser Wahrscheinlichkeit pro generiertem Token aktiviert. XTC-Sampling kann die Kreativit√§t steigern und Klischees reduzieren",
  "llm.prediction.llama.xtcProbability/info": "XTC-Sampling (Exclude Top Choices) wird nur mit dieser Wahrscheinlichkeit pro generiertem Token aktiviert. XTC-Sampling steigert normalerweise die Kreativit√§t und reduziert Klischees",
  "llm.prediction.llama.xtcThreshold/title": "XTC-Sampling-Schwellenwert",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC-Schwellenwert (Exclude Top Choices). Mit einer Wahrscheinlichkeit von `xtc-probability` werden Tokens mit Wahrscheinlichkeiten zwischen `xtc-threshold` und 0,5 gesucht und alle solchen Tokens au√üer dem unwahrscheinlichsten entfernt",
  "llm.prediction.llama.xtcThreshold/info": "XTC-Schwellenwert (Exclude Top Choices). Mit einer Wahrscheinlichkeit von `xtc-probability` werden Tokens mit Wahrscheinlichkeiten zwischen `xtc-threshold` und 0,5 gesucht und alle solchen Tokens au√üer dem unwahrscheinlichsten entfernt",
  "llm.prediction.mlx.topKSampling/title": "Top-K-Sampling",
  "llm.prediction.mlx.topKSampling/subTitle": "Beschr√§nkt das n√§chste Token auf eines der k wahrscheinlichsten Tokens. Wirkt √§hnlich wie die Temperatur",
  "llm.prediction.mlx.topKSampling/info": "Beschr√§nkt das n√§chste Token auf eines der k wahrscheinlichsten Tokens. Wirkt √§hnlich wie die Temperatur",
  "llm.prediction.onnx.topKSampling/title": "Top-K-Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Beschr√§nkt das n√§chste Token auf eines der k wahrscheinlichsten Tokens. Wirkt √§hnlich wie die Temperatur",
  "llm.prediction.onnx.topKSampling/info": "Aus der ONNX-Dokumentation:\n\nAnzahl der Vokabel-Tokens mit der h√∂chsten Wahrscheinlichkeit, die f√ºr die Top-k-Filterung beibehalten werden sollen\n\n‚Ä¢ Dieser Filter ist standardm√§√üig deaktiviert",
  "llm.prediction.onnx.repeatPenalty/title": "Wiederholungsstrafe",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Wie stark die Wiederholung desselben Tokens unterbunden werden soll",
  "llm.prediction.onnx.repeatPenalty/info": "Ein h√∂herer Wert h√§lt das Modell davon ab, sich selbst zu wiederholen",
  "llm.prediction.onnx.topPSampling/title": "Top-P-Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimale kumulative Wahrscheinlichkeit f√ºr die m√∂glichen n√§chsten Tokens. Wirkt √§hnlich wie die Temperatur",
  "llm.prediction.onnx.topPSampling/info": "Aus der ONNX-Dokumentation:\n\nNur die wahrscheinlichsten Tokens mit Wahrscheinlichkeiten, die sich zu TopP oder h√∂her summieren, werden f√ºr die Generierung beibehalten\n\n‚Ä¢ Dieser Filter ist standardm√§√üig deaktiviert",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Strukturierte Ausgabe",
  "llm.prediction.structured/info": "Strukturierte Ausgabe",
  "llm.prediction.structured/description": "Erweitert: Sie k√∂nnen ein [JSON-Schema](https://json-schema.org/learn/miscellaneous-examples) bereitstellen, um ein bestimmtes Ausgabeformat vom Modell zu erzwingen. Lesen Sie die [Dokumentation](https://lmstudio.ai/docs/advanced/structured-output), um mehr zu erfahren",
  "llm.prediction.tools/title": "Toolverwendung",
  "llm.prediction.tools/description": "Erweitert: Sie k√∂nnen eine JSON-konforme Liste von Tools bereitstellen, f√ºr die das Modell Aufrufe anfordern kann. Lesen Sie die [Dokumentation](https://lmstudio.ai/docs/advanced/tool-use), um mehr zu erfahren",
  "llm.prediction.tools/serverPageDescriptionAddon": "√úbergeben Sie dies √ºber den Anforderungstext als `tools`, wenn Sie die Server-API verwenden",
  "llm.prediction.promptTemplate/title": "Prompt-Vorlage",
  "llm.prediction.promptTemplate/subTitle": "Das Format, in dem Nachrichten im Chat an das Modell gesendet werden. Eine √Ñnderung kann zu unerwartetem Verhalten f√ºhren - stellen Sie sicher, dass Sie wissen, was Sie tun!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Zu generierende Entwurfstoken",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "Die Anzahl der Token, die mit dem Entwurfsmodell pro Hauptmodelltoken generiert werden sollen. Finden Sie den Sweet Spot zwischen Rechenaufwand und Belohnung",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Entwurfswahrscheinlichkeits-Cutoff",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Entwerfen Sie weiter, bis die Wahrscheinlichkeit eines Tokens unter diesen Schwellenwert f√§llt. H√∂here Werte bedeuten im Allgemeinen ein geringeres Risiko, eine geringere Belohnung",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min. Entwurfsgr√∂√üe",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Entw√ºrfe, die kleiner als dieser Wert sind, werden vom Hauptmodell ignoriert. H√∂here Werte bedeuten im Allgemeinen ein geringeres Risiko, eine geringere Belohnung",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max. Entwurfsgr√∂√üe",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Maximale Anzahl von Token, die in einem Entwurf zul√§ssig sind. Obergrenze, wenn alle Token-Wahrscheinlichkeiten > dem Cutoff liegen. Niedrigere Werte bedeuten im Allgemeinen ein geringeres Risiko, eine geringere Belohnung",
  "llm.prediction.speculativeDecoding.draftModel/title": "Entwurfsmodell",
  "llm.prediction.reasoning.parsing/title": "Parsing von Begr√ºndungsabschnitten",
  "llm.prediction.reasoning.parsing/subTitle": "Wie Begr√ºndungsabschnitte in der Ausgabe des Modells geparst werden sollen",

  "llm.load.mainGpu/title": "Haupt-GPU",
  "llm.load.mainGpu/subTitle": "Die GPU, die f√ºr die Modellberechnung priorisiert werden soll",
  "llm.load.mainGpu/placeholder": "Haupt-GPU ausw√§hlen...",
  "llm.load.splitStrategy/title": "Split Strategie",
  "llm.load.splitStrategy/subTitle": "Wie die Modellberechnung auf mehrere GPUs aufgeteilt werden soll",
  "llm.load.splitStrategy/placeholder": "Split Strategie ausw√§hlen...",
  "llm.load.offloadKVCacheToGpu/title": "KV Cache in GPU-Speicher verschieben",
  "llm.load.offloadKVCacheToGpu/subTitle": "Verschieben Sie den KV Cache in den GPU-Speicher. Verbessert die Leistung, ben√∂tigt aber mehr GPU-Speicher",
  "llm.load.numParallelSessions/title": "Max. gleichzeitige Vorhersagen",
  "llm.load.numParallelSessions/subTitle": "Maximale Anzahl an Vorhersagen, die gleichzeitig verarbeitet werden k√∂nnen. Die Geschwindigkeit jeder einzelnen Vorhersagen kann abnehmen, wenn mehrere gleichzeitig Vorhersagen gleichzeitig ausgef√ºhrt werden, jedoch wird die Gesamtdurchsatzrate erh√∂ht",
  "llm.load.useUnifiedKvCache/title": "Vereinigter KV Cache",
  "llm.load.useUnifiedKvCache/subTitle": "Steuert, ob gleichzeitige Vorhersagen einen gemeinsamen KV-Cache teilen, um Speicher zu sparen. Das Deaktivieren dieser Option stellt sicher, dass jede Vorhersage die volle Kontextl√§nge nutzen kann, kostet aber mehr Speicher",
  "load.gpuStrictVramCap/title": "Modell-Auslagerung auf dedizierten GPU-Speicher begrenzen",
  "load.gpuStrictVramCap/customSubTitleOff": "OFF: Erlaubt das Auslagern von Modellgewichten in den gemeinsamen Speicher, wenn der dedizierte GPU-Speicher voll ist",
  "load.gpuStrictVramCap/customSubTitleOn": "ON: Das System begrenzt das Auslagern von Modellgewichten auf dedizierten GPU-Speicher und RAM. Kontext kann weiterhin gemeinsamen Speicher verwenden",
  "load.gpuStrictVramCap/customGpuOffloadWarning": "Modell-Auslagerung auf dedizierten GPU-Speicher begrenzt. Die tats√§chliche Anzahl der ausgelagerten Schichten kann abweichen",
  "load.allGpusDisabledWarning": "Alle GPUs sind derzeit deaktiviert. Aktivieren Sie mindestens eine, um auszulagern",

  "llm.load.contextLength/title": "Kontextl√§nge",
  "llm.load.contextLength/subTitle": "Die maximale Anzahl von Token, die das Modell in einem Prompt ber√ºcksichtigen kann. Weitere M√∂glichkeiten zur Verwaltung finden Sie unter den Optionen f√ºr den Konversations√ºberlauf unter \"Inferenzparameter\"",
  "llm.load.contextLength/info": "Gibt die maximale Anzahl von Token an, die das Modell gleichzeitig ber√ºcksichtigen kann, was sich darauf auswirkt, wie viel Kontext es w√§hrend der Verarbeitung beibeh√§lt",
  "llm.load.contextLength/warning": "Das Festlegen eines hohen Werts f√ºr die Kontextl√§nge kann die Speichernutzung erheblich beeinflussen",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "Der Seed f√ºr den Zufallszahlengenerator, der bei der Textgenerierung verwendet wird. -1 ist zuf√§llig",
  "llm.load.seed/info": "Zufalls-Seed: Legt den Seed f√ºr die Zufallszahlengenerierung fest, um reproduzierbare Ergebnisse zu gew√§hrleisten",
  "llm.load.numCpuExpertLayersRatio/title": "Anzahl der Schichten, f√ºr die MoE-Gewichte auf CPU erzwungen werden",
  "llm.load.numCpuExpertLayersRatio/subTitle": "Anzahl der Schichten, f√ºr die die MoE-Gewichte auf CPU erzwungen werden. Spart VRAM und kann schneller sein als partielles GPU-Auslagern. Nicht empfohlen, wenn das Modell vollst√§ndig in den VRAM passt.",
  "llm.load.numCpuExpertLayersRatio/info": "Gibt die Anzahl der Schichten an, f√ºr die MoE-Gewichte auf CPU erzwungen werden. L√§sst Aufmerksamkeitsschichten auf GPU, spart VRAM und h√§lt die Inferenz ziemlich schnell.",
  
  "llm.load.llama.evalBatchSize/title": "Evaluierungs-Batchgr√∂√üe",
  "llm.load.llama.evalBatchSize/subTitle": "Anzahl der Eingabetoken, die gleichzeitig verarbeitet werden sollen. Eine Erh√∂hung erh√∂ht die Leistung auf Kosten der Speichernutzung",
  "llm.load.llama.evalBatchSize/info": "Legt die Anzahl der Beispiele fest, die w√§hrend der Evaluierung in einem Batch zusammen verarbeitet werden, was sich auf Geschwindigkeit und Speichernutzung auswirkt",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE-Frequenzbasis",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Benutzerdefinierte Basisfrequenz f√ºr rotierende Positions-Embeddings (RoPE). Eine Erh√∂hung kann eine bessere Leistung bei hohen Kontextl√§ngen erm√∂glichen",
  "llm.load.llama.ropeFrequencyBase/info": "[Erweitert] Passt die Basisfrequenz f√ºr die rotierende Positionskodierung an, was sich darauf auswirkt, wie Positionsinformationen eingebettet werden",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE-Frequenzskala",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Die Kontextl√§nge wird mit diesem Faktor skaliert, um den effektiven Kontext mithilfe von RoPE zu erweitern",
  "llm.load.llama.ropeFrequencyScale/info": "[Erweitert] √Ñndert die Skalierung der Frequenz f√ºr die rotierende Positionskodierung, um die Granularit√§t der Positionskodierung zu steuern",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU-Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Anzahl der diskreten Modellschichten, die zur GPU-Beschleunigung auf der GPU berechnet werden sollen",
  "llm.load.llama.acceleration.offloadRatio/info": "Legen Sie die Anzahl der Schichten fest, die auf die GPU ausgelagert werden sollen.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Verringert die Speichernutzung und die Generierungszeit bei einigen Modellen",
  "llm.load.llama.flashAttention/info": "Beschleunigt Aufmerksamkeitsmechanismen f√ºr eine schnellere und effizientere Verarbeitung",
  "llm.load.numExperts/title": "Anzahl der Experten",
  "llm.load.numExperts/subTitle": "Anzahl der im Modell zu verwendenden Experten",
  "llm.load.numExperts/info": "Die Anzahl der im Modell zu verwendenden Experten",
  "llm.load.llama.keepModelInMemory/title": "Modell im Speicher behalten",
  "llm.load.llama.keepModelInMemory/subTitle": "Reservieren Sie Systemspeicher f√ºr das Modell, auch wenn es auf die GPU ausgelagert ist. Verbessert die Leistung, erfordert aber mehr System-RAM",
  "llm.load.llama.keepModelInMemory/info": "Verhindert, dass das Modell auf die Festplatte ausgelagert wird, und gew√§hrleistet so einen schnelleren Zugriff auf Kosten einer h√∂heren RAM-Nutzung",
  "llm.load.llama.useFp16ForKVCache/title": "FP16 f√ºr KV-Cache verwenden",
  "llm.load.llama.useFp16ForKVCache/info": "Reduziert die Speichernutzung durch Speichern des Caches mit halber Genauigkeit (FP16)",
  "llm.load.llama.tryMmap/title": "mmap() versuchen",
  "llm.load.llama.tryMmap/subTitle": "Verbessert die Ladezeit f√ºr das Modell. Das Deaktivieren kann die Leistung verbessern, wenn das Modell gr√∂√üer als der verf√ºgbare System-RAM ist",
  "llm.load.llama.tryMmap/info": "Modelldateien direkt von der Festplatte in den Speicher laden",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU-Threadpoolgr√∂√üe",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Anzahl der CPU-Threads, die dem Threadpool zugewiesen werden sollen, der f√ºr die Modellberechnung verwendet wird",
  "llm.load.llama.cpuThreadPoolSize/info": "Die Anzahl der CPU-Threads, die dem Threadpool zugewiesen werden sollen, der f√ºr die Modellberechnung verwendet wird. Eine Erh√∂hung der Anzahl der Threads korreliert nicht immer mit einer besseren Leistung. Der Standardwert ist <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K-Cache-Quantisierungstyp",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Niedrigere Werte reduzieren die Speichernutzung, k√∂nnen aber die Qualit√§t beeintr√§chtigen. Der Effekt variiert erheblich zwischen den Modellen.",
  "llm.load.llama.vCacheQuantizationType/title": "V-Cache-Quantisierungstyp",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Niedrigere Werte reduzieren die Speichernutzung, k√∂nnen aber die Qualit√§t beeintr√§chtigen. Der Effekt variiert erheblich zwischen den Modellen.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Sie m√ºssen diesen Wert deaktivieren, wenn Flash Attention nicht aktiviert ist",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Kann nur aktiviert werden, wenn Flash Attention aktiviert ist",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Sie m√ºssen Flash Attention deaktivieren, wenn Sie F32 verwenden",
  "llm.load.mlx.kvCacheBits/title": "KV-Cache-Quantisierung",
  "llm.load.mlx.kvCacheBits/subTitle": "Anzahl der Bits, auf die der KV-Cache quantisiert werden soll",
  "llm.load.mlx.kvCacheBits/info": "Anzahl der Bits, auf die der KV-Cache quantisiert werden soll",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Die Einstellung f√ºr die Kontextl√§nge wird ignoriert, wenn die KV-Cache-Quantisierung verwendet wird",
  "llm.load.mlx.kvCacheGroupSize/title": "KV-Cache-Quantisierung: Gruppengr√∂√üe",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Gruppengr√∂√üe w√§hrend des Quantisierungsvorgangs f√ºr den KV-Cache. Eine h√∂here Gruppengr√∂√üe reduziert die Speichernutzung, kann aber die Qualit√§t beeintr√§chtigen",
  "llm.load.mlx.kvCacheGroupSize/info": "Anzahl der Bits, auf die der KV-Cache quantisiert werden soll",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV-Cache-Quantisierung: Beginnen Sie mit der Quantisierung, wenn der Kontext diese L√§nge √ºberschreitet",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Schwellenwert f√ºr die Kontextl√§nge, um mit der Quantisierung des KV-Caches zu beginnen",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Schwellenwert f√ºr die Kontextl√§nge, um mit der Quantisierung des KV-Caches zu beginnen",
  "llm.load.mlx.kvCacheQuantization/title": "KV-Cache-Quantisierung",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantisieren Sie den KV-Cache des Modells. Dies kann zu einer schnelleren Generierung und einem geringeren Speicherbedarf f√ºhren,\nauf Kosten der Qualit√§t der Modellausgabe.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV-Cache-Quantisierungsbits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Anzahl der Bits, auf die der KV-Cache quantisiert werden soll",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Gruppengr√∂√üenstrategie",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Genauigkeit",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Ausgeglichen",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Schnell",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Erweitert: Konfiguration der quantisierten 'matmul group size'\n\n‚Ä¢ Genauigkeit = Gruppengr√∂√üe 32\n‚Ä¢ Ausgeglichen = Gruppengr√∂√üe 64\n‚Ä¢ Schnell = Gruppengr√∂√üe 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Beginnen Sie mit der Quantisierung, wenn der Kontext diese L√§nge erreicht",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Wenn der Kontext diese Anzahl von Token erreicht,\nbeginnen Sie mit der Quantisierung des KV-Caches",

  "embedding.load.contextLength/title": "Kontextl√§nge",
  "embedding.load.contextLength/subTitle": "Die maximale Anzahl von Token, die das Modell in einem Prompt ber√ºcksichtigen kann. Weitere M√∂glichkeiten zur Verwaltung finden Sie unter den Optionen f√ºr den Konversations√ºberlauf unter \"Inferenzparameter\"",
  "embedding.load.contextLength/info": "Gibt die maximale Anzahl von Token an, die das Modell gleichzeitig ber√ºcksichtigen kann, was sich darauf auswirkt, wie viel Kontext es w√§hrend der Verarbeitung beibeh√§lt",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE-Frequenzbasis",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Benutzerdefinierte Basisfrequenz f√ºr rotierende Positions-Embeddings (RoPE). Eine Erh√∂hung kann eine bessere Leistung bei hohen Kontextl√§ngen erm√∂glichen",
  "embedding.load.llama.ropeFrequencyBase/info": "[Erweitert] Passt die Basisfrequenz f√ºr die rotierende Positionskodierung an, was sich darauf auswirkt, wie Positionsinformationen eingebettet werden",
  "embedding.load.llama.evalBatchSize/title": "Evaluierungs-Batchgr√∂√üe",
  "embedding.load.llama.evalBatchSize/subTitle": "Anzahl der Eingabetoken, die gleichzeitig verarbeitet werden sollen. Eine Erh√∂hung erh√∂ht die Leistung auf Kosten der Speichernutzung",
  "embedding.load.llama.evalBatchSize/info": "Legt die Anzahl der Token fest, die w√§hrend der Evaluierung in einem Batch zusammen verarbeitet werden",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE-Frequenzskala",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Die Kontextl√§nge wird mit diesem Faktor skaliert, um den effektiven Kontext mithilfe von RoPE zu erweitern",
  "embedding.load.llama.ropeFrequencyScale/info": "[Erweitert] √Ñndert die Skalierung der Frequenz f√ºr die rotierende Positionskodierung, um die Granularit√§t der Positionskodierung zu steuern",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU-Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Anzahl der diskreten Modellschichten, die zur GPU-Beschleunigung auf der GPU berechnet werden sollen",
  "embedding.load.llama.acceleration.offloadRatio/info": "Legen Sie die Anzahl der Schichten fest, die auf die GPU ausgelagert werden sollen.",
  "embedding.load.llama.keepModelInMemory/title": "Modell im Speicher behalten",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reservieren Sie Systemspeicher f√ºr das Modell, auch wenn es auf die GPU ausgelagert ist. Verbessert die Leistung, erfordert aber mehr System-RAM",
  "embedding.load.llama.keepModelInMemory/info": "Verhindert, dass das Modell auf die Festplatte ausgelagert wird, und gew√§hrleistet so einen schnelleren Zugriff auf Kosten einer h√∂heren RAM-Nutzung",
  "embedding.load.llama.tryMmap/title": "mmap() versuchen",
  "embedding.load.llama.tryMmap/subTitle": "Verbessert die Ladezeit f√ºr das Modell. Das Deaktivieren kann die Leistung verbessern, wenn das Modell gr√∂√üer als der verf√ºgbare System-RAM ist",
  "embedding.load.llama.tryMmap/info": "Modelldateien direkt von der Festplatte in den Speicher laden",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "Der Seed f√ºr den Zufallszahlengenerator, der bei der Textgenerierung verwendet wird. -1 ist ein zuf√§lliger Seed",

  "embedding.load.seed/info": "Zufalls-Seed: Legt den Seed f√ºr die Zufallszahlengenerierung fest, um reproduzierbare Ergebnisse zu gew√§hrleisten",

  "presetTooltip": {
    "included/title": "Voreingestellte Werte",
    "included/description": "Die folgenden Felder werden angewendet",
    "included/empty": "Keine Felder dieser Voreinstellung gelten in diesem Kontext.",
    "included/conflict": "Sie werden aufgefordert zu w√§hlen, ob dieser Wert angewendet werden soll",
    "separateLoad/title": "Ladezeitkonfiguration",
    "separateLoad/description.1": "Die Voreinstellung enth√§lt auch die folgende Ladezeitkonfiguration. Die Ladezeitkonfiguration ist modellweit und erfordert ein erneutes Laden des Modells, um wirksam zu werden. Halten Sie",
    "separateLoad/description.2": "gedr√ºckt, um auf",
    "separateLoad/description.3": "anzuwenden.",
    "excluded/title": "Gilt m√∂glicherweise nicht",
    "excluded/description": "Die folgenden Felder sind in der Voreinstellung enthalten, gelten jedoch im aktuellen Kontext nicht.",
    "legacy/title": "Legacy-Voreinstellung",
    "legacy/description": "Dies ist eine Legacy-Voreinstellung. Sie enth√§lt die folgenden Felder, die entweder jetzt automatisch verarbeitet werden oder nicht mehr anwendbar sind.",
    "button/publish": "Im Hub ver√∂ffentlichen",
    "button/pushUpdate": "√Ñnderungen an den Hub √ºbertragen",
    "button/noChangesToPush": "Keine √Ñnderungen zum √úbertragen",
    "button/export": "Exportieren",
    "hubLabel": "Hub-Preset von {{user}}",
    "ownHubLabel": "Dein Hub-Preset"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Leer>"
    },
    "checkboxNumeric": {
      "off": "AUS"
    },
    "llamaCacheQuantizationType": {
      "off": "AUS"
    },
    "mlxKvCacheBits": {
      "off": "AUS"
    },
    "stringArray": {
      "empty": "<Leer>"
    },
    "llmPromptTemplate": {
      "type": "Typ",
      "types.jinja/label": "Vorlage (Jinja)",
      "jinja.bosToken/label": "BOS-Token",
      "jinja.eosToken/label": "EOS-Token",
      "jinja.template/label": "Vorlage",
      "jinja/error": "Jinja-Vorlage konnte nicht geparst werden: {{error}}",
      "jinja/empty": "Bitte geben Sie oben eine Jinja-Vorlage ein.",
      "jinja/unlikelyToWork": "Die von Ihnen oben angegebene Jinja-Vorlage funktioniert wahrscheinlich nicht, da sie nicht auf die Variable \"messages\" verweist. Bitte √ºberpr√ºfen Sie, ob Sie eine korrekte Vorlage eingegeben haben.",
      "types.manual/label": "Manuell",
      "manual.subfield.beforeSystem/label": "Vor System",
      "manual.subfield.beforeSystem/placeholder": "Systempr√§fix eingeben...",
      "manual.subfield.afterSystem/label": "Nach System",
      "manual.subfield.afterSystem/placeholder": "Systemsuffix eingeben...",
      "manual.subfield.beforeUser/label": "Vor Benutzer",
      "manual.subfield.beforeUser/placeholder": "Benutzerpr√§fix eingeben...",
      "manual.subfield.afterUser/label": "Nach Benutzer",
      "manual.subfield.afterUser/placeholder": "Benutzersuffix eingeben...",
      "manual.subfield.beforeAssistant/label": "Vor Assistent",
      "manual.subfield.beforeAssistant/placeholder": "Assistentenpr√§fix eingeben...",
      "manual.subfield.afterAssistant/label": "Nach Assistent",
      "manual.subfield.afterAssistant/placeholder": "Assistentensuffix eingeben...",
      "stopStrings/label": "Zus√§tzliche Stopp-Strings",
      "stopStrings/subTitle": "Vorlagenspezifische Stopp-Strings, die zus√§tzlich zu benutzerdefinierten Stopp-Strings verwendet werden."
    },
    "contextLength": {
      "maxValueTooltip": "Dies ist die maximale Anzahl von Token, f√ºr die das Modell trainiert wurde. Klicken Sie hier, um den Kontext auf diesen Wert festzulegen",
      "maxValueTextStart": "Modell unterst√ºtzt bis zu",
      "maxValueTextEnd": "Token",
      "tooltipHint": "Obwohl ein Modell m√∂glicherweise eine bestimmte Anzahl von Token unterst√ºtzt, kann die Leistung nachlassen, wenn die Ressourcen Ihres Computers die Last nicht bew√§ltigen k√∂nnen - seien Sie vorsichtig, wenn Sie diesen Wert erh√∂hen"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Bei Limit stoppen",
      "stopAtLimitSub": "Generierung stoppen, sobald der Speicher des Modells voll ist",
      "truncateMiddle": "Mitte k√ºrzen",
      "truncateMiddleSub": "Entfernt Nachrichten aus der Mitte der Konversation, um Platz f√ºr neuere zu schaffen. Das Modell erinnert sich weiterhin an den Anfang der Konversation",
      "rollingWindow": "Rollierendes Fenster",
      "rollingWindowSub": "Das Modell erh√§lt immer die letzten paar Nachrichten, vergisst aber m√∂glicherweise den Anfang der Konversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "AUS"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Gleichm√§√üig",
      "favorMainGpu": "Haupt-GPU bevorzugen"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Lesen Sie, wie es funktioniert",
      "placeholder": "W√§hlen Sie ein kompatibles Entwurfsmodell aus",
      "noCompatible": "Keine kompatiblen Entwurfsmodelle f√ºr Ihre aktuelle Modellauswahl gefunden",
      "stillLoading": "Kompatible Entwurfsmodelle werden identifiziert...",
      "notCompatible": "Das ausgew√§hlte Entwurfsmodell (<draft/>) ist nicht mit der aktuellen Modellauswahl (<current/>) kompatibel.",
      "off": "AUS",
      "loadModelToSeeOptions": "Modell laden <keyboard-shortcut />, um kompatible Optionen anzuzeigen",
      "compatibleWithNumberOfModels": "Empfohlen f√ºr mindestens {{dynamicValue}} Ihrer Modelle",
      "recommendedForSomeModels": "Empfohlen f√ºr einige Modelle",
      "recommendedForLlamaModels": "Empfohlen f√ºr Llama-Modelle",
      "recommendedForQwenModels": "Empfohlen f√ºr Qwen-Modelle",
      "onboardingModal": {
        "introducing": "Einf√ºhrung",
        "speculativeDecoding": "Spekulative Dekodierung",
        "firstStepBody": "Inferenzbeschleunigung f√ºr <custom-span>llama.cpp</custom-span>- und <custom-span>MLX</custom-span>-Modelle",
        "secondStepTitle": "Inferenzbeschleunigung mit spekulativer Dekodierung",
        "secondStepBody": "Spekulative Dekodierung ist eine Technik, bei der zwei Modelle zusammenarbeiten:\n - Ein gr√∂√üeres \"Haupt\"-Modell\n - Ein kleineres \"Entwurfs\"-Modell\n\nW√§hrend der Generierung schl√§gt das Entwurfsmodell schnell Token vor, die das gr√∂√üere Hauptmodell √ºberpr√ºfen kann. Das √úberpr√ºfen von Token ist ein viel schnellerer Prozess als das tats√§chliche Generieren, was die Quelle der Geschwindigkeitssteigerungen ist. **Im Allgemeinen gilt: Je gr√∂√üer der Gr√∂√üenunterschied zwischen dem Hauptmodell und dem Entwurfsmodell ist, desto gr√∂√üer ist die Beschleunigung**.\n\nUm die Qualit√§t aufrechtzuerhalten, akzeptiert das Hauptmodell nur Token, die mit dem √ºbereinstimmen, was es selbst generiert h√§tte, wodurch die Antwortqualit√§t des gr√∂√üeren Modells bei schnelleren Inferenzgeschwindigkeiten erm√∂glicht wird. Beide Modelle m√ºssen dasselbe Vokabular verwenden.",
        "draftModelRecommendationsTitle": "Empfehlungen f√ºr Entwurfsmodelle",
        "basedOnCurrentModels": "Basierend auf Ihren aktuellen Modellen",
        "close": "Schlie√üen",
        "next": "Weiter",
        "done": "Fertig"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Bitte laden Sie zuerst ein Modell <model-badge /> ",
      "errorEngineNotSupported": "Die spekulative Dekodierung erfordert mindestens Version {{minVersion}} der Engine {{engineName}}. Bitte aktualisieren Sie die Engine (<key/>) und laden Sie das Modell neu, um diese Funktion zu verwenden.",
      "errorEngineNotSupported/noKey": "Die spekulative Dekodierung erfordert mindestens Version {{minVersion}} der Engine {{engineName}}. Bitte aktualisieren Sie die Engine und laden Sie das Modell neu, um diese Funktion zu verwenden."
    },
    "llmReasoningParsing": {
      "startString/label": "Start-String",
      "startString/placeholder": "Start-String eingeben...",
      "endString/label": "End-String",
      "endString/placeholder": "End-String eingeben..."
    }
  },
  "saveConflictResolution": {
    "title": "W√§hlen Sie aus, welche Werte in die Voreinstellung aufgenommen werden sollen",
    "description": "W√§hlen Sie aus, welche Werte beibehalten werden sollen",
    "instructions": "Klicken Sie auf einen Wert, um ihn einzuschlie√üen",
    "userValues": "Vorheriger Wert",
    "presetValues": "Neuer Wert",
    "confirm": "Best√§tigen",
    "cancel": "Abbrechen"
  },
  "applyConflictResolution": {
    "title": "Welche Werte sollen beibehalten werden?",
    "description": "Sie haben nicht festgeschriebene √Ñnderungen, die sich mit der eingehenden Voreinstellung √ºberschneiden",
    "instructions": "Klicken Sie auf einen Wert, um ihn beizubehalten",
    "userValues": "Aktueller Wert",
    "presetValues": "Eingehender Voreinstellungswert",
    "confirm": "Best√§tigen",
    "cancel": "Abbrechen"
  },
  "empty": "<Leer>",
  "noModelSelected": "Keine Modelle ausgew√§hlt",
  "apiIdentifier.label": "API-Kennung",
  "apiIdentifier.hint": "Geben Sie optional eine Kennung f√ºr dieses Modell an. Diese wird in API-Anfragen verwendet. Lassen Sie das Feld leer, um die Standardkennung zu verwenden.",
  "idleTTL.label": "Automatisch entladen bei Leerlauf (TTL)",
  "idleTTL.hint": "Wenn festgelegt, wird das Modell automatisch entladen, nachdem es f√ºr die angegebene Zeitspanne im Leerlauf war.",
  "idleTTL.mins": "Min.",

  "presets": {
    "title": "Voreinstellung",
    "saveChanges": "√Ñnderungen festschreiben",
    "saveChanges/description": "Schreiben Sie Ihre √Ñnderungen an der Voreinstellung fest.",
    "saveChanges.manual": "Neue Felder erkannt. Sie k√∂nnen ausw√§hlen, welche √Ñnderungen in die Voreinstellung aufgenommen werden sollen.",
    "saveChanges.manual.hold.0": "Halten Sie",
    "saveChanges.manual.hold.1": "gedr√ºckt, um auszuw√§hlen, welche √Ñnderungen an der Voreinstellung festgeschrieben werden sollen.",
    "saveChanges.saveAll.hold.0": "Halten Sie",
    "saveChanges.saveAll.hold.1": "gedr√ºckt, um alle √Ñnderungen zu speichern.",
    "saveChanges.saveInPreset.hold.0": "Halten Sie",
    "saveChanges.saveInPreset.hold.1": "gedr√ºckt, um nur √Ñnderungen an Feldern zu speichern, die bereits in der Voreinstellung enthalten sind.",
    "saveChanges/error": "√Ñnderungen an der Voreinstellung konnten nicht festgeschrieben werden.",
    "saveChanges.manual/description": "W√§hlen Sie aus, welche √Ñnderungen in die Voreinstellung aufgenommen werden sollen.",
    "saveAs": "Speichern als neu...",
    "presetNamePlaceholder": "Geben Sie einen Namen f√ºr die Voreinstellung ein...",
    "cannotCommitChangesLegacy": "Dies ist eine Legacy-Voreinstellung und kann nicht ge√§ndert werden. Sie k√∂nnen eine Kopie erstellen, indem Sie \"Speichern als neu...\" verwenden.",
    "cannotSaveChangesNoChanges": "Keine √Ñnderungen zum speichern.",
    "emptyNoUnsaved": "Voreinstellung ausw√§hlen...",
    "emptyWithUnsaved": "Ungespeicherte Voreinstellung",
    "saveEmptyWithUnsaved": "Voreinstellung speichern als...",
    "saveConfirm": "Speichern",
    "saveCancel": "Abbrechen",
    "saving": "Wird gespeichert...",
    "save/error": "Voreinstellung konnte nicht gespeichert werden.",
    "deselect": "Voreinstellung abw√§hlen",
    "deselect/error": "Voreinstellung konnte nicht abgew√§hlt werden.",
    "select/error": "Voreinstellung konnte nicht ausgew√§hlt werden.",
    "delete/error": "Voreinstellung konnte nicht gel√∂scht werden.",
    "discardChanges": "Ungespeicherte verwerfen",
    "discardChanges/info": "Alle nicht festgeschriebenen √Ñnderungen verwerfen und die Voreinstellung in ihren urspr√ºnglichen Zustand zur√ºckversetzen",
    "newEmptyPreset": "+ Neue Voreinstellung",
    "importPreset": "Importieren",
    "contextMenuCopyIdentifier": "Voreinstellungs-Kennung kopieren",
    "contextMenuSelect": "Voreinstellung anwenden",
    "contextMenuDelete": "L√∂schen...",
    "contextMenuShare": "Ver√∂ffentlichen...",
    "contextMenuOpenInHub": "Im Hub anzeigen",
    "contextMenuPullFromHub": "Neue Version vom Hub beziehen...",
    "contextMenuPushChanges": "√Ñnderungen an den Hub √ºbertragen",
    "contextMenuPushingChanges": "Wird √ºbertragen...",
    "contextMenuPushedChanges": "√Ñnderungen √ºbertragen",
    "contextMenuExport": "Datei exportieren",
    "contextMenuRevealInExplorer": "Im Datei-Explorer anzeigen",
    "contextMenuRevealInFinder": "Im Finder anzeigen",
    "share": {
      "title": "Voreinstellung ver√∂ffentlichen",
      "action": "Teilen Sie Ihre Voreinstellung, damit andere sie herunterladen, liken und forken k√∂nnen",
      "presetOwnerLabel": "Besitzer",
      "uploadAs": "Ihre Voreinstellung wird als {{name}} erstellt",
      "presetNameLabel": "Voreinstellungsname",
      "descriptionLabel": "Beschreibung (optional)",
      "loading": "Wird ver√∂ffentlicht...",
      "success": "Voreinstellung erfolgreich √ºbertragen",
      "presetIsLive": "<preset-name /> ist jetzt live im Hub!",
      "close": "Schlie√üen",
      "confirmViewOnWeb": "Im Web anzeigen",
      "confirmCopy": "URL kopieren",
      "confirmCopied": "Kopiert!",
      "pushedToHub": "Ihre Voreinstellung wurde an den Hub √ºbertragen",
      "descriptionPlaceholder": "Beschreibung eingeben...",
      "willBePublic": "Durch das Ver√∂ffentlichen Ihrer Voreinstellung wird sie √∂ffentlich",
      "willBePrivate": "Nur Sie k√∂nnen diese Voreinstellung sehen",
      "willBeOrgVisible": "Diese Voreinstellung ist f√ºr alle in der Organisation sichtbar.",
      "publicSubtitle": "Ihre Voreinstellung ist <custom-bold>√ñffentlich</custom-bold>. Andere k√∂nnen sie auf lmstudio.ai herunterladen und forken",
      "privateUsageReached": "Limit f√ºr private Voreinstellungen erreicht",
      "continueInBrowser": "Im Browser fortfahren...",
      "confirmShareButton": "Ver√∂ffentlichen",
      "error": "Voreinstellung konnte nicht ver√∂ffentlicht werden",
      "createFreeAccount": "Erstellen Sie ein kostenloses Konto im Hub, um Voreinstellungen zu ver√∂ffentlichen"
    },
    "update": {
      "title": "√Ñnderungen an den Hub √ºbertragen",
      "title/success": "Voreinstellung erfolgreich aktualisiert",
      "subtitle": "Nehmen Sie √Ñnderungen an <custom-preset-name /> vor und √ºbertragen Sie sie an den Hub",
      "descriptionLabel": "Beschreibung",
      "descriptionPlaceholder": "Beschreibung eingeben...",
      "loading": "Wird √ºbertragen...",
      "cancel": "Abbrechen",
      "createFreeAccount": "Erstellen Sie ein kostenloses Konto im Hub, um Voreinstellungen zu ver√∂ffentlichen",
      "error": "Update konnte nicht √ºbertragen werden",
      "confirmUpdateButton": "√úbertragen"
    },
    "resolve": {
      "title": "Konflikte aufl√∂sen...",
      "tooltip": "√ñffnen Sie ein Modal, um Unterschiede mit der Hub-Version aufzul√∂sen"
    },
    "loginToManage": {
      "title": "Zum Verwalten von Voreinstellungen im Hub anmelden"
    },
    "downloadFromHub": {
      "title": "Download",
      "downloading": "√úbertragung l√§uft...",
      "success": "√úbertragung abgeschlossen!",
      "error": "√úbertragung fehlgeschlagen"
    },
    "push": {
      "title": "√Ñnderungen an den Hub √ºbertragen",
      "pushing": "√úbertragung l√§uft...",
      "success": "√úbertragung abgeschlossen",
      "tooltip": "√úbertragen Sie Ihre lokalen √Ñnderungen auf die auf dem Hub gehostete Version",
      "error": "√úbertragung fehlgeschlagen"
    },
    "saveAsNewModal": {
      "title": "Ups! Voreinstellungen konnten nicht gefunden werden",
      "confirmSaveAsNewDescription": "M√∂chten Sie diese Voreinstellung als neue Voreinstellung ver√∂ffentlichen?",
      "confirmButton": "Als neue Voreinstellung ver√∂ffentlichen"
    },
    "pull": {
      "title": "Neueste Version vom Hub beziehen",
      "error": "√úbertragung fehlgeschlagen",
      "contextMenuErrorMessage": "√úbertragung fehlgeschlagen",
      "success": "√úbertragung abgeschlossen",
      "pulling": "√úbertragung l√§uft...",
      "upToDate": "AAuf dem neusten Stand!",
      "unsavedChangesModal": {
        "title": "Es bestehen ungespeicherte √Ñnderungen",
        "bodyContent": "Dateitransfer vom Hub erfordert, dass alle ungespeicherten √Ñnderungen verworfen werden. M√∂chten Sie fortfahren?",
        "confirmButton": "Ungespeicherte √Ñnderungen √ºberschreiben"
      }
    },
    "import": {
      "title": "Eine Voreinstellung aus einer Datei importieren",
      "dragPrompt": "Ziehen Sie JSON-Dateien mit Voreinstellungen per Drag & Drop oder <custom-link>w√§hlen Sie sie von Ihrem Computer aus</custom-link>",
      "remove": "Entfernen",
      "cancel": "Abbrechen",
      "importPreset_zero": "Voreinstellung importieren",
      "importPreset_one": "Voreinstellung importieren",
      "importPreset_other": "{{count}} Voreinstellungen importieren",
      "selectDialog": {
        "title": "Voreinstellungsdatei ausw√§hlen (.json)",
        "button": "Importieren"
      },
      "error": "Voreinstellung konnte nicht importiert werden",
      "resultsModal": {
        "titleSuccessSection_one": "1 Voreinstellung erfolgreich importiert",
        "titleSuccessSection_other": "{{count}} Voreinstellungen erfolgreich importiert",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} fehlgeschlagen)",
        "titleFailSection_other": "({{count}} fehlgeschlagen)",
        "titleAllFailed": "Voreinstellungen konnten nicht importiert werden",
        "importMore": "Mehr importieren",
        "close": "Fertig",
        "successBadge": "Erfolg",
        "alreadyExistsBadge": "Voreinstellung existiert bereits",
        "errorBadge": "Fehler",
        "invalidFileBadge": "Ung√ºltige Datei",
        "otherErrorBadge": "Voreinstellung konnte nicht importiert werden",
        "errorViewDetailsButton": "Details anzeigen",
        "seeError": "Fehler anzeigen",
        "noName": "Kein Voreinstellungsname",
        "useInChat": "Im Chat verwenden"
      },
      "importFromUrl": {
        "button": "Von URL importieren...",
        "title": "Von URL importieren",
        "back": "Aus Datei importieren...",
        "action": "F√ºgen Sie unten die LM Studio Hub-URL der Voreinstellung ein, die Sie importieren m√∂chten",
        "invalidUrl": "Ung√ºltige URL. Bitte stellen Sie sicher, dass Sie eine korrekte LM Studio Hub-URL einf√ºgen.",
        "tip": "Sie k√∂nnen die Voreinstellung direkt mit der Schaltfl√§che {{buttonName}} im LM Studio Hub installieren",
        "confirm": "Importieren",
        "cancel": "Abbrechen",
        "loading": "Wird importiert...",
        "error": "Voreinstellung konnte nicht heruntergeladen werden."
      }
    },
    "download": {
      "title": "<preset-name /> aus dem LM Studio Hub ziehen",
      "subtitle": "Speichern Sie <custom-name /> in Ihren Voreinstellungen. Dadurch k√∂nnen Sie diese Voreinstellung in der App verwenden",
      "button": "Ziehen",
      "button/loading": "Wird gezogen...",
      "cancel": "Abbrechen",
      "error": "Voreinstellung konnte nicht heruntergeladen werden."
    },
    "inclusiveness": {
      "speculativeDecoding": "In Voreinstellung einschlie√üen"
    }
  },

  "flashAttentionWarning": "Flash Attention ist eine experimentelle Funktion, die bei einigen Modellen Probleme verursachen kann. Wenn Probleme auftreten, versuchen Sie, sie zu deaktivieren.",
  "llamaKvCacheQuantizationWarning": "KV-Cache-Quantisierung ist eine experimentelle Funktion, die bei einigen Modellen Probleme verursachen kann. Flash Attention muss f√ºr die V-Cache-Quantisierung aktiviert sein. Wenn Probleme auftreten, setzen Sie auf den Standardwert \"F16\" zur√ºck.",

  "seedUncheckedHint": "Zufalls-Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",

  "hardware": {
    "environmentVariables": "Umgebungsvariablen",
    "environmentVariables.info": "Wenn Sie sich nicht sicher sind, belassen Sie diese bei ihren Standardwerten",
    "environmentVariables.reset": "Auf Standard zur√ºcksetzen",

    "gpus.information": "Erkannte Grafikbeschleuniger (GPUs) konfigurieren",
    "gpuSettings": {
      "editMaxCapacity": "Maximale Kapazit√§t bearbeiten",
      "hideEditMaxCapacity": "Verstecke Bearbeiten der maximalen Kapazit√§t",
      "allOffWarning": "Alle GPUs sind ausgeschaltet oder deaktiviert, stellen Sie sicher, dass es eine GPU-Zuweisung gibt, um Modelle zu laden",
      "split": {
        "title": "Strategie zur Aufteilung des GPU-Speichers",
        "placeholder": "GPU-Speicheraufteilung ausw√§hlen...",
        "options": {
          "generalDescription": "Konfigurieren Sie, wie der GPU-Speicher f√ºr dieses Modell auf mehrere GPUs aufgeteilt wird.",
          "evenly": {
            "title": "Gleichm√§√üige Verteilung",
            "description": "Speicher gleichm√§√üig auf alle ausgew√§hlten GPUs verteilen"
          },
          "priorityOrder": {
            "title": "Priorit√§tsreihenfolge",
            "description": "Ziehen Sie Grafikbeschleuniger, um die Priorit√§t neu anzuordnen. Das System versucht, mehr Speicher auf den zuerst aufgelisteten GPUs zuzuweisen"
          },
          "custom": {
            "title": "Benutzerdefiniert",
            "description": "Speicher zuteilen",
            "maxAllocation": "Maximale Zuweisung"
          }
        }
      },
      "deviceId.info": "Einzigartige Kennung der GPU",
      "changesOnlyAffectNewlyLoadedModels": "√Ñnderungen wirken sich nur auf neu geladene Modelle aus",
      "toggleGpu": "GPU ein-/ausschalten"
    }
  },

  "load.gpuSplitConfig/title": "GPU Aufteilungskonfiguration",
  "envVars/title": "Umgebungsvariable festlegen",
  "envVars": {
    "select": {
      "placeholder": "W√§hlen Sie eine Umgebungsvariable aus...",
      "noOptions": "Keine Umgebungsvariablen verf√ºgbar",
      "filter": {
        "placeholder": "Suchergebnisse filtern",
        "resultsFound_zero": "Keine Ergebnisse gefunden",
        "resultsFound_one": "1 Ergebnis gefunden",
        "resultsFound_other": "{{count}} Ergebnisse gefunden"
      }
    },
    "inputValue": {
      "placeholder": "Wert eingeben"
    },
    "values": {
      "title": "Aktuelle Werte"
    }
  }
}
