{
  "noInstanceSelected": "Kein Modellinstanz ausgew√§hlt",
  "resetToDefault": "Zur√ºcksetzen",
  "showAdvancedSettings": "Erweiterte Einstellungen anzeigen",
  "showAll": "Alle",
  "basicSettings": "Grundlegend",
  "configSubtitle": "Voreinstellungen laden oder speichern und mit Modellparameter-Overrides experimentieren",
  "inferenceParameters/title": "Vorhersageparameter",
  "inferenceParameters/info": "Experimentieren Sie mit Parametern, die die Vorhersage beeinflussen.",
  "generalParameters/title": "Allgemein",
  "samplingParameters/title": "Sampling",
  "basicTab": "Grundlegend",
  "advancedTab": "Erweitert",
  "advancedTab/title": "üß™ Erweiterte Konfiguration",
  "advancedTab/expandAll": "Alle erweitern",
  "advancedTab/overridesTitle": "Konfigurations√ºberschreibungen",
  "advancedTab/noConfigsText": "Sie haben keine ungespeicherten √Ñnderungen ‚Äì Bearbeiten Sie Werte oben, um hier √úberschreibungen anzuzeigen.",
  "loadInstanceFirst": "Laden Sie ein Modell, um konfigurierbare Parameter anzuzeigen",
  "noListedConfigs": "Keine konfigurierbaren Parameter",
  "generationParameters/info": "Experimentieren Sie mit grundlegenden Parametern, die die Texterstellung beeinflussen.",
  "loadParameters/title": "Ladeparameter",
  "loadParameters/description": "Einstellungen zur Steuerung der Initialisierung und des Ladens des Modells in den Speicher.",
  "loadParameters/reload": "Neu laden, um √Ñnderungen anzuwenden",
  "discardChanges": "√Ñnderungen verwerfen",
  "loadModelToSeeOptions": "Laden Sie ein Modell, um Optionen zu sehen",
  "llm.prediction.systemPrompt/title": "System-Prompt",
  "llm.prediction.systemPrompt/description": "Verwenden Sie dieses Feld, um dem Modell Hintergrundanweisungen zu geben, z. B. eine Reihe von Regeln, Einschr√§nkungen oder allgemeinen Anforderungen.",
  "llm.prediction.systemPrompt/subTitle": "Richtlinien f√ºr die KI",
  "llm.prediction.temperature/title": "Temperatur",
  "llm.prediction.temperature/subTitle": "Wie viel Zuf√§lligkeit soll eingef√ºhrt werden? 0 liefert jedes Mal dasselbe Ergebnis, w√§hrend h√∂here Werte die Kreativit√§t und Varianz erh√∂hen",
  "llm.prediction.temperature/info": "Aus den llama.cpp-Hilfedokumenten: ‚ÄûDer Standardwert ist <{{dynamicValue}}>, der ein Gleichgewicht zwischen Zuf√§lligkeit und Determinismus bietet. Bei extremen Werten f√ºhrt eine Temperatur von 0 immer zur Auswahl des wahrscheinlichsten n√§chsten Tokens, was zu identischen Ausgaben bei jedem Lauf f√ºhrt‚Äú",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top-K-Sampling",
  "llm.prediction.topKSampling/subTitle": "Beschr√§nkt das n√§chste Token auf eines der wahrscheinlichsten k Tokens. Funktioniert √§hnlich wie die Temperatur",
  "llm.prediction.topKSampling/info": "Aus den llama.cpp-Hilfedokumenten:\n\nDas Top-k-Sampling ist eine Methode zur Texterstellung, bei der das n√§chste Token nur aus den k wahrscheinlichsten Tokens ausgew√§hlt wird, die vom Modell vorhergesagt wurden.\n\nEs hilft, das Risiko der Erzeugung von Tokens mit niedriger Wahrscheinlichkeit oder unsinnigen Tokens zu verringern, kann aber auch die Vielfalt der Ausgabe einschr√§nken.\n\nEin h√∂herer Wert f√ºr k (z. B. 100) ber√ºcksichtigt mehr Tokens und f√ºhrt zu vielf√§ltigeren Texten, w√§hrend ein niedrigerer Wert (z. B. 10) sich auf die wahrscheinlichsten Tokens konzentriert und konservativere Texte erzeugt.\n\n‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU-Threads",
  "llm.prediction.llama.cpuThreads/subTitle": "Anzahl der CPU-Threads, die w√§hrend der Inferenz verwendet werden",
  "llm.prediction.llama.cpuThreads/info": "Die Anzahl der zu verwendenden Threads w√§hrend der Berechnung. Eine Erh√∂hung der Thread-Anzahl f√ºhrt nicht immer zu einer besseren Leistung. Der Standardwert ist <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "L√§nge der Antwort begrenzen",
  "llm.prediction.maxPredictedTokens/subTitle": "Optionale Begrenzung der L√§nge der Antwort der KI",
  "llm.prediction.maxPredictedTokens/info": "Steuern Sie die maximale L√§nge der Antwort des Chatbots. Aktivieren Sie die Option, um eine Begrenzung f√ºr die maximale L√§nge der Antwort festzulegen, oder deaktivieren Sie sie, um den Chatbot entscheiden zu lassen, wann er aufh√∂ren soll.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximale Antwortl√§nge (Tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Ungef√§hr {{maxWords}} W√∂rter",
  "llm.prediction.repeatPenalty/title": "Wiederholung Strafe",
  "llm.prediction.repeatPenalty/subTitle": "Wie stark Wiederholungen desselben Tokens entmutigt werden sollen",
  "llm.prediction.repeatPenalty/info": "Aus den llama.cpp-Hilfedokumenten: ‚ÄûHilft zu verhindern, dass das Modell sich wiederholende oder monotone Texte erzeugt.\n\nEin h√∂herer Wert (z. B. 1,5) bestraft Wiederholungen st√§rker, w√§hrend ein niedrigerer Wert (z. B. 0,9) gro√üz√ºgiger ist.‚Äú ‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Minimales P-Sampling",
  "llm.prediction.minPSampling/subTitle": "Minimale Basiswahrscheinlichkeit, damit ein Token zur Ausgabe ausgew√§hlt wird",
  "llm.prediction.minPSampling/info": "Aus den llama.cpp-Hilfedokumenten:\n\nDie minimale Wahrscheinlichkeit f√ºr ein Token, relativ zur Wahrscheinlichkeit des wahrscheinlichsten Tokens in Betracht gezogen zu werden. Muss in [0, 1] liegen.\n\n‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top-P-Sampling",
  "llm.prediction.topPSampling/subTitle": "Minimale kumulative Wahrscheinlichkeit f√ºr die m√∂glichen n√§chsten Tokens. Funktioniert √§hnlich wie die Temperatur",
  "llm.prediction.topPSampling/info": "Aus den llama.cpp-Hilfedokumenten:\n\nDas Top-p-Sampling, auch als Nucleus-Sampling bekannt, ist eine weitere Methode zur Texterstellung, bei der das n√§chste Token aus einem Teil der Tokens ausgew√§hlt wird, die zusammen eine kumulative Wahrscheinlichkeit von mindestens p aufweisen.\n\nDiese Methode bietet ein Gleichgewicht zwischen Vielfalt und Qualit√§t, da sowohl die Wahrscheinlichkeiten der Tokens als auch die Anzahl der zu w√§hlenden Tokens ber√ºcksichtigt werden.\n\nEin h√∂herer Wert f√ºr top-p (z. B. 0,95) f√ºhrt zu vielf√§ltigeren Texten, w√§hrend ein niedrigerer Wert (z. B. 0,5) fokussiertere und konservativere Texte erzeugt. Muss im Bereich (0, 1] liegen.\n\n‚Ä¢ Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stoppzeichen",
  "llm.prediction.stopStrings/subTitle": "Strings, die das Modell daran hindern sollen, weitere Tokens zu generieren",
  "llm.prediction.stopStrings/info": "Bestimmte Strings, bei deren Auftreten das Modell keine weiteren Tokens mehr generieren soll",
  "llm.prediction.stopStrings/placeholder": "Geben Sie einen String ein und dr√ºcken Sie ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Kontext√ºberlauf",
  "llm.prediction.contextOverflowPolicy/subTitle": "Wie sich das Modell verhalten soll, wenn das Gespr√§ch zu gro√ü wird, um verarbeitet zu werden",
  "llm.prediction.contextOverflowPolicy/info": "Entscheiden Sie, was zu tun ist, wenn das Gespr√§ch die Gr√∂√üe des Arbeitsged√§chtnisses des Modells (‚ÄöKontext‚Äò) √ºberschreitet",
  "llm.prediction.llama.frequencyPenalty/title": "Frequenzstrafe",
  "llm.prediction.llama.presencePenalty/title": "Pr√§senzstrafe",
  "llm.prediction.llama.tailFreeSampling/title": "Schwanzfreies Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Lokal typisches Sampling",
  "llm.prediction.onnx.topKSampling/title": "Top-K-Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Beschr√§nkt das n√§chste Token auf eines der Top-k wahrscheinlichsten Tokens. Funktioniert √§hnlich wie die Temperatur",
  "llm.prediction.onnx.topKSampling/info": "Aus den ONNX-Dokumentationen:\n\nAnzahl der wahrscheinlichsten Vokabel-Tokens, die f√ºr das Top-k-Filtern beibehalten werden\n\n‚Ä¢ Dieser Filter ist standardm√§√üig deaktiviert",
  "llm.prediction.onnx.repeatPenalty/title": "Wiederholungsstrafe",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Wie stark Wiederholungen desselben Tokens entmutigt werden sollen",
  "llm.prediction.onnx.repeatPenalty/info": "Ein h√∂herer Wert entmutigt das Modell, sich selbst zu wiederholen",
  "llm.prediction.onnx.topPSampling/title": "Top-P-Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimale kumulative Wahrscheinlichkeit f√ºr die m√∂glichen n√§chsten Tokens. Funktioniert √§hnlich wie die Temperatur",
  "llm.prediction.onnx.topPSampling/info": "Aus den ONNX-Dokumentationen:\n\nNur die wahrscheinlichsten Tokens, deren Wahrscheinlichkeiten sich zu TopP oder h√∂her summieren, werden zur Generierung behalten\n\n‚Ä¢ Dieser Filter ist standardm√§√üig deaktiviert",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Strukturierte Ausgabe",
  "llm.prediction.structured/info": "Strukturierte Ausgabe",
  "llm.prediction.promptTemplate/title": "Prompt-Template",
  "llm.prediction.promptTemplate/subTitle": "Das Format, in dem Nachrichten im Chat an das Modell gesendet werden. Eine √Ñnderung kann unerwartetes Verhalten hervorrufen ‚Äì stellen Sie sicher, dass Sie wissen, was Sie tun!",
  "llm.load.contextLength/title": "Kontextl√§nge",
  "llm.load.contextLength/subTitle": "Die maximale Anzahl von Tokens, die das Modell auf einmal verarbeiten kann. Siehe die Optionen f√ºr Gespr√§chs√ºberlauf unter ‚ÄûVorhersageparameter‚Äú f√ºr weitere M√∂glichkeiten zur Verwaltung.",
  "llm.load.contextLength/info": "Legt die maximale Anzahl von Tokens fest, die das Modell auf einmal ber√ºcksichtigen kann, was beeinflusst, wie viel Kontext w√§hrend der Verarbeitung beibehalten wird",
  "llm.load.contextLength/warning": "Das Festlegen eines hohen Wertes f√ºr die Kontextl√§nge kann sich erheblich auf die Speichernutzung auswirken",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "Der Seed f√ºr den Zufallszahlengenerator, der bei der Texterstellung verwendet wird. -1 ist zuf√§llig",
  "llm.load.seed/info": "Zufalls-Seed: Legt den Seed f√ºr die Zufallszahlengenerierung fest, um reproduzierbare Ergebnisse zu gew√§hrleisten",
  "llm.load.llama.evalBatchSize/title": "Evaluierungs-Batch-Gr√∂√üe",
  "llm.load.llama.evalBatchSize/subTitle": "Anzahl der gleichzeitig zu verarbeitenden Eingabetokens. Eine Erh√∂hung verbessert die Leistung auf Kosten der Speichernutzung",
  "llm.load.llama.evalBatchSize/info": "Legt die Anzahl der Beispiele fest, die gleichzeitig in einem Batch w√§hrend der Auswertung verarbeitet werden, was die Geschwindigkeit und den Speicherverbrauch beeinflusst",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE-Basisfrequenz",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Angepasste Basisfrequenz f√ºr rotatorische Positionscodierungen (RoPE). Eine Erh√∂hung kann eine bessere Leistung bei hohen Kontextl√§ngen erm√∂glichen",
  "llm.load.llama.ropeFrequencyBase/info": "[Erweitert] Passt die Basisfrequenz f√ºr rotatorische Positionscodierungen an, was beeinflusst, wie Positionsinformationen eingebettet werden",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE-Frequenz-Skalierung",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Die Kontextl√§nge wird mit diesem Faktor skaliert, um eine effektive Kontextverl√§ngerung unter Verwendung von RoPE zu erreichen",
  "llm.load.llama.ropeFrequencyScale/info": "[Erweitert] Modifiziert die Frequenzskalierung f√ºr rotatorische Positionscodierungen, um die Granularit√§t der Positionscodierung zu steuern",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU-Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Anzahl der Modellschichten, die zur GPU-Beschleunigung auf der GPU berechnet werden",
  "llm.load.llama.acceleration.offloadRatio/info": "Legt die Anzahl der Schichten fest, die auf die GPU ausgelagert werden sollen.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Verringert die Speichernutzung und die Generierungszeit bei einigen Modellen",
  "llm.load.llama.flashAttention/info": "Beschleunigt Aufmerksamkeitsmechanismen f√ºr eine schnellere und effizientere Verarbeitung",
  "llm.load.numExperts/title": "Anzahl der Experten",
  "llm.load.numExperts/subTitle": "Anzahl der Experten, die im Modell verwendet werden",
  "llm.load.numExperts/info": "Die Anzahl der Experten, die im Modell verwendet werden",
  "llm.load.llama.keepModelInMemory/title": "Modell im Speicher behalten",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserviert Systemspeicher f√ºr das Modell, auch wenn es auf die GPU ausgelagert wird. Verbessert die Leistung, erfordert jedoch mehr Systemspeicher",
  "llm.load.llama.keepModelInMemory/info": "Verhindert, dass das Modell auf die Festplatte ausgelagert wird, und stellt sicher, dass es schneller aufgerufen werden kann, allerdings auf Kosten einer h√∂heren RAM-Nutzung",
  "llm.load.llama.useFp16ForKVCache/title": "FP16 f√ºr den KV-Cache verwenden",
  "llm.load.llama.useFp16ForKVCache/info": "Reduziert die Speichernutzung durch die Speicherung des Caches in Halbpr√§zision (FP16)",
  "llm.load.llama.tryMmap/title": "mmap() versuchen",
  "llm.load.llama.tryMmap/subTitle": "Verbessert die Ladezeit f√ºr das Modell. Das Deaktivieren dieser Funktion kann die Leistung verbessern, wenn das Modell gr√∂√üer ist als der verf√ºgbare Systemspeicher",
  "llm.load.llama.tryMmap/info": "L√§dt Modelldateien direkt von der Festplatte in den Speicher",
  "embedding.load.contextLength/title": "Kontextl√§nge",
  "embedding.load.contextLength/subTitle": "Die maximale Anzahl von Tokens, die das Modell auf einmal verarbeiten kann. Siehe die Optionen f√ºr Gespr√§chs√ºberlauf unter ‚ÄûVorhersageparameter‚Äú f√ºr weitere M√∂glichkeiten zur Verwaltung.",
  "embedding.load.contextLength/info": "Legt die maximale Anzahl von Tokens fest, die das Modell auf einmal ber√ºcksichtigen kann, was beeinflusst, wie viel Kontext w√§hrend der Verarbeitung beibehalten wird",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE-Basisfrequenz",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Angepasste Basisfrequenz f√ºr rotatorische Positionscodierungen (RoPE). Eine Erh√∂hung kann eine bessere Leistung bei hohen Kontextl√§ngen erm√∂glichen",
  "embedding.load.llama.ropeFrequencyBase/info": "[Erweitert] Passt die Basisfrequenz f√ºr rotatorische Positionscodierungen an, was beeinflusst, wie Positionsinformationen eingebettet werden",
  "embedding.load.llama.evalBatchSize/title": "Evaluierungs-Batch-Gr√∂√üe",
  "embedding.load.llama.evalBatchSize/subTitle": "Anzahl der gleichzeitig zu verarbeitenden Eingabetokens. Eine Erh√∂hung verbessert die Leistung auf Kosten der Speichernutzung",
  "embedding.load.llama.evalBatchSize/info": "Legt die Anzahl der Tokens fest, die gleichzeitig in einem Batch w√§hrend der Auswertung verarbeitet werden",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE-Frequenz-Skalierung",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Die Kontextl√§nge wird mit diesem Faktor skaliert, um eine effektive Kontextverl√§ngerung unter Verwendung von RoPE zu erreichen",
  "embedding.load.llama.ropeFrequencyScale/info": "[Erweitert] Modifiziert die Frequenzskalierung f√ºr rotatorische Positionscodierungen, um die Granularit√§t der Positionscodierung zu steuern",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU-Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Anzahl der Modellschichten, die zur GPU-Beschleunigung auf der GPU berechnet werden",
  "embedding.load.llama.acceleration.offloadRatio/info": "Legt die Anzahl der Schichten fest, die auf die GPU ausgelagert werden sollen.",
  "embedding.load.llama.keepModelInMemory/title": "Modell im Speicher behalten",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserviert Systemspeicher f√ºr das Modell, auch wenn es auf die GPU ausgelagert wird. Verbessert die Leistung, erfordert jedoch mehr Systemspeicher",
  "embedding.load.llama.keepModelInMemory/info": "Verhindert, dass das Modell auf die Festplatte ausgelagert wird, und stellt sicher, dass es schneller aufgerufen werden kann, allerdings auf Kosten einer h√∂heren RAM-Nutzung",
  "embedding.load.llama.tryMmap/title": "mmap() versuchen",
  "embedding.load.llama.tryMmap/subTitle": "Verbessert die Ladezeit f√ºr das Modell. Das Deaktivieren dieser Funktion kann die Leistung verbessern, wenn das Modell gr√∂√üer ist als der verf√ºgbare Systemspeicher",
  "embedding.load.llama.tryMmap/info": "L√§dt Modelldateien direkt von der Festplatte in den Speicher",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "Der Seed f√ºr den Zufallszahlengenerator, der bei der Texterstellung verwendet wird. -1 ist zuf√§llig",
  "embedding.load.seed/info": "Zufalls-Seed: Legt den Seed f√ºr die Zufallszahlengenerierung fest, um reproduzierbare Ergebnisse zu gew√§hrleisten",
  "presetTooltip": {
    "included/title": "Voreinstellungswerte",
    "included/description": "Die folgenden Felder werden angewendet",
    "included/empty": "Keine Felder dieser Voreinstellung gelten in diesem Kontext.",
    "included/conflict": "Sie werden aufgefordert, zu entscheiden, ob Sie diesen Wert anwenden m√∂chten",
    "separateLoad/title": "Ladezeit-Konfiguration",
    "separateLoad/description.1": "Die Voreinstellung enth√§lt auch die folgende Ladezeit-Konfiguration. Ladezeitkonfigurationen gelten f√ºr das gesamte Modell und erfordern das erneute Laden des Modells. Halten Sie",
    "separateLoad/description.2": "um auf",
    "separateLoad/description.3": "anzuwenden.",
    "excluded/title": "Kann nicht angewendet werden",
    "excluded/description": "Die folgenden Felder sind in der Voreinstellung enthalten, gelten jedoch nicht im aktuellen Kontext.",
    "legacy/title": "Legacy-Voreinstellung",
    "legacy/description": "Diese Voreinstellung ist eine Legacy-Voreinstellung. Sie enth√§lt die folgenden Felder, die entweder jetzt automatisch behandelt werden oder nicht mehr anwendbar sind."
  },
  "customInputs": {
    "string": {
      "emptyParagraph": "<Leer>"
    },
    "checkboxNumeric": {
      "off": "AUS"
    },
    "stringArray": {
      "empty": "<Leer>"
    },
    "llmPromptTemplate": {
      "type": "Typ",
      "types.jinja/label": "Vorlage (Jinja)",
      "jinja.bosToken/label": "BOS-Token",
      "jinja.eosToken/label": "EOS-Token",
      "jinja.template/label": "Vorlage",
      "jinja/error": "Fehler beim Parsen der Jinja-Vorlage: {{error}}",
      "types.manual/label": "Manuell",
      "manual.subfield.beforeSystem/label": "Vor dem System",
      "manual.subfield.beforeSystem/placeholder": "Geben Sie ein Systempr√§fix ein...",
      "manual.subfield.afterSystem/label": "Nach dem System",
      "manual.subfield.afterSystem/placeholder": "Geben Sie ein Systemsuffix ein...",
      "manual.subfield.beforeUser/label": "Vor dem Benutzer",
      "manual.subfield.beforeUser/placeholder": "Geben Sie ein Benutzerpr√§fix ein...",
      "manual.subfield.afterUser/label": "Nach dem Benutzer",
      "manual.subfield.afterUser/placeholder": "Geben Sie ein Benutzersuffix ein...",
      "manual.subfield.beforeAssistant/label": "Vor dem Assistenten",
      "manual.subfield.beforeAssistant/placeholder": "Geben Sie ein Assistentenpr√§fix ein...",
      "manual.subfield.afterAssistant/label": "Nach dem Assistenten",
      "manual.subfield.afterAssistant/placeholder": "Geben Sie ein Assistentensuffix ein...",
      "stopStrings/label": "Zus√§tzliche Stoppzeichen",
      "stopStrings/subTitle": "Vorlagenspezifische Stoppzeichen, die zus√§tzlich zu den vom Benutzer angegebenen Stoppzeichen verwendet werden."
    },
    "contextLength": {
      "maxValueTooltip": "Dies ist die maximale Anzahl von Tokens, die das Modell verarbeiten kann. Klicken Sie, um den Kontext auf diesen Wert zu setzen",
      "maxValueTextStart": "Modell unterst√ºtzt bis zu",
      "maxValueTextEnd": "Tokens",
      "tooltipHint": "Obwohl ein Modell eine bestimmte Anzahl von Tokens unterst√ºtzen kann, kann die Leistung darunter leiden, wenn die Ressourcen Ihres Systems die Last nicht bew√§ltigen k√∂nnen ‚Äì seien Sie vorsichtig, wenn Sie diesen Wert erh√∂hen"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Am Limit stoppen",
      "stopAtLimitSub": "Stoppt die Generierung, wenn der Speicher des Modells voll wird",
      "truncateMiddle": "Mitte abschneiden",
      "truncateMiddleSub": "Entfernt Nachrichten aus der Mitte des Gespr√§chs, um Platz f√ºr neuere zu schaffen. Das Modell erinnert sich weiterhin an den Anfang des Gespr√§chs",
      "rollingWindow": "Rollierendes Fenster",
      "rollingWindowSub": "Das Modell erh√§lt immer die neuesten Nachrichten, kann aber den Anfang des Gespr√§chs vergessen"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "AUS"
    }
  },
  "saveConflictResolution": {
    "title": "W√§hlen Sie, welche Werte in der Voreinstellung enthalten sein sollen",
    "description": "W√§hlen Sie aus, welche Werte beibehalten werden sollen",
    "instructions": "Klicken Sie auf einen Wert, um ihn zu √ºbernehmen",
    "userValues": "Vorheriger Wert",
    "presetValues": "Neuer Wert",
    "confirm": "Best√§tigen",
    "cancel": "Abbrechen"
  },
  "applyConflictResolution": {
    "title": "Welche Werte beibehalten?",
    "description": "Sie haben ungespeicherte √Ñnderungen, die mit der eingehenden Voreinstellung √ºberlappen",
    "instructions": "Klicken Sie auf einen Wert, um ihn zu behalten",
    "userValues": "Aktueller Wert",
    "presetValues": "Eingehender Voreinstellungswert",
    "confirm": "Best√§tigen",
    "cancel": "Abbrechen"
  },
  "empty": "<Leer>",
  "presets": {
    "title": "Voreinstellung",
    "commitChanges": "√Ñnderungen √ºbernehmen",
    "commitChanges/description": "√úbernehmen Sie Ihre √Ñnderungen in der Voreinstellung.",
    "commitChanges.manual": "Neue Felder entdeckt. Sie k√∂nnen ausw√§hlen, welche √Ñnderungen in der Voreinstellung enthalten sein sollen.",
    "commitChanges.manual.hold.0": "Halten",
    "commitChanges.manual.hold.1": "um auszuw√§hlen, welche √Ñnderungen in die Voreinstellung √ºbernommen werden sollen.",
    "commitChanges.saveAll.hold.0": "Halten",
    "commitChanges.saveAll.hold.1": "um alle √Ñnderungen zu speichern.",
    "commitChanges.saveInPreset.hold.0": "Halten",
    "commitChanges.saveInPreset.hold.1": "um nur √Ñnderungen zu speichern, die bereits in der Voreinstellung enthalten sind.",
    "commitChanges/error": "Fehler beim √úbernehmen der √Ñnderungen in der Voreinstellung.",
    "commitChanges.manual/description": "W√§hlen Sie, welche √Ñnderungen in die Voreinstellung aufgenommen werden sollen.",
    "saveAs": "Speichern als neue...",
    "presetNamePlaceholder": "Geben Sie einen Namen f√ºr die Voreinstellung ein...",
    "cannotCommitChangesLegacy": "Dies ist eine Legacy-Voreinstellung und kann nicht ge√§ndert werden. Sie k√∂nnen eine Kopie erstellen, indem Sie ‚ÄûSpeichern als neue...‚Äú verwenden.",
    "cannotCommitChangesNoChanges": "Keine √Ñnderungen zum √úbernehmen.",
    "emptyNoUnsaved": "W√§hlen Sie eine Voreinstellung...",
    "emptyWithUnsaved": "Ungespeicherte Voreinstellung",
    "saveEmptyWithUnsaved": "Voreinstellung speichern als...",
    "saveConfirm": "Speichern",
    "saveCancel": "Abbrechen",
    "saving": "Speichern...",
    "save/error": "Fehler beim Speichern der Voreinstellung.",
    "deselect": "Voreinstellung abw√§hlen",
    "deselect/error": "Fehler beim Abw√§hlen der Voreinstellung.",
    "select/error": "Fehler beim Ausw√§hlen der Voreinstellung.",
    "delete/error": "Fehler beim L√∂schen der Voreinstellung.",
    "discardChanges": "Ungespeicherte √Ñnderungen verwerfen",
    "discardChanges/info": "Verwerfen Sie alle ungespeicherten √Ñnderungen und stellen Sie die Voreinstellung in den Originalzustand zur√ºck",
    "newEmptyPreset": "Neue leere Voreinstellung erstellen...",
    "contextMenuSelect": "Voreinstellung ausw√§hlen",
    "contextMenuDelete": "L√∂schen"
  },
  "flashAttentionWarning": "Flash Attention ist eine experimentelle Funktion, die bei einigen Modellen Probleme verursachen kann. Wenn Probleme auftreten, versuchen Sie, sie zu deaktivieren.",
  "seedUncheckedHint": "Zufalls-Seed",
  "ropeFrequencyBaseUncheckedHint": "Automatisch",
  "ropeFrequencyScaleUncheckedHint": "Automatisch"
}