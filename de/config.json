{
  "noInstanceSelected": "Keine Modellinstanz ausgewählt",
  "resetToDefault": "Zurücksetzen",
  "showAdvancedSettings": "Erweiterte Einstellungen anzeigen",
  "showAll": "Alle anzeigen",
  "basicSettings": "Standard",
  "configSubtitle": "Laden oder speichern Sie Voreinstellungen und experimentieren Sie mit Modellparameterüberschreibungen",
  "inferenceParameters/title": "Vorhersageparameter",
  "inferenceParameters/info": "Experimentieren Sie mit Parametern, die die Vorhersage beeinflussen.",
  "generalParameters/title": "Allgemein",
  "samplingParameters/title": "Sampling",
  "basicTab": "Standard",
  "advancedTab": "Erweitert",
  "loadInstanceFirst": "Laden Sie ein Modell, um konfigurierbare Parameter anzuzeigen",
  "generationParameters/info": "Experimentieren Sie mit grundlegenden Parametern, die die Textgenerierung beeinflussen.",
  "loadParameters/title": "Parameter laden",
  "loadParameters/description": "Das Ändern dieser Parameter erfordert ein erneutes Laden des Modells",
  "loadParameters/reload": "Neu laden, um Änderungen der Ladeparameter zu übernehmen",
  "discardChanges": "Änderungen verwerfen",
  "llm.prediction.systemPrompt/title": "Richtlinien für die KI",
  "llm.prediction.systemPrompt/description": "Verwenden Sie dieses Feld, um dem Modell Hintergrundanweisungen zu geben, z. B. eine Reihe von Regeln, Einschränkungen oder allgemeinen Anforderungen. Dieses Feld wird oft auch als \"System Prompt\" bezeichnet.",
  "llm.prediction.temperature/title": "Temperatur",
  "llm.prediction.temperature/info": "Aus den Hilfedokumenten von llama.cpp: \"Der Standardwert ist <{{dynamicValue}}>, der ein Gleichgewicht zwischen Zufälligkeit und Determinismus bietet. Im Extremfall wählt eine Temperatur von 0 immer das wahrscheinlichste nächste Token, was zu identischen Ausgaben in jedem Durchlauf führt\"",
  "llm.prediction.llama.topKSampling/title": "Top-K-Sampling",
  "llm.prediction.llama.topKSampling/info": "Aus den Hilfedokumenten von llama.cpp:\n\nTop-k-Sampling ist eine Textgenerierungsmethode, die das nächste Token nur aus den Top-k wahrscheinlichsten Token auswählt, die vom Modell vorhergesagt wurden.\n\nEs hilft, das Risiko zu reduzieren, Token mit geringer Wahrscheinlichkeit oder unsinnige Token zu generieren, kann aber auch die Vielfalt der Ausgabe einschränken.\n\nEin höherer Wert für Top-k (z. B. 100) berücksichtigt mehr Token und führt zu einem vielfältigeren Text, während ein niedrigerer Wert (z. B. 10) sich auf die wahrscheinlichsten Token konzentriert und einen konservativeren Text generiert.\n\n• Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU-Threads",
  "llm.prediction.llama.cpuThreads/info": "Die Anzahl der Threads, die während der Berechnung verwendet werden sollen. Eine Erhöhung der Anzahl der Threads korreliert nicht immer mit einer besseren Leistung. Der Standardwert ist <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Antwortlänge begrenzen",
  "llm.prediction.maxPredictedTokens/info": "Steuern Sie die maximale Länge der Antwort des Chatbots. Aktivieren Sie diese Option, um eine Beschränkung der maximalen Länge einer Antwort festzulegen, oder deaktivieren Sie sie, um den Chatbot entscheiden zu lassen, wann er stoppen soll.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximale Antwortlänge (Tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Ungefähr {{maxWords}} Wörter",
  "llm.prediction.llama.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.llama.repeatPenalty/info": "Aus den Hilfedokumenten von llama.cpp: \"Hilft zu verhindern, dass das Modell sich wiederholenden oder monotonen Text generiert.\n\nEin höherer Wert (z. B. 1.5) bestraft Wiederholungen stärker, während ein niedrigerer Wert (z. B. 0.9) nachsichtiger ist.\" • Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Min-P-Sampling",
  "llm.prediction.llama.minPSampling/info": "Aus den Hilfedokumenten von llama.cpp:\n\nDie minimale Wahrscheinlichkeit für ein Token, das berücksichtigt werden soll, relativ zur Wahrscheinlichkeit des wahrscheinlichsten Tokens. Muss in [0, 1] liegen.\n\n• Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top-P-Sampling",
  "llm.prediction.llama.topPSampling/info": "Aus den Hilfedokumenten von llama.cpp:\n\nTop-p-Sampling, auch bekannt als Nucleus-Sampling, ist eine weitere Textgenerierungsmethode, die das nächste Token aus einer Teilmenge von Token auswählt, die zusammen eine kumulative Wahrscheinlichkeit von mindestens p haben.\n\nDiese Methode bietet ein Gleichgewicht zwischen Vielfalt und Qualität, indem sie sowohl die Wahrscheinlichkeiten der Token als auch die Anzahl der zu sammelnden Token berücksichtigt.\n\nEin höherer Wert für Top-p (z. B. 0.95) führt zu einem vielfältigeren Text, während ein niedrigerer Wert (z. B. 0.5) einen fokussierteren und konservativeren Text generiert. Muss in (0, 1] liegen.\n\n• Der Standardwert ist <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stopp-Strings",
  "llm.prediction.stopStrings/info": "Spezifische Zeichenfolgen, die, wenn sie gefunden werden, das Modell daran hindern, weitere Token zu generieren",
  "llm.prediction.stopStrings/placeholder": "Geben Sie eine Zeichenfolge ein und drücken Sie ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Konversationsüberlauf",
  "llm.prediction.contextOverflowPolicy/info": "Entscheiden Sie, was zu tun ist, wenn die Konversation die Größe des Arbeitsspeichers des Modells ('Kontext') überschreitet",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Bei Limit stoppen",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Beenden Sie die Generierung, sobald der Speicher des Modells voll ist",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Mitte kürzen",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Entfernt Nachrichten aus der Mitte der Konversation, um Platz für neuere zu schaffen. Das Modell wird sich trotzdem an den Anfang der Konversation erinnern",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Rollendes Fenster",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Das Modell erhält immer die letzten paar Nachrichten, kann aber den Anfang der Konversation vergessen",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency Penalty",
  "llm.prediction.llama.presencePenalty/title": "Presence Penalty",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.mlx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.mlx.repeatPenalty/info": "Ein höherer Wert hält das Modell davon ab, sich zu wiederholen",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Strukturierte Ausgabe",
  "llm.prediction.structured/info": "Strukturierte Ausgabe",
  "llm.load.contextLength/title": "Kontextlänge",
  "llm.load.contextLength/info": "Gibt die maximale Anzahl von Token an, die das Modell gleichzeitig berücksichtigen kann, was sich darauf auswirkt, wie viel Kontext es während der Verarbeitung behält",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/info": "Zufälliger Seed: Legt den Seed für die Zufallszahlengenerierung fest, um reproduzierbare Ergebnisse zu gewährleisten",
  "llm.load.llama.evalBatchSize/title": "Evaluierungs-Batch-Größe",
  "llm.load.llama.evalBatchSize/info": "Legt die Anzahl der Beispiele fest, die während der Evaluierung in einem Batch zusammen verarbeitet werden, was sich auf die Geschwindigkeit und die Speichernutzung auswirkt",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE-Frequenzbasis",
  "llm.load.llama.ropeFrequencyBase/info": "[Erweitert] Passt die Basisfrequenz für die Rotary Positional Encoding an, was sich darauf auswirkt, wie Positionsinformationen eingebettet werden",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE-Frequenzskala",
  "llm.load.llama.ropeFrequencyScale/info": "[Erweitert] Ändert die Skalierung der Frequenz für die Rotary Positional Encoding, um die Granularität der Positionskodierung zu steuern",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU-Offload",
  "llm.load.llama.acceleration.offloadRatio/info": "Legen Sie das Verhältnis der Berechnungen fest, die auf die GPU ausgelagert werden sollen. Stellen Sie auf \"Aus\" ein, um das GPU-Offloading zu deaktivieren, oder auf \"Auto\", um das Modell entscheiden zu lassen.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Beschleunigt Aufmerksamkeitsmechanismen für eine schnellere und effizientere Verarbeitung",
  "llm.load.llama.keepModelInMemory/title": "Modell im Speicher behalten",
  "llm.load.llama.keepModelInMemory/info": "Verhindert, dass das Modell auf die Festplatte ausgelagert wird, was einen schnelleren Zugriff auf Kosten einer höheren RAM-Nutzung gewährleistet",
  "llm.load.llama.useFp16ForKVCache/title": "FP16 für KV-Cache verwenden",
  "llm.load.llama.useFp16ForKVCache/info": "Reduziert die Speichernutzung, indem der Cache in halber Genauigkeit (FP16) gespeichert wird",
  "llm.load.llama.tryMmap/title": "Versuchen Sie mmap()",
  "llm.load.llama.tryMmap/info": "Modelldateien direkt von der Festplatte in den Speicher laden"
}
