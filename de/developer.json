{
  "tabs/server": "Lokaler Server",
  "tabs/extensions": "LM-Laufzeiten",
  "loadSettings/title": "Einstellungen laden",
  "modelSettings/placeholder": "W√§hlen Sie ein Modell aus, um es zu konfigurieren",

  "loadedModels/noModels": "Keine Modelle geladen",

  "serverOptions/title": "Serveroptionen",
  "serverOptions/configurableTitle": "Konfigurierbare Optionen",
  "serverOptions/port/hint": "Legen Sie fest, welchen Netzwerkport der lokale Server verwenden soll. Standardm√§√üig verwendet LM Studio den Port 1234. Sie m√ºssen diesen m√∂glicherweise √§ndern, wenn der Port bereits verwendet wird.",
  "serverOptions/port/subtitle": "Der Port, auf dem gelauscht wird",
  "serverOptions/autostart/title": "Server automatisch starten",
  "serverOptions/autostart/hint": "Starten Sie den lokalen LLM-Server von LM Studio automatisch beim Start der App oder des Dienstes",
  "serverOptions/port/integerWarning": "Die Portnummer muss eine ganze Zahl sein",
  "serverOptions/port/invalidPortWarning": "Der Port muss zwischen 1 und 65535 liegen",
  "serverOptions/cors/title": "CORS aktivieren",
  "serverOptions/cors/hint1": "Wenn Sie CORS (Cross-Origin Resource Sharing) aktivieren, k√∂nnen Websites, die Sie besuchen, Anfragen an den LM Studio-Server senden.",
  "serverOptions/cors/hint2": "CORS kann erforderlich sein, wenn Anfragen von einer Webseite oder einer VS Code-/anderen Erweiterung gesendet werden.",
  "serverOptions/cors/subtitle": "Cross-Origin-Anfragen zulassen",
  "serverOptions/network/title": "Im lokalen Netzwerk bereitstellen",
  "serverOptions/network/subtitle": "Server f√ºr Ger√§te im Netzwerk verf√ºgbar machen",
  "serverOptions/network/hint1": "Ob Verbindungen von anderen Ger√§ten im Netzwerk zugelassen werden sollen.",
  "serverOptions/network/hint2": "Wenn dies nicht aktiviert ist, lauscht der Server nur auf localhost.",
  "serverOptions/verboseLogging/title": "Ausf√ºhrliche Protokollierung",
  "serverOptions/verboseLogging/subtitle": "Ausf√ºhrliche Protokollierung f√ºr den lokalen Server aktivieren",
  "serverOptions/contentLogging/title": "Prompts und Antworten protokollieren",
  "serverOptions/contentLogging/subtitle": "Einstellungen f√ºr die lokale Anfrage-/Antwortprotokollierung",
  "serverOptions/contentLogging/hint": "Ob Prompts und/oder die Antwort in der lokalen Serverprotokolldatei protokolliert werden sollen.",
  "serverOptions/fileLoggingMode/title": "Dateiprotokollierungsmodus",
  "serverOptions/fileLoggingMode/off/title": "AUS",
  "serverOptions/fileLoggingMode/off/hint": "Keine Protokolldateien erstellen",
  "serverOptions/fileLoggingMode/succinct/title": "Pr√§gnant",
  "serverOptions/fileLoggingMode/succinct/hint": "Protokollieren Sie denselben Inhalt wie in der Konsole. Lange Anfragen werden gek√ºrzt.",
  "serverOptions/fileLoggingMode/full/title": "Vollst√§ndig",
  "serverOptions/fileLoggingMode/full/hint": "Lange Anfragen nicht k√ºrzen.",
  "serverOptions/jitModelLoading/title": "Just-in-Time-Modellladen",
  "serverOptions/jitModelLoading/hint": "Wenn aktiviert und eine Anfrage ein nicht geladenes Modell angibt, wird es automatisch geladen und verwendet. Au√üerdem enth√§lt der Endpunkt \"/v1/models\" auch Modelle, die noch nicht geladen sind.",
  "serverOptions/loadModel/error": "Modell konnte nicht geladen werden",
  "serverOptions/jitModelLoadingTTL/title": "Nicht verwendete JIT-geladene Modelle automatisch entladen",
  "serverOptions/jitModelLoadingTTL/hint": "Ein Modell, das Just-in-Time (JIT) geladen wurde, um eine API-Anfrage zu bedienen, wird automatisch entladen, nachdem es f√ºr eine bestimmte Dauer (TTL) nicht verwendet wurde.",
  "serverOptions/jitModelLoadingTTL/ttl/label": "Maximale Leerlauf-TTL",
  "serverOptions/jitModelLoadingTTL/ttl/unit": "Minuten",
  "serverOptions/unloadPreviousJITModelOnLoad/title": "Nur das letzte JIT-geladene Modell behalten",
  "serverOptions/unloadPreviousJITModelOnLoad/hint": "Stellen Sie sicher, dass zu jedem Zeitpunkt h√∂chstens 1 Modell √ºber JIT geladen ist (entl√§dt das vorherige Modell)",

  "serverLogs/scrollToBottom": "Zum Ende springen",
  "serverLogs/clearLogs": "Protokolle l√∂schen ({{shortcut}})",
  "serverLogs/openLogsFolder": "Serverprotokollordner √∂ffnen",

  "runtimeSettings/title": "Laufzeiteinstellungen",
  "runtimeSettings/chooseRuntime/title": "Standardauswahl",
  "runtimeSettings/chooseRuntime/description": "W√§hlen Sie eine Standardlaufzeit f√ºr jedes Modellformat aus",
  "runtimeSettings/chooseRuntime/showAllVersions/label": "Alle Laufzeiten anzeigen",
  "runtimeSettings/chooseRuntime/showAllVersions/hint": "Standardm√§√üig zeigt LM Studio nur die neueste Version jeder kompatiblen Laufzeit an. Aktivieren Sie diese Option, um alle verf√ºgbaren Laufzeiten anzuzeigen.",
  "runtimeSettings/chooseRuntime/select/placeholder": "W√§hlen Sie eine Laufzeit aus",

  "runtimeOptions/uninstall": "Deinstallieren",
  "runtimeOptions/uninstallDialog/title": "{{runtimeName}} deinstallieren?",
  "runtimeOptions/uninstallDialog/body": "Durch das Deinstallieren dieser Laufzeit wird sie vom System entfernt. Diese Aktion ist unwiderruflich.",
  "runtimeOptions/uninstallDialog/body/caveats": "Einige Dateien k√∂nnen m√∂glicherweise erst nach einem Neustart von LM Studio entfernt werden.",
  "runtimeOptions/uninstallDialog/error": "Laufzeit konnte nicht deinstalliert werden",
  "runtimeOptions/uninstallDialog/confirm": "Fortfahren und deinstallieren",
  "runtimeOptions/uninstallDialog/cancel": "Abbrechen",
  "runtimeOptions/noCompatibleRuntimes": "Keine kompatiblen Laufzeiten gefunden",
  "runtimeOptions/downloadIncompatibleRuntime": "Diese Laufzeit wurde als inkompatibel mit Ihrem Computer eingestuft. Sie wird h√∂chstwahrscheinlich nicht funktionieren.",
  "runtimeOptions/noRuntimes": "Keine Laufzeiten gefunden",

  "runtimes": {
    "manageLMRuntimes": "LM-Laufzeiten verwalten",
    "includeOlderRuntimeVersions": "√Ñltere Laufzeitversionen einschlie√üen",
    "dismiss": "Verwerfen",
    "updateAvailableToast": {
      "title": "LM-Laufzeit-Update verf√ºgbar!"
    },
    "updatedToast": {
      "title": " ‚úÖ LM-Laufzeit aktualisiert: {{runtime}} ‚Üí v{{version}}",
      "preferencesUpdated": "Neu geladene {{compatibilityTypes}}-Modelle verwenden die aktualisierte Laufzeit."
    },
    "noAvx2ErrorMessage": "Alle LM-Laufzeiten erfordern derzeit eine CPU mit AVX2-Unterst√ºtzung",
    "downloadableRuntimes": {
      "runtimeExtensionPacks": "Laufzeit-Erweiterungspakete",
      "refresh": "Aktualisieren",
      "refreshing": "Wird aktualisiert...",
      "filterSegment": {
        "compatibleOnly": "Nur kompatibel",
        "all": "Alle"
      },
      "card": {
        "releaseNotes": "Versionshinweise",
        "latestVersionInstalled": "Neueste Version installiert",
        "updateAvailable": "Update verf√ºgbar"
      }
    },
    "installedRuntimes": {
      "manage": {
        "title": "Aktive Laufzeiten verwalten"
      },
      "dropdownOptions": {
        "installedVersions": "Versionen verwalten",
        "close": "Schlie√üen"
      },
      "tabs": {
        "all": "Alle",
        "frameworks": "Meine Frameworks",
        "engines": "Meine Engines"
      },
      "detailsModal": {
        "installedVersions": "Installierte Versionen f√ºr {{runtimeName}}",
        "manifestJsonTitle": "Manifest-JSON (erweitert)",
        "releaseNotesTitle": "Versionshinweise",
        "noReleaseNotes": "Keine Versionshinweise f√ºr diese Version verf√ºgbar",
        "back": "Zur√ºck",
        "close": "Schlie√üen"
      },
      "noEngines": "Keine Engines installiert",
      "noFrameworks": "Keine Frameworks installiert"
    }
  },

  "inferenceParams/noParams": "F√ºr diesen Modelltyp sind keine konfigurierbaren Inferenzparameter verf√ºgbar",

  "quickDocs": {
    "tabChipTitle": "Kurzdokumentation",
    "newToolUsePopover": "Codeausschnitte sind jetzt hier in der \"Kurzdokumentation\" verf√ºgbar. Klicken Sie hier, um mit der Toolverwendung zu beginnen!",
    "newToolUsePopoverTitle": "üìö Kurzdokumentation",
    "learnMore": "‚ÑπÔ∏è üëæ Um mehr √ºber die lokalen Serverendpunkte von LM Studio zu erfahren, besuchen Sie die [Dokumentation](https://lmstudio.ai/docs).",
    "helloWorld": {
      "title": "Hallo Welt!"
    },
    "chat": {
      "title": "Chat"
    },
    "structuredOutput": {
      "title": "Strukturierte Ausgabe"
    },
    "imageInput": {
      "title": "Bildeingabe"
    },
    "embeddings": {
      "title": "Einbettungen"
    },
    "toolUse": {
      "title": "Toolverwendung",
      "tab": {
        "saveAsPythonFile": "Als Python-Datei speichern",
        "runTheScript": "F√ºhren Sie das Skript aus:",
        "savePythonFileCopyPaste": "Als Python-Datei speichern f√ºr einen Kopier- und Einf√ºgebefehl"
      }
    },
    "newBadge": "Neu"
  },

  "endpoints/openaiCompatRest/title": "Unterst√ºtzte Endpunkte (OpenAI-√§hnlich)",
  "endpoints/openaiCompatRest/getModels": "Die aktuell geladenen Modelle auflisten",
  "endpoints/openaiCompatRest/postCompletions": "Textvervollst√§ndigungsmodus. Sagen Sie das/die n√§chste(n) Token(s) f√ºr einen gegebenen Prompt voraus. Hinweis: OpenAI betrachtet diesen Endpunkt als 'veraltet'.",
  "endpoints/openaiCompatRest/postChatCompletions": "Chat-Vervollst√§ndigungen. Senden Sie einen Chatverlauf an das Modell, um die n√§chste Assistentenantwort vorherzusagen",
  "endpoints/openaiCompatRest/postEmbeddings": "Texteinbettung. Generieren Sie Texteinbettungen f√ºr eine gegebene Texteingabe. Akzeptiert eine Zeichenfolge oder ein Array von Zeichenfolgen.",

  "model.createVirtualModelFromInstance": "Einstellungen als neues virtuelles Modell speichern",
  "model.createVirtualModelFromInstance/error": "Einstellungen konnten nicht als neues virtuelles Modell gespeichert werden",

  "model": {
    "toolUseSectionTitle": "Toolverwendung",
    "toolUseDescription": "Dieses Modell wurde erkannt, um f√ºr die Toolverwendung trainiert worden zu sein\n\n√ñffnen Sie die <custom-link>Kurzdokumentation</custom-link> f√ºr weitere Informationen"
  },

  "apiConfigOptions/title": "API-Konfiguration"
}
