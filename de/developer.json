{
  "tabs/server": "Lokaler Server",
  "tabs/extensions": "LM-Laufzeiten",
  "loadSettings/title": "Einstellungen laden",
  "modelSettings/placeholder": "Wählen Sie ein Modell aus, um es zu konfigurieren",

  "loadedModels/noModels": "Keine Modelle geladen",

  "serverOptions/title": "Serveroptionen",
  "serverOptions/configurableTitle": "Konfigurierbare Optionen",
  "serverOptions/port/hint": "Legen Sie fest, welchen Netzwerkport der lokale Server verwenden soll. Standardmäßig verwendet LM Studio den Port 1234. Sie müssen diesen möglicherweise ändern, wenn der Port bereits verwendet wird.",
  "serverOptions/port/subtitle": "Der Port, auf dem gelauscht wird",
  "serverOptions/autostart/title": "Server automatisch starten",
  "serverOptions/autostart/hint": "Starten Sie den lokalen Server automatisch, wenn ein Modell geladen wird",
  "serverOptions/port/integerWarning": "Die Portnummer muss eine ganze Zahl sein",
  "serverOptions/port/invalidPortWarning": "Der Port muss zwischen 1 und 65535 liegen",
  "serverOptions/cors/title": "CORS aktivieren",
  "serverOptions/cors/hint1": "Wenn Sie CORS (Cross-Origin Resource Sharing) aktivieren, können Websites, die Sie besuchen, Anfragen an den LM Studio-Server senden.",
  "serverOptions/cors/hint2": "CORS könnte erforderlich sein, wenn Anfragen von einer Webseite oder von VS Code / anderen Erweiterungen gesendet werden.",
  "serverOptions/cors/subtitle": "Cross-Origin-Anfragen zulassen",
  "serverOptions/network/title": "Im lokalen Netzwerk bereitstellen",
  "serverOptions/network/subtitle": "Server für Geräte im Netzwerk verfügbar machen",
  "serverOptions/network/hint1": "Ob Verbindungen von anderen Geräten im Netzwerk zugelassen werden sollen.",
  "serverOptions/network/hint2": "Wenn dies nicht aktiviert ist, lauscht der Server nur auf localhost.",
  "serverOptions/verboseLogging/title": "Detaillierte Protokollierung",
  "serverOptions/verboseLogging/subtitle": "Detaillierte Protokollierung für den lokalen Server aktivieren",
  "serverOptions/contentLogging/title": "Protokollierung von Prompts und Antworten",
  "serverOptions/contentLogging/subtitle": "Lokale Anfrage-/Antwortprotokollierungseinstellungen",
  "serverOptions/contentLogging/hint": "Gibt an, ob Prompts und/oder die Antwort in der lokalen Server-Protokolldatei protokolliert werden sollen.",
  "serverOptions/loadModel/error": "Fehler beim Laden des Modells",

  "serverLogs/scrollToBottom": "Zum Ende springen",
  "serverLogs/clearLogs": "Protokolle löschen ({{shortcut}})",
  "serverLogs/openLogsFolder": "Serverprotokollordner öffnen",

  "runtimeSettings/title": "Laufzeiteinstellungen",
  "runtimeSettings/chooseRuntime/title": "Laufzeiten konfigurieren",
  "runtimeSettings/chooseRuntime/description": "Wählen Sie eine Laufzeit für jedes Modellformat aus",
  "runtimeSettings/chooseRuntime/showAllVersions/label": "Alle Laufzeiten anzeigen",
  "runtimeSettings/chooseRuntime/showAllVersions/hint": "Standardmäßig zeigt LM Studio nur die neueste Version jeder kompatiblen Laufzeit an. Aktivieren Sie diese Option, um alle verfügbaren Laufzeiten zu sehen.",
  "runtimeSettings/chooseRuntime/select/placeholder": "Wählen Sie eine Laufzeit",

  "runtimeOptions/uninstall": "Deinstallieren",
  "runtimeOptions/uninstallDialog/title": "{{runtimeName}} deinstallieren?",
  "runtimeOptions/uninstallDialog/body": "Durch das Deinstallieren dieser Laufzeit wird sie vom System entfernt. Diese Aktion ist unwiderruflich.",
  "runtimeOptions/uninstallDialog/body/caveats": "Einige Dateien können erst entfernt werden, nachdem LM Studio neu gestartet wurde.",
  "runtimeOptions/uninstallDialog/error": "Fehler beim Deinstallieren der Laufzeit",
  "runtimeOptions/uninstallDialog/confirm": "Fortfahren und deinstallieren",
  "runtimeOptions/uninstallDialog/cancel": "Abbrechen",
  "runtimeOptions/noCompatibleRuntimes": "Keine kompatiblen Laufzeiten gefunden",
  "runtimeOptions/downloadIncompatibleRuntime": "Diese Laufzeit wurde als nicht mit Ihrem Computer kompatibel ermittelt. Sie wird höchstwahrscheinlich nicht funktionieren.",
  "runtimeOptions/noRuntimes": "Keine Laufzeiten gefunden",

  "inferenceParams/noParams": "Keine konfigurierbaren Inferenzparameter für diesen Modelltyp verfügbar",

  "endpoints/openaiCompatRest/title": "Unterstützte Endpunkte (OpenAI-ähnlich)",
  "endpoints/openaiCompatRest/getModels": "Liste der derzeit geladenen Modelle",
  "endpoints/openaiCompatRest/postCompletions": "Textvervollständigungsmodus. Sagen Sie das nächste Token voraus, das zu einer Eingabe passt. Hinweis: OpenAI betrachtet diesen Endpunkt als 'veraltet'.",
  "endpoints/openaiCompatRest/postChatCompletions": "Chat-Vervollständigungen. Senden Sie dem Modell einen Chatverlauf, um die nächste Antwort des Assistenten vorherzusagen",
  "endpoints/openaiCompatRest/postEmbeddings": "Texteinbettung. Generieren Sie Texteinbettungen für eine gegebene Texteingabe. Akzeptiert eine Zeichenkette oder ein Array von Zeichenketten.",

  "model.createVirtualModelFromInstance": "Einstellungen als neues virtuelles Modell speichern",
  "model.createVirtualModelFromInstance/error": "Fehler beim Speichern der Einstellungen als neues virtuelles Modell",

  "apiConfigOptions/title": "API-Konfiguration"
}