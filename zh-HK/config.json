{
  "noInstanceSelected": "未選擇模型實例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "顯示高級設定",
  "showAll": "所有",
  "basicSettings": "基本",
  "configSubtitle": "加載或保存預設，並實驗模型參數覆蓋",
  "inferenceParameters/title": "推理參數",
  "inferenceParameters/info": "實驗影響推理的參數。",
  "generalParameters/title": "通用",
  "samplingParameters/title": "取樣",
  "basicTab": "基礎",
  "advancedTab": "高級",
  "advancedTab/title": "🧪 高級配置",
  "advancedTab/expandAll": "展開全部",
  "advancedTab/overridesTitle": "配置覆蓋",
  "advancedTab/noConfigsText": "沒有未保存的更改 - 編輯上面的值以在這裡查看覆蓋。",
  "loadInstanceFirst": "加載模型以檢視可配置參數",
  "noListedConfigs": "無可配置參數",
  "generationParameters/info": "實驗影響文本生成的基本參數。",
  "loadParameters/title": "加載參數",
  "loadParameters/description": "控制模型初始化和加載到內存的方式。",
  "loadParameters/reload": "重新加載以應用更改",
  "discardChanges": "放棄更改",
  "llm.prediction.systemPrompt/title": "系統提示詞",
  "llm.prediction.systemPrompt/description": "使用此字段為模型提供背景指示，例如一組規則、約束或一般要求。此字段通常也被稱為“系統提示詞”。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "溫度",
  "llm.prediction.temperature/subTitle": "引入多少隨機性。0 將每次生成相同的結果，而較高的值將增加創造性和多樣性",
  "llm.prediction.temperature/info": "來自 llama.cpp 幫助文檔：“默認值為 <{{dynamicValue}}>，在隨機性和確定性之間提供平衡。在極端情況下，溫度為 0 時總是會選擇最有可能的下一個令牌，導致每次運行都生成相同的輸出”",
  "llm.prediction.llama.sampling/title": "取樣",
  "llm.prediction.llama.topKSampling/title": "Top K 取樣",
  "llm.prediction.llama.topKSampling/subTitle": "將下一個令牌限制為 top-k 最可能的令牌之一。其作用類似於溫度",
  "llm.prediction.llama.topKSampling/info": "來自 llama.cpp 幫助文檔：\n\nTop-k 取樣是一種文本生成方法，僅從模型預測的 top k 最可能的令牌中選擇下一個令牌。\n\n它有助於減少生成低概率或無意義令牌的風險，但也可能限制輸出的多樣性。\n\n較高的 top-k 值（例如 100）將考慮更多令牌，並導致更具多樣性的文本，而較低的值（例如 10）將專注於最可能的令牌，並生成更保守的文本。\n\n• 默認值為 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 線程數",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期間使用的 CPU 線程數",
  "llm.prediction.llama.cpuThreads/info": "計算期間使用的線程數。增加線程數並不總是能與性能提升成正比。默認值為 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制回應長度",
  "llm.prediction.maxPredictedTokens/subTitle": "可選擇設置 AI 回應的最大長度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天機器人的回應最大長度。啟用此選項以設置回應的最大長度，或關閉此選項以讓聊天機器人決定何時停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（令牌）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 字",
  "llm.prediction.llama.repeatPenalty/title": "重複懲罰",
  "llm.prediction.llama.repeatPenalty/subTitle": "限制生成重複令牌的頻率",
  "llm.prediction.llama.repeatPenalty/info": "來自 llama.cpp 幫助文檔：“有助於防止模型生成重複或單調的文本。\n\n較高的值（例如 1.5）將更強烈地懲罰重複，而較低的值（例如 0.9）則更寬容。” • 默認值為 <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "最小 P 取樣",
  "llm.prediction.llama.minPSampling/subTitle": "選擇輸出令牌的最小基本概率",
  "llm.prediction.llama.minPSampling/info": "來自 llama.cpp 幫助文檔：\n\n選擇令牌的最小概率，相對於最可能令牌的概率。必須在 [0, 1] 之間。\n\n• 默認值為 <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P 取樣",
  "llm.prediction.llama.topPSampling/subTitle": "可能的下一個令牌的最小累計概率。其作用類似於溫度",
  "llm.prediction.llama.topPSampling/info": "來自 llama.cpp 幫助文檔：\n\nTop-p 取樣，也稱為核取樣，是另一種文本生成方法，從一組累計概率至少為 p 的令牌中選擇下一個令牌。\n\n這種方法通過同時考慮令牌的概率和要取樣的令牌數量，在多樣性和質量之間提供平衡。\n\n較高的 top-p 值（例如 0.95）將導致更具多樣性的文本，而較低的值（例如 0.5）將生成更專注和保守的文本。必須在 (0, 1] 之間。\n\n• 默認值為 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "應停止模型生成更多令牌的字符串",
  "llm.prediction.stopStrings/info": "特定字符串，當遇到時將停止模型生成更多的令牌",
  "llm.prediction.stopStrings/placeholder": "輸入字符串並按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "對話溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "當對話超出模型能夠處理的大小時，模型應如何表現",
  "llm.prediction.contextOverflowPolicy/info": "決定當對話超出模型的工作內存（“上下文”）時應如何處理",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "達到限制時停止",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "一旦模型的內存滿了就停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "截斷中間",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "從對話中間刪除消息以騰出空間給新消息。模型仍會記住對話的開頭",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滾動窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型將始終獲取最近的幾條消息，但可能會忘記對話的開頭",
  "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
  "llm.prediction.llama.presencePenalty/title": "存在懲罰",
  "llm.prediction.llama.tailFreeSampling/title": "尾部自由取樣",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型取樣",
  "llm.prediction.mlx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.mlx.repeatPenalty/subTitle": "限制生成重複令牌的頻率",
  "llm.prediction.mlx.repeatPenalty/info": "較高的值會限制模型自我重複的行為",
  "llm.prediction.onnx.topKSampling/title": "Top K 取樣",
  "llm.prediction.onnx.topKSampling/subTitle": "將下一個令牌限制為 top-k 最可能的令牌之一。其作用類似於溫度",
  "llm.prediction.onnx.topKSampling/info": "來自 ONNX 文檔：\n\n保留 top-k 最高概率的詞彙令牌數量進行篩選。\n\n• 此篩選器默認關閉",
  "llm.prediction.onnx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.onnx.repeatPenalty/subTitle": "限制生成重複令牌的頻率",
  "llm.prediction.onnx.repeatPenalty/info": "較高的值會限制模型自我重複的行為",
  "llm.prediction.onnx.topPSampling/title": "Top P 取樣",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一個令牌的最小累計概率。其作用類似於溫度",
  "llm.prediction.onnx.topPSampling/info": "來自 ONNX 文檔：\n\n僅保留概率累計到 TopP 或更高的最可能令牌進行生成。\n\n• 此篩選器默認關閉",
  "llm.prediction.seed/title": "隨機種子",
  "llm.prediction.structured/title": "結構化輸出",
  "llm.prediction.structured/info": "結構化輸出",
  "llm.prediction.promptTemplate/title": "提示詞模板",
  "llm.prediction.promptTemplate/subTitle": "消息發送給模型的格式。更改此內容可能會引發意外行為 - 確保你知道自己在做什麼！",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Jinja 模板解析失敗：{{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手動",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "系統前綴",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "輸入系統前綴...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "系統後綴",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "輸入系統後綴...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "用戶前綴",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "輸入用戶前綴...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "用戶後綴",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "輸入用戶後綴...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "助手前綴",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "輸入助手前綴...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "助手後綴",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "輸入助手後綴...",
  "llm.prediction.promptTemplate.stopStrings/label": "額外停止字符串",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "除了用戶指定的停止字符串外，還將使用與模板相關的特定停止字符串。",
  "llm.load.contextLength/title": "上下文長度",
  "llm.load.contextLength/subTitle": "模型一次可以處理的最大令牌數。查看'推理參數'中的對話溢出選項以獲取更多方法",
  "llm.load.contextLength/info": "指定模型一次可以考慮的最大令牌數，影響其在處理期間保留的上下文",
  "llm.load.seed/title": "隨機種子",
  "llm.load.seed/subTitle": "文本生成中使用的隨機數生成器的種子。-1 表示隨機",
  "llm.load.seed/info": "隨機種子：設置隨機數生成器的種子以確保結果可重現",
  "llm.load.llama.evalBatchSize/title": "評估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次處理的輸入令牌數量。增加此值會提升性能，但也會增加內存使用量",
  "llm.load.llama.evalBatchSize/info": "設置評估期間一次處理的示例數量，影響速度和內存使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 基頻",
  "llm.load.llama.ropeFrequencyBase/subTitle": "自定義旋轉位置編碼（RoPE）的基頻。增加此值可能會提高長上下文下的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高級] 調整旋轉位置編碼的基頻，影響位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 頻率縮放",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此因子縮放以使用 RoPE 擴展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高級] 修改旋轉位置編碼頻率的縮放，控制位置編碼的粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的離散模型層數",
  "llm.load.llama.acceleration.offloadRatio/info": "設置要卸載到 GPU 的層數。",
  "llm.load.llama.flashAttention/title": "閃存注意力",
  "llm.load.llama.flashAttention/subTitle": "減少某些模型的內存使用和生成時間",
  "llm.load.llama.flashAttention/info": "加速注意力機制以實現更快、更高效的處理",
  "llm.load.llama.keepModelInMemory/title": "保持模型在內存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使模型已卸載到 GPU，仍為模型保留系統內存。提高性能，但需要更多的系統 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交換到磁盤，確保更快的訪問速度，代價是更高的 RAM 使用量",
  "llm.load.llama.useFp16ForKVCache/title": "為 KV 緩存使用 FP16",
  "llm.load.llama.useFp16ForKVCache/info": "通過以半精度（FP16）存儲緩存來減少內存使用",
  "llm.load.llama.tryMmap/title": "嘗試 mmap()",
  "llm.load.llama.tryMmap/subTitle": "提高模型加載時間。禁用此選項可能會提高性能，特別是當模型大於可用系統 RAM 時",
  "llm.load.llama.tryMmap/info": "直接從磁盤加載模型文件到內存",
  "embedding.load.contextLength/title": "上下文長度",
  "embedding.load.contextLength/subTitle": "模型一次可以處理的最大令牌數。查看'推理參數'中的對話溢出選項以獲取更多方法",
  "embedding.load.contextLength/info": "指定模型一次可以考慮的最大令牌數，影響其在處理期間保留的上下文",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 基頻",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "自定義旋轉位置編碼（RoPE）的基頻。增加此值可能會提高長上下文下的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高級] 調整旋轉位置編碼的基頻，影響位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "評估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次處理的輸入令牌數量。增加此值會提升性能，但也會增加內存使用量",
  "embedding.load.llama.evalBatchSize/info": "設置評估期間一次處理的令牌數量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 頻率縮放",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此因子縮放以使用 RoPE 擴展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高級] 修改旋轉位置編碼頻率的縮放，控制位置編碼的粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用于 GPU 加速的離散模型層數",
  "embedding.load.llama.acceleration.offloadRatio/info": "設置要卸載到 GPU 的層數。",
  "embedding.load.llama.keepModelInMemory/title": "保持模型在內存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使模型已卸載到 GPU，仍為模型保留系統內存。提高性能，但需要更多的系統 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交換到磁盤，確保更快的訪問速度，代價是更高的 RAM 使用量",
  "embedding.load.llama.tryMmap/title": "嘗試 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "提高模型加載時間。禁用此選項可能會提高性能，特別是當模型大於可用系統 RAM 時",
  "embedding.load.llama.tryMmap/info": "直接從磁盤加載模型文件到內存",
  "embedding.load.seed/title": "隨機種子",
  "embedding.load.seed/subTitle": "文本生成中使用的隨機數生成器的種子。-1 表示隨機種子",
  "embedding.load.seed/info": "隨機種子：設置隨機數生成器的種子以確保結果可重現",
  "customInputs": {
    "contextLength": {
      "maxValueTooltip": "這是模型訓練時可處理的最大令牌數。點擊將上下文設置為此值",
      "maxValueTextStart": "模型最多支持",
      "maxValueTextEnd": "令牌"
    }
  },
  "flashAttentionWarning": "Flash Attention 是一項實驗性功能，可能會在某些模型上導致問題。如果遇到問題，請嘗試禁用它。"
}
