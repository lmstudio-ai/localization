{
  "noInstanceSelected": "לא נבחר מודל",
  "resetToDefault": "איפוס",
  "showAdvancedSettings": "הצג הגדרות מתקדמות",
  "showAll": "הצג הכל",
  "basicSettings": "בסיסי",
  "configSubtitle": "טען או שמור קביעות מוגדרות מראש וניסוי עם הגדרות פרמטרים של המודל",
  "inferenceParameters/title": "פרמטרים לחיזוי",
  "inferenceParameters/info": "התנסות עם פרמטרים המשפיעים על החיזוי",
  "generalParameters/title": "כללי",
  "samplingParameters/title": "דגימה",
  "basicTab": "בסיסי",
  "advancedTab": "מתקדם",
  "loadInstanceFirst": "טען מודל כדי להציג פרמטרים הניתנים להגדרה",
  "generationParameters/info": "התנסות עם פרמטרים בסיסיים המשפיעים על יצירת טקסט",
  "loadParameters/title": "טען פרמטרים",
  "loadParameters/description": "שינוי פרמטרים אלו דורש טעינה מחדש של המודל",
  "loadParameters/reload": "טען מחדש כדי להחיל את שינויי הפרמטרים",
  "discardChanges": "בטל שינויים",
  "llm.prediction.systemPrompt/title": "הנחיות לבינה המלאכותית",
  "llm.prediction.systemPrompt/description": "השתמש בשדה זה כדי לספק הנחיות רקע למודל, כגון קבוצת כללים, אילוצים או דרישות כלליות. שדה זה מכונה לעתים קרובות \"הנחיית מערכת\".",
  "llm.prediction.temperature/title": "טמפרטורה",
  "llm.prediction.temperature/info": "מתוך מסמכי העזרה של llama.cpp: \"ערך ברירת המחדל הוא <{{dynamicValue}}>, המספק איזון בין אקראיות לדטרמיניזם. במקרה הקיצוני, טמפרטורה של 0 תבחר תמיד באסימון הסביר ביותר הבא, מה שיוביל לפלטים זהים בכל הרצה\"",
  "llm.prediction.topKSampling/title": "דגימת Top K",
  "llm.prediction.topKSampling/info": "מתוך מסמכי העזרה של llama.cpp:\n\nדגימת Top-k היא שיטת יצירת טקסט הבוחרת את האסימון הבא רק מתוך k האסימונים הסבירים ביותר שחזה המודל.\n\nשיטה זו עוזרת להפחית את הסיכון לייצור אסימונים בעלי סבירות נמוכה או חסרי משמעות, אך היא עשויה להגביל גם את מגוון הפלט.\n\nערך גבוה יותר עבור top-k (למשל, 100) ישקול יותר אסימונים ויוביל לטקסט מגוון יותר, בעוד שערך נמוך יותר (למשל, 10) יתמקד באסימונים הסבירים ביותר וייצר טקסט שמרני יותר.\n\n• ערך ברירת המחדל הוא <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "תהליכוני CPU",
  "llm.prediction.llama.cpuThreads/info": "מספר תהליכוני המעבד לשימוש במהלך החישוב. הגדלת מספר התהליכונים לא תמיד מתואמת עם ביצועים טובים יותר. ברירת המחדל היא <{{dynamicValue}}>",
  "llm.prediction.maxPredictedTokens/title": "הגבלת אורך תגובה",
  "llm.prediction.maxPredictedTokens/info": "שליטה באורך המרבי של תגובת הצ'אטבוט. הפעל אפשרות זו כדי להגדיר מגבלה על האורך המרבי של תגובה, או כבה אותה כדי לאפשר לצ'אטבוט להחליט מתי לעצור",
  "llm.prediction.maxPredictedTokens/inputLabel": "אורך תגובה מרבי (אסימונים)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "כ- {{maxWords}} מילים",
  "llm.prediction.repeatPenalty/title": "קנס חזרה",
  "llm.prediction.repeatPenalty/info": "מתוך מסמכי העזרה של llama.cpp: \"עוזר למנוע מהמודל לייצר טקסט חוזר או מונוטוני.\n\nערך גבוה יותר (למשל, 1.5) יקנוס חזרות בצורה חזקה יותר, בעוד שערך נמוך יותר (למשל, 0.9) יהיה סלחני יותר.\" • ערך ברירת המחדל הוא <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "דגימת Min P",
  "llm.prediction.minPSampling/info": "מתוך מסמכי העזרה של llama.cpp:\n\nההסתברות המינימלית שאסימון ייחשב, יחסית להסתברות של האסימון הסביר ביותר. חייב להיות בטווח [0, 1].\n\n• ערך ברירת המחדל הוא <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "דגימת Top P",
  "llm.prediction.topPSampling/info": "מתוך מסמכי העזרה של llama.cpp:\n\nדגימת Top-p, הידועה גם כדגימת גרעין, היא שיטת יצירת טקסט נוספת הבוחרת את האסימון הבא מתת-קבוצה של אסימונים של יחד יש הסתברות מצטברת של לפחות p.\n\nשיטה זו מספקת איזון בין גיוון לאיכות על ידי התחשבות הן בהסתברויות של אסימונים והן במספר האסימונים מהם ניתן לדגום.\n\nערך גבוה יותר עבור top-p (למשל, 0.95) יוביל לטקסט מגוון יותר, בעוד שערך נמוך יותר (למשל, 0.5) ייצר טקסט ממוקד ושמרני יותר. חייב להיות בטווח (0, 1].\n\n• ערך ברירת המחדל הוא <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "הפסק מחרוזות",
  "llm.prediction.stopStrings/info": "מחרוזות ספציפיות שכאשר יתקבלו, יגרמו למודל להפסיק לייצר אסימונים נוספים",
  "llm.prediction.stopStrings/placeholder": "הזן מחרוזת והקש ⏎",
  "llm.prediction.contextOverflowPolicy/title": "מדיניות גלישת שיחה",
  "llm.prediction.contextOverflowPolicy/info": "קבע מה לעשות כאשר השיחה חורגת מגודל זיכרון העבודה של המודל ('הקשר')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "עצור במגבלה",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "הפסק יצירה ברגע שזיכרון המודל מתמלא",
  "customInputs.contextOverflowPolicy.truncateMiddle": "חתוך אמצע",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "הסר הודעות מאמצע השיחה כדי לפנות מקום להודעות חדשות יותר. המודל עדיין יזכור את תחילת השיחה",
  "customInputs.contextOverflowPolicy.rollingWindow": "חלון גלילה",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "המודל תמיד יקבל את ההודעות האחרונות אך עשוי לשכוח את תחילת השיחה",
  "llm.prediction.llama.frequencyPenalty/title": "קנס תדירות",
  "llm.prediction.llama.presencePenalty/title": "קנס נוכחות",
  "llm.prediction.llama.tailFreeSampling/title": "דגימה ללא זנב",
  "llm.prediction.llama.locallyTypicalSampling/title": "דגימה אופיינית מקומית",
  "llm.prediction.seed/title": "זרע",
  "llm.prediction.structured/title": "פלט מובנה",
  "llm.prediction.structured/info": "פלט מובנה",
  "llm.load.contextLength/title": "אורך הקשר",
  "llm.load.contextLength/info": "מציין את המספר המרבי של אסימונים שהמודל יכול לשקול בו-זמנית, מה שמשפיע על כמות ההקשר שהוא שומר במהלך העיבוד",
  "llm.load.seed/title": "זרע",
  "llm.load.seed/info": "זרע אקראי: קובע את הזרע עבור יצירת מספרים אקראיים כדי להבטיח תוצאות לשחזור",
  "llm.load.llama.evalBatchSize/title": "גודל אצווה להערכה",
  "llm.load.llama.evalBatchSize/info": "מגדיר את מספר הדוגמאות המעובדות יחד באצווה אחת במהלך ההערכה, מה שמשפיע על המהירות ועל צריכת הזיכרון",
  "llm.load.llama.ropeFrequencyBase/title": "תדר בסיס RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[מתקדם] מכוון את תדר הבסיס עבור קידוד מיקום סיבובי, מה שמשפיע על אופן הטמעת מידע המיקום",
  "llm.load.llama.ropeFrequencyScale/title": "קנה מידה לתדר RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[מתקדם] משנה את קנה המידה של התדר עבור קידוד מיקום סיבובי כדי לשלוט ברמת הפירוט של קידוד המיקום",
  "llm.load.llama.acceleration.offloadRatio/title": "פריקת GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "הגדר את יחס החישוב לפריקה ל-GPU. הגדר כבוי כדי להשבית פריקה ל-GPU, או אוטומטי כדי לאפשר למודל להחליט.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "מאיץ מנגנוני קשב לעיבוד מהיר ויעיל יותר",
  "llm.load.llama.keepModelInMemory/title": "שמור מודל בזיכרון",
  "llm.load.llama.keepModelInMemory/info": "מונע מהמודל להיות מוחלף לדיסק, מה שמבטיח גישה מהירה יותר אך צורך יותר זיכרון RAM",
  "llm.load.llama.useFp16ForKVCache/title": "השתמש ב-FP16 עבור מטמון מפתחות/ערכים",
  "llm.load.llama.useFp16ForKVCache/info": "מפחית את צריכת הזיכרון על ידי אחסון המטמון בחצי דיוק (FP16)",
  "llm.load.llama.tryMmap/title": "נסה mmap()",
  "llm.load.llama.tryMmap/info": "טען קבצי מודל ישירות מהדיסק לזיכרון"
}
