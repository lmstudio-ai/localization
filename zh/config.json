{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部显示",
  "basicSettings": "基本显示",
  "configSubtitle": "加载或保存预设，并进行模型参数覆盖测试",
  "inferenceParameters/title": "推理参数",
  "inferenceParameters/info": "测试影响推理的参数.",
  "generalParameters/title": "常规",
  "samplingParameters/title": "采样",
  "basicTab": "基本",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开全部",
  "advancedTab/overridesTitle": "配置重写",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上面的值以查看此处的重写值.",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "测试影响文本生成的基本参数.",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "用于控制模型初始化和加载到内存中的方式的设置.",
  "loadParameters/reload": "重新加载以应用更改",
  "discardChanges": "放弃更改",
  "llm.prediction.systemPrompt/title": "系统提示词",
  "llm.prediction.systemPrompt/description": "使用该字段为模型提供背景说明，如一组规则、约束或一般要求. 此字段通常也称为 \"系统提示词\".",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档: \"默认值为 <{{dynamicValue}}>, 在随机性和确定性之间取得平衡. 在极端情况下，温度为 0 将始终选择最有可能的下一个 token，从而导致每次运行的输出完全相同\"",
  "llm.prediction.llama.topKSampling/title": "Top K 采样",
  "llm.prediction.llama.topKSampling/info": "来自 llama.cpp 帮助文档:\n\nTop-k 采样是一种文本生成方法，它仅从模型推理的前 k 个最有可能的 tokens 中选择下一个 token.\n\n它有助于降低生成低概率或无意义 tokens 的风险，但也可能限制输出的多样性.\n\n较高数值的 top-k (例如 100) 将考虑更多的 tokens 并导致更多样化的文本，而较低的值 (例如 10) 将专注于最可能的标记并生成更保守的文本.\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程",
  "llm.prediction.llama.cpuThreads/info": "计算过程中使用的线程数。增加线程数并不一定能提高性能. 默认值为 <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选择限制 AI 回复的长度",
  "llm.prediction.maxPredictedTokens/info": "控制对话机器人回复的最大长度。打开可设置回复的最大长度限制，关闭可让对话机器人自行决定何时停止.",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度 (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "约 {{maxWords}} 字符",
  "llm.prediction.llama.repeatPenalty/title": "重复惩罚",
  "llm.prediction.llama.repeatPenalty/info": "来自 llama.cpp 帮助文档: \"有助于防止模型生成重复或单调的文本.\n\n数值越高 (如 1.5)，对重复的处罚越重，数值越低 (如 0.9)，惩罚越轻.\" • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Min P 采样",
  "llm.prediction.llama.minPSampling/info": "来自 llama.cpp 帮助文档:\n\n相对于最有可能出现的 token 的概率而言，需要考虑的 token 的最小概率。必须在 [0, 1] 范围内.\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P 采样",
  "llm.prediction.llama.topPSampling/info": "来自 llama.cpp 帮助文档:\n\nTop-p 采样，也称为核心采样，是另一种文本生成方法，它从累积概率至少为 P 的 tokens 子集中选择下一个 token.\n\n这种方法同时考虑了 tokens 的概率和抽样 tokens 的数量，从而在多样性和质量之间取得了平衡.\n\ntop-p 的值越大 (如 0.95)，文本就越多样化，而值越小 (如 0.5)，文本就越集中和保守。范围必须在 (0，1].\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "用来阻止模型生成更多 tokens 的字符串",
  "llm.prediction.stopStrings/info": "遇到这种特定字符串时，将阻止模型生成更多 tokens",
  "llm.prediction.stopStrings/placeholder": "输入字符串并按下回车 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "对话溢出",
  "llm.prediction.contextOverflowPolicy/info": "当对话超出模型工作内存 ('上下文') 的范围时，决定该如何处理",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "限制停驻",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "一旦模型的内存满了，就停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "中间截断",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "删除对话中间的信息，为新信息腾出空间。模型仍会记得对话的开头",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滚动窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型将始终获取最新的几条消息，但可能会忘记对话的开头",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "无尾采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.mlx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.mlx.repeatPenalty/info": "数值越大，模型越不会重复运行",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档:\n\n为 top-k 过滤保留的最高概率词汇 tokens \n\n• 该过滤器默认关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/info": "数值越大，模型越不会重复运行",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档:\n\n只有概率加起来达到 TopP 或更高的最有可能的 tokens 才会被保留下来用于生成\n\n• 该过滤器默认关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "无法解析 Jinja 模板: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手动",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "系统提示前缀",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "输入系统提示前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "系统提示后缀",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "输入系统提示后缀...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "用户提示前缀",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": " 输入用户提示前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "用户提示后缀",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "输入用户提示后缀...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "助理提示前缀",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "输入助理提示前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "助理提示后缀",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "输入助理提示后缀...",
  "llm.prediction.promptTemplate.stopStrings/label": "附加停止字符串",
  "llm.prediction.promptTemplate.stopStrings/hint": "除了用户指定的停止字符串之外，还将使用的特定于模板的停止字符串.",
  
  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/info": "指定模型一次可考虑的最大 tokens，这将影响模型在处理过程中保留的上下文数量",
  "llm.load.seed/title": "种子",
  "llm.load.seed/info": "随机种子: 设置随机数生成的种子，确保结果的可重复性",
  
  "llm.load.llama.evalBatchSize/title": "评估批量大小",
  "llm.load.llama.evalBatchSize/info": "设置评估过程中批量处理的示例数量，这将影响速度和内存使用量",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 基础频率",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 缩放频率",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码粒度",
  "llm.load.llama.gpuOffload/title": "GPU 卸载",
  "llm.load.llama.gpuOffload/info": "设置计算与卸载到 GPU 的比率。设置为 off 可禁用 GPU 卸载，或设置为 auto 可让模型决定.",
  "llm.load.llama.flashAttention/title": "Flash 注意力",
  "llm.load.llama.flashAttention/info": "加速注意力机制，实现更快、更高效的处理",
  "llm.load.llama.keepModelInMemory/title": "模型保留在内存中",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问速度，但代价是更高的内存使用率",
  "llm.load.llama.useFp16ForKVCache/title": "使用 FP16 For KV 缓存",
  "llm.load.llama.useFp16ForKVCache/info": "以半精度（FP16）存储高速缓存，减少内存使用量",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/info": "将模型文件直接从磁盘载入内存",
  
  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/info": "指定模型一次可考虑的最大 tokens，这将影响模型在处理过程中保留的上下文数量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 基础频率",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批量大小",
  "embedding.load.llama.evalBatchSize/info": "设置评估过程中一批一起处理的标记数",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 缩放频率",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码粒度",
  "embedding.load.llama.gpuOffload/title": "GPU 卸载",
  "embedding.load.llama.gpuOffload/info": "设置计算与卸载到 GPU 的比率。设置为 off 可禁用 GPU 卸载，或设置为 auto 可让模型决定.",
  "embedding.load.llama.keepModelInMemory/title": "模型保留在内存中",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问速度，但代价是更高的内存使用率",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/info": "将模型文件直接从磁盘载入内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/info": "随机种子: 设置随机数生成的种子，确保结果的可重复性"
}
