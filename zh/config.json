{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部显示",
  "basicSettings": "基本显示",
  "configSubtitle": "加载或保存预设，并进行模型参数覆盖测试",
  "inferenceParameters/title": "推理参数",
  "inferenceParameters/info": "测试影响推理的参数。",
  "generalParameters/title": "常规",
  "samplingParameters/title": "采样",
  "basicTab": "基本",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开全部",
  "advancedTab/overridesTitle": "配置重写",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上面的值以查看此处的重写。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "无可配置参数",
  "generationParameters/info": "E测试影响文本生成的基本参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "用于控制模型初始化和加载到内存中的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "discardChanges": "放弃更改",
  "llm.prediction.systemPrompt/title": "系统提示词",
  "llm.prediction.systemPrompt/description": "使用该字段为模型提供背景说明，如一组规则、约束或一般要求. 此字段通常也称为 \'系统提示词' \ 。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "引入多少随机性。设置为0时，每次运行的结果都会一样；如果选择更高的数值，则可以提升创造性和多样性。",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档: \'默认值为 <{{dynamicValue}}>, 在随机性和确定性之间取得平衡. 在极端情况下，温度为 0 将始终选择最有可能的下一个 token，从而导致每次运行的输出完全相同 \'",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.llama.topKSampling/title": "Top K 采样",
  "llm.prediction.llama.topKSampling/subTitle": "将下一个 token 限制为最有可能的 top-k 个标记中的一个。其作用方式与调整温度相似。",
  "llm.prediction.llama.topKSampling/info": "来自 llama.cpp 帮助文档:\n\nTop-k 采样是一种文本生成方法，它仅从模型推理的前 k 个最有可能的 tokens 中选择下一个 token.\n\n它有助于降低生成低概率或无意义 tokens 的风险，但也可能限制输出的多样性.\n\n较高数值的 top-k (例如 100) 将考虑更多的 tokens 并导致更多样化的文本，而较低的值 (例如 10) 将专注于最可能的标记并生成更保守的文本.\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程数",
  "llm.prediction.llama.cpuThreads/subTitle": "在推理过程中使用的 CPU 线程数",
  "llm.prediction.llama.cpuThreads/info": "算过程中使用的线程数。增加线程数并不一定能提高性能. 默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制响应长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选择限制 AI 回复的长度",
  "llm.prediction.maxPredictedTokens/info": "控制对话机器人回复的最大长度。打开可设置回复的最大长度限制，关闭可让对话机器人自行决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大响应长度 (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "约 {{maxWords}} 字符",
  "llm.prediction.llama.repeatPenalty/title": "重复惩罚",
  "llm.prediction.llama.repeatPenalty/subTitle": "为了降低重复使用相同标记的频率，应该采取哪些措施",
  "llm.prediction.llama.repeatPenalty/info": "来自 llama.cpp 帮助文档: \"有助于防止模型生成重复或单调的文本.\n\n数值越高 (如 1.5)，对重复的处罚越重，数值越低 (如 0.9)，惩罚越轻.\" • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Min P 采样",
  "llm.prediction.llama.minPSampling/subTitle": "最小基础概率，用于选择输出 token",
  "llm.prediction.llama.minPSampling/info": "来自 llama.cpp 帮助文档:\n\n相对于最有可能出现的 token 的概率而言，需要考虑的 token 的最小概率。必须在 [0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P 采样",
  "llm.prediction.llama.topPSampling/subTitle": "最小累积概率用于可能的下一个 token。与温度的作用类似。",
  "llm.prediction.llama.topPSampling/info": "来自 llama.cpp 帮助文档:\n\nTop-p 采样，也称为核心采样，是另一种文本生成方法，它从累积概率至少为 P 的 tokens 子集中选择下一个 token。\n\n这种方法同时考虑了 tokens 的概率和抽样 tokens 的数量，从而在多样性和质量之间取得了平衡。\n\ntop-p 的值越大 (如 0.95)，文本就越多样化，而值越小 (如 0.5)，文本就越集中和保守。范围必须在 (0，1]。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "用来阻止模型生成更多 tokens 的字符串",
  "llm.prediction.stopStrings/info": "遇到这种特定字符串时，将阻止模型生成更多 tokens",
  "llm.prediction.stopStrings/placeholder": "输入字符串并按下回车 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "对话溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话规模超出模型处理能力时，模型应采取何种行为策略",
  "llm.prediction.contextOverflowPolicy/info": "当对话超出模型工作内存 ('上下文') 的范围时，决定该如何处理",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "限制停驻",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "旦模型的内存满了，就停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "中间截断",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "删除对话中间的信息，为新信息腾出空间。模型仍会记得对话的开头",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滚动窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型将始终获取最新的几条消息，但可能会忘记对话的开头",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "无尾采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.mlx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.mlx.repeatPenalty/subTitle": "如何减少重复使用相同 token 的频率",
  "llm.prediction.mlx.repeatPenalty/info": "数值越大，模型越不会重复运行",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个 token 限制为最有可能的前 k 个 token 之一。其作用方式与调整温度相似。",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档:\n\n为 top-k 过滤保留的最高概率词汇 tokens。 \n\n• 该过滤器默认关闭",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "如何减少重复使用相同 token 的频率",
  "llm.prediction.onnx.repeatPenalty/info": "数值越大，模型越不会重复运行",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "最小累积概率，用于可能的下一个 tokens。与温度的作用类似。",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档:\n\n只有概率加起来达到 TopP 或更高的最有可能的 tokens 才会被保留下来用于生成。\n\n• 该过滤器默认关闭",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.promptTemplate/title": "提示词模板",
  "llm.prediction.promptTemplate/subTitle": "在对话中，消息是以特定格式发送给模型的。如果改变这种格式，可能会出现预料之外的问题——请确保你清楚自己在做什么！",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "无法解析 Jinja 模板: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手动",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "前缀系统提示词",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "输入前缀系统提示词...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "后缀系统提示词",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "输入后缀系统提示词...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "前缀用户提示词",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "输入前缀用户提示词...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "后缀用户提示词",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "输入后缀用户提示词...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "前缀助理提示词",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "输入前缀助理提示词...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "后缀助理提示词",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": " 输入后缀助理提示词...",
  "llm.prediction.promptTemplate.stopStrings/label": "附加停止字符串",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "除了用户指定的停止字符串之外，还将使用的特定于模板的停止字符串。",
  
  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型在一个 Prompt (提示词) 中可以处理的最大 token 数。有关管理此限制的更多方法，请参阅 \'推理参数\' 下的 '对话溢出' 选项",
  "llm.load.contextLength/info": "指定模型一次可考虑的最大 tokens，这将影响模型在处理过程中保留的上下文数量",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于生成文本的随机数生成器的种子，如果设置为-1，则表示随机选择",
  "llm.load.seed/info": "随机种子: 设置随机数生成的种子，确保结果的可重复性",
  
  "llm.load.llama.evalBatchSize/title": "评估批量大小",
  "llm.load.llama.evalBatchSize/subTitle": "每次处理时可以同时处理的输入 token 数量。增加这个数量可以提升处理速度，但内存消耗也会增加。",
  "llm.load.llama.evalBatchSize/info": "设置评估过程中批量处理的示例数量，这将影响速度和内存使用量",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 基础频率",
  "llm.load.llama.ropeFrequencyBase/subTitle": "为旋转位置嵌入 (RoPE) 设定一个特定的基本频率。提高这个频率有助于在处理较长的上下文信息时获得更好的性能效果。",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 缩放频率",
  "llm.load.llama.ropeFrequencyScale/subTitle": "为了通过 RoPE 技术扩展上下文的有效性，上下文长度会按照这个系数进行相应的调整。",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "为了在 GPU 上加速计算，需要处理的离散模型层数量",
  "llm.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "llm.load.llama.flashAttention/title": "Flash 注意力",
  "llm.load.llama.flashAttention/subTitle": "减少某些模型在内存使用和生成时间上的消耗",
  "llm.load.llama.flashAttention/info": "加速注意力机制，以实现更快、更高效的处理",
  "llm.load.llama.keepModelInMemory/title": "模型保留在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使模型的工作被转移到 GPU 上，也要确保为模型保留足够的系统内存。这样可以提升运行效率，但系统也需要更多的 RAM 空间。",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问速度，但代价是更高的内存使用率",
  "llm.load.llama.useFp16ForKVCache/title": "为 KV 缓存使用 FP16",
  "llm.load.llama.useFp16ForKVCache/info": "以半精度 (FP16) 存储高速缓存，减少内存使用量",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/subTitle": "优化模型加载速度。当模型体积超过电脑内存容量时，关闭这个选项可能会让运行更顺畅。",
  "llm.load.llama.tryMmap/info": "将模型文件直接从磁盘载入内存",
  
  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型在一个 Prompt (提示词) 中可以处理的最大 token 数。有关管理此限制的更多方法，请参阅 \'推理参数\' 下的 '对话溢出' 选项",
  "embedding.load.contextLength/info": "指定模型一次可考虑的最大 tokens，这将影响模型在处理过程中保留的上下文数量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 基础频率",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "为旋转位置嵌入 (RoPE) 设定一个特定的基本频率。提高这个频率有助于在处理较长的上下文信息时获得更好的性能效果。",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批量大小",
  "embedding.load.llama.evalBatchSize/subTitle": "每次处理输入 token 的数量。增加这个值可以提高性能，但会以内存使用为代价。",
  "embedding.load.llama.evalBatchSize/info": "设置评估过程中一批一起处理的标记数",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 缩放频率",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "为了通过 RoPE 技术扩展上下文的有效性，上下文长度会按照这个系数进行相应的调整。",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "为了在 GPU 上加速计算，需要处理的离散模型层数量",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "embedding.load.llama.keepModelInMemory/title": "模型保留在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使模型的工作被转移到 GPU 上，也要确保为模型保留足够的系统内存。这样可以提升运行效率，但系统也需要更多的 RAM 空间。",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保更快的访问速度，但代价是更高的内存使用率",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "优化模型加载速度。当模型体积超过电脑内存容量时，关闭这个选项可能会让运行更顺畅。",
  "embedding.load.llama.tryMmap/info": "将模型文件直接从磁盘载入内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于生成文本的随机数生成器的种子，如果设置为-1，则表示随机选择",
  "embedding.load.seed/info": "随机种子: 设置随机数生成的种子，确保结果的可重复性"
}
