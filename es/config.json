{
  "noInstanceSelected": "No se seleccionó ninguna instancia de modelo",
  "resetToDefault": "Restablecer",
  "showAdvancedSettings": "Mostrar configuraciones avanzadas",
  "showAll": "Mostrar todo",
  "basicSettings": "Básico",
  "configSubtitle": "Cargar o guardar presets y experimentar con la anulación de parámetros del modelo",
  "inferenceParameters/title": "Parámetros de Predicción",
  "inferenceParameters/info": "Experimenta con parámetros que impactan la predicción.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Muestreo",
  "basicTab": "Básico",
  "advancedTab": "Avanzado",
  "advancedTab/title": "Configuraciones de Ejecución",
  "advancedTab/expandAll": "Expandir todo",
  "advancedTab/overridesTitle": "Anulaciones Activas",
  "advancedTab/noConfigsText": "No tienes cambios sin guardar - edita los valores arriba para ver las anulaciones aquí.",
  "loadInstanceFirst": "Cargue un modelo para ver los parámetros configurables",
  "noListedConfigs": "No hay parámetros configurables",
  "generationParameters/info": "Experimenta con parámetros básicos que impactan la generación de texto.",
  "loadParameters/title": "Cargar Parámetros",
  "loadParameters/description": "Cambiar estos parámetros requiere recargar el modelo",
  "loadParameters/reload": "Recargar para aplicar los cambios de parámetros de carga",
  "discardChanges": "Descartar cambios",
  "llm.prediction.systemPrompt/title": "Mensaje del Sistema",
  "llm.prediction.systemPrompt/description": "Usa este campo para proporcionar instrucciones de fondo al modelo, como un conjunto de reglas, restricciones o requisitos generales. Este campo también se conoce como el \"mensaje del sistema\".",
  "llm.prediction.systemPrompt/subTitle": "Directrices para la IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "El subtítulo va aquí",
  "llm.prediction.temperature/info": "De los documentos de ayuda de llama.cpp: \"El valor predeterminado es <{{dynamicValue}}>, que proporciona un equilibrio entre aleatoriedad y determinismo. En el extremo, una temperatura de 0 siempre elegirá el siguiente token más probable, lo que lleva a salidas idénticas en cada ejecución\"",
  "llm.prediction.topKSampling/title": "Muestreo Top K",
  "llm.prediction.topKSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.topKSampling/info": "De los documentos de ayuda de llama.cpp:\n\nEl muestreo top-k es un método de generación de texto que selecciona el siguiente token solo de los k tokens más probables predichos por el modelo.\n\nAyuda a reducir el riesgo de generar tokens de baja probabilidad o sin sentido, pero también puede limitar la diversidad de la salida.\n\nUn valor más alto para top-k (por ejemplo, 100) considerará más tokens y generará un texto más diverso, mientras que un valor más bajo (por ejemplo, 10) se centrará en los tokens más probables y generará un texto más conservador.\n\n• El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Hilos de CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "El subtítulo va aquí",
  "llm.prediction.llama.cpuThreads/info": "El número de hilos a utilizar durante el cálculo. Aumentar el número de hilos no siempre se correlaciona con un mejor rendimiento. El valor predeterminado es <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitar Longitud de Respuesta",
  "llm.prediction.maxPredictedTokens/subTitle": "El subtítulo va aquí",
  "llm.prediction.maxPredictedTokens/info": "Controla la longitud máxima de la respuesta del chatbot. Actívalo para establecer un límite en la longitud máxima de una respuesta, o desactívalo para permitir que el chatbot decida cuándo detenerse.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longitud máxima de respuesta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadamente {{maxWords}} palabras",
  "llm.prediction.repeatPenalty/title": "Penalización por Repetición",
  "llm.prediction.repeatPenalty/subTitle": "El subtítulo va aquí",
  "llm.prediction.repeatPenalty/info": "De los documentos de ayuda de llama.cpp: \"Ayuda a prevenir que el modelo genere texto repetitivo o monótono.\n\nUn valor más alto (por ejemplo, 1.5) penalizará las repeticiones más fuertemente, mientras que un valor más bajo (por ejemplo, 0.9) será más indulgente.\" • El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Muestreo Min P",
  "llm.prediction.minPSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.minPSampling/info": "De los documentos de ayuda de llama.cpp:\n\nLa probabilidad mínima para que un token sea considerado, relativa a la probabilidad del token más probable. Debe estar en [0, 1].\n\n• El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Muestreo Top P",
  "llm.prediction.topPSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.topPSampling/info": "De los documentos de ayuda de llama.cpp:\n\nEl muestreo top-p, también conocido como muestreo de núcleo, es otro método de generación de texto que selecciona el siguiente token de un subconjunto de tokens que juntos tienen una probabilidad acumulada de al menos p.\n\nEste método proporciona un equilibrio entre diversidad y calidad al considerar tanto las probabilidades de los tokens como el número de tokens a muestrear.\n\nUn valor más alto para top-p (por ejemplo, 0.95) generará un texto más diverso, mientras que un valor más bajo (por ejemplo, 0.5) generará un texto más enfocado y conservador. Debe estar en (0, 1].\n\n• El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenas de Parada",
  "llm.prediction.stopStrings/subTitle": "El subtítulo va aquí",
  "llm.prediction.stopStrings/info": "Cadenas específicas que cuando se encuentran detendrán al modelo de generar más tokens",
  "llm.prediction.stopStrings/placeholder": "Ingrese una cadena y presione ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Desbordamiento de Conversación",
  "llm.prediction.contextOverflowPolicy/subTitle": "El subtítulo va aquí",
  "llm.prediction.contextOverflowPolicy/info": "Decida qué hacer cuando la conversación exceda el tamaño de la memoria de trabajo del modelo ('contexto')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Detener en el Límite",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Dejar de generar una vez que la memoria del modelo esté llena",
  "customInputs.contextOverflowPolicy.truncateMiddle": "Truncar el Medio",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Elimina mensajes del medio de la conversación para hacer espacio para los más nuevos. El modelo aún recordará el comienzo de la conversación",
  "customInputs.contextOverflowPolicy.rollingWindow": "Ventana Deslizante",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "El modelo siempre obtendrá los últimos pocos mensajes, pero puede olvidar el comienzo de la conversación",
  "llm.prediction.llama.frequencyPenalty/title": "Penalización por Frecuencia",
  "llm.prediction.llama.frequencyPenalty/subTitle": "El subtítulo va aquí",
  "llm.prediction.llama.presencePenalty/title": "Penalización por Presencia",
  "llm.prediction.llama.presencePenalty/subTitle": "El subtítulo va aquí",
  "llm.prediction.llama.tailFreeSampling/title": "Muestreo Libre de Cola",
  "llm.prediction.llama.tailFreeSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.llama.locallyTypicalSampling/title": "Muestreo Típico Local",
  "llm.prediction.llama.locallyTypicalSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.onnx.topKSampling/title": "Muestreo Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.onnx.topKSampling/info": "De la documentación de ONNX:\n\nNúmero de tokens de vocabulario de mayor probabilidad a mantener para el filtrado top-k\n\n• Este filtro está desactivado por defecto",
  "llm.prediction.onnx.repeatPenalty/title": "Penalización por Repetición",
  "llm.prediction.onnx.repeatPenalty/subTitle": "El subtítulo va aquí",
  "llm.prediction.onnx.repeatPenalty/info": "Un valor más alto desalienta al modelo de repetirse",
  "llm.prediction.onnx.topPSampling/title": "Muestreo Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "El subtítulo va aquí",
  "llm.prediction.onnx.topPSampling/info": "De la documentación de ONNX:\n\nSolo se mantienen los tokens más probables con probabilidades que suman TopP o más para la generación\n\n• Este filtro está desactivado por defecto",
  "llm.prediction.seed/title": "Semilla",
  "llm.prediction.seed/subTitle": "El subtítulo va aquí",
  "llm.prediction.structured/title": "Salida Estructurada",
  "llm.prediction.structured/subTitle": "El subtítulo va aquí",
  "llm.prediction.structured/info": "Salida Estructurada",
  "llm.load.contextLength/title": "Longitud del Contexto",
  "llm.load.contextLength/subTitle": "El subtítulo va aquí",
  "llm.load.contextLength/info": "Especifica el número máximo de tokens que el modelo puede considerar a la vez, impactando cuánto contexto retiene durante el procesamiento",
  "llm.load.seed/title": "Semilla",
  "llm.load.seed/subTitle": "El subtítulo va aquí",
  "llm.load.seed/info": "Semilla Aleatoria: Establece la semilla para la generación de números aleatorios para asegurar resultados reproducibles",
  "llm.load.llama.evalBatchSize/title": "Tamaño del Lote de Evaluación",
  "llm.load.llama.evalBatchSize/subTitle": "El subtítulo va aquí",
  "llm.load.llama.evalBatchSize/info": "Establece el número de ejemplos procesados juntos en un lote durante la evaluación, afectando la velocidad y el uso de memoria",
  "llm.load.llama.ropeFrequencyBase/title": "Frecuencia Base RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "El subtítulo va aquí",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanzado] Ajusta la frecuencia base para la Codificación Posicional Rotatoria, afectando cómo se embebe la información posicional",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frecuencia RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "El subtítulo va aquí",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanzado] Modifica la escala de frecuencia para la Codificación Posicional Rotatoria para controlar la granularidad de la codificación posicional",
  "llm.load.llama.acceleration.offloadRatio/title": "Descarga a GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "El subtítulo va aquí",
  "llm.load.llama.acceleration.offloadRatio/info": "Establece la proporción de cálculo para descargar a la GPU. Configúralo en apagado para desactivar la descarga a la GPU, o en automático para que el modelo decida.",
  "llm.load.llama.flashAttention/title": "Atención Flash",
  "llm.load.llama.flashAttention/subTitle": "El subtítulo va aquí",
  "llm.load.llama.flashAttention/info": "Acelera los mecanismos de atención para un procesamiento más rápido y eficiente",
  "llm.load.llama.keepModelInMemory/title": "Mantener el Modelo en Memoria",
  "llm.load.llama.keepModelInMemory/subTitle": "El subtítulo va aquí",
  "llm.load.llama.keepModelInMemory/info": "Evita que el modelo sea intercambiado al disco, asegurando un acceso más rápido a costa de un mayor uso de RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Caché KV",
  "llm.load.llama.useFp16ForKVCache/subTitle": "El subtítulo va aquí",
  "llm.load.llama.useFp16ForKVCache/info": "Reduce el uso de memoria almacenando la caché en precisión media (FP16)",
  "llm.load.llama.tryMmap/title": "Intentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "El subtítulo va aquí",
  "llm.load.llama.tryMmap/info": "Cargar archivos de modelo directamente del disco a la memoria"
}
