{
  "noInstanceSelected": "No se ha seleccionado ninguna instancia del modelo",
  "resetToDefault": "Restablecer",
  "showAdvancedSettings": "Mostrar configuraciones avanzadas",
  "configSubtitle": "Cargar o guardar ajustes preestablecidos y experimentar con anulaciones de parámetros del modelo",
  "inferenceParameters/title": "Parámetros de Predicción",
  "inferenceParameters/info": "Experimentar con parámetros que afectan la predicción.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Muestreo",
  "basicTab": "Básico",
  "advancedTab": "Avanzado",
  "loadInstanceFirst": "Seleccione primero una instancia del modelo para ver los parámetros",
  "generationParameters/info": "Experimentar con parámetros básicos que afectan la generación de texto.",
  "loadParameters/title": "Cargar Parámetros",
  "loadParameters/description": "Cambiar estos parámetros requiere recargar el modelo",
  "loadParameters/reload": "Recargar modelo para aplicar cambios",
  "llm.prediction.systemPrompt/title": "Directrices para la IA",
  "llm.prediction.systemPrompt/description": "Use este campo para proporcionar instrucciones de fondo al modelo, como un conjunto de reglas, restricciones o requisitos generales. Este campo también se conoce a menudo como el \"prompt del sistema\".",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/info": "Desde los documentos de ayuda llama.cpp: \"El valor predeterminado es <{{dynamicValue}}>, que proporciona un equilibrio entre aleatoriedad y determinismo. En el extremo, una temperatura de 0 siempre elegirá el token siguiente más probable, lo que conducirá a salidas idénticas en cada ejecución\"",
  "llm.prediction.llama.topKSampling/title": "Muestreo Top K",
  "llm.prediction.llama.topKSampling/info": "Desde los documentos de ayuda llama.cpp:\n\nEl muestreo top-k es un método de generación de texto que selecciona el siguiente token solo de los k tokens más probables predichos por el modelo.\n\nAyuda a reducir el riesgo de generar tokens de baja probabilidad o sin sentido, pero también puede limitar la diversidad de la salida.\n\nUn valor más alto para top-k (por ejemplo, 100) considerará más tokens y conducirá a un texto más diverso, mientras que un valor más bajo (por ejemplo, 10) se centrará en los tokens más probables y generará un texto más conservador.\n\n• El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Hilos de CPU",
  "llm.prediction.llama.cpuThreads/info": "Número de hilos a utilizar durante el cálculo. Aumentar el número de hilos no siempre se correlaciona con un mejor rendimiento. El valor predeterminado es <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitar Longitud de Respuesta",
  "llm.prediction.maxPredictedTokens/info": "Controla la longitud máxima de la respuesta del chatbot. Activar para establecer un límite en la longitud máxima de una respuesta, o desactivar para permitir que el chatbot decida cuándo detenerse.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longitud máxima de la respuesta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadamente {{maxWords}} palabras",
  "llm.prediction.llama.repeatPenalty/title": "Penalización por Repetición",
  "llm.prediction.llama.repeatPenalty/info": "Desde los documentos de ayuda llama.cpp: \"Ayuda a evitar que el modelo genere texto repetitivo o monótono.\n\nUn valor más alto (por ejemplo, 1.5) penalizará más las repeticiones, mientras que un valor más bajo (por ejemplo, 0.9) será más permisivo.\" • El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Muestreo Min P",
  "llm.prediction.llama.minPSampling/info": "Desde los documentos de ayuda llama.cpp:\n\nLa probabilidad mínima para que un token sea considerado, relativa a la probabilidad del token más probable. Debe estar en [0, 1].\n\n• El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Muestreo Top P",
  "llm.prediction.llama.topPSampling/info": "Desde los documentos de ayuda llama.cpp:\n\nEl muestreo top-p, también conocido como muestreo de núcleo, es otro método de generación de texto que selecciona el siguiente token de un subconjunto de tokens que juntos tienen una probabilidad acumulativa de al menos p.\n\nEste método proporciona un equilibrio entre diversidad y calidad al considerar tanto las probabilidades de los tokens como la cantidad de tokens a muestrear.\n\nUn valor más alto para top-p (por ejemplo, 0.95) conducirá a un texto más diverso, mientras que un valor más bajo (por ejemplo, 0.5) generará un texto más enfocado y conservador. Debe estar en (0, 1].\n\n• El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenas de Parada",
  "llm.prediction.stopStrings/info": "Cadenas específicas que, al encontrarse, detendrán al modelo de generar más tokens",
  "llm.prediction.stopStrings/placeholder": "Ingrese una cadena y presione ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Desbordamiento de Conversación",
  "llm.prediction.contextOverflowPolicy/info": "Decide qué hacer cuando la conversación excede el tamaño de la memoria de trabajo ('contexto') del modelo",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Detener en el Límite",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Detener la generación una vez que la memoria del modelo se llene",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Truncar en el Medio",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Elimina mensajes del medio de la conversación para hacer espacio para los más recientes. El modelo seguirá recordando el inicio de la conversación",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Ventana Deslizante",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "El modelo siempre obtendrá los mensajes más recientes pero puede olvidar el inicio de la conversación",
  "llm.prediction.llama.frequencyPenalty/title": "Penalización por Frecuencia",
  "llm.prediction.llama.presencePenalty/title": "Penalización por Presencia",
  "llm.prediction.llama.tailFreeSampling/title": "Muestreo Libre de Cola",
  "llm.prediction.llama.locallyTypicalSampling/title": "Muestreo Localmente Típico",
  "llm.prediction.seed/title": "Semilla",
  "llm.prediction.structured/title": "Salida Estructurada",
  "llm.prediction.structured/info": "Salida Estructurada",
  "llm.load.contextLength/title": "Longitud de Contexto",
  "llm.load.contextLength/info": "Especifica el número máximo de tokens que el modelo puede considerar a la vez, lo que afecta cuánto contexto retiene durante el procesamiento",
  "llm.load.seed/title": "Semilla",
  "llm.load.seed/info": "Semilla Aleatoria: Establece la semilla para la generación de números aleatorios para garantizar resultados reproducibles",
  "llm.load.llama.evalBatchSize/title": "Tamaño del Lote de Evaluación",
  "llm.load.llama.evalBatchSize/info": "Establece el número de ejemplos procesados juntos en un lote durante la evaluación, afectando la velocidad y el uso de memoria",
  "llm.load.llama.ropeFrequencyBase/title": "Base de Frecuencia de RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanzado] Ajusta la frecuencia base para el Codificador Posicional Rotatorio, afectando cómo se incorpora la información posicional",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frecuencia de RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanzado] Modifica la escala de frecuencia para el Codificador Posicional Rotatorio para controlar la granularidad de la codificación posicional",
  "llm.load.llama.gpuOffload/title": "Desvío de GPU",
  "llm.load.llama.gpuOffload/info": "Establece la proporción de cómputo a desviar a la GPU. Establecer en off para desactivar el desvío de GPU, o auto para que el modelo decida.",
  "llm.load.llama.flashAttention/title": "Atención Flash",
  "llm.load.llama.flashAttention/info": "Acelera los mecanismos de atención para un procesamiento más rápido y eficiente",
  "llm.load.llama.keepModelInMemory/title": "Mantener Modelo en Memoria",
  "llm.load.llama.keepModelInMemory/info": "Evita que el modelo se intercambie a disco, asegurando un acceso más rápido a costa de un mayor uso de RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Caché KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduce el uso de memoria almacenando la caché en media precisión (FP16)",
  "llm.load.llama.tryMmap/title": "Intentar mmap()",
  "llm.load.llama.tryMmap/info": "Cargar archivos de modelo directamente desde el disco a la memoria"
}