{
  "noInstanceSelected": "Ninguna instancia de modelo seleccionada",
  "resetToDefault": "Restablecer",
  "showAdvancedSettings": "Mostrar configuraci√≥n avanzada",
  "showAll": "Todo",
  "basicSettings": "B√°sico",
  "configSubtitle": "Carga o guarda presets y experimenta con modificaciones de par√°metros del modelo",
  "inferenceParameters/title": "Par√°metros de Predicci√≥n",
  "inferenceParameters/info": "Experimenta con par√°metros que afectan la predicci√≥n.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Muestreo",
  "basicTab": "B√°sico",
  "advancedTab": "Avanzado",
  "advancedTab/title": "üß™ Configuraci√≥n Avanzada",
  "advancedTab/expandAll": "Expandir todo",
  "advancedTab/overridesTitle": "Modificaciones de Configuraci√≥n",
  "advancedTab/noConfigsText": "No tienes cambios sin guardar - edita los valores arriba para ver las modificaciones aqu√≠.",
  "loadInstanceFirst": "Carga un modelo para ver los par√°metros configurables",
  "noListedConfigs": "No hay par√°metros configurables",
  "generationParameters/info": "Experimenta con par√°metros b√°sicos que afectan la generaci√≥n de texto.",
  "loadParameters/title": "Par√°metros de Carga",
  "loadParameters/description": "Configuraciones para controlar la forma en que el modelo se inicializa y se carga en memoria.",
  "loadParameters/reload": "Recargar para aplicar cambios",
  "loadParameters/reload/error": "Error al recargar el modelo",
  "discardChanges": "Descartar cambios",
  "loadModelToSeeOptions": "Carga un modelo para ver las opciones",
  "schematicsError.title": "Los esquemas de configuraci√≥n contienen errores en los siguientes campos:",
  "manifestSections": {
    "structuredOutput/title": "Salida Estructurada",
    "speculativeDecoding/title": "Decodificaci√≥n Especulativa",
    "sampling/title": "Muestreo",
    "settings/title": "Configuraciones",
    "toolUse/title": "Uso de Herramientas",
    "promptTemplate/title": "Plantilla de Prompt",
    "customFields/title": "Campos Personalizados"
  },

  "llm.prediction.systemPrompt/title": "Prompt del Sistema",
  "llm.prediction.systemPrompt/description": "Usa este campo para proporcionar instrucciones de fondo al modelo, como un conjunto de reglas, restricciones o requisitos generales.",
  "llm.prediction.systemPrompt/subTitle": "Gu√≠as para la IA",
  "llm.prediction.systemPrompt/openEditor": "Editor",
  "llm.prediction.systemPrompt/closeEditor": "Cerrar Editor",
  "llm.prediction.systemPrompt/openedEditor": "Abierto en el Editor...",
  "llm.prediction.systemPrompt/edit": "Editar Prompt del Sistema...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "Agregar instrucciones...",
  "llm.prediction.systemPrompt/addInstructions": "Agregar instrucciones",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Cu√°nta aleatoriedad introducir. 0 dar√° el mismo resultado cada vez, mientras que valores m√°s altos aumentar√°n la creatividad y variaci√≥n",
  "llm.prediction.temperature/info": "De la documentaci√≥n de llama.cpp: \"El valor predeterminado es <{{dynamicValue}}>, que proporciona un equilibrio entre aleatoriedad y determinismo. En el extremo, una temperatura de 0 siempre elegir√° el token m√°s probable siguiente, llevando a salidas id√©nticas en cada ejecuci√≥n\"",
  "llm.prediction.llama.sampling/title": "Muestreo",
  "llm.prediction.topKSampling/title": "Muestreo Top K",
  "llm.prediction.topKSampling/subTitle": "Limita el siguiente token a uno de los k tokens m√°s probables. Act√∫a de manera similar a la temperatura",
  "llm.prediction.topKSampling/info": "De la documentaci√≥n de llama.cpp:\n\nEl muestreo top-k es un m√©todo de generaci√≥n de texto que selecciona el siguiente token solo de los k tokens m√°s probables predichos por el modelo.\n\nAyuda a reducir el riesgo de generar tokens de baja probabilidad o sin sentido, pero tambi√©n puede limitar la diversidad de la salida.\n\nUn valor m√°s alto para top-k (ej., 100) considerar√° m√°s tokens y llevar√° a texto m√°s diverso, mientras que un valor m√°s bajo (ej., 10) se enfocar√° en los tokens m√°s probables y generar√° texto m√°s conservador.\n\n‚Ä¢ El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Hilos de CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "N√∫mero de hilos de CPU a usar durante la inferencia",
  "llm.prediction.llama.cpuThreads/info": "El n√∫mero de hilos a usar durante el c√°lculo. Aumentar el n√∫mero de hilos no siempre se correlaciona con mejor rendimiento. El valor predeterminado es <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitar Longitud de Respuesta",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente limita la longitud de la respuesta de la IA",
  "llm.prediction.maxPredictedTokens/info": "Controla la longitud m√°xima de la respuesta del chatbot. Act√≠valo para establecer un l√≠mite en la longitud m√°xima de una respuesta, o desact√≠valo para que el chatbot decida cu√°ndo parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longitud m√°xima de respuesta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadamente {{maxWords}} palabras",
  "llm.prediction.repeatPenalty/title": "Penalizaci√≥n por Repetici√≥n",
  "llm.prediction.repeatPenalty/subTitle": "Cu√°nto desalentar la repetici√≥n del mismo token",
  "llm.prediction.repeatPenalty/info": "De la documentaci√≥n de llama.cpp: \"Ayuda a prevenir que el modelo genere texto repetitivo o mon√≥tono.\n\nUn valor m√°s alto (ej., 1.5) penalizar√° las repeticiones m√°s fuertemente, mientras que un valor m√°s bajo (ej., 0.9) ser√° m√°s indulgente.\" ‚Ä¢ El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Muestreo Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilidad base m√≠nima para que un token sea seleccionado para la salida",
  "llm.prediction.minPSampling/info": "De la documentaci√≥n de llama.cpp:\n\nLa probabilidad m√≠nima para que un token sea considerado, relativa a la probabilidad del token m√°s probable. Debe estar en [0, 1].\n\n‚Ä¢ El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Muestreo Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilidad acumulativa m√≠nima para los posibles siguientes tokens. Act√∫a de manera similar a la temperatura",
  "llm.prediction.topPSampling/info": "De la documentaci√≥n de llama.cpp:\n\nEl muestreo top-p, tambi√©n conocido como muestreo de n√∫cleo, es otro m√©todo de generaci√≥n de texto que selecciona el siguiente token de un subconjunto de tokens que juntos tienen una probabilidad acumulativa de al menos p.\n\nEste m√©todo proporciona un equilibrio entre diversidad y calidad al considerar tanto las probabilidades de los tokens como el n√∫mero de tokens a muestrear.\n\nUn valor m√°s alto para top-p (ej., 0.95) llevar√° a texto m√°s diverso, mientras que un valor m√°s bajo (ej., 0.5) generar√° texto m√°s enfocado y conservador. Debe estar en (0, 1].\n\n‚Ä¢ El valor predeterminado es <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenas de Parada",
  "llm.prediction.stopStrings/subTitle": "Cadenas que deber√≠an hacer que el modelo deje de generar m√°s tokens",
  "llm.prediction.stopStrings/info": "Cadenas espec√≠ficas que cuando se encuentren har√°n que el modelo deje de generar m√°s tokens",
  "llm.prediction.stopStrings/placeholder": "Ingresa una cadena y presiona ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Desbordamiento de Contexto",
  "llm.prediction.contextOverflowPolicy/subTitle": "C√≥mo debe comportarse el modelo cuando la conversaci√≥n crece demasiado para que pueda manejarla",
  "llm.prediction.contextOverflowPolicy/info": "Decide qu√© hacer cuando la conversaci√≥n excede el tama√±o de la memoria de trabajo del modelo ('contexto')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalizaci√≥n por Frecuencia",
  "llm.prediction.llama.presencePenalty/title": "Penalizaci√≥n por Presencia",
  "llm.prediction.llama.tailFreeSampling/title": "Muestreo Libre de Cola",
  "llm.prediction.llama.locallyTypicalSampling/title": "Muestreo Localmente T√≠pico",
  "llm.prediction.llama.xtcProbability/title": "Probabilidad de Muestreo XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "El muestreador XTC (Excluir Opciones Principales) solo se activar√° con esta probabilidad por token generado. El muestreo XTC puede aumentar la creatividad y reducir clich√©s",
  "llm.prediction.llama.xtcProbability/info": "El muestreo XTC (Excluir Opciones Principales) solo se activar√° con esta probabilidad, por token generado. El muestreo XTC usualmente aumenta la creatividad y reduce clich√©s",
  "llm.prediction.llama.xtcThreshold/title": "Umbral de Muestreo XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Umbral XTC (Excluir Opciones Principales). Con una probabilidad de `xtc-probability`, busca tokens con probabilidades entre `xtc-threshold` y 0.5, y elimina todos esos tokens excepto el menos probable",
  "llm.prediction.llama.xtcThreshold/info": "Umbral XTC (Excluir Opciones Principales). Con una probabilidad de `xtc-probability`, busca tokens con probabilidades entre `xtc-threshold` y 0.5, y elimina todos esos tokens excepto el menos probable",
  "llm.prediction.mlx.topKSampling/title": "Muestreo Top K",
  "llm.prediction.mlx.topKSampling/subTitle": "Limita el siguiente token a uno de los k tokens m√°s probables. Act√∫a de manera similar a la temperatura",
  "llm.prediction.mlx.topKSampling/info": "Limita el siguiente token a uno de los k tokens m√°s probables. Act√∫a de manera similar a la temperatura",
  "llm.prediction.onnx.topKSampling/title": "Muestreo Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita el siguiente token a uno de los k tokens m√°s probables. Act√∫a de manera similar a la temperatura",
  "llm.prediction.onnx.topKSampling/info": "De la documentaci√≥n de ONNX:\n\nN√∫mero de tokens de vocabulario de mayor probabilidad a mantener para el filtrado top-k\n\n‚Ä¢ Este filtro est√° desactivado por defecto",
  "llm.prediction.onnx.repeatPenalty/title": "Penalizaci√≥n por Repetici√≥n",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Cu√°nto desalentar la repetici√≥n del mismo token",
  "llm.prediction.onnx.repeatPenalty/info": "Un valor m√°s alto desalienta al modelo de repetirse a s√≠ mismo",
  "llm.prediction.onnx.topPSampling/title": "Muestreo Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilidad acumulativa m√≠nima para los posibles siguientes tokens. Act√∫a de manera similar a la temperatura",
  "llm.prediction.onnx.topPSampling/info": "De la documentaci√≥n de ONNX:\n\nSolo los tokens m√°s probables con probabilidades que sumen TopP o m√°s alto se mantienen para la generaci√≥n\n\n‚Ä¢ Este filtro est√° desactivado por defecto",
  "llm.prediction.seed/title": "Semilla",
  "llm.prediction.structured/title": "Salida Estructurada",
  "llm.prediction.structured/info": "Salida Estructurada",
  "llm.prediction.structured/description": "Avanzado: puedes proporcionar un [Esquema JSON](https://json-schema.org/learn/miscellaneous-examples) para forzar un formato de salida particular del modelo. Lee la [documentaci√≥n](https://lmstudio.ai/docs/advanced/structured-output) para aprender m√°s",
  "llm.prediction.tools/title": "Uso de Herramientas",
  "llm.prediction.tools/description": "Avanzado: puedes proporcionar una lista compatible con JSON de herramientas para que el modelo solicite llamadas a ellas. Lee la [documentaci√≥n](https://lmstudio.ai/docs/advanced/tool-use) para aprender m√°s",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pasa esto a trav√©s del cuerpo de la solicitud como `tools` cuando uses la API del servidor",
  "llm.prediction.promptTemplate/title": "Plantilla de Prompt",
  "llm.prediction.promptTemplate/subTitle": "El formato en el cual los mensajes en el chat se env√≠an al modelo. ¬°Cambiar esto puede introducir comportamiento inesperado - aseg√∫rate de saber lo que est√°s haciendo!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Tokens de Borrador a Generar",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "El n√∫mero de tokens a generar con el modelo de borrador por token del modelo principal. Encuentra el punto ideal entre c√≥mputo y recompensa",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Umbral de Probabilidad de Borrador",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Contin√∫a el borrador hasta que la probabilidad de un token caiga por debajo de este umbral. Valores m√°s altos generalmente significan menor riesgo, menor recompensa",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Tama√±o M√≠nimo de Borrador",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Los borradores m√°s peque√±os que esto ser√°n ignorados por el modelo principal. Valores m√°s altos generalmente significan menor riesgo, menor recompensa",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Tama√±o M√°ximo de Borrador",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "N√∫mero m√°ximo de tokens permitidos en un borrador. Techo si todas las probabilidades de tokens son > el umbral. Valores m√°s bajos generalmente significan menor riesgo, menor recompensa",
  "llm.prediction.speculativeDecoding.draftModel/title": "Modelo de Borrador",
  "llm.prediction.reasoning.parsing/title": "An√°lisis de Secci√≥n de Razonamiento",
  "llm.prediction.reasoning.parsing/subTitle": "C√≥mo analizar las secciones de razonamiento en la salida del modelo",

  "llm.load.mainGpu/title": "GPU Principal",
  "llm.load.mainGpu/subTitle": "La GPU a priorizar para el c√≥mputo del modelo",
  "llm.load.mainGpu/placeholder": "Seleccionar GPU principal...",
  "llm.load.splitStrategy/title": "Estrategia de Divisi√≥n",
  "llm.load.splitStrategy/subTitle": "C√≥mo dividir el c√≥mputo del modelo entre GPUs",
  "llm.load.splitStrategy/placeholder": "Seleccionar estrategia de divisi√≥n...",
  "llm.load.offloadKVCacheToGpu/title": "Descargar Cache KV a Memoria GPU",
  "llm.load.offloadKVCacheToGpu/subTitle": "Descarga el cache KV a memoria GPU. Mejora el rendimiento pero requiere m√°s memoria GPU",
  "load.gpuStrictVramCap/title": "Limitar Descarga del Modelo a Memoria GPU Dedicada",
  "load.gpuStrictVramCap.customSubTitleOff": "DESACTIVADO: Permitir que los pesos del modelo se descarguen a memoria compartida si la memoria GPU dedicada est√° llena",
  "load.gpuStrictVramCap.customSubTitleOn": "ACTIVADO: El sistema limitar√° la descarga de pesos del modelo solo a memoria GPU dedicada y RAM. El contexto a√∫n puede usar memoria compartida",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "Descarga del modelo limitada a memoria GPU dedicada. El n√∫mero real de capas descargadas puede diferir",
  "load.allGpusDisabledWarning": "Todas las GPUs est√°n actualmente deshabilitadas. Habilita al menos una para descargar",

  "llm.load.contextLength/title": "Longitud de Contexto",
  "llm.load.contextLength/subTitle": "El n√∫mero m√°ximo de tokens que el modelo puede atender en un prompt. Ve las opciones de Desbordamiento de Conversaci√≥n bajo \"Par√°metros de inferencia\" para m√°s formas de manejar esto",
  "llm.load.contextLength/info": "Especifica el n√∫mero m√°ximo de tokens que el modelo puede considerar a la vez, impactando cu√°nto contexto retiene durante el procesamiento",
  "llm.load.contextLength/warning": "Establecer un valor alto para la longitud de contexto puede impactar significativamente el uso de memoria",
  "llm.load.seed/title": "Semilla",
  "llm.load.seed/subTitle": "La semilla para el generador de n√∫meros aleatorios usado en la generaci√≥n de texto. -1 es aleatorio",
  "llm.load.seed/info": "Semilla Aleatoria: Establece la semilla para la generaci√≥n de n√∫meros aleatorios para asegurar resultados reproducibles",
  "llm.load.numCpuExpertLayersRatio/title": "Forzar Pesos de Expertos del Modelo en CPU",
  "llm.load.numCpuExpertLayersRatio/subTitle": "Si forzar los pesos de expertos MoE en RAM de CPU. Ahorra VRAM y puede ser m√°s r√°pido que la descarga parcial de GPU. No recomendado si el modelo cabe completamente en VRAM.",
  "llm.load.numCpuExpertLayersRatio/info": "Especifica si poner o no todas las capas de expertos MoE en RAM de CPU. Deja las capas de atenci√≥n en GPU, ahorrando VRAM mientras mantiene la inferencia bastante r√°pida",

  "llm.load.llama.evalBatchSize/title": "Tama√±o de Lote de Evaluaci√≥n",
  "llm.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a procesar a la vez. Aumentar esto incrementa el rendimiento a costa del uso de memoria",
  "llm.load.llama.evalBatchSize/info": "Establece el n√∫mero de ejemplos procesados juntos en un lote durante la evaluaci√≥n, afectando velocidad y uso de memoria",
  "llm.load.llama.ropeFrequencyBase/title": "Base de Frecuencia RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frecuencia base personalizada para incrustaciones posicionales rotatorias (RoPE). Aumentar esto puede habilitar mejor rendimiento en longitudes de contexto altas",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanzado] Ajusta la frecuencia base para la Codificaci√≥n Posicional Rotatoria, afectando c√≥mo se incrusta la informaci√≥n posicional",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frecuencia RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "La longitud de contexto se escala por este factor para extender el contexto efectivo usando RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanzado] Modifica el escalado de frecuencia para la Codificaci√≥n Posicional Rotatoria para controlar la granularidad de codificaci√≥n posicional",
  "llm.load.llama.acceleration.offloadRatio/title": "Descarga GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de capas discretas del modelo a computar en la GPU para aceleraci√≥n GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Establece el n√∫mero de capas a descargar a la GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Disminuye el uso de memoria y tiempo de generaci√≥n en algunos modelos",
  "llm.load.llama.flashAttention/info": "Acelera los mecanismos de atenci√≥n para un procesamiento m√°s r√°pido y eficiente",
  "llm.load.numExperts/title": "N√∫mero de Expertos",
  "llm.load.numExperts/subTitle": "N√∫mero de expertos a usar en el modelo",
  "llm.load.numExperts/info": "El n√∫mero de expertos a usar en el modelo",
  "llm.load.llama.keepModelInMemory/title": "Mantener Modelo en Memoria",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserva memoria del sistema para el modelo, incluso cuando se descarga a GPU. Mejora el rendimiento pero requiere m√°s RAM del sistema",
  "llm.load.llama.keepModelInMemory/info": "Previene que el modelo sea intercambiado al disco, asegurando acceso m√°s r√°pido a costa de mayor uso de RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 Para Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduce el uso de memoria almacenando el cache en media precisi√≥n (FP16)",
  "llm.load.llama.tryMmap/title": "Intentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "Mejora el tiempo de carga del modelo. Deshabilitar esto puede mejorar el rendimiento cuando el modelo es m√°s grande que la RAM del sistema disponible",
  "llm.load.llama.tryMmap/info": "Carga archivos del modelo directamente del disco a memoria",
  "llm.load.llama.cpuThreadPoolSize/title": "Tama√±o del Pool de Hilos de CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "N√∫mero de hilos de CPU a asignar al pool de hilos usado para c√≥mputo del modelo",
  "llm.load.llama.cpuThreadPoolSize/info": "El n√∫mero de hilos de CPU a asignar al pool de hilos usado para c√≥mputo del modelo. Aumentar el n√∫mero de hilos no siempre se correlaciona con mejor rendimiento. El valor predeterminado es <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Tipo de Cuantizaci√≥n de Cache K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Valores m√°s bajos reducen el uso de memoria pero pueden disminuir la calidad. El efecto var√≠a significativamente entre modelos.",
  "llm.load.llama.vCacheQuantizationType/title": "Tipo de Cuantizaci√≥n de Cache V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Valores m√°s bajos reducen el uso de memoria pero pueden disminuir la calidad. El efecto var√≠a significativamente entre modelos.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Debes deshabilitar este valor si Flash Attention no est√° habilitado",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Solo puede activarse cuando Flash Attention est√° habilitado",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Debes deshabilitar flash attention cuando uses F32",
  "llm.load.mlx.kvCacheBits/title": "Cuantizaci√≥n de Cache KV",
  "llm.load.mlx.kvCacheBits/subTitle": "N√∫mero de bits al que el cache KV debe ser cuantizado",
  "llm.load.mlx.kvCacheBits/info": "N√∫mero de bits al que el cache KV debe ser cuantizado",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "La configuraci√≥n de Longitud de Contexto se ignora cuando se usa Cuantizaci√≥n de Cache KV",
  "llm.load.mlx.kvCacheGroupSize/title": "Cuantizaci√≥n de Cache KV: Tama√±o de Grupo",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Tama√±o de grupo durante la operaci√≥n de cuantizaci√≥n para el cache KV. Un tama√±o de grupo m√°s alto reduce el uso de memoria pero puede disminuir la calidad",
  "llm.load.mlx.kvCacheGroupSize/info": "N√∫mero de bits al que el cache KV debe ser cuantizado",
  "llm.load.mlx.kvCacheQuantizationStart/title": "Cuantizaci√≥n de Cache KV: Empezar a cuantizar cuando el contexto cruce esta longitud",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Umbral de longitud de contexto para empezar a cuantizar el cache KV",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Umbral de longitud de contexto para empezar a cuantizar el cache KV",
  "llm.load.mlx.kvCacheQuantization/title": "Cuantizaci√≥n de Cache KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Cuantiza el cache KV del modelo. Esto puede resultar en generaci√≥n m√°s r√°pida y menor huella de memoria, a expensas de la calidad de la salida del modelo.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "Bits de cuantizaci√≥n del cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "N√∫mero de bits para cuantizar el cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Estrategia de tama√±o de grupo",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Precisi√≥n",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Equilibrado",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "R√°pido",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Avanzado: Configuraci√≥n de 'tama√±o de grupo matmul' cuantizado\n\n‚Ä¢ Precisi√≥n = tama√±o de grupo 32\n‚Ä¢ Equilibrado = tama√±o de grupo 64\n‚Ä¢ R√°pido = tama√±o de grupo 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Empezar a cuantizar cuando el contexto alcance esta longitud",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Cuando el contexto alcance esta cantidad de tokens,\nempezar a cuantizar el cache KV",

  "embedding.load.contextLength/title": "Longitud de Contexto",
  "embedding.load.contextLength/subTitle": "El n√∫mero m√°ximo de tokens que el modelo puede atender en un prompt. Ve las opciones de Desbordamiento de Conversaci√≥n bajo \"Par√°metros de inferencia\" para m√°s formas de manejar esto",
  "embedding.load.contextLength/info": "Especifica el n√∫mero m√°ximo de tokens que el modelo puede considerar a la vez, impactando cu√°nto contexto retiene durante el procesamiento",
  "embedding.load.llama.ropeFrequencyBase/title": "Base de Frecuencia RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frecuencia base personalizada para incrustaciones posicionales rotatorias (RoPE). Aumentar esto puede habilitar mejor rendimiento en longitudes de contexto altas",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avanzado] Ajusta la frecuencia base para la Codificaci√≥n Posicional Rotatoria, afectando c√≥mo se incrusta la informaci√≥n posicional",
  "embedding.load.llama.evalBatchSize/title": "Tama√±o de Lote de Evaluaci√≥n",
  "embedding.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a procesar a la vez. Aumentar esto incrementa el rendimiento a costa del uso de memoria",
  "embedding.load.llama.evalBatchSize/info": "Establece el n√∫mero de tokens procesados juntos en un lote durante la evaluaci√≥n",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frecuencia RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "La longitud de contexto se escala por este factor para extender el contexto efectivo usando RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avanzado] Modifica el escalado de frecuencia para la Codificaci√≥n Posicional Rotatoria para controlar la granularidad de codificaci√≥n posicional",
  "embedding.load.llama.acceleration.offloadRatio/title": "Descarga GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de capas discretas del modelo a computar en la GPU para aceleraci√≥n GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Establece el n√∫mero de capas a descargar a la GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Mantener Modelo en Memoria",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserva memoria del sistema para el modelo, incluso cuando se descarga a GPU. Mejora el rendimiento pero requiere m√°s RAM del sistema",
  "embedding.load.llama.keepModelInMemory/info": "Previene que el modelo sea intercambiado al disco, asegurando acceso m√°s r√°pido a costa de mayor uso de RAM",
  "embedding.load.llama.tryMmap/title": "Intentar mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Mejora el tiempo de carga del modelo. Deshabilitar esto puede mejorar el rendimiento cuando el modelo es m√°s grande que la RAM del sistema disponible",
  "embedding.load.llama.tryMmap/info": "Carga archivos del modelo directamente del disco a memoria",
  "embedding.load.seed/title": "Semilla",
  "embedding.load.seed/subTitle": "La semilla para el generador de n√∫meros aleatorios usado en la generaci√≥n de texto. -1 es semilla aleatoria",

  "embedding.load.seed/info": "Semilla Aleatoria: Establece la semilla para la generaci√≥n de n√∫meros aleatorios para asegurar resultados reproducibles",

  "presetTooltip": {
    "included/title": "Valores del Preset",
    "included/description": "Los siguientes campos ser√°n aplicados",
    "included/empty": "Ning√∫n campo de este preset aplica en este contexto.",
    "included/conflict": "Se te preguntar√° si elegir aplicar este valor",
    "separateLoad/title": "Configuraci√≥n de Tiempo de Carga",
    "separateLoad/description.1": "El preset tambi√©n incluye la siguiente configuraci√≥n de tiempo de carga. La configuraci√≥n de tiempo de carga es para todo el modelo y requiere recargar el modelo para tomar efecto. Mant√©n presionado",
    "separateLoad/description.2": "para aplicar a",
    "separateLoad/description.3": ".",
    "excluded/title": "Puede no aplicar",
    "excluded/description": "Los siguientes campos est√°n incluidos en el preset pero no aplican en el contexto actual.",
    "legacy/title": "Preset Heredado",
    "legacy/description": "Este preset es un preset heredado. Incluye los siguientes campos que ahora se manejan autom√°ticamente, o ya no son aplicables.",
    "button/publish": "Publicar en el Hub",
    "button/pushUpdate": "Subir Cambios al Hub",
    "button/noChangesToPush": "No hay cambios que subir",
    "button/export": "Exportar",
    "hubLabel": "Preset del Hub por {{user}}",
    "ownHubLabel": "Tu preset del Hub"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Vac√≠o>"
    },
    "checkboxNumeric": {
      "off": "DESACTIVADO"
    },
    "llamaCacheQuantizationType": {
      "off": "DESACTIVADO"
    },
    "mlxKvCacheBits": {
      "off": "DESACTIVADO"
    },
    "stringArray": {
      "empty": "<Vac√≠o>"
    },
    "llmPromptTemplate": {
      "type": "Tipo",
      "types.jinja/label": "Plantilla (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Plantilla",
      "jinja/error": "Error al analizar plantilla Jinja: {{error}}",
      "jinja/empty": "Por favor ingresa una plantilla Jinja arriba.",
      "jinja/unlikelyToWork": "La plantilla Jinja que proporcionaste arriba es poco probable que funcione ya que no hace referencia a la variable \"messages\". Por favor verifica si has ingresado una plantilla correcta.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Antes del Sistema",
      "manual.subfield.beforeSystem/placeholder": "Ingresa prefijo del Sistema...",
      "manual.subfield.afterSystem/label": "Despu√©s del Sistema",
      "manual.subfield.afterSystem/placeholder": "Ingresa sufijo del Sistema...",
      "manual.subfield.beforeUser/label": "Antes del Usuario",
      "manual.subfield.beforeUser/placeholder": "Ingresa prefijo del Usuario...",
      "manual.subfield.afterUser/label": "Despu√©s del Usuario",
      "manual.subfield.afterUser/placeholder": "Ingresa sufijo del Usuario...",
      "manual.subfield.beforeAssistant/label": "Antes del Asistente",
      "manual.subfield.beforeAssistant/placeholder": "Ingresa prefijo del Asistente...",
      "manual.subfield.afterAssistant/label": "Despu√©s del Asistente",
      "manual.subfield.afterAssistant/placeholder": "Ingresa sufijo del Asistente...",
      "stopStrings/label": "Cadenas de Parada Adicionales",
      "stopStrings/subTitle": "Cadenas de parada espec√≠ficas de la plantilla que se usar√°n adem√°s de las cadenas de parada especificadas por el usuario."
    },
    "contextLength": {
      "maxValueTooltip": "Este es el n√∫mero m√°ximo de tokens que el modelo fue entrenado para manejar. Haz clic para establecer el contexto a este valor",
      "maxValueTextStart": "El modelo soporta hasta",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "Aunque un modelo puede soportar hasta cierto n√∫mero de tokens, el rendimiento puede deteriorarse si los recursos de tu m√°quina no pueden manejar la carga - usa precauci√≥n al aumentar este valor"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Parar en el L√≠mite",
      "stopAtLimitSub": "Deja de generar una vez que la memoria del modelo se llene",
      "truncateMiddle": "Truncar en el Medio",
      "truncateMiddleSub": "Elimina mensajes del medio de la conversaci√≥n para hacer espacio para los m√°s nuevos. El modelo a√∫n recordar√° el inicio de la conversaci√≥n",
      "rollingWindow": "Ventana Deslizante",
      "rollingWindowSub": "El modelo siempre obtendr√° los mensajes m√°s recientes pero puede olvidar el inicio de la conversaci√≥n"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "M√ÅXIMO",
      "off": "DESACTIVADO"
    },
    "gpuSplitStrategy": {
      "evenly": "Uniformemente",
      "favorMainGpu": "Favorecer GPU Principal"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Lee c√≥mo funciona",
      "placeholder": "Selecciona un modelo de borrador compatible",
      "noCompatible": "No se encontraron modelos de borrador compatibles para tu selecci√≥n de modelo actual",
      "stillLoading": "Identificando modelos de borrador compatibles...",
      "notCompatible": "El modelo de borrador seleccionado (<draft/>) no es compatible con la selecci√≥n de modelo actual (<current/>).",
      "off": "DESACTIVADO",
      "loadModelToSeeOptions": "Carga el modelo <keyboard-shortcut /> para ver opciones compatibles",
      "compatibleWithNumberOfModels": "Recomendado para al menos {{dynamicValue}} de tus modelos",
      "recommendedForSomeModels": "Recomendado para algunos modelos",
      "recommendedForLlamaModels": "Recomendado para modelos Llama",
      "recommendedForQwenModels": "Recomendado para modelos Qwen",
      "onboardingModal": {
        "introducing": "Presentando",
        "speculativeDecoding": "Decodificaci√≥n Especulativa",
        "firstStepBody": "Aceleraci√≥n de inferencia para modelos <custom-span>llama.cpp</custom-span> y <custom-span>MLX</custom-span>",
        "secondStepTitle": "Aceleraci√≥n de Inferencia con Decodificaci√≥n Especulativa",
        "secondStepBody": "La Decodificaci√≥n Especulativa es una t√©cnica que involucra la colaboraci√≥n de dos modelos:\n - Un modelo \"principal\" m√°s grande\n - Un modelo \"de borrador\" m√°s peque√±o\n\nDurante la generaci√≥n, el modelo de borrador propone r√°pidamente tokens para que el modelo principal m√°s grande los verifique. Verificar tokens es un proceso mucho m√°s r√°pido que generarlos realmente, que es la fuente de las ganancias de velocidad. **Generalmente, mientras mayor sea la diferencia de tama√±o entre el modelo principal y el modelo de borrador, mayor ser√° la aceleraci√≥n**.\n\nPara mantener la calidad, el modelo principal solo acepta tokens que se alinean con lo que habr√≠a generado por s√≠ mismo, habilitando la calidad de respuesta del modelo m√°s grande a velocidades de inferencia m√°s r√°pidas. Ambos modelos deben compartir el mismo vocabulario.",
        "draftModelRecommendationsTitle": "Recomendaciones de modelo de borrador",
        "basedOnCurrentModels": "Basado en tus modelos actuales",
        "close": "Cerrar",
        "next": "Siguiente",
        "done": "Listo"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Por favor carga un modelo primero <model-badge /> ",
      "errorEngineNotSupported": "La decodificaci√≥n especulativa requiere al menos la versi√≥n {{minVersion}} del motor {{engineName}}. Por favor actualiza el motor (<key/>) y recarga el modelo para usar esta caracter√≠stica.",
      "errorEngineNotSupported/noKey": "La decodificaci√≥n especulativa requiere al menos la versi√≥n {{minVersion}} del motor {{engineName}}. Por favor actualiza el motor y recarga el modelo para usar esta caracter√≠stica."
    },
    "llmReasoningParsing": {
      "startString/label": "Cadena de Inicio",
      "startString/placeholder": "Ingresa la cadena de inicio...",
      "endString/label": "Cadena de Final",
      "endString/placeholder": "Ingresa la cadena de final..."
    }
  },
  "saveConflictResolution": {
    "title": "Elige qu√© valores incluir en el Preset",
    "description": "Selecciona y elige qu√© valores mantener",
    "instructions": "Haz clic en un valor para incluirlo",
    "userValues": "Valor Anterior",
    "presetValues": "Valor Nuevo",
    "confirm": "Confirmar",
    "cancel": "Cancelar"
  },
  "applyConflictResolution": {
    "title": "¬øQu√© valores mantener?",
    "description": "Tienes cambios sin confirmar que se superponen con el Preset entrante",
    "instructions": "Haz clic en un valor para mantenerlo",
    "userValues": "Valor Actual",
    "presetValues": "Valor del Preset Entrante",
    "confirm": "Confirmar",
    "cancel": "Cancelar"
  },
  "empty": "<Vac√≠o>",
  "noModelSelected": "No hay modelos seleccionados",
  "apiIdentifier.label": "Identificador de API",
  "apiIdentifier.hint": "Opcionalmente proporciona un identificador para este modelo. Esto se usar√° en solicitudes de API. Deja en blanco para usar el identificador predeterminado.",
  "idleTTL.label": "Descargar Autom√°ticamente si Est√° Inactivo (TTL)",
  "idleTTL.hint": "Si se establece, el modelo se descargar√° autom√°ticamente despu√©s de estar inactivo durante la cantidad de tiempo especificada.",
  "idleTTL.mins": "mins",

  "presets": {
    "title": "Preset",
    "commitChanges": "Confirmar Cambios",
    "commitChanges/description": "Confirma tus cambios al preset.",
    "commitChanges.manual": "Nuevos campos detectados. Podr√°s elegir qu√© cambios incluir en el preset.",
    "commitChanges.manual.hold.0": "Mant√©n presionado",
    "commitChanges.manual.hold.1": "para elegir qu√© cambios confirmar al preset.",
    "commitChanges.saveAll.hold.0": "Mant√©n presionado",
    "commitChanges.saveAll.hold.1": "para guardar todos los cambios.",
    "commitChanges.saveInPreset.hold.0": "Mant√©n presionado",
    "commitChanges.saveInPreset.hold.1": "para solo guardar cambios en campos que ya est√°n incluidos en el preset.",
    "commitChanges/error": "Error al confirmar cambios al preset.",
    "commitChanges.manual/description": "Elige qu√© cambios incluir en el preset.",
    "saveAs": "Guardar Como Nuevo...",
    "presetNamePlaceholder": "Ingresa un nombre para el preset...",
    "cannotCommitChangesLegacy": "Este es un preset heredado y no puede ser modificado. Puedes crear una copia usando \"Guardar Como Nuevo...\".",
    "cannotCommitChangesNoChanges": "No hay cambios que confirmar.",
    "emptyNoUnsaved": "Selecciona un Preset...",
    "emptyWithUnsaved": "Preset Sin Guardar",
    "saveEmptyWithUnsaved": "Guardar Preset Como...",
    "saveConfirm": "Guardar",
    "saveCancel": "Cancelar",
    "saving": "Guardando...",
    "save/error": "Error al guardar preset.",
    "deselect": "Deseleccionar Preset",
    "deselect/error": "Error al deseleccionar preset.",
    "select/error": "Error al seleccionar preset.",
    "delete/error": "Error al eliminar preset.",
    "discardChanges": "Descartar Sin Guardar",
    "discardChanges/info": "Descarta todos los cambios sin confirmar y restaura el preset a su estado original",
    "newEmptyPreset": "+ Nuevo Preset",
    "importPreset": "Importar",
    "contextMenuCopyIdentifier": "Copiar Identificador del Preset",
    "contextMenuSelect": "Aplicar Preset",
    "contextMenuDelete": "Eliminar...",
    "contextMenuShare": "Publicar...",
    "contextMenuOpenInHub": "Ver en la Web",
    "contextMenuPullFromHub": "Descargar √öltimo",
    "contextMenuPushChanges": "Subir Cambios al Hub",
    "contextMenuPushingChanges": "Subiendo...",
    "contextMenuPushedChanges": "Cambios subidos",
    "contextMenuExport": "Exportar Archivo",
    "contextMenuRevealInExplorer": "Mostrar en Explorador de Archivos",
    "contextMenuRevealInFinder": "Mostrar en Finder",
    "share": {
      "title": "Publicar Preset",
      "action": "Comparte tu preset para que otros lo descarguen, les guste y lo bifurquen",
      "presetOwnerLabel": "Propietario",
      "uploadAs": "Tu preset se crear√° como {{name}}",
      "presetNameLabel": "Nombre del Preset",
      "descriptionLabel": "Descripci√≥n (opcional)",
      "loading": "Publicando...",
      "success": "Preset Subido Exitosamente",
      "presetIsLive": "¬°<preset-name /> ya est√° en vivo en el Hub!",
      "close": "Cerrar",
      "confirmViewOnWeb": "Ver en la web",
      "confirmCopy": "Copiar URL",
      "confirmCopied": "¬°Copiado!",
      "pushedToHub": "Tu preset fue subido al Hub",
      "descriptionPlaceholder": "Ingresa una descripci√≥n...",
      "willBePublic": "Este preset ser√° p√∫blico. Cualquiera en internet podr√° verlo.",
      "willBePrivate": "Solo t√∫ podr√°s ver este preset",
      "willBeOrgVisible": "Este preset ser√° visible para todos en la organizaci√≥n.",
      "publicSubtitle": "Tu preset es <custom-bold>P√∫blico</custom-bold>. Otros pueden descargarlo y bifurcarlo en lmstudio.ai",
      "privateUsageReached": "L√≠mite de n√∫mero de presets privados alcanzado.",
      "continueInBrowser": "Continuar en el Navegador",
      "confirmShareButton": "Publicar",
      "error": "Error al publicar preset",
      "createFreeAccount": "Crea una cuenta gratuita en el Hub para publicar presets"
    },
    "update": {
      "title": "Subir Cambios al Hub",
      "title/success": "Preset Actualizado Exitosamente",
      "subtitle": "Haz cambios a <custom-preset-name /> y s√∫belos al Hub",
      "descriptionLabel": "Descripci√≥n",
      "descriptionPlaceholder": "Ingresa una descripci√≥n...",
      "loading": "Subiendo...",
      "cancel": "Cancelar",
      "createFreeAccount": "Crea una cuenta gratuita en el Hub para publicar presets",
      "error": "Error al subir actualizaci√≥n",
      "confirmUpdateButton": "Subir"
    },
    "resolve": {
      "title": "Resolver conflictos...",
      "tooltip": "Abre un modal para resolver diferencias con la versi√≥n del Hub"
    },
    "loginToManage": {
      "title": "Inicia sesi√≥n para administrar..."
    },
    "downloadFromHub": {
      "title": "Descargar",
      "downloading": "Descargando...",
      "success": "¬°Descargado!",
      "error": "Error al descargar"
    },
    "push": {
      "title": "Subir cambios",
      "pushing": "Subiendo...",
      "success": "Subido",
      "tooltip": "Sube tus cambios locales a la versi√≥n remota alojada en el Hub",
      "error": "Error al subir"
    },
    "saveAsNewModal": {
      "title": "¬°Ups! No se encontr√≥ el preset en el Hub",
      "confirmSaveAsNewDescription": "¬øQuieres publicar este preset como uno nuevo?",
      "confirmButton": "Publicar como Nuevo"
    },
    "pull": {
      "title": "Descargar √öltimo",
      "error": "Error al descargar",
      "contextMenuErrorMessage": "Error al descargar",
      "success": "Descargado",
      "pulling": "Descargando...",
      "upToDate": "¬°Actualizado!",
      "unsavedChangesModal": {
        "title": "Tienes cambios sin guardar.",
        "bodyContent": "Descargar del remoto sobrescribir√° tus cambios sin guardar. ¬øContinuar?",
        "confirmButton": "Sobrescribir Cambios Sin Guardar"
      }
    },
    "import": {
      "title": "Importar un Preset desde Archivo",
      "dragPrompt": "Arrastra y suelta archivos de preset (.tar.gz o preset.json) o <custom-link>selecciona desde tu computadora</custom-link>",
      "remove": "Eliminar",
      "cancel": "Cancelar",
      "importPreset_zero": "Importar Preset",
      "importPreset_one": "Importar Preset",
      "importPreset_other": "Importar {{count}} Presets",
      "selectDialog": {
        "title": "Seleccionar Archivo de Preset (preset.json o .tar.gz)",
        "button": "Importar"
      },
      "error": "Error al importar preset",
      "resultsModal": {
        "titleSuccessSection_one": "1 preset importado exitosamente",
        "titleSuccessSection_other": "{{count}} presets importados exitosamente",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} fall√≥)",
        "titleFailSection_other": "({{count}} fallaron)",
        "titleAllFailed": "Error al importar presets",
        "importMore": "Importar M√°s",
        "close": "Listo",
        "successBadge": "√âxito",
        "alreadyExistsBadge": "El preset ya existe",
        "errorBadge": "Error",
        "invalidFileBadge": "Archivo inv√°lido",
        "otherErrorBadge": "Error al importar preset",
        "errorViewDetailsButton": "Ver Detalles",
        "seeError": "Ver Error",
        "noName": "Sin nombre de preset",
        "useInChat": "Usar en Chat"
      },
      "importFromUrl": {
        "button": "Importar desde URL...",
        "title": "Importar desde URL",
        "back": "Importar desde Archivo...",
        "action": "Pega la URL del Hub de LM Studio del preset que quieres importar abajo",
        "invalidUrl": "URL inv√°lida. Por favor aseg√∫rate de estar pegando una URL correcta del Hub de LM Studio.",
        "tip": "Puedes instalar el preset directamente con el bot√≥n {{buttonName}} en el Hub de LM Studio",
        "confirm": "Importar",
        "cancel": "Cancelar",
        "loading": "Importando...",
        "error": "Error al descargar preset."
      }
    },
    "download": {
      "title": "Descargar <preset-name /> del Hub de LM Studio",
      "subtitle": "Guarda <custom-name /> en tus presets. Hacer esto te permitir√° usar este preset en la aplicaci√≥n",
      "button": "Descargar",
      "button/loading": "Descargando...",
      "cancel": "Cancelar",
      "error": "Error al descargar preset."
    },
    "inclusiveness": {
      "speculativeDecoding": "Incluir en Preset"
    }
  },

  "flashAttentionWarning": "Flash Attention es una caracter√≠stica experimental que puede causar problemas con algunos modelos. Si encuentras problemas, intenta deshabilitarla.",
  "llamaKvCacheQuantizationWarning": "La Cuantizaci√≥n de Cache KV es una caracter√≠stica experimental que puede causar problemas con algunos modelos. Flash Attention debe estar habilitado para la cuantizaci√≥n de cache V. Si encuentras problemas, restablece al valor predeterminado \"F16\".",

  "seedUncheckedHint": "Semilla Aleatoria",
  "ropeFrequencyBaseUncheckedHint": "Autom√°tico",
  "ropeFrequencyScaleUncheckedHint": "Autom√°tico",

  "hardware": {
    "environmentVariables": "Variables de Entorno",
    "environmentVariables.info": "Si no est√°s seguro, d√©jalas en sus valores predeterminados",
    "environmentVariables.reset": "Restablecer a predeterminado",

    "gpus.information": "Configura las unidades de procesamiento gr√°fico (GPUs) detectadas en tu m√°quina",
    "gpuSettings": {
      "editMaxCapacity": "Editar Capacidad M√°xima",
      "hideEditMaxCapacity": "Ocultar Editar Capacidad M√°xima",
      "allOffWarning": "Todas las GPUs est√°n apagadas o deshabilitadas, aseg√∫rate de que haya alguna asignaci√≥n de GPU para habilitar la carga de modelos",
      "split": {
        "title": "Estrategia",
        "placeholder": "Selecciona una asignaci√≥n de memoria GPU",
        "options": {
          "generalDescription": "Configura c√≥mo se cargar√°n los modelos en tus GPUs",
          "evenly": {
            "title": "Dividir uniformemente",
            "description": "Asignar memoria uniformemente entre GPUs"
          },
          "priorityOrder": {
            "title": "Orden de prioridad",
            "description": "Arrastra para reordenar prioridad. El sistema intentar√° asignar m√°s en GPUs listadas primero"
          },
          "custom": {
            "title": "Personalizado",
            "description": "Asignar memoria",
            "maxAllocation": "Asignaci√≥n M√°xima"
          }
        }
      },
      "deviceId.info": "Identificador √∫nico para este dispositivo",
      "changesOnlyAffectNewlyLoadedModels": "Los cambios solo afectar√°n modelos reci√©n cargados",
      "toggleGpu": "Habilitar/Deshabilitar GPU"
    }
  },

  "load.gpuSplitConfig/title": "Configuraci√≥n de Divisi√≥n GPU",
  "envVars/title": "Establecer una Variable de Entorno",
  "envVars": {
    "select": {
      "placeholder": "Selecciona una variable de entorno...",
      "noOptions": "No hay m√°s disponibles",
      "filter": {
        "placeholder": "Filtrar resultados de b√∫squeda",
        "resultsFound_zero": "No se encontraron resultados",
        "resultsFound_one": "1 resultado encontrado",
        "resultsFound_other": "{{count}} resultados encontrados"
      }
    },
    "inputValue": {
      "placeholder": "Ingresa un valor"
    },
    "values": {
      "title": "Valores Actuales"
    }
  }
}