{
  "noInstanceSelected": "Chưa chọn phiên bản mô hình",
  "resetToDefault": "Đặt lại",
  "showAdvancedSettings": "Hiển thị cài đặt nâng cao",
  "showAll": "Hiển thị tất cả",
  "basicSettings": "Cơ bản",
  "configSubtitle": "Tải hoặc lưu cấu hình sẵn và thử nghiệm với các tham số mô hình tùy chỉnh",
  "inferenceParameters/title": "Tham số Dự đoán",
  "inferenceParameters/info": "Thử nghiệm với các tham số ảnh hưởng đến dự đoán.",
  "generalParameters/title": "Chung",
  "samplingParameters/title": "Lấy mẫu",
  "basicTab": "Cơ bản",
  "advancedTab": "Nâng cao",
  "advancedTab/title": "🧪 Cấu hình Nâng cao",
  "advancedTab/expandAll": "Mở rộng tất cả",
  "advancedTab/overridesTitle": "Ghi đè Cấu hình",
  "advancedTab/noConfigsText": "Bạn không có thay đổi chưa lưu nào - chỉnh sửa giá trị ở trên để thấy ghi đè ở đây.",
  "loadInstanceFirst": "Tải mô hình để xem các tham số có thể cấu hình",
  "noListedConfigs": "Không có tham số có thể cấu hình",
  "generationParameters/info": "Thử nghiệm với các tham số cơ bản ảnh hưởng đến việc tạo văn bản.",
  "loadParameters/title": "Tải Tham số",
  "loadParameters/description": "Cài đặt để kiểm soát cách mô hình được khởi tạo và tải vào bộ nhớ.",
  "loadParameters/reload": "Tải lại để áp dụng thay đổi",
  "discardChanges": "Hủy bỏ thay đổi",
  "llm.prediction.systemPrompt/title": "Prompt Hệ Thống",
  "llm.prediction.systemPrompt/description": "Sử dụng trường này để cung cấp các hướng dẫn nền tảng cho mô hình, chẳng hạn như một tập hợp quy tắc, ràng buộc hoặc yêu cầu chung. Trường này cũng thường được gọi là \"prompt hệ thống\".",
  "llm.prediction.systemPrompt/subTitle": "Hướng dẫn cho AI",
  "llm.prediction.temperature/title": "Nhiệt độ",
  "llm.prediction.temperature/info": "Từ tài liệu llama.cpp: \"Giá trị mặc định là <{{dynamicValue}}>, cung cấp sự cân bằng giữa ngẫu nhiên và xác định. Ở mức cực đoan, nhiệt độ 0 sẽ luôn chọn token có khả năng xảy ra cao nhất tiếp theo, dẫn đến các kết quả giống hệt nhau trong mỗi lần chạy\"",
  "llm.prediction.topKSampling/title": "Lấy mẫu Top K",
  "llm.prediction.topKSampling/info": "Từ tài liệu llama.cpp:\n\nLấy mẫu Top-k là phương pháp tạo văn bản chỉ chọn token tiếp theo từ top k token có khả năng xảy ra cao nhất được mô hình dự đoán.\n\nNó giúp giảm rủi ro tạo ra các token có xác suất thấp hoặc vô nghĩa, nhưng nó cũng có thể hạn chế sự đa dạng của đầu ra.\n\nGiá trị top-k cao hơn (ví dụ, 100) sẽ xem xét nhiều token hơn và dẫn đến văn bản đa dạng hơn, trong khi giá trị thấp hơn (ví dụ, 10) sẽ tập trung vào các token có khả năng xảy ra cao nhất và tạo ra văn bản thận trọng hơn.\n\n• Giá trị mặc định là <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Số luồng CPU",
  "llm.prediction.llama.cpuThreads/info": "Số lượng luồng được sử dụng trong quá trình tính toán. Tăng số luồng không phải lúc nào cũng liên quan đến hiệu suất tốt hơn. Giá trị mặc định là <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Giới hạn Độ dài Phản hồi",
  "llm.prediction.maxPredictedTokens/subTitle": "Tùy chọn giới hạn độ dài phản hồi của AI",
  "llm.prediction.maxPredictedTokens/info": "Kiểm soát độ dài tối đa của phản hồi chatbot. Bật để đặt giới hạn cho độ dài tối đa của phản hồi, hoặc tắt để chatbot tự quyết định khi nào dừng lại.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Độ dài phản hồi tối đa (token)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Khoảng {{maxWords}} từ",
  "llm.prediction.repeatPenalty/title": "Phạt Lặp lại",
  "llm.prediction.repeatPenalty/info": "Từ tài liệu llama.cpp: \"Giúp ngăn mô hình tạo ra văn bản lặp lại hoặc đơn điệu.\n\nGiá trị cao hơn (ví dụ, 1.5) sẽ phạt các lặp lại mạnh mẽ hơn, trong khi giá trị thấp hơn (ví dụ, 0.9) sẽ dễ dãi hơn.\" • Giá trị mặc định là <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Lấy mẫu Min P",
  "llm.prediction.minPSampling/info": "Từ tài liệu llama.cpp:\n\nXác suất tối thiểu để một token được xem xét, so với xác suất của token có khả năng xảy ra cao nhất. Phải nằm trong khoảng [0, 1].\n\n• Giá trị mặc định là <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Lấy mẫu Top P",
  "llm.prediction.topPSampling/info": "Từ tài liệu llama.cpp:\n\nLấy mẫu top-p, còn được gọi là lấy mẫu hạt nhân, là một phương pháp tạo văn bản khác chọn token tiếp theo từ một tập hợp token có xác suất cộng lại ít nhất là p.\n\nPhương pháp này cung cấp sự cân bằng giữa đa dạng và chất lượng bằng cách xem xét cả xác suất của các token và số lượng token để lấy mẫu.\n\nGiá trị top-p cao hơn (ví dụ, 0.95) sẽ dẫn đến văn bản đa dạng hơn, trong khi giá trị thấp hơn (ví dụ, 0.5) sẽ tạo ra văn bản tập trung và thận trọng hơn. Phải nằm trong khoảng (0, 1].\n\n• Giá trị mặc định là <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Chuỗi Dừng",
  "llm.prediction.stopStrings/subTitle": "Các chuỗi nên dừng mô hình tạo thêm token",
  "llm.prediction.stopStrings/info": "Các chuỗi cụ thể khi gặp phải sẽ dừng mô hình tạo thêm token",
  "llm.prediction.stopStrings/placeholder": "Nhập chuỗi và nhấn ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Tràn Bộ nhớ Hội thoại",
  "llm.prediction.contextOverflowPolicy/info": "Quyết định việc làm gì khi cuộc hội thoại vượt quá kích thước bộ nhớ làm việc của mô hình ('ngữ cảnh')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Dừng lại ở Giới hạn",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Dừng tạo văn bản khi bộ nhớ của mô hình đầy",

  "customInputs.contextOverflowPolicy.truncateMiddle": "Cắt bỏ Phần giữa",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Loại bỏ các tin nhắn từ giữa cuộc hội thoại để tạo chỗ cho các tin nhắn mới hơn. Mô hình vẫn sẽ nhớ phần đầu của cuộc hội thoại",
  "customInputs.contextOverflowPolicy.rollingWindow": "Cửa sổ Cuộn",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "Mô hình sẽ luôn nhận được một vài tin nhắn gần đây nhất nhưng có thể quên phần đầu của cuộc hội thoại",
  "llm.prediction.llama.frequencyPenalty/title": "Phạt Tần suất",
  "llm.prediction.llama.presencePenalty/title": "Phạt Sự hiện diện",
  "llm.prediction.llama.tailFreeSampling/title": "Lấy mẫu Không Đuôi",
  "llm.prediction.llama.locallyTypicalSampling/title": "Lấy mẫu Điển hình Cục bộ",
  "llm.prediction.onnx.topKSampling/title": "Lấy mẫu Top K",
  "llm.prediction.onnx.topKSampling/info": "Từ tài liệu ONNX:\n\nSố lượng token có xác suất cao nhất được giữ lại để lọc top-k\n\n• Bộ lọc này mặc định tắt",
  "llm.prediction.onnx.repeatPenalty/title": "Phạt Lặp lại",
  "llm.prediction.onnx.repeatPenalty/info": "Giá trị cao hơn làm cho mô hình ít lặp lại chính nó",
  "llm.prediction.onnx.topPSampling/title": "Lấy mẫu Top P",
  "llm.prediction.onnx.topPSampling/info": "Từ tài liệu ONNX:\n\nChỉ giữ lại những token có xác suất cao nhất với tổng xác suất đạt TopP trở lên để sinh văn bản\n\n• Bộ lọc này mặc định tắt",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Đầu ra có Cấu trúc",
  "llm.prediction.structured/info": "Đầu ra có Cấu trúc",
  "llm.prediction.promptTemplate/title": "Mẫu Prompt",
  "customInputs.llmPromptTemplate.types.jinja/label": "Jinja",
  "customInputs.llmPromptTemplate.jinja/error": "Không thể phân tích mẫu Jinja: {{error}}",
  "customInputs.llmPromptTemplate.types.manual/label": "Thủ công",
  "customInputs.llmPromptTemplate.manual.subfield.beforeSystem/label": "Trước Hệ thống",
  "customInputs.llmPromptTemplate.manual.subfield.beforeSystem/placeholder": "Nhập tiền tố Hệ thống...",
  "customInputs.llmPromptTemplate.manual.subfield.afterSystem/label": "Sau Hệ thống",
  "customInputs.llmPromptTemplate.manual.subfield.afterSystem/placeholder": "Nhập hậu tố Hệ thống...",
  "customInputs.llmPromptTemplate.manual.subfield.beforeUser/label": "Trước Người dùng",
  "customInputs.llmPromptTemplate.manual.subfield.beforeUser/placeholder": "Nhập tiền tố Người dùng...",
  "customInputs.llmPromptTemplate.manual.subfield.afterUser/label": "Sau Người dùng",
  "customInputs.llmPromptTemplate.manual.subfield.afterUser/placeholder": "Nhập hậu tố Người dùng...",
  "customInputs.llmPromptTemplate.manual.subfield.beforeAssistant/label": "Trước Trợ lý",
  "customInputs.llmPromptTemplate.manual.subfield.beforeAssistant/placeholder": "Nhập tiền tố Trợ lý...",
  "customInputs.llmPromptTemplate.manual.subfield.afterAssistant/label": "Sau Trợ lý",
  "customInputs.llmPromptTemplate.manual.subfield.afterAssistant/placeholder": "Nhập hậu tố Trợ lý...",
  "customInputs.llmPromptTemplate.stopStrings/label": "Chuỗi Dừng Bổ sung",
  "customInputs.llmPromptTemplate.stopStrings/hint": "Chuỗi dừng cụ thể trong mẫu sẽ được sử dụng thêm cùng với chuỗi dừng do người dùng chỉ định.",

  "llm.load.contextLength/title": "Độ dài Ngữ cảnh",
  "llm.load.contextLength/info": "Chỉ định số lượng tối đa token mà mô hình có thể xem xét một lúc, ảnh hưởng đến mức độ ngữ cảnh mà nó giữ lại trong quá trình xử lý",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/info": "Seed ngẫu nhiên: Đặt giá trị seed để tạo số ngẫu nhiên nhằm đảm bảo kết quả có thể lặp lại",

  "llm.load.llama.evalBatchSize/title": "Kích thước Lô Đánh giá",
  "llm.load.llama.evalBatchSize/info": "Đặt số lượng ví dụ được xử lý cùng lúc trong một lô trong quá trình đánh giá, ảnh hưởng đến tốc độ và sử dụng bộ nhớ",
  "llm.load.llama.ropeFrequencyBase/title": "Cơ sở Tần suất RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Nâng cao] Điều chỉnh tần số cơ bản cho Mã hóa Vị trí Quay, ảnh hưởng đến cách thông tin vị trí được nhúng",
  "llm.load.llama.ropeFrequencyScale/title": "Thang đo Tần suất RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Nâng cao] Điều chỉnh thang đo tần số cho Mã hóa Vị trí Quay để kiểm soát độ chi tiết mã hóa vị trí",
  "llm.load.llama.gpuOffload/title": "Giảm tải GPU",
  "llm.load.llama.gpuOffload/info": "Đặt tỷ lệ tính toán để giảm tải GPU. Đặt thành tắt để vô hiệu hóa giảm tải GPU, hoặc tự động để mô hình tự quyết định.",
  "llm.load.llama.flashAttention/title": "Tăng tốc Chú ý",
  "llm.load.llama.flashAttention/info": "Tăng tốc cơ chế chú ý để xử lý nhanh hơn và hiệu quả hơn",
  "llm.load.llama.keepModelInMemory/title": "Giữ Mô hình trong Bộ nhớ",
  "llm.load.llama.keepModelInMemory/info": "Ngăn mô hình khỏi bị hoán đổi ra đĩa, đảm bảo truy cập nhanh hơn với chi phí sử dụng RAM cao hơn",
  "llm.load.llama.useFp16ForKVCache/title": "Sử dụng FP16 cho Bộ nhớ Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Giảm sử dụng bộ nhớ bằng cách lưu trữ bộ nhớ cache ở độ chính xác nửa (FP16)",
  "llm.load.llama.tryMmap/title": "Thử mmap()",
  "llm.load.llama.tryMmap/info": "Tải tệp mô hình trực tiếp từ đĩa vào bộ nhớ",

  "embedding.load.contextLength/title": "Độ dài Ngữ cảnh",
  "embedding.load.contextLength/info": "Chỉ định số lượng tối đa token mà mô hình có thể xem xét một lúc, ảnh hưởng đến mức độ ngữ cảnh mà nó giữ lại trong quá trình xử lý",
  "embedding.load.llama.ropeFrequencyBase/title": "Cơ sở Tần suất RoPE",
  "embedding.load.llama.ropeFrequencyBase/info": "[Nâng cao] Điều chỉnh tần số cơ bản cho Mã hóa Vị trí Quay, ảnh hưởng đến cách thông tin vị trí được nhúng",
  "embedding.load.llama.evalBatchSize/title": "Kích thước Lô Đánh giá",
  "embedding.load.llama.evalBatchSize/info": "Đặt số lượng token được xử lý cùng lúc trong một lô trong quá trình đánh giá",
  "embedding.load.llama.ropeFrequencyScale/title": "Thang đo Tần suất RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Nâng cao] Điều chỉnh thang đo tần số cho Mã hóa Vị trí Quay để kiểm soát độ chi tiết mã hóa vị trí",
  "embedding.load.llama.gpuOffload/title": "Giảm tải GPU",
  "embedding.load.llama.gpuOffload/info": "Đặt tỷ lệ tính toán để giảm tải GPU. Đặt thành tắt để vô hiệu hóa giảm tải GPU, hoặc tự động để mô hình tự quyết định.",
  "embedding.load.llama.keepModelInMemory/title": "Giữ Mô hình trong Bộ nhớ",
  "embedding.load.llama.keepModelInMemory/info": "Ngăn mô hình khỏi bị hoán đổi ra đĩa, đảm bảo truy cập nhanh hơn với chi phí sử dụng RAM cao hơn",
  "embedding.load.llama.tryMmap/title": "Thử mmap()",
  "embedding.load.llama.tryMmap/info": "Tải tệp mô hình trực tiếp từ đĩa vào bộ nhớ",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/info": "Seed ngẫu nhiên: Đặt giá trị seed để tạo số ngẫu nhiên để đảm bảo kết quả có thể lặp lại"
}
