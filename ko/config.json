{
  "noInstanceSelected": "선택된 모델 인스턴스 없음",
  "resetToDefault": "기본값으로 재설정",
  "showAdvancedSettings": "고급 설정 표시",
  "showAll": "모두 보기",
  "basicSettings": "기본",
  "configSubtitle": "프리셋 불러오기 또는 저장 및 모델 매개변수 재정의 실험",
  "inferenceParameters/title": "예측 매개변수",
  "inferenceParameters/info": "예측에 영향을 미치는 매개변수를 실험해보세요.",
  "generalParameters/title": "일반",
  "samplingParameters/title": "샘플링",
  "basicTab": "기본",
  "advancedTab": "고급",
  "advancedTab/title": "🧪 고급 구성",
  "advancedTab/expandAll": "모두 펼치기",
  "advancedTab/overridesTitle": "구성 재정의",
  "advancedTab/noConfigsText": "저장되지 않은 변경 사항이 없습니다 - 위의 값을 편집하여 여기에서 재정의를 확인하세요.",
  "loadInstanceFirst": "구성 가능한 매개변수를 보려면 모델을 불러오세요",
  "noListedConfigs": "구성 가능한 매개변수 없음",
  "generationParameters/info": "텍스트 생성에 영향을 미치는 기본 매개변수를 실험해보세요.",
  "loadParameters/title": "매개변수 불러오기",
  "loadParameters/description": "모델 초기화 및 메모리 로드 방식을 제어하는 설정입니다.",
  "loadParameters/reload": "변경 사항을 적용하려면 다시 불러오세요",
  "discardChanges": "변경 취소",
  "llm.prediction.systemPrompt/title": "시스템 프롬프트",
  "llm.prediction.systemPrompt/description": "모델에 배경 지침을 제공하는 데 사용합니다. 규칙, 제약 조건 또는 일반 요구 사항 등을 포함할 수 있습니다. 이 필드는 '시스템 프롬프트'라고도 합니다.",
  "llm.prediction.systemPrompt/subTitle": "AI를 위한 지침",
  "llm.prediction.temperature/title": "온도",
  "llm.prediction.temperature/subTitle": "무작위성 정도를 조절합니다. 0은 항상 같은 결과를 생성하며, 높은 값은 창의성과 다양성을 증가시킵니다",
  "llm.prediction.temperature/info": "llama.cpp 도움말: \"기본값은 <{{dynamicValue}}>로, 무작위성과 결정론적 특성 사이의 균형을 제공합니다. 극단적으로, 온도 0은 항상 가장 가능성 높은 다음 토큰을 선택하여 매번 동일한 출력을 생성합니다\"",
  "llm.prediction.llama.sampling/title": "샘플링",
  "llm.prediction.topKSampling/title": "Top K 샘플링",
  "llm.prediction.topKSampling/subTitle": "다음 토큰을 Top k의 가장 가능성 높은 토큰으로 제한합니다. 온도와 유사하게 작용합니다",
  "llm.prediction.topKSampling/info": "llama.cpp 도움말:\n\n상위 k 샘플링은 모델이 예측한 가장 가능성 높은 상위 k개의 토큰 중에서만 다음 토큰을 선택하는 텍스트 생성 방법입니다.\n\n낮은 확률이나 의미 없는 토큰 생성 위험을 줄이지만, 출력의 다양성도 제한할 수 있습니다.\n\n상위 k 값이 높을수록(예: 100) 더 다양한 텍스트를 생성하고, 낮은 값(예: 10)은 가장 가능성 높은 토큰에 집중하여 더 보수적인 텍스트를 생성합니다.\n\n• 기본값은 <{{dynamicValue}}>입니다",
  "llm.prediction.llama.cpuThreads/title": "CPU 스레드",
  "llm.prediction.llama.cpuThreads/subTitle": "추론 중 사용할 CPU 스레드 수",
  "llm.prediction.llama.cpuThreads/info": "계산 중 사용할 스레드 수입니다. 스레드 수를 늘리는 것이 항상 더 나은 성능으로 이어지지는 않습니다. 기본값은 <{{dynamicValue}}>입니다.",
  "llm.prediction.maxPredictedTokens/title": "응답 길이 제한",
  "llm.prediction.maxPredictedTokens/subTitle": "AI 응답 길이를 선택적으로 제한",
  "llm.prediction.maxPredictedTokens/info": "챗봇 응답의 최대 길이를 제어합니다. 켜면 응답의 최대 길이를 설정하고, 끄면 챗봇이 스스로 멈출 시점을 결정하도록 합니다.",
  "llm.prediction.maxPredictedTokens/inputLabel": "최대 응답 길이 (토큰)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "약 {{maxWords}}단어",
  "llm.prediction.repeatPenalty/title": "반복 패널티",
  "llm.prediction.repeatPenalty/subTitle": "같은 토큰 반복을 얼마나 억제할지 설정",
  "llm.prediction.repeatPenalty/info": "llama.cpp 도움말: \"모델이 반복적이거나 단조로운 텍스트를 생성하는 것을 방지합니다.\n\n높은 값(예: 1.5)은 반복을 더 강하게 패널티를 주고, 낮은 값(예: 0.9)은 더 관대합니다.\" • 기본값은 <{{dynamicValue}}>입니다",
  "llm.prediction.minPSampling/title": "Min P 샘플링",
  "llm.prediction.minPSampling/subTitle": "출력을 위해 토큰이 선택될 수 있는 최소 기본 확률",
  "llm.prediction.minPSampling/info": "llama.cpp 도움말 문서에서:\n\n가장 가능성 있는 토큰의 확률에 대한 상대적인 토큰의 최소 확률입니다. [0, 1] 범위 내에 있어야 합니다.\n\n• 기본값은 <{{dynamicValue}}>입니다",
  "llm.prediction.topPSampling/title": "Top P 샘플링",
  "llm.prediction.topPSampling/subTitle": "가능한 다음 토큰들의 최소 누적 확률. 온도와 유사하게 작용합니다",
  "llm.prediction.topPSampling/info": "llama.cpp 도움말 문서에서:\n\nTop-p 샘플링은 nucleus 샘플링이라고도 알려져 있으며, 누적 확률이 적어도 p인 토큰들의 부분집합에서 다음 토큰을 선택하는 또 다른 텍스트 생성 방법입니다.\n\n이 방법은 토큰의 확률과 샘플링할 토큰의 수를 모두 고려하여 다양성과 품질 사이의 균형을 제공합니다.\n\n상위-p 값이 높을수록(예: 0.95) 더 다양한 텍스트가 생성되고, 낮은 값(예: 0.5)은 더 집중적이고 보수적인 텍스트를 생성합니다. 반드시 (0, 1] 범위 내에 있어야 합니다.\n\n• 기본값은 <{{dynamicValue}}>입니다",
  "llm.prediction.stopStrings/title": "중지 문자열 (Stop Strings)",
  "llm.prediction.stopStrings/subTitle": "모델이 더 이상 토큰을 생성하지 않도록 중지할 문자열",
  "llm.prediction.stopStrings/info": "특정 문자열이 나타나면 모델이 더 이상 토큰을 생성하지 않도록 중지합니다",
  "llm.prediction.stopStrings/placeholder": "문자열을 입력하고 ⏎ 키를 누르세요",
  "llm.prediction.contextOverflowPolicy/title": "대화 초과 정책 (Conversation Overflow)",
  "llm.prediction.contextOverflowPolicy/subTitle": "대화가 너무 길어질 때 모델의 동작 방식",
  "llm.prediction.contextOverflowPolicy/info": "대화가 모델의 작업 메모리('컨텍스트') 크기를 초과할 때의 처리 방법을 결정합니다",
  "llm.prediction.llama.frequencyPenalty/title": "빈도 페널티 (Frequency Penalty)",
  "llm.prediction.llama.presencePenalty/title": "존재 페널티 (Presence Penalty)",
  "llm.prediction.llama.tailFreeSampling/title": "테일 프리 샘플링 (Tail-Free Sampling)",
  "llm.prediction.llama.locallyTypicalSampling/title": "로컬리 티피컬 샘플링 (Locally Typical Sampling)",
  "llm.prediction.onnx.topKSampling/title": "탑 K 샘플링 (Top K Sampling)",
  "llm.prediction.onnx.topKSampling/subTitle": "다음 토큰을 상위 k개의 가장 가능성 높은 토큰 중 하나로 제한합니다. 온도와 유사하게 작동합니다",
  "llm.prediction.onnx.topKSampling/info": "ONNX 문서에서 발췌:\n\n상위 k개의 가장 높은 확률의 어휘 토큰을 유지하여 필터링합니다\n\n• 이 필터는 기본적으로 꺼져 있습니다",
  "llm.prediction.onnx.repeatPenalty/title": "반복 페널티 (Repeat Penalty)",
  "llm.prediction.onnx.repeatPenalty/subTitle": "동일한 토큰 반복을 얼마나 억제할지",
  "llm.prediction.onnx.repeatPenalty/info": "값이 높을수록 모델이 자신을 반복하는 것을 억제합니다",
  "llm.prediction.onnx.topPSampling/title": "탑 P 샘플링 (Top P Sampling)",
  "llm.prediction.onnx.topPSampling/subTitle": "가능한 다음 토큰의 최소 누적 확률. 온도와 유사하게 작동합니다",
  "llm.prediction.onnx.topPSampling/info": "ONNX 문서에서 발췌:\n\n가장 가능성 높은 토큰 중 누적 확률이 TopP 이상인 토큰만 생성에 사용됩니다\n\n• 이 필터는 기본적으로 꺼져 있습니다",
  "llm.prediction.seed/title": "시드 (Seed)",
  "llm.prediction.structured/title": "구조화된 출력 (Structured Output)",
  "llm.prediction.structured/info": "구조화된 출력",
  "llm.prediction.promptTemplate/title": "프롬프트 템플릿",
  "llm.prediction.promptTemplate/subTitle": "채팅 메시지가 모델에 전송되는 형식입니다. 이를 변경하면 예상치 못한 동작이 발생할 수 있으므로 신중히 사용하세요!",
  "llm.load.contextLength/title": "컨텍스트 길이 (Context Length)",
  "llm.load.contextLength/subTitle": "모델이 하나의 프롬프트에서 처리할 수 있는 최대 토큰 수입니다. '추론 매개변수' 아래의 대화 초과 옵션을 참조하여 이를 관리하는 방법을 확인하세요",
  "llm.load.contextLength/info": "모델이 한 번에 고려할 수 있는 최대 토큰 수를 지정하여 처리 중에 유지되는 컨텍스트 양에 영향을 줍니다",
  "llm.load.contextLength/warning": "컨텍스트 길이에 높은 값을 설정하면 메모리 사용량에 상당한 영향을 미칠 수 있습니다",
  "llm.load.seed/title": "시드 (Seed)",
  "llm.load.seed/subTitle": "텍스트 생성에 사용되는 난수 생성기의 시드입니다. -1은 무작위입니다",
  "llm.load.seed/info": "랜덤 시드: 재현 가능한 결과를 보장하기 위해 난수 생성의 시드를 설정합니다",

  "llm.load.llama.evalBatchSize/title": "평가 배치 크기 (Evaluation Batch Size)",
  "llm.load.llama.evalBatchSize/subTitle": "한 번에 처리할 입력 토큰 수입니다. 이를 늘리면 성능이 향상되지만 메모리 사용량이 증가합니다",
  "llm.load.llama.evalBatchSize/info": "평가 중에 한 번에 처리되는 예제 수를 설정하여 속도와 메모리 사용량에 영향을 줍니다",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 기본 주파수 (RoPE Frequency Base)",
  "llm.load.llama.ropeFrequencyBase/subTitle": "회전 위치 임베딩(RoPE)을 위한 사용자 정의 기본 주파수입니다. 이를 증가시키면 높은 컨텍스트 길이에서 성능이 향상될 수 있습니다",
  "llm.load.llama.ropeFrequencyBase/info": "[고급] 회전 위치 인코딩의 기본 주파수를 조정하여 위치 정보가 임베딩되는 방식을 변경합니다",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 주파수 스케일 (RoPE Frequency Scale)",
  "llm.load.llama.ropeFrequencyScale/subTitle": "RoPE를 사용하여 효과적인 컨텍스트를 확장하기 위해 이 계수로 컨텍스트 길이를 스케일링합니다",
  "llm.load.llama.ropeFrequencyScale/info": "[고급] 회전 위치 인코딩의 주파수 스케일링을 수정하여 위치 인코딩의 세분성을 제어합니다",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 오프로딩 (GPU Offload)",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "GPU 가속을 위해 GPU에서 계산할 개별 모델 레이어 수",
  "llm.load.llama.acceleration.offloadRatio/info": "GPU로 오프로딩할 레이어 수를 설정합니다",
  "llm.load.llama.flashAttention/title": "플래시 어텐션 (Flash Attention)",
  "llm.load.llama.flashAttention/subTitle": "일부 모델에서 메모리 사용량과 생성 시간을 줄입니다",
  "llm.load.llama.flashAttention/info": "어텐션 메커니즘을 가속화하여 더 빠르고 효율적인 처리를 제공합니다",
  "llm.load.numExperts/title": "전문가 수",
  "llm.load.numExperts/subTitle": "모델에서 사용할 전문가 수",
  "llm.load.numExperts/info": "모델에서 사용할 전문가 수",
  "llm.load.llama.keepModelInMemory/title": "모델을 메모리에 유지",
  "llm.load.llama.keepModelInMemory/subTitle": "모델을 GPU로 오프로딩할 때도 시스템 메모리에 예약합니다. 성능이 향상되지만 시스템 RAM이 더 많이 필요합니다",
  "llm.load.llama.keepModelInMemory/info": "모델이 디스크로 스왑되지 않도록 하여 더 빠른 접근을 보장하지만 RAM 사용량이 증가합니다",
  "llm.load.llama.useFp16ForKVCache/title": "KV 캐시에 FP16 사용",
  "llm.load.llama.useFp16ForKVCache/info": "캐시를 반정밀도(FP16)로 저장하여 메모리 사용량을 줄입니다",
  "llm.load.llama.tryMmap/title": "mmap() 시도",
  "llm.load.llama.tryMmap/subTitle": "모델의 로드 시간을 개선합니다. 모델이 사용 가능한 시스템 RAM보다 클 때 이를 비활성화하면 성능이 향상될 수 있습니다",
  "llm.load.llama.tryMmap/info": "모델 파일을 디스크에서 메모리로 직접 로드합니다",

  "embedding.load.contextLength/title": "컨텍스트 길이 (Context Length)",
  "embedding.load.contextLength/subTitle": "모델이 하나의 프롬프트에서 처리할 수 있는 최대 토큰 수입니다. '추론 매개변수' 아래의 대화 초과 옵션을 참조하여 이를 관리하는 방법을 확인하세요",
  "embedding.load.contextLength/info": "모델이 한 번에 고려할 수 있는 최대 토큰 수를 지정하여 처리 중에 유지되는 컨텍스트 양에 영향을 줍니다",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 기본 주파수 (RoPE Frequency Base)",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "회전 위치 임베딩(RoPE)을 위한 사용자 정의 기본 주파수입니다. 이를 증가시키면 높은 컨텍스트 길이에서 성능이 향상될 수 있습니다",
  "embedding.load.llama.ropeFrequencyBase/info": "[고급] 회전 위치 인코딩의 기본 주파수를 조정하여 위치 정보가 임베딩되는 방식을 변경합니다",
  "embedding.load.llama.evalBatchSize/title": "평가 배치 크기 (Evaluation Batch Size)",
  "embedding.load.llama.evalBatchSize/subTitle": "한 번에 처리할 입력 토큰 수입니다. 이를 늘리면 성능이 향상되지만 메모리 사용량이 증가합니다",
  "embedding.load.llama.evalBatchSize/info": "평가 중에 한 번에 처리되는 토큰 수를 설정합니다",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 주파수 스케일 (RoPE Frequency Scale)",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "RoPE를 사용하여 효과적인 컨텍스트를 확장하기 위해 이 계수로 컨텍스트 길이를 스케일링합니다",
  "embedding.load.llama.ropeFrequencyScale/info": "[고급] 회전 위치 인코딩의 주파수 스케일링을 수정하여 위치 인코딩의 세분성을 제어합니다",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 오프로딩 (GPU Offload)",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "GPU 가속을 위해 GPU에서 계산할 개별 모델 레이어 수",
  "embedding.load.llama.acceleration.offloadRatio/info": "GPU로 오프로딩할 레이어 수를 설정합니다",
  "embedding.load.llama.keepModelInMemory/title": "모델을 메모리에 유지",
  "embedding.load.llama.keepModelInMemory/subTitle": "모델을 GPU로 오프로딩할 때도 시스템 메모리에 예약합니다. 성능이 향상되지만 시스템 RAM이 더 많이 필요합니다",
  "embedding.load.llama.keepModelInMemory/info": "모델이 디스크로 스왑되지 않도록 하여 더 빠른 접근을 보장하지만 RAM 사용량이 증가합니다",
  "embedding.load.llama.tryMmap/title": "mmap() 시도",
  "embedding.load.llama.tryMmap/subTitle": "모델의 로드 시간을 개선합니다. 모델이 사용 가능한 시스템 RAM보다 클 때 이를 비활성화하면 성능이 향상될 수 있습니다",
  "embedding.load.llama.tryMmap/info": "모델 파일을 디스크에서 메모리로 직접 로드합니다",
  "embedding.load.seed/title": "시드 (Seed)",
  "embedding.load.seed/subTitle": "텍스트 생성에 사용되는 난수 생성기의 시드입니다. -1은 무작위 시드입니다",

  "embedding.load.seed/info": "랜덤 시드: 재현 가능한 결과를 보장하기 위해 난수 생성의 시드를 설정합니다",

  "presetTooltip": {
    "included/title": "적용됨",
    "included/description": "다음 필드가 적용됩니다",
    "included/empty": "이 컨텍스트에서는 이 프리셋의 필드가 적용되지 않습니다.",
    "separateLoad/title": "로드 시간 구성",
    "separateLoad/description.1": "프리셋에는 다음과 같은 로드 시간 구성도 포함됩니다. 로드 시간 구성은 모델 전체에 적용되며 효과를 적용하려면 모델을 다시 로드해야 합니다.",
    "separateLoad/description.2": "을(를) 눌러 적용하세요",
    "separateLoad/description.3": ".",
    "excluded/title": "제외됨",
    "excluded/description": "다음 필드는 프리셋에 포함되어 있지만 현재 컨텍스트에서는 적용되지 않습니다.",
    "legacy/title": "레거시 프리셋",
    "legacy/description": "이 프리셋은 레거시 프리셋입니다. 현재 자동으로 처리되거나 더 이상 적용되지 않는 다음 필드를 포함합니다."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<비어 있음>"
    },
    "checkboxNumeric": {
      "off": "꺼짐"
    },
    "stringArray": {
      "empty": "<비어 있음>"
    },
    "llmPromptTemplate": {
      "type": "유형",
      "types.jinja/label": "Jinja",
      "jinja.bosToken/label": "BOS 토큰",
      "jinja.eosToken/label": "EOS 토큰",
      "jinja.template/label": "템플릿",
      "jinja/error": "진자 템플릿 파싱 실패: {{error}}",
      "types.manual/label": "수동",
      "manual.subfield.beforeSystem/label": "시스템 전",
      "manual.subfield.beforeSystem/placeholder": "시스템 접두사 입력...",
      "manual.subfield.afterSystem/label": "시스템 후",
      "manual.subfield.afterSystem/placeholder": "시스템 접미사 입력...",
      "manual.subfield.beforeUser/label": "사용자 전",
      "manual.subfield.beforeUser/placeholder": "사용자 접두사 입력...",
      "manual.subfield.afterUser/label": "사용자 후",
      "manual.subfield.afterUser/placeholder": "사용자 접미사 입력...",
      "manual.subfield.beforeAssistant/label": "어시스턴트 전",
      "manual.subfield.beforeAssistant/placeholder": "어시스턴트 접두사 입력...",
      "manual.subfield.afterAssistant/label": "어시스턴트 후",
      "manual.subfield.afterAssistant/placeholder": "어시스턴트 접미사 입력...",
      "stopStrings/label": "추가 중지 문자열",
      "stopStrings/subTitle": "사용자 지정 중지 문자열 외에 사용될 템플릿 특정 중지 문자열입니다."
    },
    "contextLength": {
      "maxValueTooltip": "이는 모델이 처리하도록 훈련된 최대 토큰 수입니다. 클릭하여 컨텍스트를 이 값으로 설정하세요",
      "maxValueTextStart": "모델은 최대",
      "maxValueTextEnd": "토큰을 지원합니다",
      "tooltipHint": "모델이 특정 수의 토큰을 지원할 수 있지만, 기계의 리소스가 부하를 처리할 수 없는 경우 성능이 저하될 수 있습니다 - 이 값을 증가시킬 때는 주의하세요"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "한계에서 중지",
      "stopAtLimitSub": "모델의 메모리가 가득 차면 생성을 중지합니다",
      "truncateMiddle": "중간 잘라내기",
      "truncateMiddleSub": "새로운 메시지를 위한 공간을 만들기 위해 대화 중간에서 메시지를 제거합니다. 모델은 여전히 대화의 시작을 기억할 것입니다",
      "rollingWindow": "롤링 윈도우",
      "rollingWindowSub": "모델은 항상 가장 최근의 몇 개 메시지를 받지만 대화의 시작을 잊을 수 있습니다"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "최대",
      "off": "꺼짐"
    }
  },
  "conflictResolution": {
    "title": "충돌",
    "description": "저장되지 않은 설정과 적용하려는 프리셋 사이에 충돌이 있습니다. 어떻게 해결하시겠습니까?",
    "userValues": "귀하의 변경사항",
    "presetValues": "들어오는 변경사항",
    "confirm": "확인",
    "cancel": "취소"
  },
  "empty": "<비어 있음>",

  "flashAttentionWarning": "플래시 어텐션은 일부 모델에서 문제를 일으킬 수 있는 실험적 기능입니다. 문제가 발생하면 비활성화해 보세요."
}