{
  "noInstanceSelected": "Δεν έχει επιλεγεί κάποιο μοντέλο",
  "resetToDefault": "Επαναφορά",
  "showAdvancedSettings": "Εμφάνιση προχωρημένων ρυθμίσεων",
  "showAll": "Εμφάνιση όλων",
  "basicSettings": "Βασικά",
  "configSubtitle": "Φορτώστε ή αποθηκεύστε προεπιλογές και πειραματιστείτε με παρακάμψεις παραμέτρων μοντέλου",
  "inferenceParameters/title": "Παράμετροι προβλέψεων",
  "inferenceParameters/info": "Πειραματιστείτε με παραμέτρους που επηρεάζουν την πρόβλεψη.",
  "generalParameters/title": "Γενικά",
  "samplingParameters/title": "Δειγματοληψία",
  "basicTab": "Βασικά",
  "advancedTab": "Προχωρημένα",
  "loadInstanceFirst": "Φορτώστε ένα μοντέλο για να δείτε τις παραμέτρους που μπορούν να ρυθμιστούν",
  "generationParameters/info": "Πειραματιστείτε με βασικές παραμέτρους που επηρεάζουν τη δημιουργία κειμένου.",
  "loadParameters/title": "Παράμετροι φόρτωσης",
  "loadParameters/description": "Η αλλαγή αυτών των παραμέτρων απαιτεί επαναφόρτωση του μοντέλου",
  "loadParameters/reload": "Επαναφόρτωση για την εφαρμογή αλλαγών στις παραμέτρους φόρτωσης",
  "discardChanges": "Απόρριψη αλλαγών",
  "llm.prediction.systemPrompt/title": "Κατευθυντήριες γραμμές για το AI",
  "llm.prediction.systemPrompt/description": "Χρησιμοποιήστε αυτό το πεδίο για να δώσετε οδηγίες υποβάθρου στο μοντέλο, όπως ένα σύνολο κανόνων, περιορισμών ή γενικών απαιτήσεων. Το πεδίο αυτό αναφέρεται επίσης συχνά ως \"προτροπή συστήματος\".",
  "llm.prediction.temperature/title": "Θερμοκρασία",
  "llm.prediction.temperature/info": "Από τα βοηθητικά έγγραφα του llama.cpp: \"Η προεπιλεγμένη τιμή είναι <{{dynamicValue}}>, η οποία παρέχει μια ισορροπία μεταξύ τυχαιότητας και ντετερμινισμού. Στην ακραία περίπτωση, μια θερμοκρασία 0 θα επιλέγει πάντα το πιο πιθανό επόμενο token, οδηγώντας σε πανομοιότυπες εξόδους σε κάθε εκτέλεση\"",
  "llm.prediction.llama.topKSampling/title": "Δειγματοληψία Top K",
  "llm.prediction.llama.topKSampling/info": "Από το αρχείο llama.cpp help docs:\n\nΗ δειγματοληψία top-k είναι μια μέθοδος παραγωγής κειμένου που επιλέγει το επόμενο token μόνο από τα k πιο πιθανά token που προβλέπει το μοντέλο.\n\nΒοηθά στη μείωση του κινδύνου δημιουργίας token χαμηλής πιθανότητας ή μη λογικών tokens, αλλά μπορεί επίσης να περιορίσει την ποικιλομορφία της εξόδου.\n\nΜια υψηλότερη τιμή για το top-k (π.χ., 100) θα εξετάσει περισσότερα tokens και θα οδηγήσει σε πιο ποικιλόμορφο κείμενο, ενώ μια χαμηλότερη τιμή (π.χ. 10) θα επικεντρωθεί στα πιο πιθανά tokens και θα παράγει πιο συντηρητικό κείμενο.\n\n- Η προεπιλεγμένη τιμή είναι <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Νήματα CPU",
  "llm.prediction.llama.cpuThreads/info": "Ο αριθμός των νημάτων που θα χρησιμοποιηθούν κατά τη διάρκεια του υπολογισμού. Η αύξηση του αριθμού των νημάτων δεν συσχετίζεται πάντα με καλύτερη απόδοση. Η προεπιλογή είναι <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Όριο μήκους απάντησης",
  "llm.prediction.maxPredictedTokens/info": "Ελέγξτε το μέγιστο μήκος της απάντησης του chatbot. Ενεργοποιήστε το για να ορίσετε ένα όριο στη μέγιστη διάρκεια μιας απάντησης ή απενεργοποιήστε το για να αφήσετε το chatbot να αποφασίσει πότε θα σταματήσει.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Μέγιστο μήκος απάντησης (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Περίπου {{maxWords}} λέξεις",
  "llm.prediction.llama.repeatPenalty/title": "Επαναλαμβανόμενη ποινή",
  "llm.prediction.llama.repeatPenalty/info": "Από τα βοηθητικά έγγραφα του llama.cpp: \"Βοηθά στην αποτροπή του μοντέλου από τη δημιουργία επαναλαμβανόμενου ή μονότονου κειμένου.\n\nΜια υψηλότερη τιμή (π.χ. 1,5) θα τιμωρεί αυστηρότερα τις επαναλήψεις, ενώ μια χαμηλότερη τιμή (π.χ. 0,9) θα είναι πιο ήπια.\" • Η προεπιλεγμένη τιμή είναι <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Δειγματοληψία Min P",
  "llm.prediction.llama.minPSampling/info": "Από τα βοηθητικά έγγραφα του llama.cpp:\n\nΗ ελάχιστη πιθανότητα να ληφθεί υπόψη ένα σύμβολο, σε σχέση με την πιθανότητα του πιο πιθανού συμβόλου. Πρέπει να είναι στο [0, 1].\n\n• Η προεπιλεγμένη τιμή είναι <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Δειγματοληψία Top P",
  "llm.prediction.llama.topPSampling/info": "Από τα βοηθητικά έγγραφα του llama.cpp:\n\nΗ δειγματοληψία Top-P, επίσης γνωστή ως δειγματοληψία πυρήνων, είναι μια άλλη μέθοδος παραγωγής κειμένου που επιλέγει το επόμενο token από ένα υποσύνολο tokens που έχουν μαζί μια αθροιστική πιθανότητα τουλάχιστον p.\n\nΗ μέθοδος αυτή παρέχει μια ισορροπία μεταξύ ποικιλίας και ποιότητας λαμβάνοντας υπόψη τόσο τις πιθανότητες των tokens όσο και τον αριθμό των tokens για δειγματοληψία.\n\n\nΜια υψηλότερη τιμή για το top-p (π.χ., 0,95) θα οδηγήσει σε πιο ποικιλόμορφο κείμενο, ενώ μια χαμηλότερη τιμή (π.χ. 0,5) θα δημιουργήσει πιο εστιασμένο και συντηρητικό κείμενο. Πρέπει να είναι στο (0, 1).\n\n• Η προεπιλεγμένη τιμή είναι <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Συμβολοσειρές Διακοπής",
  "llm.prediction.stopStrings/info": "Specific strings that when encountered will stop the model from generating more tokens",
  "llm.prediction.stopStrings/placeholder": "Εισάγετε μια συμβολοσειρά και πατήστε ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Υπερχείλιση συνομιλίας",
  "llm.prediction.contextOverflowPolicy/info": "Αποφασίστε τι πρέπει να γίνει όταν η συζήτηση υπερβαίνει το μέγεθος της μνήμης εργασίας του μοντέλου ('πλαίσιο')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Διακοπή στο όριο",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Διακοπή της δημιουργίας μόλις γεμίσει η μνήμη του μοντέλου",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Αποκοπή Μέσης",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Αφαιρεί μηνύματα από τη μέση της συνομιλίας για να κάνει χώρο για νεότερα μηνύματα. Το μοντέλο θα εξακολουθεί να θυμάται την αρχή της συνομιλίας.",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Παράθυρο κύλισης",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Το μοντέλο θα λαμβάνει πάντα τα πιο πρόσφατα μηνύματα, αλλά μπορεί να ξεχάσει την αρχή της συνομιλίας.",
  "llm.prediction.llama.frequencyPenalty/title": "Ποινή συχνότητας",
  "llm.prediction.llama.presencePenalty/title": "Ποινή παρουσίας",
  "llm.prediction.llama.tailFreeSampling/title": "Δειγματοληψία χωρίς ουρά (Tail-Free Sampling)",
  "llm.prediction.llama.locallyTypicalSampling/title": "Τοπικά τυπική δειγματοληψία",
  "llm.prediction.mlx.repeatPenalty/title": "Επαναλαμβανόμενη ποινή",
  "llm.prediction.mlx.repeatPenalty/info": "Μια υψηλότερη τιμή αποθαρρύνει το μοντέλο από το να επαναλαμβάνεται",
  "llm.prediction.seed/title": "Σπόρος",
  "llm.prediction.structured/title": "Δομημένη έξοδος",
  "llm.prediction.structured/info": "Δομημένη έξοδος",
  "llm.load.contextLength/title": "Μήκος πλαισίου",
  "llm.load.contextLength/info": "Καθορίζει τον μέγιστο αριθμό των tokens που μπορεί να εξετάσει το μοντέλο ταυτόχρονα, επηρεάζοντας το πόσα δεδομένα διατηρεί κατά τη διάρκεια της επεξεργασίας",
  "llm.load.seed/title": "Σπόρος",
  "llm.load.seed/info": "Τυχαίος σπόρος: Ορίζει τον σπόρο για τη δημιουργία τυχαίων αριθμών, ώστε να διασφαλίζονται αναπαραγώγιμα αποτελέσματα.",
  "llm.load.llama.evalBatchSize/title": "Μέγεθος παρτίδας αξιολόγησης",
  "llm.load.llama.evalBatchSize/info": "Ορίζει τον αριθμό των παραδειγμάτων που επεξεργάζονται μαζί σε μια παρτίδα κατά την αξιολόγηση, επηρεάζοντας την ταχύτητα και τη χρήση μνήμης",
  "llm.load.llama.ropeFrequencyBase/title": "Βάση συχνοτήτων RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Για προχωρημένους] Ρυθμίζει τη βασική συχνότητα για την περιστροφική κωδικοποίηση θέσης (RoPE), επηρεάζοντας τον τρόπο ενσωμάτωσης των πληροφοριών θέσης.",
  "llm.load.llama.ropeFrequencyScale/title": "Κλίμακα συχνοτήτων RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Για προχωρημένους] Τροποποιεί την κλιμάκωση της συχνότητας για την περιστροφική κωδικοποίηση θέσης (RoPE) για τον έλεγχο της κοκκομετρίας της κωδικοποίησης θέσης",
  "llm.load.llama.gpuOffload/title": "Εκφόρτωση GPU",
  "llm.load.llama.gpuOffload/info": "Ορίστε την αναλογία των υπολογισμών που θα εκφορτωθούν στην GPU. Ορίστε το off για να απενεργοποιήσετε την εκφόρτωση της GPU ή auto για να αφήσετε το μοντέλο να αποφασίσει.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Επιταχύνει τους μηχανισμούς προσοχής για ταχύτερη και αποτελεσματικότερη επεξεργασία",
  "llm.load.llama.keepModelInMemory/title": "Διατήρηση μοντέλου στη μνήμη",
  "llm.load.llama.keepModelInMemory/info": "Αποτρέπει την εναλλαγή του μοντέλου στο δίσκο, εξασφαλίζοντας ταχύτερη πρόσβαση με κόστος την υψηλότερη χρήση της μνήμης RAM.",
  "llm.load.llama.useFp16ForKVCache/title": "Χρήση FP16 για τη μνήμη KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Μειώνει τη χρήση μνήμης αποθηκεύοντας την κρυφή μνήμη σε μισή ακρίβεια (FP16)",
  "llm.load.llama.tryMmap/title": "Δοκιμασία mmap()",
  "llm.load.llama.tryMmap/info": "Φόρτωση αρχείων μοντέλων απευθείας από το δίσκο στη μνήμη"
}
