{
  "noInstanceSelected": "No model instance selected",
  "resetToDefault": "Reset",
  "showAdvancedSettings": "Show advanced settings",
  "showAll": "Show all",
  "basicSettings": "Basic",
  "configSubtitle": "Load or save presets and experiment with model parameter overrides",
  "inferenceParameters/title": "Prediction Parameters",
  "inferenceParameters/info": "Experiment with parameters that impact the prediction.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Sampling",
  "basicTab": "Basic",
  "advancedTab": "Advanced",
  "advancedTab/title": "üß™ Advanced Configuration",
  "advancedTab/expandAll": "Expand all",
  "advancedTab/overridesTitle": "Config Overrides",
  "advancedTab/noConfigsText": "You have no unsaved changes - edit values above to see overrides here.",
  "loadInstanceFirst": "Load a model to view configurable parameters",
  "noListedConfigs": "No configurable parameters",
  "generationParameters/info": "Experiment with basic parameters which impact text generation.",
  "loadParameters/title": "Load Parameters",
  "loadParameters/description": "Settings to control the way the model is initialized and loaded into memory.",
  "loadParameters/reload": "Reload to apply changes",
  "discardChanges": "Discard changes",
  "llm.prediction.systemPrompt/title": "System Prompt",
  "llm.prediction.systemPrompt/description": "Use this field to provide background instructions to the model, such as a set of rules, constraints, or general requirements. This field is also often referred to as the \"system prompt\".",
  "llm.prediction.systemPrompt/subTitle": "Guidelines for the AI",
  "llm.prediction.temperature/title": "Temperature",
  "llm.prediction.temperature/info": "From llama.cpp help docs: \"The default value is <{{dynamicValue}}>, which provides a balance between randomness and determinism. At the extreme, a temperature of 0 will always pick the most likely next token, leading to identical outputs in each run\"",
  "llm.prediction.llama.topKSampling/title": "Top K Sampling",
  "llm.prediction.llama.topKSampling/info": "From llama.cpp help docs:\n\nTop-k sampling is a text generation method that selects the next token only from the top k most likely tokens predicted by the model.\n\nIt helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output.\n\nA higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text.\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU Threads",
  "llm.prediction.llama.cpuThreads/info": "The number of threads to use during computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limit Response Length",
  "llm.prediction.maxPredictedTokens/subTitle": "Optionally cap the length of the AI's response",
  "llm.prediction.maxPredictedTokens/info": "Control the max length of the chatbot's response. Turn on to set a limit on the max length of a response, or turn off to let the chatbot decide when to stop.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximum response length (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "About {{maxWords}} words",
  "llm.prediction.llama.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.llama.repeatPenalty/info": "From llama.cpp help docs: \"Helps prevent the model from generating repetitive or monotonous text.\n\nA higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.\" ‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Min P Sampling",
  "llm.prediction.llama.minPSampling/info": "From llama.cpp help docs:\n\nThe minimum probability for a token to be considered, relative to the probability of the most likely token. Must be in [0, 1].\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P Sampling",
  "llm.prediction.llama.topPSampling/info": "From llama.cpp help docs:\n\nTop-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p.\n\nThis method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from.\n\nA higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. Must be in (0, 1].\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stop Strings",
  "llm.prediction.stopStrings/subTitle": "Strings that should stop the model from generating more tokens",
  "llm.prediction.stopStrings/info": "Specific strings that when encountered will stop the model from generating more tokens",
  "llm.prediction.stopStrings/placeholder": "Enter a string and press ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Conversation Overflow",
  "llm.prediction.contextOverflowPolicy/info": "Decide what to do when the conversation exceeds the size of the model's working memory ('context')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Stop at Limit",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Stop generating once the model's memory gets full",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Truncate Middle",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Rolling Window",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency Penalty",
  "llm.prediction.llama.presencePenalty/title": "Presence Penalty",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.mlx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.mlx.repeatPenalty/info": "A higher value discourages the model from repeating itself",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/info": "From ONNX documentation:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.onnx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.onnx.repeatPenalty/info": "A higher value discourages the model from repeating itself",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/info": "From ONNX documentation:\n\nOnly the most probable tokens with probabilities that add up to TopP or higher are kept for generation\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Structured Output",
  "llm.prediction.structured/info": "Structured Output",
  "llm.prediction.promptTemplate/title": "Prompt Template",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Failed to parse Jinja template: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manual",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Before System Prompt",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "After System Prompt",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Before Each User Message",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "After Each User Message",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Before Each AI Message",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "After Each AI Message",
  "llm.prediction.promptTemplate.stopStrings/label": "Additional Stop Strings",
  "llm.prediction.promptTemplate.stopStrings/hint": "Template specific stop strings that will be used in addition to user-specified stop strings.",
  
  "llm.load.contextLength/title": "Context Length",
  "llm.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",
  
  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/info": "Sets the number of examples processed together in one batch during evaluation, affecting speed and memory usage",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "llm.load.llama.gpuOffload/title": "GPU Offload",
  "llm.load.llama.gpuOffload/info": "Set the ratio of computation to offload to the GPU. Set to off to disable GPU offloading, or auto to let the model decide.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Accelerates attention mechanisms for faster and more efficient processing",
  "llm.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "llm.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "llm.load.llama.useFp16ForKVCache/title": "Use FP16 For KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Reduces memory usage by storing cache in half-precision (FP16)",
  "llm.load.llama.tryMmap/title": "Try mmap()",
  "llm.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  
  "embedding.load.contextLength/title": "Context Length",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.gpuOffload/title": "GPU Offload",
  "embedding.load.llama.gpuOffload/info": "Set the ratio of computation to offload to the GPU. Set to off to disable GPU offloading, or auto to let the model decide.",
  "embedding.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Try mmap()",
  "embedding.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results"
}
