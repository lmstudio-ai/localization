{
  "noInstanceSelected": "Niciun model selectat",
  "resetToDefault": "Reset",
  "showAdvancedSettings": "Setari avansate",
  "showAll": "Toate",
  "basicSettings": "Simple",
  "configSubtitle": "Incarca sau salveaza Preseturi si testeaza parametri ai modelului",
  "inferenceParameters/title": "Parametri de predictie",
  "inferenceParameters/info": "Testarea parametrilor ce influenteaza predictia.",
  "generalParameters/title": "Generic",
  "samplingParameters/title": "Prelevare",
  "basicTab": "Simplu",
  "advancedTab": "Avansate",
  "advancedTab/title": "üß™ Configurare avansata",
  "advancedTab/expandAll": "Extindere",
  "advancedTab/overridesTitle": "Alte configuratii",
  "advancedTab/noConfigsText": "Nu exista schimbari neaplicate - valorile editate sus se reflecta aici in cele noi.",
  "loadInstanceFirst": "Incarca un model pentru a vedea parametrii configurabili",
  "noListedConfigs": "Nu exista parametri configurabili",
  "generationParameters/info": "Testarea unor paremetri de baza ce influenteaza generarea textului.",
  "loadParameters/title": "Incarca parametrii",
  "loadParameters/description": "Setari pentru controlarea initializarii si incarcarii in memorie a modelului.",
  "loadParameters/reload": "Reincarc pentru aplicarea modificarilor",
  "discardChanges": "Anularea modificarilor",
  "loadModelToSeeOptions": "Incarca un model pentru a vedea optiunile",
  "llm.prediction.systemPrompt/title": "System Prompt",
  "llm.prediction.systemPrompt/description": "Aici se scriu instructiuni preliminare pentru model, de exemplu reguli/conditii, restrictii, cerinte generice.",
  "llm.prediction.systemPrompt/subTitle": "Linii directoare pentru AI",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Cat de aleatoare sa fie replica. 0 va determina aceeasi replica de fiecare data, iar valori tot mai mari induc creativitate si variatie tot mai mari",
  "llm.prediction.temperature/info": "Din docum. llama.cpp: \"Valoarea implicita e <{{dynamicValue}}>, care asigura echilibrul intre aleator si cauza->efect. La extrem, Temperatura 0 va duce mereu la alegerea celui mai probabil token-urmator, ceea ce duce la replici identice de fiecare data\"",
  "llm.prediction.llama.sampling/title": "Prelevare (Sampling)",
  "llm.prediction.topKSampling/title": "Prelevare: Top K",
  "llm.prediction.topKSampling/subTitle": "Limiteaza tokenul-urmator la unul din cele mai probabile top-k tokenuri. Similar cu Temperatura",
  "llm.prediction.topKSampling/info": "Din docum. llama.cpp:\n\nPrelevarea Top-k e o metoda de generare a textului ce selecteaza tokenul-urmator numai din cele mai probabile top k tokenuri prezise de model.\n\nAjuta la reducerea riscului de generare a unor tokenuri putin probabile sau irelevante, dar totodata poate limita varietatea replicilor.\n\nO valoare mai mare a top-k (de ex. 100) implica mai multe tokenuri si duce la un text mai complex, pe cand una mai mica (de ex. 10) va tinti cele mai probabile tokenuri si va genera text mai simplu.\n\n‚Ä¢ Valoarea implicita e <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Thread-uri CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Numarul thread-urilor CPU de folosit la inferenta",
  "llm.prediction.llama.cpuThreads/info": "Numarul thread-urilor de folosit in timpul calculelor. Cresterea sa nu implica mereu performanta mai buna. Implicit: <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitarea lungimii replicii",
  "llm.prediction.maxPredictedTokens/subTitle": "Optional - limiteaza lungimea replicii de la AI",
  "llm.prediction.maxPredictedTokens/info": "Controlul lungimii maxime a replicii. Activat: stabileste limita; dezactivat: AI va decide cand sa se opreasca.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Lungimea maxima a replicii (tokenuri)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Despre {{maxWords}} cuvinte",
  "llm.prediction.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.repeatPenalty/subTitle": "Cat de mult sa se inhibe repetarea unui token",
  "llm.prediction.repeatPenalty/info": "Din docum. llama.cpp: \"Ajuta la impiedicarea modelului de a genera text repetitiv sau monoton.\n\nO valoare mai mare (de ex. 1.5) va penaliza mai mult repetarea, pe cand o valoare mai mica (de ex. 0.9) e mai ingaduitoare.\" ‚Ä¢ Valoarea implicita e <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Prelevare: Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilitatea minima a selectarii unui token la crearea replicii",
  "llm.prediction.minPSampling/info": "Din docum. llama.cpp:\n\nProbabilitatea minima a unui token de considerat, relativ la probabilitatea celui mai apropiat/probabil token. Trebuie sa fie in intervalul [0, 1].\n\n‚Ä¢ Valoarea implicita e <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Prelevare: Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilitatea minima cumulata a tokeurilor-urmatoare posibile. Similar cu Temperatura",
  "llm.prediction.topPSampling/info": "Din docum. llama.cpp:\n\nPrelevarea Top-p, numita si nucleus sampling, e alta metoda de generare a textului ce selecteaza tokenul-urmator dintr-un subset de tokenuri care impreuna au probabilitatea cumulata de cel putin p.\n\nAsigura echilibrul intre variatie si calitate, considerand probabilitatea tokenurilor si numarul tokenurilor din care se va preleva.\n\nO valoare mai mare a top-p (de ex. 0.95) duce la n text mai complex, pe cand una mai mica (de ex. 0.5) va genera text mai concentrat pe subiect si mai simplu. Trebuie sa fie in intervalul (0, 1].\n\n‚Ä¢ Valoarea implicita e <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stop Strings",
  "llm.prediction.stopStrings/subTitle": "Text(String) ce ar trebui sa opreasca modelul din generarea de tokenuri",
  "llm.prediction.stopStrings/info": "Text (String) anume care atunci cand e intalnit de model, opreste modelul din generat tokenuri",
  "llm.prediction.stopStrings/placeholder": "Scrie un text si apasa ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Context Overflow",
  "llm.prediction.contextOverflowPolicy/subTitle": "Comportarea modelului cand conversatia devine prea lunga pentru a fi asimilata",
  "llm.prediction.contextOverflowPolicy/info": "Ce sa se intample cand lungimea conversatiei depaseste dimensiunea memoriei de lucru a modelului ('contextul')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalizare: Frecventa",
  "llm.prediction.llama.presencePenalty/title": "Penalizare: Prezenta",
  "llm.prediction.llama.tailFreeSampling/title": "Prelevare: Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Prelevare: Locally Typical",
  "llm.prediction.onnx.topKSampling/title": "Prelevare: Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limiteaza tokenul-urmator la unul din cele mai probabile top-k tokenuri. Similar cu Temperatura",
  "llm.prediction.onnx.topKSampling/info": "Din docum. ONNX:\n\nNumarul tokenurilor lexicale cu probabilitatea maxima de pastrat pentru filtrarea-top-k\n\n‚Ä¢ Initial, acest filtru e DEZactivat",
  "llm.prediction.onnx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Cat de mult sa se inhibe repetarea unui token",
  "llm.prediction.onnx.repeatPenalty/info": "O valoare mai mare descurajeaza modelul de la a se repeta",
  "llm.prediction.onnx.topPSampling/title": "Prelevare: Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilitatea minima cumulata a tokeurilor-urmatoare posibile. Similar cu Temperatura",
  "llm.prediction.onnx.topPSampling/info": "Din docum. ONNX:\n\nPentru generarea textului vor fi considerate numai cele mai probabile tokenuri - cu probabilitatea cumulata de TopP sau mai mare\n\n‚Ä¢ Initial, acest filtru e DEZactivat",
  "llm.prediction.seed/title": "Seed (Initializare)",
  "llm.prediction.structured/title": "Output (Replica) Structurata",
  "llm.prediction.structured/info": "Output (Replica) Structurata",
  "llm.prediction.structured/description": "Avansat: se poate furniza o schema JSON pentru a formata replica modelului. Mai multe info: [documentation](https://lmstudio.ai/docs/advanced/structured-output)",
  "llm.prediction.promptTemplate/title": "Prompt Template",
  "llm.prediction.promptTemplate/subTitle": "Formatul in care mesajele din chat sunt trimise modelului. Modificarea sa poate provoca reactii neasteptate - asigura-te ca stii ce faci!",

  "llm.load.contextLength/title": "Context Length (Lungimea Contextului)",
  "llm.load.contextLength/subTitle": "Numarul maxim al tokenurilor pe care modelul le poate asimila intr-un singur prompt. Vezi si optiunile Conversation Overflow din \"Parametrii Inferentei\" pentru alte ajustari",
  "llm.load.contextLength/info": "Specifica numarul maxim al tokenurilor pe care modelul le poate asimila dintr-o data, influentand capacitatea de asimilare a contextului in timpul procesarii",
  "llm.load.contextLength/warning": "O valoare mare a Lungimii Contextului poate ocupa multa memorie",
  "llm.load.seed/title": "Seed (Initializare)",
  "llm.load.seed/subTitle": "Valoarea de initializare a creatorului de numere aleatoare folosite la generarea de text. -1 indica o valoare la intamplare",
  "llm.load.seed/info": "Random Seed: Stabileste valoarea de initializare pentru crearea de numere aleatoare si rezultate reproductibile",

  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Numarul tokenurilor furnizate, de procesat intr-un pas. Cresterea sa creste performanta, in detrimentul ocuparii memoriei",
  "llm.load.llama.evalBatchSize/info": "Stabileste numarul exemplelor procesate impreuna intr-un set (Batch) la evaluare, afectand viteza si memoria",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frecventa de baza pentru Embedding-uri Pozitionale Rotationale (RoPE). Cresterea sa poate induce performanta mai buna cu lungimi mari ale contextului",
  "llm.load.llama.ropeFrequencyBase/info": "[Avansate] Ajusteaza frecventa de baza pentru RoPE, afectand modul de Embedding a informatiei pozitionale",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Lungimea Contextului e ajustata in raport cu acest factor pentru a extinde contextul efectiv prin RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avansate] Modifica amploarea frecventei pentru RoPE, in a controla granularitatea codificarii pozitionale",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Numarul layerelor distincte ale modelului care sa fie procesate in GPU, pentru accelerare cu GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Stabileste numarul layerelor de trimis catre GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Micsoreaza ocuparea memoriei si timpul de generare, pentru unele modele",
  "llm.load.llama.flashAttention/info": "Accelereaza mecanismele de atentie pentru procesare mai rapida si mai eficienta",
  "llm.load.numExperts/title": "Numarul Expertilor",
  "llm.load.numExperts/subTitle": "Numarul Expertilor de utilizat in model",
  "llm.load.numExperts/info": "Numarul Expertilor de utilizat in model",
  "llm.load.llama.keepModelInMemory/title": "Pastreaza modelul in memorie",
  "llm.load.llama.keepModelInMemory/subTitle": "Rezerva memorie pentru model, chiar cand e trimis catre GPU. Mareste performanta, dar necesita mai multa memorie RAM",
  "llm.load.llama.keepModelInMemory/info": "Impiedica modelul de la a fi accesat de pe disc, asigurand acces mai rapid cu costul folosirii mai intense a RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Foloseste FP16 pentru KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Reduce folosirea memoriei prin stocarea Cache-ului in precizie FP16",
  "llm.load.llama.tryMmap/title": "Incearca mmap()",
  "llm.load.llama.tryMmap/subTitle": "Optimizeaza timpul de incarcare a modelului. Dezactivarea aici poate imbunatati performanta cand modelul e mai mare decat memoria RAM disponibila",
  "llm.load.llama.tryMmap/info": "Incarca fisierele modelului direct de pe disc in memorie",

  "embedding.load.contextLength/title": "Context Length",
  "embedding.load.contextLength/subTitle": "Numarul maxim de tokenuri pe care modelul le poate asimila intr-un prompt. Vezi si optiunile Conversation Overflow din \"Parametrii Inferentei\" pentru alte ajustari",
  "embedding.load.contextLength/info": "Specifica numarul maxim al tokenurilor pe care modelul le poate asimila dintr-o data, influentand capacitatea de asimilare a contextului in timpul procesarii",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frecventa de baza pentru Embedding-uri Pozitionale Rotationale (RoPE). Cresterea sa poate induce performanta mai buna cu lungimi mari ale contextului",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avansate] Ajusteaza frecventa de baza pentru RoPE, afectand modul de Embedding a informatiei pozitionale",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Numarul tokenurilor furnizate, de procesat intr-un pas. Cresterea sa creste performanta, in detrimentul ocuparii memoriei",
  "embedding.load.llama.evalBatchSize/info": "Stabileste numarul exemplelor procesate impreuna intr-un set (Batch) la evaluare",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Lungimea Contextului e ajustata in raport cu acest factor pentru a extinde contextul efectiv prin RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avansate] Modifica amploarea frecventei pentru RoPE, in a controla granularitatea codificarii pozitionale",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Numarul layerelor distincte ale modelului care sa fie procesate in GPU, pentru accelerare cu GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Stabileste numarul layerelor de trimis catre GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Pastreaza modelul in memorie",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Incearca mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Optimizeaza timpul de incarcare a modelului. Dezactivarea aici poate imbunatati performanta cand modelul e mai mare decat memoria RAM disponibila",
  "embedding.load.llama.tryMmap/info": "Incarca fisierele modelului direct de pe disc in memorie",
  "embedding.load.seed/title": "Seed (Initializare)",
  "embedding.load.seed/subTitle": "Valoarea de initializare a creatorului de numere aleatoare folosite la generarea de text. -1 indica o valoare la intamplare",

  "embedding.load.seed/info": "Random Seed: Stabileste valoarea de initializare pentru asigurarea unor rezultate reproductibile",

  "presetTooltip": {
    "included/title": "Valorile Presetului",
    "included/description": "Vor fi aplicate aceste campuri",
    "included/empty": "Niciun camp al acestui Preset nu poate fi aplicat in acest context.",
    "included/conflict": "Ti se va cere sa confirmi aplicarea acestei valori",
    "separateLoad/title": "Configurare in timpul incarcarii",
    "separateLoad/description.1": "Presetul include si urmatoarea configuratie de aplicat la incarcare. Astfel de configuratii sunt cunoscute in tot modelul si necesita reincarcarea modelului pentru a fi aplicate. Stai un pic",
    "separateLoad/description.2": "pentru a se aplica la",
    "separateLoad/description.3": ".",
    "excluded/title": "Poate fi inaplicabil",
    "excluded/description": "Urmatoarele campuri sunt incluse in Preset, dar nu pot fi aplicate in contextul curent.",
    "legacy/title": "Preset vechi",
    "legacy/description": "Acesta e un Preset vechi. Include urmatoarele campuri ce fie acum sunt ajustate automat, fie nu mai sunt aplicabile."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Template",
      "jinja/error": "NU s-a putut citi template-ul Jinja: {{error}}",
      "jinja/empty": "Specifica un template Jinja.",
      "jinja/unlikelyToWork": "Template-ul Jinja specificat nu va functiona, fiindca nu refera variabila \"messages\". Verifica daca template-ul este corect.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Inainte de System",
      "manual.subfield.beforeSystem/placeholder": "Specifica prefixul la System...",
      "manual.subfield.afterSystem/label": "Dupa System",
      "manual.subfield.afterSystem/placeholder": "Specifica sufixul la System...",
      "manual.subfield.beforeUser/label": "Inainte de User",
      "manual.subfield.beforeUser/placeholder": "Specifica prefixul la User...",
      "manual.subfield.afterUser/label": "Dupa User",
      "manual.subfield.afterUser/placeholder": "Specifica sufixul la User...",
      "manual.subfield.beforeAssistant/label": "Inainte de Asistent",
      "manual.subfield.beforeAssistant/placeholder": "Specifica prefixul la Asistent...",
      "manual.subfield.afterAssistant/label": "Dupa Asistent",
      "manual.subfield.afterAssistant/placeholder": "Specifica sufixul la Assistent...",
      "stopStrings/label": "Stop String-uri suplimentare",
      "stopStrings/subTitle": "Modele de texte (Stringuri) ce vor fi folosite impreuna cu cele furnizate de utilizator."
    },
    "contextLength": {
      "maxValueTooltip": "Acesta e numarul maxim de tokenuri cu care modelul a fost instruit sa le asimileze. Click pentru a seta contextul acestei valori",
      "maxValueTextStart": "Modelul suporta maximum",
      "maxValueTextEnd": "tokenuri",
      "tooltipHint": "Desi un model poate suporta un anumit numar de tokenuri, performanta se poate deteriora daca resursele sistemului sunt insuficiente - atentie! la cresterea acestei valori"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Oprire la Limita",
      "stopAtLimitSub": "Opreste generarea cand memoria modelului e plina",
      "truncateMiddle": "Trunchierea Mijlocului",
      "truncateMiddleSub": "Sterge mesajele din mijlocul conversatiei pentru a face loc unora noi. Totusi, modelul va tine minte inceputul conversatiei",
      "rollingWindow": "Fereastra Glisanta (Rolling Window)",
      "rollingWindowSub": "Modelul va intelege mereu cele mai recente cateva mesaje, dar poate uita inceputul conversatiei"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    }
  },
  "saveConflictResolution": {
    "title": "Alege valorile ce vor fi incluse in Preset",
    "description": "Alege valorile ce vor fi pastrate",
    "instructions": "Click pe o valoare pentru a o include",
    "userValues": "Valoarea precedenta",
    "presetValues": "Valoare noua",
    "confirm": "Confirm",
    "cancel": "Anulez"
  },
  "applyConflictResolution": {
    "title": "Care valori sa fie pastrate?",
    "description": "Exista modificari nesalvate interferand cu Presetul in pregatire",
    "instructions": "Click pe o valoare pentru a o pastra",
    "userValues": "Valoarea curenta",
    "presetValues": "Valoarea din Presetul in pregatire",
    "confirm": "Confirm",
    "cancel": "Anulez"
  },
  "empty": "<Empty>",
  "presets": {
    "title": "Preset",
    "commitChanges": "Salvez modificarile",
    "commitChanges/description": "Salveaza modificarile Presetului.",
    "commitChanges.manual": "Am detectat campuri noi. Vei putea alege modificarile de inclus in Preset.",
    "commitChanges.manual.hold.0": "Stai un pic",
    "commitChanges.manual.hold.1": "pentru a alege ce modificari sa fie salvate in Preset.",
    "commitChanges.saveAll.hold.0": "Stai un pic",
    "commitChanges.saveAll.hold.1": "pentru a salva toate modificarile.",
    "commitChanges.saveInPreset.hold.0": "Stai un pic",
    "commitChanges.saveInPreset.hold.1": "pentru a salva numai modificarile campurilor deja incluse in Preset.",
    "commitChanges/error": "NU s-au putut salva modificarile Presetului.",
    "commitChanges.manual/description": "Alege care modificari sa fie incluse in Preset.",
    "saveAs": "Salvez ca...",
    "presetNamePlaceholder": "Specifica denumirea Presetului...",
    "cannotCommitChangesLegacy": "Acesta e un Preset vechi si nu poate fi modificat. Poti crea o copie a sa folosind \"Salvez ca...\".",
    "cannotCommitChangesNoChanges": "Nicio modificare de salvat.",
    "emptyNoUnsaved": "Selecteaza un Preset...",
    "emptyWithUnsaved": "Preset nesalvat",
    "saveEmptyWithUnsaved": "Salvez Presetul ca...",
    "saveConfirm": "Salvez",
    "saveCancel": "Cancel",
    "saving": "Se salveaza...",
    "save/error": "NU s-a putut salva Presetul.",
    "deselect": "Deselectez Presetul",
    "deselect/error": "NU s-a putut deselecta Presetul.",
    "select/error": "NU s-a putut selecta Presetul.",
    "delete/error": "NU s-a putut sterge Presetul.",
    "discardChanges": "Anulez ce nu e salvat",
    "discardChanges/info": "Anuleaza toate modificarile neaplicate si restaureaza Presetul original",
    "newEmptyPreset": "Creez un Preset gol...",
    "contextMenuSelect": "Selectez un Preset",
    "contextMenuDelete": "Sterg"
  },

  "flashAttentionWarning": "Flash Attention e o functionalitate experimentala ce poate cauza probleme cu unele modele. Daca se intampla asa ceva, trebuie dezactivata.",

  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}
