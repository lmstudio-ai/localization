{
  "noInstanceSelected": "Geen model instance geselecteerd",
  "resetToDefault": "Opnieuw instellen",
  "showAdvancedSettings": "Geavanceerde instellingen weergeven",
  "showAll": "Alles",
  "basicSettings": "Basis",
  "configSubtitle": "Laad of bewaar presets en experimenteer met modelparameter-overschrijvingen",
  "inferenceParameters/title": "Voorspellingsparameters",
  "inferenceParameters/info": "Experimenteer met parameters die van invloed zijn op de voorspelling.",
  "generalParameters/title": "Algemeen",
  "samplingParameters/title": "Sampling",
  "basicTab": "Basis",
  "advancedTab": "Geavanceerd",
  "advancedTab/title": "üß™ Gedvanceerde Instellingen",
  "advancedTab/expandAll": "Alles uitvouwen",
  "advancedTab/overridesTitle": "Configuratie-overschrijvingen",
  "advancedTab/noConfigsText": "Er zijn geen wijzigingen die u niet hebt opgeslagen. Bewerk de waarden hierboven om de overschrijvingen hier te zien.",
  "loadInstanceFirst": "Laad een model om configureerbare parameters te bekijken",
  "noListedConfigs": "Geen configureerbare parameters",
  "generationParameters/info": "Experimenteer met basisparameters die van invloed zijn op de tekstgeneratie.",
  "loadParameters/title": "Laadparameters",
  "loadParameters/description": "Instellingen om te bepalen hoe het model wordt ge√Ønitialiseerd en in het geheugen wordt geladen.",
  "loadParameters/reload": "Opnieuw laden om wijzigingen toe te passen",
  "discardChanges": "Wijzigingen negeren",
  "loadModelToSeeOptions": "Laad een model om opties te zien",
  "llm.prediction.systemPrompt/title": "Systeemprompt",
  "llm.prediction.systemPrompt/description": "Gebruik dit veld om achtergrondinformatie voor het model te verstrekken, zoals een reeks regels, beperkingen of algemene vereisten.",
  "llm.prediction.systemPrompt/subTitle": "Richtlijnen voor de AI",
  "llm.prediction.temperature/title": "Temperatuur",
  "llm.prediction.temperature/subTitle": "Hoeveel willekeur moet er worden ge√Øntroduceerd? 0 zal elke keer hetzelfde resultaat opleveren, terwijl hogere waarden de creativiteit en variantie zullen vergroten",
  "llm.prediction.temperature/info": "Uit llama.cpp help docs: \"De standaardwaarde is <{{dynamicValue}}>, wat een balans biedt tussen willekeur en determinisme. In het uiterste geval zal een temperatuur van 0 altijd het meest waarschijnlijke volgende token kiezen, wat leidt tot identieke uitvoer in elke run\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top K Sampling",
  "llm.prediction.topKSampling/subTitle": "Beperkt het volgende token tot een van de top-k meest waarschijnlijke tokens. Werkt op een vergelijkbare manier als temperatuur",
  "llm.prediction.topKSampling/info": "Uit de helpdocumenten van llama.cpp:\n\nTop-k-sampling is een tekstgeneratiemethode die alleen het volgende token selecteert uit de bovenste k meest waarschijnlijke tokens die door het model zijn voorspeld.\n\nHet helpt het risico te verkleinen dat er tokens met een lage waarschijnlijkheid of onzinnige tokens worden gegenereerd, maar het kan ook de diversiteit van de uitvoer beperken.\n\nEen hogere waarde voor top-k (bijv. 100) zal meer tokens in overweging nemen en leiden tot meer diverse tekst, terwijl een lagere waarde (bijv. 10) zich zal richten op de meest waarschijnlijke tokens en conservatievere tekst zal genereren.\n\n‚Ä¢ De standaardwaarde is <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU-threads",
  "llm.prediction.llama.cpuThreads/subTitle": "Aantal CPU-threads dat tijdens inferentie moet worden gebruikt",
  "llm.prediction.llama.cpuThreads/info": "Het aantal threads dat tijdens de berekening wordt gebruikt. Het verhogen van het aantal threads correleert niet altijd met betere prestaties. De standaardwaarde is <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limiet responslengte",
  "llm.prediction.maxPredictedTokens/subTitle": "Optioneel de lengte van de AI-reactie beperken",
  "llm.prediction.maxPredictedTokens/info": "Bepaal de maximale lengte van het antwoord van de chatbot. Schakel in om een ‚Äã‚Äãlimiet in te stellen voor de maximale lengte van een antwoord, of schakel uit om de chatbot te laten beslissen wanneer te stoppen.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maximale responslengte (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Ongeveer {{maxWords}} woorden",
  "llm.prediction.repeatPenalty/title": "Herhaalde straf",
  "llm.prediction.repeatPenalty/subTitle": "Hoezeer moet het worden ontmoedigd om hetzelfde te herhalen?",
  "llm.prediction.repeatPenalty/info": "Uit de helpdocumenten van llama.cpp: \"Helpt voorkomen dat het model repetitieve of monotone tekst genereert.\n\nEen hogere waarde (bijv. 1,5) zal herhalingen sterker bestraffen, terwijl een lagere waarde (bijv. 0,9) milder zal zijn.\" ‚Ä¢ De standaardwaarde is <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P Sampling",
  "llm.prediction.minPSampling/subTitle": "Minimale basiswaarschijnlijkheid voor een token om te worden geselecteerd voor uitvoer",
  "llm.prediction.minPSampling/info": "Uit llama.cpp help docs:\n\nDe minimale waarschijnlijkheid dat een token in aanmerking komt, ten opzichte van de waarschijnlijkheid van het meest waarschijnlijke token. Moet in [0, 1] zijn.\n\n‚Ä¢ De standaardwaarde is <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P Sampling",
  "llm.prediction.topPSampling/subTitle": "Minimale cumulatieve waarschijnlijkheid voor de mogelijke volgende tokens. Werkt vergelijkbaar met temperatuur",
  "llm.prediction.topPSampling/info": "Uit llama.cpp help docs:\n\nTop-p sampling, ook bekend als nucleus sampling, is een andere tekstgeneratiemethode die het volgende token selecteert uit een subset van tokens die samen een cumulatieve waarschijnlijkheid van ten minste p hebben.\n\nDeze methode biedt een balans tussen diversiteit en kwaliteit door zowel de waarschijnlijkheid van tokens als het aantal tokens waaruit moet worden gesampled te overwegen.\n\nEen hogere waarde voor top-p (bijv. 0,95) leidt tot meer diverse tekst, terwijl een lagere waarde (bijv. 0,5) meer gerichte en conservatieve tekst genereert. Moet in (0, 1] zijn.\n\n‚Ä¢ De standaardwaarde is <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stop Strings",
  "llm.prediction.stopStrings/subTitle": "Strings die het model ervan moeten weerhouden meer tokens te genereren",
  "llm.prediction.stopStrings/info": "Specifieke strings die, wanneer ze worden aangetroffen, het model ervan weerhouden meer tokens te genereren",
  "llm.prediction.stopStrings/placeholder": "Voer een string in en druk op ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Contextoverloop",
  "llm.prediction.contextOverflowPolicy/subTitle": "Hoe het model zich moet gedragen als het gesprek te groot wordt om het te kunnen verwerken",
  "llm.prediction.contextOverflowPolicy/info": "Beslis wat u moet doen als het gesprek de omvang van het werkgeheugen van het model ('context') overschrijdt",
  "llm.prediction.llama.frequencyPenalty/title": "Frequentiestraf",
  "llm.prediction.llama.presencePenalty/title": "Aanwezigheidsstraf",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Beperkt het volgende token tot een van de top-k meest waarschijnlijke tokens. Werkt op een vergelijkbare manier als temperatuur",
  "llm.prediction.onnx.topKSampling/info": "Uit ONNX-documentatie:\n\nAantal woordenschattokens met de hoogste waarschijnlijkheid dat moet worden bewaard voor top-k-filtering\n\n‚Ä¢ Dit filter is standaard uitgeschakeld",
  "llm.prediction.onnx.repeatPenalty/title": "Herhaalde straf",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Hoezeer moet het worden ontmoedigd om hetzelfde te herhalen?",
  "llm.prediction.onnx.repeatPenalty/info": "Een hogere waarde ontmoedigt het model om zichzelf te herhalen",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimale cumulatieve waarschijnlijkheid voor de mogelijke volgende tokens. Werkt vergelijkbaar met temperatuur",
  "llm.prediction.onnx.topPSampling/info": "Uit de ONNX-documentatie:\n\nAlleen de meest waarschijnlijke tokens met waarschijnlijkheden die optellen tot TopP of hoger worden bewaard voor generatie\n\n‚Ä¢ Dit filter is standaard uitgeschakeld",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Gestructureerde output",
  "llm.prediction.structured/info": "Gestructureerde output",
  "llm.prediction.promptTemplate/title": "Prompt-sjabloon",
  "llm.prediction.promptTemplate/subTitle": "Het formaat waarin berichten in chat naar het model worden verzonden. Als u dit wijzigt, kan dit onverwacht gedrag veroorzaken. Zorg ervoor dat u weet wat u doet!",

  "llm.load.contextLength/title": "Contextlengte",
  "llm.load.contextLength/subTitle": "Het maximale aantal tokens dat het model in √©√©n prompt kan behandelen. Zie de Conversation Overflow-opties onder \"Inference params\" voor meer manieren om dit te beheren",
  "llm.load.contextLength/info": "Geeft het maximale aantal tokens aan dat het model tegelijk kan overwegen, wat van invloed is op de hoeveelheid context die het tijdens de verwerking behoudt",
  "llm.load.contextLength/warning": "Het instellen van een hoge waarde voor de contextlengte kan een aanzienlijke impact hebben op het geheugengebruik",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "De seed voor de willekeurige nummergenerator die wordt gebruikt bij het genereren van tekst. -1 is willekeurig",
  "llm.load.seed/info": "Willekeurige seed: stelt de seed in voor het genereren van willekeurige getallen om reproduceerbare resultaten te garanderen",

  "llm.load.llama.evalBatchSize/title": "Evaluatie Batchgrootte",
  "llm.load.llama.evalBatchSize/subTitle": "Aantal inputtokens dat tegelijk verwerkt moet worden. Als u dit verhoogt, neemt de prestatie toe ten koste van het geheugengebruik.",
  "llm.load.llama.evalBatchSize/info": "Stelt het aantal voorbeelden in dat in √©√©n batch wordt verwerkt tijdens de evaluatie, wat de snelheid en het geheugengebruik be√Ønvloedt",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE-frequentiebasis",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Aangepaste basisfrequentie voor roterende positionele inbeddingen (RoPE). Door deze te verhogen, kunnen betere prestaties bij hoge contextlengtes worden bereikt.",
  "llm.load.llama.ropeFrequencyBase/info": "[Geavanceerd] Past de basisfrequentie voor Rotary Positional Encoding aan, wat van invloed is op de manier waarop positie-informatie wordt ingebed",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE-frequentieschaal",
  "llm.load.llama.ropeFrequencyScale/subTitle": "De contextlengte wordt met deze factor geschaald om de effectieve context uit te breiden met behulp van RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Geavanceerd] Wijzigt de schaal van de frequentie voor Rotary Positional Encoding om de granulariteit van de positionele codering te regelen",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Aantal discrete modellagen om te berekenen op de GPU voor GPU-versnelling",
  "llm.load.llama.acceleration.offloadRatio/info": "Stel het aantal lagen in dat naar de GPU moet worden verplaatst.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Vermindert het geheugengebruik en de generatietijd op sommige modellen",
  "llm.load.llama.flashAttention/info": "Versnelt aandachtsmechanismen voor snellere en effici√´ntere verwerking",
  "llm.load.numExperts/title": "Aantal experts",
  "llm.load.numExperts/subTitle": "Aantal experts dat in het model moet worden gebruikt",
  "llm.load.numExperts/info": "Het aantal experts dat in het model moet worden gebruikt",
  "llm.load.llama.keepModelInMemory/title": "Model in geheugen bewaren",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserveer systeemgeheugen voor het model, zelfs wanneer het naar de GPU wordt overgeheveld. Verbetert de prestaties, maar vereist meer systeem-RAM",
  "llm.load.llama.keepModelInMemory/info": "Voorkomt dat het model naar schijf wordt geswapt, waardoor snellere toegang wordt gegarandeerd ten koste van een hoger RAM-gebruik",
  "llm.load.llama.useFp16ForKVCache/title": "Gebruik FP16 voor KV-cache",
  "llm.load.llama.useFp16ForKVCache/info": "Vermindert het geheugengebruik door cache op te slaan in halve precisie (FP16)",
  "llm.load.llama.tryMmap/title": "Probeer mmap()",
  "llm.load.llama.tryMmap/subTitle": "Verbetert de laadtijd voor het model. Als u dit uitschakelt, kan dit de prestaties verbeteren wanneer het model groter is dan het beschikbare systeem-RAM.",
  "llm.load.llama.tryMmap/info": "Laad modelbestanden rechtstreeks van schijf naar geheugen",

  "embedding.load.contextLength/title": "Contextlengte",
  "embedding.load.contextLength/subTitle": "Het maximale aantal tokens dat het model in √©√©n prompt kan behandelen. Zie de Conversation Overflow-opties onder \"Inference params\" voor meer manieren om dit te beheren",
  "embedding.load.contextLength/info": "Geeft het maximale aantal tokens aan dat het model tegelijk kan overwegen, wat van invloed is op de hoeveelheid context die het tijdens de verwerking behoudt",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Aangepaste basisfrequentie voor roterende positionele inbeddingen (RoPE). Door deze te verhogen, kunnen betere prestaties bij hoge contextlengtes worden bereikt.",
  "embedding.load.llama.ropeFrequencyBase/info": "[Geavanceerd] Past de basisfrequentie voor Rotary Positional Encoding aan, wat van invloed is op de manier waarop positie-informatie wordt ingebed",
  "embedding.load.llama.evalBatchSize/title": "Evaluatie Batchgrootte",
  "embedding.load.llama.evalBatchSize/subTitle": "Aantal inputtokens dat tegelijk verwerkt moet worden. Als u dit verhoogt, neemt de prestatie toe ten koste van het geheugengebruik.",
  "embedding.load.llama.evalBatchSize/info": "Stelt het aantal tokens in dat samen in √©√©n batch wordt verwerkt tijdens de evaluatie",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE-frequentieschaal",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "De contextlengte wordt met deze factor geschaald om de effectieve context uit te breiden met behulp van RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Geavanceerd] Wijzigt de schaal van de frequentie voor Rotary Positional Encoding om de granulariteit van de positionele codering te regelen",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Aantal discrete modellagen om te berekenen op de GPU voor GPU-versnelling",
  "embedding.load.llama.acceleration.offloadRatio/info": "Stel het aantal lagen in dat naar de GPU moet worden verplaatst.",
  "embedding.load.llama.keepModelInMemory/title": "Model in geheugen bewaren",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserveer systeemgeheugen voor het model, zelfs wanneer het naar de GPU wordt overgeheveld. Verbetert de prestaties, maar vereist meer systeem-RAM",
  "embedding.load.llama.keepModelInMemory/info": "Voorkomt dat het model naar schijf wordt geswapt, waardoor snellere toegang wordt gegarandeerd ten koste van een hoger RAM-gebruik",
  "embedding.load.llama.tryMmap/title": "Probeer mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Verbetert de laadtijd voor het model. Als u dit uitschakelt, kan dit de prestaties verbeteren wanneer het model groter is dan het beschikbare systeem-RAM.",
  "embedding.load.llama.tryMmap/info": "Laad modelbestanden rechtstreeks van schijf naar geheugen",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "De seed voor de willekeurige nummergenerator die wordt gebruikt bij het genereren van tekst. -1 is willekeurig zaad",

  "embedding.load.seed/info": "Willekeurige seed: stelt de seed in voor het genereren van willekeurige getallen om reproduceerbare resultaten te garanderen",

  "presetTooltip": {
    "included/title": "Vooraf ingestelde waarden",
    "included/description": "De volgende velden worden toegepast",
    "included/empty": "Er zijn geen velden van deze voorinstelling van toepassing in deze context.",
    "included/conflict": "U wordt gevraagd om te kiezen of u deze waarde wilt toepassen",
    "separateLoad/title": "Laadtijdconfiguratie",
    "separateLoad/description.1": "De preset bevat ook de volgende laadtijdconfiguratie. Laadtijdconfiguraties zijn modelbreed en vereisen het opnieuw laden van het model om van kracht te worden. Houd",
    "separateLoad/description.2": "toepassen op",
    "separateLoad/description.3": ".",
    "excluded/title": "Mogelijk niet van toepassing",
    "excluded/description": "De volgende velden zijn opgenomen in de voorinstelling, maar zijn niet van toepassing in de huidige context.",
    "legacy/title": "Legacy-voorinstelling",
    "legacy/description": "Deze preset is een legacy preset. Het bevat de volgende velden die nu automatisch worden verwerkt of niet langer van toepassing zijn."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Sjabloon",
      "jinja/error": "Het parseren van de Jinja-sjabloon is mislukt: {{error}}",
      "types.manual/label": "Handmatig",
      "manual.subfield.beforeSystem/label": "Voor Systeem",
      "manual.subfield.beforeSystem/placeholder": "Voer systeemprefix in...",
      "manual.subfield.afterSystem/label": "Na Systeem",
      "manual.subfield.afterSystem/placeholder": "Voer systeemprefix in...",
      "manual.subfield.beforeUser/label": "Voor Gebruiker",
      "manual.subfield.beforeUser/placeholder": "Voer gebruikersvoorvoegsel in...",
      "manual.subfield.afterUser/label": "Na Gebruiker",
      "manual.subfield.afterUser/placeholder": "Voer gebruikersachtervoegsel in...",
      "manual.subfield.beforeAssistant/label": "Voor Assistent",
      "manual.subfield.beforeAssistant/placeholder": "Voer het Assistent-voorvoegsel in...",
      "manual.subfield.afterAssistant/label": "Na Assistent",
      "manual.subfield.afterAssistant/placeholder": "Voer het Assistent-achtervoegsel in...",
      "stopStrings/label": "Extra stopstrings",
      "stopStrings/subTitle": "Sjabloonspecifieke stopstrings die naast door de gebruiker opgegeven stopstrings worden gebruikt."
    },
    "contextLength": {
      "maxValueTooltip": "Dit is het maximale aantal tokens dat het model is getraind om te verwerken. Klik om de context op deze waarde in te stellen",
      "maxValueTextStart": "Model ondersteunt tot",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "Hoewel een model maximaal een bepaald aantal tokens kan ondersteunen, kunnen de prestaties verslechteren als de bronnen van uw machine de belasting niet aankunnen. Wees voorzichtig bij het verhogen van deze waarde."
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Stop bij limiet",
      "stopAtLimitSub": "Stop met genereren zodra het geheugen van het model vol is",
      "truncateMiddle": "Midden afkappen",
      "truncateMiddleSub": "Verwijdert berichten uit het midden van het gesprek om ruimte te maken voor nieuwere. Het model zal nog steeds het begin van het gesprek onthouden",
      "rollingWindow": "Rollend venster",
      "rollingWindowSub": "Het model ontvangt altijd de meest recente berichten, maar kan het begin van het gesprek vergeten"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    }
  },
  "saveConflictResolution": {
    "title": "Kies welke waarden u in de voorinstelling wilt opnemen",
    "description": "Kies welke waarden u wilt behouden",
    "instructions": "Klik op een waarde om deze op te nemen",
    "userValues": "Vorige waarde",
    "presetValues": "Nieuwe waarde",
    "confirm": "Bevestig",
    "cancel": "Annuleer"
  },
  "applyConflictResolution": {
    "title": "Welke waarden wilt U behouden?",
    "description": "U hebt niet-vastgelegde wijzigingen die overlappen met de binnenkomende voorinstelling",
    "instructions": "Klik op een waarde om deze te behouden",
    "userValues": "Huidige waarde",
    "presetValues": "Inkomende voorinstellingswaarde",
    "confirm": "Bevestig",
    "cancel": "Annuleer"
  },
  "empty": "<Empty>",
  "presets": {
    "title": "Voorinstelling",
    "commitChanges": "Wijzigingen vastleggen",
    "commitChanges/description": "Sla uw wijzigingen op in de voorinstelling.",
    "commitChanges.manual": "Nieuwe velden gedetecteerd. U kunt kiezen welke wijzigingen u in de voorinstelling wilt opnemen.",
    "commitChanges.manual.hold.0": "Wacht",
    "commitChanges.manual.hold.1": "om te kiezen welke wijzigingen u in de voorinstelling wilt doorvoeren.",
    "commitChanges.saveAll.hold.0": "Wacht",
    "commitChanges.saveAll.hold.1": "om alle wijzigingen op te slaan.",
    "commitChanges.saveInPreset.hold.0": "Wacht",
    "commitChanges.saveInPreset.hold.1": "om alleen wijzigingen op te slaan in velden die al in de voorinstelling zijn opgenomen.",
    "commitChanges/error": "Het is niet gelukt om de wijzigingen in de voorinstelling door te voeren.",
    "commitChanges.manual/description": "Kies welke wijzigingen u in de voorinstelling wilt opnemen.",
    "saveAs": "Opslaan als nieuw...",
    "presetNamePlaceholder": "Voer een naam in voor de voorinstelling...",
    "cannotCommitChangesLegacy": "Dit is een legacy preset en kan niet worden gewijzigd. U kunt een kopie maken door te kiezen voor \"Opslaan als nieuw...\".",
    "cannotCommitChangesNoChanges": "Geen wijzigingen door te voeren.",
    "emptyNoUnsaved": "Selecteer een voorinstelling...",
    "emptyWithUnsaved": "Niet-opgeslagen voorinstelling",
    "saveEmptyWithUnsaved": "Voorinstelling opslaan als...",
    "saveConfirm": "Opslaan",
    "saveCancel": "Annuleer",
    "saving": "Opslaan...",
    "save/error": "Het opslaan van de voorinstelling is mislukt.",
    "deselect": "Voorinstelling deselecteren",
    "deselect/error": "Het is niet gelukt om de voorinstelling te deselecteren.",
    "select/error": "Het selecteren van een voorinstelling is mislukt.",
    "delete/error": "Het verwijderen van de voorinstelling is mislukt.",
    "discardChanges": "Niet-opgeslagen items verwijderen",
    "discardChanges/info": "Alle niet-vastgelegde wijzigingen negeren en de voorinstelling herstellen naar de oorspronkelijke staat",
    "newEmptyPreset": "Nieuwe lege voorinstelling maken...",
    "contextMenuSelect": "Voorinstelling selecteren",
    "contextMenuDelete": "Verwijder"
  },

  "flashAttentionWarning": "Flash Attention is een experimentele functie die problemen kan veroorzaken bij sommige modellen. Als u problemen ondervindt, probeer deze dan uit te schakelen.",

  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}
