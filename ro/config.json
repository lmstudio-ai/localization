{
  "noInstanceSelected": "Nicio instanță de model selectată",
  "resetToDefault": "Resetare",
  "showAdvancedSettings": "Afișare setări avansate",
  "showAll": "Toate",
  "basicSettings": "De bază",
  "configSubtitle": "Încarcă sau salvează presetări și experimentează cu suprascrierea parametrilor modelului",
  "inferenceParameters/title": "Parametri de predicție",
  "inferenceParameters/info": "Experimentează cu parametrii care influențează predicția",
  "generalParameters/title": "General",
  "samplingParameters/title": "Eșantionare",
  "basicTab": "De bază",
  "advancedTab": "Avansat",
  "advancedTab/title": "Configurație avansată",
  "advancedTab/expandAll": "Expandează tot",
  "advancedTab/overridesTitle": "Suprascrie configurația",
  "advancedTab/noConfigsText": "Nu ai modificări nesalvate - editează valorile de mai sus pentru a vedea suprascrierea aici",
  "loadInstanceFirst": "Încarcă un model pentru a vedea parametrii configurabili",
  "noListedConfigs": "Nu există parametri configurabili",
  "generationParameters/info": "Experimentează cu parametrii de bază care influențează generarea textului",
  "loadParameters/title": "Parametri de încărcare",
  "loadParameters/description": "Setări pentru a controla modul în care modelul este inițializat și încărcat în memorie",
  "loadParameters/reload": "Reîncarcă pentru a aplica modificările",
  "discardChanges": "Renunță la modificări",
  "loadModelToSeeOptions": "Încarcă un model pentru a vedea opțiunile",
  "llm.prediction.systemPrompt/title": "Prompt de Sistem",
  "llm.prediction.systemPrompt/description": "Folosește acest câmp pentru a oferi instrucțiuni de fundal modelului, cum ar fi reguli, constrângeri sau cerințe generale.",
  "llm.prediction.systemPrompt/subTitle": "Instrucțiuni pentru AI",
  "llm.prediction.temperature/title": "Temperatură",
  "llm.prediction.temperature/subTitle": "Gradul de aleatorietate introdus. 0 va da același rezultat de fiecare dată, valorile mai mari cresc creativitatea și variația",
  "llm.prediction.temperature/info": "Din documentația llama.cpp: Valoarea implicită este <{{dynamicValue}}>, oferind un echilibru între aleatorietate și determinism. La extrem, o temperatură de 0 va alege întotdeauna cel mai probabil token următor, ducând la ieșiri identice la fiecare rulare",
  "llm.prediction.llama.sampling/title": "Eșantionare",
  "llm.prediction.topKSampling/title": "Eșantionare Top K",
  "llm.prediction.topKSampling/subTitle": "Limitează următorul token la unul dintre cei mai probabili k tokeni. Acționează similar cu temperatura",
  "llm.prediction.topKSampling/info": "Din documentația llama.cpp:\n\nEșantionarea Top-k selectează următorul token doar dintre cei mai probabili k tokeni prezişi de model.\n\nAjută la reducerea riscului de generare a tokenilor improbabili sau fără sens, dar poate limita diversitatea ieșirii.\n\nO valoare mai mare pentru top-k (ex. 100) va lua în considerare mai mulți tokeni și va duce la text mai divers, în timp ce o valoare mai mică (ex. 10) se va concentra pe tokenii cei mai probabili și va genera text mai conservator.\n\n• Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Fire CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Numărul de fire CPU de utilizat în timpul inferenței",
  "llm.prediction.llama.cpuThreads/info": "Numărul de fire de utilizat în timpul calculului. Creșterea numărului de fire nu corelează întotdeauna cu performanța mai bună. Implicit este <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitare Lungime Răspuns",
  "llm.prediction.maxPredictedTokens/subTitle": "Opțional limitează lungimea răspunsului AI",
  "llm.prediction.maxPredictedTokens/info": "Controlează lungimea maximă a răspunsului chatbot-ului. Activează pentru a seta o limită pentru lungimea maximă a unui răspuns, sau dezactivează pentru a lăsa chatbot-ul să decidă când să se oprească.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Lungime maximă răspuns (tokeni)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximativ {{maxWords}} cuvinte",
  "llm.prediction.repeatPenalty/title": "Penalizare Repetiție",
  "llm.prediction.repeatPenalty/subTitle": "Cât de mult să descurajeze repetarea aceluiași token",
  "llm.prediction.repeatPenalty/info": "Din documentația llama.cpp: \"Ajută la prevenirea generării de text repetitiv sau monoton.\n\nO valoare mai mare (ex. 1.5) va penaliza repetițiile mai puternic, în timp ce o valoare mai mică (ex. 0.9) va fi mai permisivă.\" • Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Eșantionare Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilitatea minimă de bază pentru ca un token să fie selectat pentru ieșire",
  "llm.prediction.minPSampling/info": "Din documentația llama.cpp:\n\nProbabilitatea minimă pentru ca un token să fie considerat, relativă la probabilitatea celui mai probabil token. Trebuie să fie în [0, 1].\n\n• Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Eșantionare Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilitatea cumulativă minimă pentru tokenii următori posibili. Acționează similar cu temperatura",
  "llm.prediction.topPSampling/info": "Din documentația llama.cpp:\n\nEșantionarea Top-p, cunoscută și ca eșantionare nucleu, selectează următorul token dintr-un subset de tokeni care împreună au o probabilitate cumulativă de cel puțin p.\n\nAceastă metodă oferă un echilibru între diversitate și calitate.\n\nO valoare mai mare pentru top-p (ex. 0.95) va duce la text mai divers, în timp ce o valoare mai mică (ex. 0.5) va genera text mai focusat și conservator. Trebuie să fie în (0, 1].\n\n• Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Șiruri de Oprire",
  "llm.prediction.stopStrings/subTitle": "Șiruri care ar trebui să oprească modelul din generarea mai multor tokeni",
  "llm.prediction.stopStrings/info": "Șiruri specifice care când sunt întâlnite vor opri modelul din generarea mai multor tokeni",
  "llm.prediction.stopStrings/placeholder": "Introduceți un șir și apăsați ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Depășire Context",
  "llm.prediction.contextOverflowPolicy/subTitle": "Cum ar trebui să se comporte modelul când conversația devine prea mare pentru a fi gestionată",
  "llm.prediction.contextOverflowPolicy/info": "Decideți ce să faceți când conversația depășește dimensiunea memoriei de lucru a modelului ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalizare Frecvență",
  "llm.prediction.llama.presencePenalty/title": "Penalizare Prezență",
  "llm.prediction.llama.tailFreeSampling/title": "Eșantionare Fără Coadă",
  "llm.prediction.llama.locallyTypicalSampling/title": "Eșantionare Local Tipică",
  "llm.prediction.onnx.topKSampling/title": "Eșantionare Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limitează următorul token la unul dintre cei mai probabili k tokeni. Acționează similar cu temperatura",
  "llm.prediction.onnx.topKSampling/info": "Din documentația ONNX:\n\nNumărul de tokeni de vocabular cu cea mai mare probabilitate de păstrat pentru filtrarea top-k\n\n• Acest filtru este dezactivat implicit",
  "llm.prediction.onnx.repeatPenalty/title": "Penalizare Repetiție",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Cât de mult să descurajeze repetarea aceluiași token",
  "llm.prediction.onnx.repeatPenalty/info": "O valoare mai mare descurajează modelul să se repete",
  "llm.prediction.onnx.topPSampling/title": "Eșantionare Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilitatea cumulativă minimă pentru tokenii următori posibili. Acționează similar cu temperatura",
  "llm.prediction.onnx.topPSampling/info": "Din documentația ONNX:\n\nDoar tokenii cei mai probabili cu probabilități care se sumează la TopP sau mai mare sunt păstrați pentru generare\n\n• Acest filtru este dezactivat implicit",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Ieșire Structurată",
  "llm.prediction.structured/info": "Ieșire Structurată",
  "llm.prediction.structured/description": "Avansat: puteți furniza o Schemă JSON pentru a impune un format specific de ieșire din model. Citiți documentația pentru mai multe informații",
  "llm.prediction.promptTemplate/title": "Șablon Prompt",
  "llm.prediction.promptTemplate/subTitle": "Formatul în care mesajele din chat sunt trimise către model. Modificarea acestuia poate introduce comportament neașteptat - asigurați-vă că știți ce faceți!",
  
  "llm.load.contextLength/title": "Lungime Context",
  "llm.load.contextLength/subTitle": "Numărul maxim de tokeni pe care modelul îi poate procesa într-un prompt. Vezi opțiunile de Depășire Conversație din \"Parametri inferență\"",
  "llm.load.contextLength/info": "Specifică numărul maxim de tokeni pe care modelul îi poate lua în considerare simultan, impactând cât context reține în timpul procesării",
  "llm.load.contextLength/warning": "Setarea unei valori mari pentru lungimea contextului poate afecta semnificativ utilizarea memoriei",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "Seed-ul pentru generatorul de numere aleatorii folosit în generarea textului. -1 este aleator",
  "llm.load.seed/info": "Seed Aleator: Setează seed-ul pentru generarea numerelor aleatorii pentru a asigura rezultate reproductibile",
  
  "llm.load.llama.evalBatchSize/title": "Dimensiune Lot Evaluare",
  "llm.load.llama.evalBatchSize/subTitle": "Numărul de tokeni de intrare procesați simultan. Creșterea acestuia mărește performanța dar consumă mai multă memorie",
  "llm.load.llama.evalBatchSize/info": "Setează numărul de exemple procesate împreună într-un lot în timpul evaluării, afectând viteza și utilizarea memoriei",
  "llm.load.llama.ropeFrequencyBase/title": "Bază Frecvență RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frecvență de bază personalizată pentru încorporări poziționale rotative (RoPE). Creșterea poate îmbunătăți performanța la lungimi mari de context",
  "llm.load.llama.ropeFrequencyBase/info": "[Avansat] Ajustează frecvența de bază pentru Codificarea Pozițională Rotativă, afectând modul de încorporare a informației poziționale",
  "llm.load.llama.ropeFrequencyScale/title": "Scală Frecvență RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Lungimea contextului este scalată cu acest factor pentru a extinde contextul efectiv folosind RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avansat] Modifică scalarea frecvenței pentru Codificarea Pozițională Rotativă pentru a controla granularitatea codificării",
  "llm.load.llama.acceleration.offloadRatio/title": "Descărcare GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Numărul de straturi discrete ale modelului de calculat pe GPU pentru accelerare",
  "llm.load.llama.acceleration.offloadRatio/info": "Setează numărul de straturi de descărcat pe GPU",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Reduce utilizarea memoriei și timpul de generare pentru unele modele",
  "llm.load.llama.flashAttention/info": "Accelerează mecanismele de atenție pentru procesare mai rapidă și eficientă",
  "llm.load.numExperts/title": "Număr de Experți",
  "llm.load.numExperts/subTitle": "Numărul de experți de utilizat în model",
  "llm.load.numExperts/info": "Numărul de experți de utilizat în model",
  "llm.load.llama.keepModelInMemory/title": "Păstrează Modelul în Memorie",
  "llm.load.llama.keepModelInMemory/subTitle": "Rezervă memoria sistemului pentru model, chiar când e descărcat pe GPU. Îmbunătățește performanța dar necesită mai mult RAM",
  "llm.load.llama.keepModelInMemory/info": "Previne descărcarea modelului pe disc, asigurând acces mai rapid în schimbul utilizării mai mari de RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Folosește FP16 Pentru Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduce utilizarea memoriei prin stocarea cache-ului în precizie jumătate (FP16)",
  "llm.load.llama.tryMmap/title": "Încearcă mmap()",
  "llm.load.llama.tryMmap/subTitle": "Îmbunătățește timpul de încărcare. Dezactivarea poate îmbunătăți performanța când modelul e mai mare decât RAM-ul disponibil",
  "llm.load.llama.tryMmap/info": "Încarcă fișierele modelului direct de pe disc în memorie",
  
  "embedding.load.contextLength/title": "Lungime Context",
  "embedding.load.contextLength/subTitle": "Numărul maxim de tokeni procesați într-un prompt. Vezi opțiunile de Depășire Conversație din \"Parametri inferență\"",
  "embedding.load.contextLength/info": "Specifică numărul maxim de tokeni considerați simultan, influențând contextul păstrat în procesare",
  "embedding.load.llama.ropeFrequencyBase/title": "Bază Frecvență RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frecvență de bază personalizată pentru încorporări poziționale rotative (RoPE). Creșterea poate îmbunătăți performanța",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avansat] Ajustează frecvența de bază pentru Codificarea Pozițională Rotativă",
  "embedding.load.llama.evalBatchSize/title": "Dimensiune Lot Evaluare",
  "embedding.load.llama.evalBatchSize/subTitle": "Numărul de tokeni procesați simultan. Creșterea mărește performanța dar consumă mai multă memorie",
  "embedding.load.llama.evalBatchSize/info": "Setează numărul de tokeni procesați împreună într-un lot în timpul evaluării",
  "embedding.load.llama.ropeFrequencyScale/title": "Scală Frecvență RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Lungimea contextului e scalată cu acest factor pentru context efectiv extins",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avansat] Modifică scalarea frecvenței pentru controlul granularității codificării",
  "embedding.load.llama.acceleration.offloadRatio/title": "Descărcare GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Numărul de straturi discrete calculate pe GPU pentru accelerare",
  "embedding.load.llama.acceleration.offloadRatio/info": "Setează numărul de straturi de descărcat pe GPU",
  "embedding.load.llama.keepModelInMemory/title": "Păstrează Modelul în Memorie",
  "embedding.load.llama.keepModelInMemory/subTitle": "Rezervă memoria sistemului pentru model. Îmbunătățește performanța dar necesită mai mult RAM",
  "embedding.load.llama.keepModelInMemory/info": "Previne descărcarea pe disc pentru acces mai rapid cu consum mai mare de RAM",
  "embedding.load.llama.tryMmap/title": "Încearcă mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Îmbunătățește timpul de încărcare. Dezactivarea poate ajuta când modelul e mai mare decât RAM-ul",
  "embedding.load.llama.tryMmap/info": "Încarcă fișierele modelului direct în memorie",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "Seed-ul pentru generatorul de numere aleatorii. -1 înseamnă seed aleator",
  
  "embedding.load.seed/info": "Seed Aleator: Setează seed-ul pentru generatorul de numere aleatorii pentru a asigura rezultate reproductibile",
  
  "presetTooltip": {
    "included/title": "Valori Presetate",
    "included/description": "Următoarele câmpuri vor fi aplicate",
    "included/empty": "Niciun câmp din această presetare nu se aplică în acest context.",
    "included/conflict": "Vi se va cere să alegeți dacă doriți să aplicați această valoare",
    "separateLoad/title": "Configurație la Încărcare",
    "separateLoad/description.1": "Presetarea include și următoarea configurație de încărcare. Configurația de încărcare se aplică la nivel de model și necesită reîncărcarea modelului. Țineți apăsat",
    "separateLoad/description.2": "pentru a aplica la",
    "separateLoad/description.3": ".",
    "excluded/title": "Posibil să nu se aplice",
    "excluded/description": "Următoarele câmpuri sunt incluse în presetare dar nu se aplică în contextul curent.",
    "legacy/title": "Presetare Moștenită",
    "legacy/description": "Aceasta este o presetare moștenită. Include câmpuri care fie sunt gestionate automat acum, fie nu mai sunt aplicabile."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Gol>"
    },
    "checkboxNumeric": {
      "off": "OPRIT"
    },
    "stringArray": {
      "empty": "<Gol>"
    },
    "llmPromptTemplate": {
      "type": "Tip",
      "types.jinja/label": "Șablon (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Șablon",
      "jinja/error": "Eroare la parsarea șablonului Jinja: {{error}}",
      "jinja/empty": "Introduceți un șablon Jinja mai sus.",
      "jinja/unlikelyToWork": "Șablonul Jinja furnizat probabil nu va funcționa deoarece nu face referire la variabila \"messages\". Verificați dacă ați introdus un șablon corect.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Înaintea Sistemului",
      "manual.subfield.beforeSystem/placeholder": "Introduceți prefix Sistem...",
      "manual.subfield.afterSystem/label": "După Sistem",
      "manual.subfield.afterSystem/placeholder": "Introduceți sufix Sistem...",
      "manual.subfield.beforeUser/label": "Înaintea Utilizatorului",
      "manual.subfield.beforeUser/placeholder": "Introduceți prefix Utilizator...",
      "manual.subfield.afterUser/label": "După Utilizator",
      "manual.subfield.afterUser/placeholder": "Introduceți sufix Utilizator...",
      "manual.subfield.beforeAssistant/label": "Înaintea Asistentului",
      "manual.subfield.beforeAssistant/placeholder": "Introduceți prefix Asistent...",
      "manual.subfield.afterAssistant/label": "După Asistent",
      "manual.subfield.afterAssistant/placeholder": "Introduceți sufix Asistent...",
      "stopStrings/label": "Șiruri de Oprire Adiționale",
      "stopStrings/subTitle": "Șiruri specifice șablonului folosite în plus față de cele specificate de utilizator"
    },
    "contextLength": {
      "maxValueTooltip": "Acesta este numărul maxim de tokeni pentru care modelul a fost antrenat. Clic pentru a seta contextul la această valoare",
      "maxValueTextStart": "Modelul suportă până la",
      "maxValueTextEnd": "tokeni",
      "tooltipHint": "Deși un model poate suporta un anumit număr de tokeni, performanța poate scădea dacă resursele calculatorului nu fac față - folosiți cu precauție"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Oprire la Limită",
      "stopAtLimitSub": "Oprește generarea când memoria modelului se umple",
      "truncateMiddle": "Trunchiază Mijlocul",
      "truncateMiddleSub": "Elimină mesaje din mijlocul conversației pentru mesaje noi. Modelul va reține începutul",
      "rollingWindow": "Fereastră Rulantă",
      "rollingWindowSub": "Modelul va primi mereu ultimele mesaje dar poate uita începutul conversației"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAXIM",
      "off": "OPRIT"
    }
  },
  "saveConflictResolution": {
    "title": "Alegeți valorile de inclus în Presetare",
    "description": "Selectați valorile de păstrat",
    "instructions": "Clic pe o valoare pentru includere",
    "userValues": "Valoarea Anterioară",
    "presetValues": "Valoarea Nouă",
    "confirm": "Confirmă",
    "cancel": "Anulează"
  },
  "applyConflictResolution": {
    "title": "Ce valori păstrăm?",
    "description": "Aveți modificări neconfirmate care se suprapun cu Presetarea",
    "instructions": "Clic pe o valoare pentru păstrare",
    "userValues": "Valoarea Curentă",
    "presetValues": "Valoarea din Presetare",
    "confirm": "Confirmă",
    "cancel": "Anulează"
  },
  "empty": "<Gol>",
  "presets": {
    "title": "Presetare",
    "commitChanges": "Confirmă Modificările",
    "commitChanges/description": "Confirmă modificările în presetare",
    "commitChanges.manual": "Câmpuri noi detectate. Veți putea alege ce modificări să includeți",
    "commitChanges.manual.hold.0": "Țineți",
    "commitChanges.manual.hold.1": "pentru a alege modificările de confirmat",
    "commitChanges.saveAll.hold.0": "Țineți",
    "commitChanges.saveAll.hold.1": "pentru a salva toate modificările",
    "commitChanges.saveInPreset.hold.0": "Țineți",
    "commitChanges.saveInPreset.hold.1": "pentru a salva doar modificările câmpurilor existente",
    "commitChanges/error": "Eroare la confirmarea modificărilor",
    "commitChanges.manual/description": "Alegeți modificările de inclus",
    "saveAs": "Salvează Ca Nou...",
    "presetNamePlaceholder": "Introduceți un nume pentru presetare...",
    "cannotCommitChangesLegacy": "Aceasta e o presetare moștenită ce nu poate fi modificată. Puteți crea o copie folosind \"Salvează Ca Nou...\"",
    "cannotCommitChangesNoChanges": "Nu există modificări de confirmat",
    "emptyNoUnsaved": "Selectați o Presetare...",
    "emptyWithUnsaved": "Presetare Nesalvată",
    "saveEmptyWithUnsaved": "Salvează Presetarea Ca...",
    "saveConfirm": "Salvează",
    "saveCancel": "Anulează",
    "saving": "Se salvează...",
    "save/error": "Eroare la salvarea presetării",
    "deselect": "Deselectează Presetarea",
    "deselect/error": "Eroare la deselectarea presetării",
    "select/error": "Eroare la selectarea presetării",
    "delete/error": "Eroare la ștergerea presetării",
    "discardChanges": "Renunță la Nesalvate",
    "discardChanges/info": "Renunță la modificările neconfirmate și restaurează starea originală",
    "newEmptyPreset": "Creează presetare nouă goală...",
    "contextMenuSelect": "Selectează Presetare",
    "contextMenuDelete": "Șterge"
  },

  "flashAttentionWarning": "Flash Attention este o funcție experimentală ce poate cauza probleme cu unele modele. Dacă întâmpinați probleme, încercați să o dezactivați.",
  
  "seedUncheckedHint": "Seed Aleator",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}
