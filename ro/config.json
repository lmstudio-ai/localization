{
  "noInstanceSelected": "Nicio instanță de model selectată",
  "resetToDefault": "Resetare",
  "showAdvancedSettings": "Arată setările avansate",
  "showAll": "Toate",
  "basicSettings": "De bază",
  "configSubtitle": "Încărcați sau salvați presetările și experimentați cu suprascrieri de parametri de model",
  "inferenceParameters/title": "Parametri de predicție",
  "inferenceParameters/info": "Experimentați cu parametrii care influențează predicția.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Eșantionare",
  "basicTab": "De bază",
  "advancedTab": "Avansat",
  "advancedTab/title": "🧪 Configurare avansată",
  "advancedTab/expandAll": "Extinde tot",
  "advancedTab/overridesTitle": "Suprascrieri de configurare",
  "advancedTab/noConfigsText": "Nu aveți modificări nesalvate - editați valorile de mai sus pentru a vedea suprascrierile aici.",
  "loadInstanceFirst": "Încărcați un model pentru a vizualiza parametrii configurabili",
  "noListedConfigs": "Niciun parametru configurabil",
  "generationParameters/info": "Experimentați cu parametrii de bază care influențează generarea textului.",
  "loadParameters/title": "Parametri de încărcare",
  "loadParameters/description": "Setări pentru a controla modul în care modelul este inițializat și încărcat în memorie.",
  "loadParameters/reload": "Reîncărcați pentru a aplica modificările",
  "discardChanges": "Renunță la modificări",
  "loadModelToSeeOptions": "Încărcați un model pentru a vedea opțiunile",
  "llm.prediction.systemPrompt/title": "Prompt de sistem",
  "llm.prediction.systemPrompt/description": "Utilizați acest câmp pentru a furniza instrucțiuni de fundal modelului, cum ar fi un set de reguli, constrângeri sau cerințe generale.",
  "llm.prediction.systemPrompt/subTitle": "Ghid pentru AI",
  "llm.prediction.temperature/title": "Temperatură",
  "llm.prediction.temperature/subTitle": "Cât de multă aleatorietate să se introducă. 0 va da același rezultat de fiecare dată, în timp ce valori mai mari vor crește creativitatea și variația",
  "llm.prediction.temperature/info": "Din documentația de ajutor a llama.cpp: \"Valoarea implicită este <{{dynamicValue}}>, care oferă un echilibru între aleatorietate și determinism. În extreme, o temperatură de 0 va alege întotdeauna următorul token cel mai probabil, conducând la ieșiri identice la fiecare rulare\"",
  "llm.prediction.llama.sampling/title": "Eșantionare",
  "llm.prediction.topKSampling/title": "Eșantionare Top K",
  "llm.prediction.topKSampling/subTitle": "Limitează următorul token la unul dintre cei top-k tokeni cei mai probabili. Funcționează similar cu temperatura",
  "llm.prediction.topKSampling/info": "Din documentația de ajutor a llama.cpp:\n\nEșantionarea top-k este o metodă de generare a textului care selectează următorul token doar dintre cei k tokeni cei mai probabili prezis de model.\n\nAjută la reducerea riscului de a genera tokeni cu probabilitate scăzută sau fără sens, dar poate limita și diversitatea ieșirii.\n\nO valoare mai mare pentru top-k (de exemplu, 100) va considera mai mulți tokeni și va conduce la un text mai divers, în timp ce o valoare mai mică (de exemplu, 10) se va concentra pe cei mai probabili tokeni și va genera un text mai conservator.\n\n• Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Fire CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Numărul de fire CPU utilizate în timpul inferenței",
  "llm.prediction.llama.cpuThreads/info": "Numărul de fire utilizate în timpul calculului. Creșterea numărului de fire nu se corelează întotdeauna cu o performanță mai bună. Implicit este <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitează lungimea răspunsului",
  "llm.prediction.maxPredictedTokens/subTitle": "Opțional, limitați lungimea răspunsului AI",
  "llm.prediction.maxPredictedTokens/info": "Controlați lungimea maximă a răspunsului chatbot-ului. Activați pentru a seta o limită maximă de răspuns, sau dezactivați pentru a lăsa chatbot-ul să decidă când să se oprească.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Lungime maximă a răspunsului (tokeni)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximativ {{maxWords}} cuvinte",
  "llm.prediction.repeatPenalty/title": "Penalizare repetare",
  "llm.prediction.repeatPenalty/subTitle": "Cât de mult să descurajeze repetarea aceluiași token",
  "llm.prediction.repeatPenalty/info": "Din documentația de ajutor a llama.cpp: \"Ajută la prevenirea generării de text repetitiv sau monoton.\n\nO valoare mai mare (de exemplu, 1.5) va penaliza repetițiile mai sever, în timp ce o valoare mai mică (de exemplu, 0.9) va fi mai indulgentă.\" • Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Eșantionare Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilitatea de bază minimă pentru ca un token să fie selectat pentru ieșire",
  "llm.prediction.minPSampling/info": "Din documentația de ajutor a llama.cpp:\n\nProbabilitatea minimă pentru ca un token să fie considerat, relativ la probabilitatea celui mai probabil token. Trebuie să fie în [0, 1].\n\n• Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Eșantionare Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilitatea cumulativă minimă pentru următorii tokeni posibili. Funcționează similar cu temperatura",
  "llm.prediction.topPSampling/info": "Din documentația de ajutor a llama.cpp:\n\nEșantionarea top-p, cunoscută și sub numele de eșantionare nucleu, este o altă metodă de generare a textului care selectează următorul token dintr-un subset de tokeni care împreună au o probabilitate cumulativă de cel puțin p.\n\nAceastă metodă oferă un echilibru între diversitate și calitate, luând în considerare atât probabilitățile tokenilor, cât și numărul de tokeni din care se alege.\n\nO valoare mai mare pentru top-p (de exemplu, 0.95) va conduce la un text mai divers, în timp ce o valoare mai mică (de exemplu, 0.5) va genera un text mai concentrat și conservator. Trebuie să fie în intervalul (0, 1].\n\n• Valoarea implicită este <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Șiruri de oprire",
  "llm.prediction.stopStrings/subTitle": "Șiruri care ar trebui să oprească modelul din generarea altor tokeni",
  "llm.prediction.stopStrings/info": "Șiruri specifice care, atunci când sunt întâlnite, vor opri modelul din generarea de tokeni suplimentari",
  "llm.prediction.stopStrings/placeholder": "Introduceți un șir și apăsați ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Depășirea contextului",
  "llm.prediction.contextOverflowPolicy/subTitle": "Cum ar trebui să se comporte modelul atunci când conversația devine prea mare pentru a fi gestionată",
  "llm.prediction.contextOverflowPolicy/info": "Decideți ce să faceți atunci când conversația depășește dimensiunea memoriei de lucru a modelului ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalizare de frecvență",
  "llm.prediction.llama.presencePenalty/title": "Penalizare de prezență",
  "llm.prediction.llama.tailFreeSampling/title": "Eșantionare fără coadă",
  "llm.prediction.llama.locallyTypicalSampling/title": "Eșantionare local tipică",
  "llm.prediction.onnx.topKSampling/title": "Eșantionare Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limitează următorul token la unul dintre cei top-k tokeni cei mai probabili. Funcționează similar cu temperatura",
  "llm.prediction.onnx.topKSampling/info": "Din documentația ONNX:\n\nNumărul de tokeni cu cea mai mare probabilitate din vocabular care se păstrează pentru filtrarea top-k\n\n• Acest filtru este dezactivat implicit",
  "llm.prediction.onnx.repeatPenalty/title": "Penalizare repetare",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Cât de mult să descurajeze repetarea aceluiași token",
  "llm.prediction.onnx.repeatPenalty/info": "O valoare mai mare descurajează modelul să se repete",
  "llm.prediction.onnx.topPSampling/title": "Eșantionare Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilitatea cumulativă minimă pentru următorii tokeni posibili. Funcționează similar cu temperatura",
  "llm.prediction.onnx.topPSampling/info": "Din documentația ONNX:\n\nSe păstrează doar tokenii cei mai probabili ale căror probabilități însumează TopP sau mai mult pentru generare\n\n• Acest filtru este dezactivat implicit",
  "llm.prediction.seed/title": "Sămânță",
  "llm.prediction.structured/title": "Ieșire structurată",
  "llm.prediction.structured/info": "Ieșire structurată",
  "llm.prediction.structured/description": "Avansat: puteți furniza un JSON Schema pentru a impune un anumit format de ieșire de la model. Citiți [documentația](https://lmstudio.ai/docs/advanced/structured-output) pentru a afla mai multe",
  "llm.prediction.promptTemplate/title": "Șablon de prompt",
  "llm.prediction.promptTemplate/subTitle": "Formatul în care mesajele din chat sunt trimise către model. Schimbarea acestuia poate introduce comportamente neașteptate - asigurați-vă că știți ce faceți!",
  "llm.load.contextLength/title": "Lungimea contextului",
  "llm.load.contextLength/subTitle": "Numărul maxim de tokeni la care modelul poate presta atenție într-un singur prompt. Consultați opțiunile de depășire a conversației din secțiunea \"Parametri de inferență\" pentru mai multe modalități de gestionare a acestui aspect",
  "llm.load.contextLength/info": "Specificați numărul maxim de tokeni pe care modelul îi poate considera simultan, influențând cât de mult context reține în timpul procesării",
  "llm.load.contextLength/warning": "Setarea unei valori mari pentru lungimea contextului poate afecta semnificativ utilizarea memoriei",
  "llm.load.seed/title": "Sămânță",
  "llm.load.seed/subTitle": "Sămânța pentru generatorul de numere aleatorii folosit în generarea textului. -1 înseamnă aleatoriu",
  "llm.load.seed/info": "Sămânță aleatorie: Setează sămânța pentru generarea numerelor aleatorii pentru a asigura rezultate reproductibile",
  "llm.load.llama.evalBatchSize/title": "Dimensiunea lotului de evaluare",
  "llm.load.llama.evalBatchSize/subTitle": "Numărul de tokeni de intrare procesați simultan. Creșterea acestuia îmbunătățește performanța, dar crește utilizarea memoriei",
  "llm.load.llama.evalBatchSize/info": "Setează numărul de exemple procesate împreună într-un singur lot în timpul evaluării, afectând viteza și utilizarea memoriei",
  "llm.load.llama.ropeFrequencyBase/title": "Bază de frecvență RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frecvență de bază personalizată pentru încorporările poziționale rotative (RoPE). Creșterea acesteia poate permite o performanță mai bună la lungimi mari de context",
  "llm.load.llama.ropeFrequencyBase/info": "[Avansat] Ajustează frecvența de bază pentru codarea pozițională rotativă, influențând modul în care informațiile poziționale sunt încorporate",
  "llm.load.llama.ropeFrequencyScale/title": "Scală de frecvență RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Lungimea contextului este scalată prin acest factor pentru a extinde contextul efectiv folosind RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avansat] Modifică scalarea frecvenței pentru codarea pozițională rotativă pentru a controla granularitatea codării poziționale",
  "llm.load.llama.acceleration.offloadRatio/title": "Descărcare GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Numărul de straturi discrete ale modelului care vor fi procesate pe GPU pentru accelerare",
  "llm.load.llama.acceleration.offloadRatio/info": "Setați numărul de straturi de descărcat pe GPU.",
  "llm.load.llama.flashAttention/title": "Atenție Flash",
  "llm.load.llama.flashAttention/subTitle": "Reduce utilizarea memoriei și timpul de generare la unele modele",
  "llm.load.llama.flashAttention/info": "Accelerează mecanismele de atenție pentru o procesare mai rapidă și mai eficientă",
  "llm.load.numExperts/title": "Numărul de experți",
  "llm.load.numExperts/subTitle": "Numărul de experți de utilizat în model",
  "llm.load.numExperts/info": "Numărul de experți de utilizat în model",
  "llm.load.llama.keepModelInMemory/title": "Păstrează modelul în memorie",
  "llm.load.llama.keepModelInMemory/subTitle": "Rezervați memorie sistemului pentru model, chiar și când este descărcat pe GPU. Îmbunătățește performanța, dar necesită mai multă memorie RAM a sistemului",
  "llm.load.llama.keepModelInMemory/info": "Previne ca modelul să fie mutat pe disc, asigurând un acces mai rapid în schimbul unei utilizări mai mari a memoriei RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Utilizează FP16 pentru cache-ul KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduce utilizarea memoriei prin stocarea cache-ului în precizie redusă (FP16)",
  "llm.load.llama.tryMmap/title": "Încearcă mmap()",
  "llm.load.llama.tryMmap/subTitle": "Îmbunătățește timpul de încărcare al modelului. Dezactivarea acestuia poate îmbunătăți performanța când modelul este mai mare decât memoria RAM disponibilă",
  "llm.load.llama.tryMmap/info": "Încărcați fișierele modelului direct de pe disc în memorie",
  "embedding.load.contextLength/title": "Lungimea contextului",
  "embedding.load.contextLength/subTitle": "Numărul maxim de tokeni la care modelul poate presta atenție într-un singur prompt. Consultați opțiunile de depășire a conversației din secțiunea \"Parametri de inferență\" pentru mai multe modalități de gestionare a acestui aspect",
  "embedding.load.contextLength/info": "Specificați numărul maxim de tokeni pe care modelul îi poate considera simultan, influențând cât de mult context reține în timpul procesării",
  "embedding.load.llama.ropeFrequencyBase/title": "Bază de frecvență RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frecvență de bază personalizată pentru încorporările poziționale rotative (RoPE). Creșterea acesteia poate permite o performanță mai bună la lungimi mari de context",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avansat] Ajustează frecvența de bază pentru codarea pozițională rotativă, influențând modul în care informațiile poziționale sunt încorporate",
  "embedding.load.llama.evalBatchSize/title": "Dimensiunea lotului de evaluare",
  "embedding.load.llama.evalBatchSize/subTitle": "Numărul de tokeni de intrare procesați simultan. Creșterea acestuia îmbunătățește performanța, dar crește utilizarea memoriei",
  "embedding.load.llama.evalBatchSize/info": "Setează numărul de tokeni procesați împreună într-un singur lot în timpul evaluării",
  "embedding.load.llama.ropeFrequencyScale/title": "Scală de frecvență RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Lungimea contextului este scalată prin acest factor pentru a extinde contextul efectiv folosind RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avansat] Modifică scalarea frecvenței pentru codarea pozițională rotativă pentru a controla granularitatea codării poziționale",
  "embedding.load.llama.acceleration.offloadRatio/title": "Descărcare GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Numărul de straturi discrete ale modelului care vor fi procesate pe GPU pentru accelerare",
  "embedding.load.llama.acceleration.offloadRatio/info": "Setați numărul de straturi de descărcat pe GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Păstrează modelul în memorie",
  "embedding.load.llama.keepModelInMemory/subTitle": "Rezervați memorie sistemului pentru model, chiar și când este descărcat pe GPU. Îmbunătățește performanța, dar necesită mai multă memorie RAM",
  "embedding.load.llama.keepModelInMemory/info": "Previne ca modelul să fie mutat pe disc, asigurând un acces mai rapid în schimbul unei utilizări mai mari a memoriei RAM",
  "embedding.load.llama.tryMmap/title": "Încearcă mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Îmbunătățește timpul de încărcare al modelului. Dezactivarea acestuia poate îmbunătăți performanța când modelul este mai mare decât memoria RAM disponibilă",
  "embedding.load.llama.tryMmap/info": "Încărcați fișierele modelului direct de pe disc în memorie",
  "embedding.load.seed/title": "Sămânță",
  "embedding.load.seed/subTitle": "Sămânța pentru generatorul de numere aleatorii folosit în generarea textului. -1 înseamnă sămânță aleatorie",
  "embedding.load.seed/info": "Sămânță aleatorie: Setează sămânța pentru generarea numerelor aleatorii pentru a asigura rezultate reproductibile",
  "presetTooltip": {
    "included/title": "Valori presetate",
    "included/description": "Câmpurile următoare vor fi aplicate",
    "included/empty": "Niciun câmp al acestei presetări nu se aplică în acest context.",
    "included/conflict": "Vi se va cere să alegeți dacă să aplicați această valoare",
    "separateLoad/title": "Configurare la încărcare",
    "separateLoad/description.1": "Presetarea include, de asemenea, următoarea configurare la încărcare. Configurările la încărcare sunt pentru întregul model și necesită reîncărcarea modelului pentru a intra în vigoare. Țineți apăsat",
    "separateLoad/description.2": "pentru a aplica la",
    "separateLoad/description.3": ".",
    "excluded/title": "Este posibil să nu se aplice",
    "excluded/description": "Câmpurile următoare sunt incluse în presetare, dar nu se aplică în contextul curent.",
    "legacy/title": "Presetare veche",
    "legacy/description": "Această presetare este o presetare veche. Include următoarele câmpuri care fie sunt gestionate automat acum, fie nu mai sunt aplicabile."
  },
  "customInputs": {
    "string": {
      "emptyParagraph": "<Gol>"
    },
    "checkboxNumeric": {
      "off": "Oprit"
    },
    "stringArray": {
      "empty": "<Gol>"
    },
    "llmPromptTemplate": {
      "type": "Tip",
      "types.jinja/label": "Șablon (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Șablon",
      "jinja/error": "Parcurgerea șablonului Jinja a eșuat: {{error}}",
      "jinja/empty": "Vă rugăm să introduceți un șablon Jinja mai sus.",
      "jinja/unlikelyToWork": "Șablonul Jinja furnizat mai sus este puțin probabil să funcționeze deoarece nu face referire la variabila \"messages\". Vă rugăm verificați dacă ați introdus un șablon corect.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Înainte de Sistem",
      "manual.subfield.beforeSystem/placeholder": "Introduceți prefixul pentru sistem...",
      "manual.subfield.afterSystem/label": "După Sistem",
      "manual.subfield.afterSystem/placeholder": "Introduceți sufixul pentru sistem...",
      "manual.subfield.beforeUser/label": "Înainte de Utilizator",
      "manual.subfield.beforeUser/placeholder": "Introduceți prefixul pentru utilizator...",
      "manual.subfield.afterUser/label": "După Utilizator",
      "manual.subfield.afterUser/placeholder": "Introduceți sufixul pentru utilizator...",
      "manual.subfield.beforeAssistant/label": "Înainte de Asistent",
      "manual.subfield.beforeAssistant/placeholder": "Introduceți prefixul pentru asistent...",
      "manual.subfield.afterAssistant/label": "După Asistent",
      "manual.subfield.afterAssistant/placeholder": "Introduceți sufixul pentru asistent...",
      "stopStrings/label": "Șiruri suplimentare de oprire",
      "stopStrings/subTitle": "Șiruri de oprire specifice șablonului care vor fi utilizate pe lângă șirurile de oprire specificate de utilizator."
    },
    "contextLength": {
      "maxValueTooltip": "Acesta este numărul maxim de tokeni pe care modelul a fost antrenat să îi gestioneze. Faceți clic pentru a seta contextul la această valoare",
      "maxValueTextStart": "Modelul suportă până la",
      "maxValueTextEnd": "tokeni",
      "tooltipHint": "Deși un model poate suporta un anumit număr de tokeni, performanța poate scădea dacă resursele mașinii dvs. nu pot gestiona încărcarea - folosiți precauție când măriți această valoare"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Oprește la limită",
      "stopAtLimitSub": "Oprește generarea odată ce memoria modelului se umple",
      "truncateMiddle": "Trunchiază mijlocul",
      "truncateMiddleSub": "Elimină mesajele din mijlocul conversației pentru a face loc celor noi. Modelul își va aminti totuși începutul conversației",
      "rollingWindow": "Fereastră derulantă",
      "rollingWindowSub": "Modelul va primi întotdeauna ultimele câteva mesaje, dar poate uita începutul conversației"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "Oprit"
    }
  },
  "saveConflictResolution": {
    "title": "Alegeți care valori să fie incluse în presetare",
    "description": "Alegeți valorile de păstrat",
    "instructions": "Faceți clic pe o valoare pentru a o include",
    "userValues": "Valoare anterioară",
    "presetValues": "Valoare nouă",
    "confirm": "Confirmă",
    "cancel": "Anulează"
  },
  "applyConflictResolution": {
    "title": "Ce valori să păstrați?",
    "description": "Aveți modificări nesalvate care se suprapun cu presetarea primită",
    "instructions": "Faceți clic pe o valoare pentru a o păstra",
    "userValues": "Valoare curentă",
    "presetValues": "Valoare din presetarea primită",
    "confirm": "Confirmă",
    "cancel": "Anulează"
  },
  "empty": "<Gol>",
  "presets": {
    "title": "Presetare",
    "commitChanges": "Confirmă modificările",
    "commitChanges/description": "Confirmă modificările în presetare.",
    "commitChanges.manual": "Câmpuri noi detectate. Veți putea alege care modificări să fie incluse în presetare.",
    "commitChanges.manual.hold.0": "Țineți",
    "commitChanges.manual.hold.1": "pentru a alege care modificări să fie confirmate în presetare.",
    "commitChanges.saveAll.hold.0": "Țineți",
    "commitChanges.saveAll.hold.1": "pentru a salva toate modificările.",
    "commitChanges.saveInPreset.hold.0": "Țineți",
    "commitChanges.saveInPreset.hold.1": "pentru a salva doar modificările la câmpurile care sunt deja incluse în presetare.",
    "commitChanges/error": "Confirmarea modificărilor în presetare a eșuat.",
    "commitChanges.manual/description": "Alegeți care modificări să fie incluse în presetare.",
    "saveAs": "Salvează ca nou...",
    "presetNamePlaceholder": "Introduceți un nume pentru presetare...",
    "cannotCommitChangesLegacy": "Aceasta este o presetare veche și nu poate fi modificată. Puteți crea o copie folosind \"Salvează ca nou...\".",
    "cannotCommitChangesNoChanges": "Nicio modificare de confirmat.",
    "emptyNoUnsaved": "Selectați o presetare...",
    "emptyWithUnsaved": "Presetare nesalvată",
    "saveEmptyWithUnsaved": "Salvează presetarea ca...",
    "saveConfirm": "Salvează",
    "saveCancel": "Anulează",
    "saving": "Se salvează...",
    "save/error": "Salvarea presetării a eșuat.",
    "deselect": "Deselectează presetarea",
    "deselect/error": "Deselectarea presetării a eșuat.",
    "select/error": "Selectarea presetării a eșuat.",
    "delete/error": "Ștergerea presetării a eșuat.",
    "discardChanges": "Renunță la nesalvat",
    "discardChanges/info": "Renunță la toate modificările nesalvate și restaurează presetarea la starea sa originală",
    "newEmptyPreset": "Creează o presetare nouă și goală...",
    "contextMenuSelect": "Selectează presetarea",
    "contextMenuDelete": "Șterge"
  },
  "flashAttentionWarning": "Flash Attention este o caracteristică experimentală care poate cauza probleme la unele modele. Dacă întâmpinați probleme, încercați să o dezactivați.",
  "seedUncheckedHint": "Sămânță aleatorie",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}
