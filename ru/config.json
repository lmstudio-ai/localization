{
  "noInstanceSelected": "Модель не выбрана",
  "resetToDefault": "Сбросить настройки",
  "showAdvancedSettings": "Расширенные настройки",
  "showAll": "Показать все",
  "basicSettings": "Основные",
  "configSubtitle": "Управление пресетами и тонкая настройка параметров модели",
  "inferenceParameters/title": "Параметры инференса",
  "inferenceParameters/info": "Экспериментируйте с параметрами, влияющими на процесс инференса.",
  "generalParameters/title": "Общие",
  "samplingParameters/title": "Сэмплирование",
  "basicTab": "Базовые",
  "advancedTab": "Продвинутые",
  "loadInstanceFirst": "Загрузите модель для доступа к настраиваемым параметрам",
  "generationParameters/info": "Настройте базовые параметры генерации текста.",
  "loadParameters/title": "Параметры загрузки",
  "loadParameters/description": "Изменение этих параметров требует перезагрузки модели",
  "loadParameters/reload": "Перезагрузить для применения изменений параметров загрузки",
  "discardChanges": "Отменить изменения",
  "llm.prediction.systemPrompt/title": "Системный промпт",
  "llm.prediction.systemPrompt/description": "Используйте это поле для задания базовых инструкций модели, таких как правила, ограничения или общие требования. Также известно как 'системный промпт'.",
  "llm.prediction.temperature/title": "Температура",
  "llm.prediction.temperature/info": "Из документации llama.cpp: 'Значение по умолчанию <{{dynamicValue}}> обеспечивает баланс между случайностью и детерминизмом. При температуре 0 модель всегда выбирает наиболее вероятный следующий токен, что приводит к идентичным результатам при каждом запуске'",
  "llm.prediction.llama.topKSampling/title": "Top-K сэмплирование",
  "llm.prediction.llama.topKSampling/info": "Из документации llama.cpp:\n\nTop-K сэмплирование - метод генерации текста, выбирающий следующий токен только из K наиболее вероятных токенов, предсказанных моделью.\n\nЭто снижает риск генерации маловероятных или бессмысленных токенов, но может ограничить разнообразие вывода.\n\nБолее высокое значение K (например, 100) учитывает больше токенов и приводит к более разнообразному тексту, в то время как меньшее значение (например, 10) фокусируется на наиболее вероятных токенах и генерирует более консервативный текст.\n\n• Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Потоки CPU",
  "llm.prediction.llama.cpuThreads/info": "Количество потоков, используемых при вычислениях. Увеличение числа потоков не всегда коррелирует с улучшением производительности. По умолчанию: <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Ограничить длину ответа",
  "llm.prediction.maxPredictedTokens/info": "Управление максимальной длиной ответа чат-бота. Включите, чтобы установить ограничение на максимальную длину ответа, или выключите, чтобы позволить чат-боту самостоятельно определять момент остановки.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Максимальная длина ответа (в токенах)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Примерно {{maxWords}} слов",
  "llm.prediction.llama.repeatPenalty/title": "Штраф за повторения",
  "llm.prediction.llama.repeatPenalty/info": "Из документации llama.cpp: \"Помогает предотвратить генерацию повторяющегося или монотонного текста.\n\nБолее высокое значение (например, 1.5) сильнее штрафует повторения, в то время как более низкое значение (например, 0.9) будет более снисходительным.\" • Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Min-P сэмплирование",
  "llm.prediction.llama.minPSampling/info": "Из документации llama.cpp:\n\nМинимальная вероятность для рассмотрения токена относительно вероятности наиболее вероятного токена. Должно быть в диапазоне [0, 1].\n\n• Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top-P сэмплирование",
  "llm.prediction.llama.topPSampling/info": "Из документации llama.cpp:\n\nTop-P сэмплирование, также известное как nucleus sampling, - это метод генерации текста, который выбирает следующий токен из подмножества токенов, суммарная вероятность которых составляет не менее P.\n\nЭтот метод обеспечивает баланс между разнообразием и качеством, учитывая как вероятности токенов, так и количество токенов для выборки.\n\nБолее высокое значение P (например, 0.95) приведет к более разнообразному тексту, в то время как более низкое значение (например, 0.5) будет генерировать более сфокусированный и консервативный текст. Должно быть в диапазоне (0, 1].\n\n• Значение по умолчанию: <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Стоп-строки",
  "llm.prediction.stopStrings/info": "Конкретные строки, при обнаружении которых модель прекратит генерацию токенов",
  "llm.prediction.stopStrings/placeholder": "Введите строку и нажмите ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Переполнение контекста",
  "llm.prediction.contextOverflowPolicy/info": "Определите действие при превышении размера рабочей памяти ('контекста') модели",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Остановка при достижении лимита",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Прекращение генерации при заполнении памяти модели",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Усечение середины",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Удаляет сообщения из середины разговора, чтобы освободить место для новых. Модель по-прежнему помнит начало разговора",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Скользящее окно",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Модель всегда получает несколько последних сообщений, но может забыть начало разговора",
  "llm.prediction.llama.frequencyPenalty/title": "Штраф за частоту",
  "llm.prediction.llama.presencePenalty/title": "Штраф за присутствие",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free сэмплирование",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical сэмплирование",
  "llm.prediction.mlx.repeatPenalty/title": "Штраф за повторения",
  "llm.prediction.mlx.repeatPenalty/info": "Более высокое значение сильнее препятствует самоповторам модели",
  "llm.prediction.seed/title": "Сид",
  "llm.prediction.structured/title": "Структурированный вывод",
  "llm.prediction.structured/info": "Структурированный вывод",
  "llm.load.contextLength/title": "Длина контекста",
  "llm.load.contextLength/info": "Определяет максимальное количество токенов, которые модель может учитывать одновременно, влияя на объем сохраняемого контекста при обработке",
  "llm.load.seed/title": "Сид",
  "llm.load.seed/info": "Случайное начальное число: устанавливает сид для генерации случайных чисел, обеспечивая воспроизводимость результатов",
  "llm.load.llama.evalBatchSize/title": "Размер пакета при оценке",
  "llm.load.llama.evalBatchSize/info": "Устанавливает количество примеров, обрабатываемых вместе в одном пакете во время оценки, влияя на скорость и использование памяти",
  "llm.load.llama.ropeFrequencyBase/title": "Базовая частота RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Продвинутое] Настраивает базовую частоту для Rotary Positional Encoding, влияя на способ внедрения позиционной информации",
  "llm.load.llama.ropeFrequencyScale/title": "Масштаб частоты RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Продвинутое] Изменяет масштабирование частоты для Rotary Positional Encoding для контроля гранулярности позиционного кодирования",
  "llm.load.llama.gpuOffload/title": "Выгрузка на GPU",
  "llm.load.llama.gpuOffload/info": "Установите долю вычислений для выгрузки на GPU. Установите 'off' для отключения выгрузки на GPU, или 'auto', чтобы позволить модели решать самостоятельно.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Ускоряет механизмы внимания для более быстрой и эффективной обработки",
  "llm.load.llama.keepModelInMemory/title": "Держать модель в памяти",
  "llm.load.llama.keepModelInMemory/info": "Предотвращает выгрузку модели на диск, обеспечивая более быстрый доступ за счет повышенного использования оперативной памяти",
  "llm.load.llama.useFp16ForKVCache/title": "Использовать FP16 для KV-кэша",
  "llm.load.llama.useFp16ForKVCache/info": "Уменьшает использование памяти, храня кэш в половинной точности (FP16)",
  "llm.load.llama.tryMmap/title": "Пробовать mmap()",
  "llm.load.llama.tryMmap/info": "Загружает файлы модели напрямую с диска в память"
  }