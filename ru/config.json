{
  "noInstanceSelected": "Не выбрана модель",
  "resetToDefault": "Сбросить на значения по умолчанию",
  "showAdvancedSettings": "Показать дополнительные настройки",
  "showAll": "Показать все",
  "basicSettings": "Основные",
  "configSubtitle": "Загрузка или сохранение пресетов и эксперименты с перекрытием параметров модели",
  "inferenceParameters/title": "Параметры предсказания",
  "inferenceParameters/info": "Экспериментируйте с параметрами, которые влияют на предсказание.",
  "generalParameters/title": "Общие",
  "samplingParameters/title": "Создание выборки",
  "basicTab": "Основной",
  "advancedTab": "Дополнительный",
  "loadInstanceFirst": "Загрузите модель, чтобы просмотреть настраиваемые параметры",
  "generationParameters/info": "Экспериментируйте с базовыми параметрами, которые влияют на генерацию текста.",
  "loadParameters/title": "Параметры загрузки",
  "loadParameters/description": "Изменение этих параметров требует перезагрузки модели",
  "loadParameters/reload": "Перезагрузить, чтобы применить изменения в параметрах загрузки",
  "discardChanges": "Отменить изменения",
  "llm.prediction.systemPrompt/title": "Системный промпт",
  "llm.prediction.systemPrompt/description": "Используйте это поле для предоставления фоновых инструкций модели, таких как набор правил, ограничений или общих требований. Это поле также часто называют \"системным промптом\".",
  "llm.prediction.temperature/title": "Температура",
  "llm.prediction.temperature/info": "Из документации llama.cpp: \"Значение по умолчанию <{{dynamicValue}}> обеспечивает баланс между случайностью и детерминизмом. В крайнем случае, температура 0 всегда будет выбирать наиболее вероятный следующий токен, приводя к идентичным результатам при каждом запуске\"",
  "llm.prediction.llama.topKSampling/title": "Сэмплирование Top K",
  "llm.prediction.llama.topKSampling/info": "Из документации llama.cpp:\n\nСэмплирование Top-k - это метод генерации текста, который выбирает следующий токен только из k наиболее вероятных токенов, предсказанных моделью.\n\nЭто помогает снизить риск генерации маловероятных или бессмысленных токенов, но также может ограничить разнообразие вывода.\n\nБолее высокое значение для top-k (например, 100) будет учитывать больше токенов и приведет к более разнообразному тексту, в то время как более низкое значение (например, 10) сосредоточится на наиболее вероятных токенах и сгенерирует более консервативный текст.\n\n• Значение по умолчанию <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Потоки CPU",
  "llm.prediction.llama.cpuThreads/info": "Количество потоков, используемых во время вычислений. Увеличение числа потоков не всегда коррелирует с лучшей производительностью. По умолчанию <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Ограничить длину ответа",
  "llm.prediction.maxPredictedTokens/info": "Контролирует максимальную длину ответа чат-бота. Включите, чтобы установить ограничение на максимальную длину ответа, или выключите, чтобы позволить чат-боту самому решать, когда остановиться.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Максимальная длина ответа (в токенах)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Примерно {{maxWords}} слов",
  "llm.prediction.llama.repeatPenalty/title": "Штраф за повторение",
  "llm.prediction.llama.repeatPenalty/info": "Из документации llama.cpp: \"Помогает предотвратить генерацию моделью повторяющегося или монотонного текста.\n\nБолее высокое значение (например, 1.5) будет сильнее штрафовать повторения, в то время как более низкое значение (например, 0.9) будет более снисходительным.\" • Значение по умолчанию <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Сэмплирование Min P",
  "llm.prediction.llama.minPSampling/info": "Из документации llama.cpp:\n\nМинимальная вероятность для рассмотрения токена, относительно вероятности наиболее вероятного токена. Должна быть в диапазоне [0, 1].\n\n• Значение по умолчанию <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Сэмплирование Top P",
  "llm.prediction.llama.topPSampling/info": "Из документации llama.cpp:\n\nСэмплирование Top-p, также известное как ядерное сэмплирование, - это еще один метод генерации текста, который выбирает следующий токен из подмножества токенов, которые вместе имеют кумулятивную вероятность не менее p.\n\nЭтот метод обеспечивает баланс между разнообразием и качеством, учитывая как вероятности токенов, так и количество токенов для выборки.\n\nБолее высокое значение для top-p (например, 0.95) приведет к более разнообразному тексту, в то время как более низкое значение (например, 0.5) сгенерирует более сфокусированный и консервативный текст. Должно быть в диапазоне (0, 1].\n\n• Значение по умолчанию <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Стоп-строки",
  "llm.prediction.stopStrings/info": "Определенные строки, при встрече которых модель прекратит генерацию дополнительных токенов",
  "llm.prediction.stopStrings/placeholder": "Введите строку и нажмите ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Переполнение контекста разговора",
  "llm.prediction.contextOverflowPolicy/info": "Определите, что делать, когда разговор превышает размер рабочей памяти модели ('контекст')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Остановиться на пределе",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Прекратить генерацию, как только память модели заполнится",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Обрезать середину",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Удаляет сообщения из середины разговора, чтобы освободить место для новых. Модель все еще будет помнить начало разговора",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Скользящее окно",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Модель всегда будет получать несколько последних сообщений, но может забыть начало разговора",
  "llm.prediction.llama.frequencyPenalty/title": "Штраф за частоту",
  "llm.prediction.llama.presencePenalty/title": "Штраф за присутствие",
  "llm.prediction.llama.tailFreeSampling/title": "Сэмплирование без хвоста",
  "llm.prediction.llama.locallyTypicalSampling/title": "Локально типичное сэмплирование",
  "llm.prediction.mlx.repeatPenalty/title": "Штраф за повторение",
  "llm.prediction.mlx.repeatPenalty/info": "Более высокое значение препятствует модели повторять себя",
  "llm.prediction.seed/title": "Сид",
  "llm.prediction.structured/title": "Структурированный вывод",
  "llm.prediction.structured/info": "Структурированный вывод",
  "llm.load.contextLength/title": "Длина контекста",
  "llm.load.contextLength/info": "Определяет максимальное количество токенов, которое модель может рассматривать одновременно, влияя на то, сколько контекста она сохраняет во время обработки",
  "llm.load.seed/title": "Сид",
  "llm.load.seed/info": "Случайный сид: Устанавливает сид для генерации случайных чисел, чтобы обеспечить воспроизводимые результаты",
  "llm.load.llama.evalBatchSize/title": "Размер пакета оценки",
  "llm.load.llama.evalBatchSize/info": "Устанавливает количество примеров, обрабатываемых вместе в одном пакете во время оценки, влияя на скорость и использование памяти",
  "llm.load.llama.ropeFrequencyBase/title": "Базовая частота RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Продвинутое] Настраивает базовую частоту для Rotary Positional Encoding, влияя на то, как встраивается позиционная информация",
  "llm.load.llama.ropeFrequencyScale/title": "Масштаб частоты RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Продвинутое] Изменяет масштабирование частоты для Rotary Positional Encoding для контроля гранулярности позиционного кодирования",
  "llm.load.llama.acceleration.offloadRatio/title": "Разгрузка GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Установите соотношение вычислений для разгрузки на GPU. Установите 'выкл', чтобы отключить разгрузку GPU, или 'авто', чтобы позволить модели решать самостоятельно.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Ускоряет механизмы внимания для более быстрой и эффективной обработки",
  "llm.load.llama.keepModelInMemory/title": "Держать модель в памяти",
  "llm.load.llama.keepModelInMemory/info": "Предотвращает выгрузку модели на диск, обеспечивая более быстрый доступ за счет повышенного использования ОЗУ",
  "llm.load.llama.useFp16ForKVCache/title": "Использовать FP16 для KV-кэша",
  "llm.load.llama.useFp16ForKVCache/info": "Уменьшает использование памяти за счет хранения кэша в половинной точности (FP16)",
  "llm.load.llama.tryMmap/title": "Попробовать mmap()",
  "llm.load.llama.tryMmap/info": "Загружает файлы модели напрямую с диска в память"
}

