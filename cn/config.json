{
  "noInstanceSelected": "未选择模型实例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "显示高级设置",
  "showAll": "全部",
  "basicSettings": "基本",
  "configSubtitle": "加载或保存预设并尝试模型参数覆盖",
  "inferenceParameters/title": "预测参数",
  "inferenceParameters/info": "试验影响预测的参数。",
  "generalParameters/title": "通用",
  "samplingParameters/title": "采样",
  "basicTab": "基本",
  "advancedTab": "高级",
  "advancedTab/title": "🧪 高级配置",
  "advancedTab/expandAll": "展开全部",
  "advancedTab/overridesTitle": "配置覆盖",
  "advancedTab/noConfigsText": "您没有未保存的更改 - 编辑上面的值以在此处查看覆盖。",
  "loadInstanceFirst": "加载模型以查看可配置参数",
  "noListedConfigs": "没有可配置参数",
  "generationParameters/info": "试验影响文本生成的基本参数。",
  "loadParameters/title": "加载参数",
  "loadParameters/description": "控制模型初始化和加载到内存中的方式的设置。",
  "loadParameters/reload": "重新加载以应用更改",
  "discardChanges": "放弃更改",
  "llm.prediction.systemPrompt/title": "系统提示",
  "llm.prediction.systemPrompt/description": "使用此字段向模型提供背景说明，例如一组规则、约束或一般要求。此字段也常称为“系统提示”。",
  "llm.prediction.systemPrompt/subTitle": "AI 指导原则",
  "llm.prediction.temperature/title": "温度",
  "llm.prediction.temperature/subTitle": "要引入多少随机性。0 每次都会产生相同的结果，而较高的值会增加创造力和差异",
  "llm.prediction.temperature/info": "来自 llama.cpp 帮助文档：“默认值为 <{{dynamicValue}}>，它在随机性和确定性之间提供了平衡。在极端情况下，温度为 0 将始终选择最有可能的下一个标记，从而导致每次运行的输出相同”",
  "llm.prediction.llama.sampling/title": "采样",
  "llm.prediction.llama.topKSampling/title": "Top K 采样",
  "llm.prediction.llama.topKSampling/subTitle": "将下一个标记限制为概率最高的 k 个标记之一。作用类似于温度",
  "llm.prediction.llama.topKSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-k 采样是一种文本生成方法，它仅从模型预测的概率最高的 k 个标记中选择下一个标记。\n\n它有助于降低生成低概率或无意义标记的风险，但也可能限制输出的多样性。\n\n较高的 top-k 值（例如，100）将考虑更多标记并导致更多样化的文本，而较低的值（例如，10）将侧重于最可能的标记并生成更保守的文本。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 线程数",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期间要使用的 CPU 线程数",
  "llm.prediction.llama.cpuThreads/info": "计算期间要使用的线程数。增加线程数并不总是与更好的性能相关。默认值为 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制回复长度",
  "llm.prediction.maxPredictedTokens/subTitle": "可选地限制 AI 回复的长度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天机器人回复的最大长度。打开以设置回复的最大长度限制，或关闭以让聊天机器人决定何时停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大回复长度（标记）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "大约 {{maxWords}} 个字",
  "llm.prediction.llama.repeatPenalty/title": "重复惩罚",
  "llm.prediction.llama.repeatPenalty/subTitle": "重复相同标记的抑制程度",
  "llm.prediction.llama.repeatPenalty/info": "来自 llama.cpp 帮助文档：“帮助防止模型生成重复或单调的文本。\n\n较高的值（例如，1.5）将更强烈地惩罚重复，而较低的值（例如，0.9）将更宽松。” • 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "最小 P 采样",
  "llm.prediction.llama.minPSampling/subTitle": "要选择用于输出的标记的最小基准概率",
  "llm.prediction.llama.minPSampling/info": "来自 llama.cpp 帮助文档：\n\n相对于最可能标记的概率，要考虑的标记的最小概率。必须在 [0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Top P 采样",
  "llm.prediction.llama.topPSampling/subTitle": "可能的下一个标记的最小累积概率。作用类似于温度",
  "llm.prediction.llama.topPSampling/info": "来自 llama.cpp 帮助文档：\n\nTop-p 采样，也称为核采样，是另一种文本生成方法，它从累积概率至少为 p 的标记子集中选择下一个标记。\n\n此方法通过同时考虑标记的概率和要从中采样的标记数量来在多样性和质量之间取得平衡。\n\n较高的 top-p 值（例如，0.95）将导致更多样化的文本，而较低的值（例如，0.5）将生成更集中和保守的文本。必须在 (0, 1] 范围内。\n\n• 默认值为 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字符串",
  "llm.prediction.stopStrings/subTitle": "应阻止模型生成更多标记的字符串",
  "llm.prediction.stopStrings/info": "遇到时将阻止模型生成更多标记的特定字符串",
  "llm.prediction.stopStrings/placeholder": "输入字符串并按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "对话溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "当对话增长到模型无法处理时，模型应该如何表现",
  "llm.prediction.contextOverflowPolicy/info": "决定当对话超过模型工作内存（“上下文”）的大小时该怎么做",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "在限制处停止",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "一旦模型的内存已满，就停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "截断中间",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "从对话中间删除消息，为较新的消息腾出空间。模型仍将记住对话的开头",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滚动窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型将始终获取最近的几条消息，但可能会忘记对话的开头",
  "llm.prediction.llama.frequencyPenalty/title": "频率惩罚",
  "llm.prediction.llama.presencePenalty/title": "存在惩罚",
  "llm.prediction.llama.tailFreeSampling/title": "无尾采样",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型采样",
  "llm.prediction.mlx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.mlx.repeatPenalty/subTitle": "重复相同标记的抑制程度",
  "llm.prediction.mlx.repeatPenalty/info": "较高的值会阻止模型重复自身",
  "llm.prediction.onnx.topKSampling/title": "Top K 采样",
  "llm.prediction.onnx.topKSampling/subTitle": "将下一个标记限制为概率最高的 k 个标记之一。作用类似于温度",
  "llm.prediction.onnx.topKSampling/info": "来自 ONNX 文档：\n\n要保留用于 top-k 过滤的最高概率词汇标记的数量\n\n• 默认情况下，此过滤器处于关闭状态",
  "llm.prediction.onnx.repeatPenalty/title": "重复惩罚",
  "llm.prediction.onnx.repeatPenalty/subTitle": "重复相同标记的抑制程度",
  "llm.prediction.onnx.repeatPenalty/info": "较高的值会阻止模型重复自身",
  "llm.prediction.onnx.topPSampling/title": "Top P 采样",
  "llm.prediction.onnx.topPSampling/subTitle": "可能的下一个标记的最小累积概率。作用类似于温度",
  "llm.prediction.onnx.topPSampling/info": "来自 ONNX 文档：\n\n仅保留概率总计为 TopP 或更高的最可能标记以用于生成\n\n• 默认情况下，此过滤器处于关闭状态",
  "llm.prediction.seed/title": "种子",
  "llm.prediction.structured/title": "结构化输出",
  "llm.prediction.structured/info": "结构化输出",
  "llm.prediction.promptTemplate/title": "提示模板",
  "llm.prediction.promptTemplate/subTitle": "聊天中的消息发送到模型的格式。更改此设置可能会导致意外行为 - 确保您知道自己在做什么！",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "解析 Jinja 模板失败：{{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手动",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "系统之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "输入系统前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "系统之后",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "输入系统后缀...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "用户之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "输入用户前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "用户之后",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "输入用户后缀...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "助手之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "输入助手前缀...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "助手之后",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "输入助手后缀...",
  "llm.prediction.promptTemplate.stopStrings/label": "其他停止字符串",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "除了用户指定的停止字符串之外，还将使用的特定于模板的停止字符串。",
  "llm.load.contextLength/title": "上下文长度",
  "llm.load.contextLength/subTitle": "模型可以在一个提示中处理的最大标记数。有关管理此问题的更多方法，请参阅“推理参数”下的“对话溢出”选项",
  "llm.load.contextLength/info": "指定模型一次可以考虑的最大标记数，这会影响它在处理过程中保留多少上下文",
  "llm.load.seed/title": "种子",
  "llm.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 是随机的",
  "llm.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",
  "llm.load.llama.evalBatchSize/title": "评估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次处理的输入标记数。增加此值会以内存使用量为代价提高性能",
  "llm.load.llama.evalBatchSize/info": "设置在评估期间在一个批次中一起处理的示例数，从而影响速度和内存使用量",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 频率基数",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入 (RoPE) 的自定义基频。增加此值可以在高上下文长度下实现更好的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 频率比例",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放，以使用 RoPE 扩展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "要在 GPU 上计算以进行 GPU 加速的离散模型层数",
  "llm.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "llm.load.llama.flashAttention/title": "Flash 注意力",
  "llm.load.llama.flashAttention/subTitle": "减少某些模型的内存使用量和生成时间",
  "llm.load.llama.flashAttention/info": "加速注意力机制以实现更快、更高效的处理",
  "llm.load.llama.keepModelInMemory/title": "将模型保留在内存中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使卸载到 GPU，也要为模型保留系统内存。提高性能，但需要更多系统 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保以更高的 RAM 使用量为代价更快地访问",
  "llm.load.llama.useFp16ForKVCache/title": "对 KV 缓存使用 FP16",
  "llm.load.llama.useFp16ForKVCache/info": "通过以半精度 (FP16) 存储缓存来减少内存使用量",
  "llm.load.llama.tryMmap/title": "尝试 mmap()",
  "llm.load.llama.tryMmap/subTitle": "缩短模型的加载时间。当模型大于可用系统 RAM 时，禁用此选项可以提高性能",
  "llm.load.llama.tryMmap/info": "将模型文件直接从磁盘加载到内存",
  "embedding.load.contextLength/title": "上下文长度",
  "embedding.load.contextLength/subTitle": "模型可以在一个提示中处理的最大标记数。有关管理此问题的更多方法，请参阅“推理参数”下的“对话溢出”选项",
  "embedding.load.contextLength/info": "指定模型一次可以考虑的最大标记数，这会影响它在处理过程中保留多少上下文",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 频率基数",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋转位置嵌入 (RoPE) 的自定义基频。增加此值可以在高上下文长度下实现更好的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高级] 调整旋转位置编码的基频，影响位置信息的嵌入方式",
  "embedding.load.llama.evalBatchSize/title": "评估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次处理的输入标记数。增加此值会以内存使用量为代价提高性能",
  "embedding.load.llama.evalBatchSize/info": "设置在评估期间在一个批次中一起处理的标记数",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 频率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文长度按此因子缩放，以使用 RoPE 扩展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高级] 修改旋转位置编码的频率缩放，以控制位置编码粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸载",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "要在 GPU 上计算以进行 GPU 加速的离散模型层数",
  "embedding.load.llama.acceleration.offloadRatio/info": "设置要卸载到 GPU 的层数。",
  "embedding.load.llama.keepModelInMemory/title": "将模型保留在内存中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使卸载到 GPU，也要为模型保留系统内存。提高性能，但需要更多系统 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交换到磁盘，确保以更高的 RAM 使用量为代价更快地访问",
  "embedding.load.llama.tryMmap/title": "尝试 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "缩短模型的加载时间。当模型大于可用系统 RAM 时，禁用此选项可以提高性能",
  "embedding.load.llama.tryMmap/info": "将模型文件直接从磁盘加载到内存",
  "embedding.load.seed/title": "种子",
  "embedding.load.seed/subTitle": "用于文本生成的随机数生成器的种子。-1 是随机种子",
  "embedding.load.seed/info": "随机种子：设置随机数生成的种子以确保结果可重现",
  "customInputs": {
    "contextLength": {
      "maxValueTooltip": "这是模型训练处理的最大标记数。单击以将上下文设置为该值",
      "maxValueTextStart": "模型最多支持",
      "maxValueTextEnd": "个标记"
    }
  },
  "flashAttentionWarning": "Flash 注意力是一项实验性功能，可能会导致某些模型出现问题。如果遇到问题，请尝试禁用它。"
}
