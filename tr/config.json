{
  "noInstanceSelected": "Model Ã¶rneÄŸi seÃ§ilmedi",
  "resetToDefault": "SÄ±fÄ±rla",
  "showAdvancedSettings": "GeliÅŸmiÅŸ ayarlarÄ± gÃ¶ster",
  "showAll": "Hepsini gÃ¶ster",
  "basicSettings": "Temel",
  "configSubtitle": "Ã–n ayarlarÄ± yÃ¼kleyin veya kaydedin ve model parametre geÃ§ersiz kÄ±lmalarÄ±nÄ± deneyin",
  "inferenceParameters/title": "Tahmin Parametreleri",
  "inferenceParameters/info": "Tahmini etkileyen parametrelerle deneyler yapÄ±n.",
  "generalParameters/title": "Genel",
  "samplingParameters/title": "Ã–rnekleme",
  "basicTab": "Temel",
  "advancedTab": "GeliÅŸmiÅŸ",
  "advancedTab/title": "ğŸ§ª GeliÅŸmiÅŸ YapÄ±landÄ±rma",
  "advancedTab/expandAll": "TÃ¼mÃ¼nÃ¼ geniÅŸlet",
  "advancedTab/overridesTitle": "YapÄ±landÄ±rma GeÃ§ersiz KÄ±lmalarÄ±",
  "advancedTab/noConfigsText": "KaydedilmemiÅŸ deÄŸiÅŸikliÄŸiniz yok - buradaki geÃ§ersiz kÄ±lmalarÄ± gÃ¶rmek iÃ§in yukarÄ±daki deÄŸerleri dÃ¼zenleyin.",
  "loadInstanceFirst": "YapÄ±landÄ±rÄ±labilir parametreleri gÃ¶rÃ¼ntÃ¼lemek iÃ§in bir model yÃ¼kleyin",
  "noListedConfigs": "YapÄ±landÄ±rÄ±labilir parametre yok",
  "generationParameters/info": "Metin oluÅŸturmayÄ± etkileyen temel parametreleri deneyin.",
  "loadParameters/title": "Parametreleri YÃ¼kle",
  "loadParameters/description": "Modelin baÅŸlatÄ±lma ve belleÄŸe yÃ¼klenme ÅŸeklini kontrol etmek iÃ§in ayarlar.",
  "loadParameters/reload": "DeÄŸiÅŸiklikleri uygulamak iÃ§in yeniden yÃ¼kle",
  "discardChanges": "DeÄŸiÅŸiklikleri iptal et",
  "llm.prediction.systemPrompt/title": "Sistem Komutu",
  "llm.prediction.systemPrompt/description": "Bu alanÄ± modele bir dizi kural, kÄ±sÄ±tlama veya genel gereklilik gibi arka plan talimatlarÄ± saÄŸlamak iÃ§in kullanÄ±n. Bu alan genellikle â€œsistem komutuâ€ olarak da adlandÄ±rÄ±lÄ±r.",
  "llm.prediction.systemPrompt/subTitle": "Yapay Zeka YÃ¶nergeleri",
  "llm.prediction.temperature/title": "Temperature",
  "llm.prediction.temperature/info": "VarsayÄ±lan deÄŸer <{{dynamicValue}}> olup rastgelelik ile deterministiklik arasÄ±nda bir denge saÄŸlar. AÅŸÄ±rÄ± durumda, 0 tempature her zaman en olasÄ± sonraki belirteci seÃ§ecek ve her Ã§alÄ±ÅŸtÄ±rmada aynÄ± Ã§Ä±ktÄ±lara yol aÃ§acaktÄ±r",
  "llm.prediction.llama.topKSampling/title": "Top K Ã–rnekleme",
  "llm.prediction.llama.topKSampling/info": "Top-k Ã¶rnekleme, model tarafÄ±ndan tahmin edilen en olasÄ± k belirtecinden bir sonraki belirteci seÃ§en bir metin oluÅŸturma yÃ¶ntemidir.\n\nBu yÃ¶ntem, dÃ¼ÅŸÃ¼k olasÄ±lÄ±klÄ± veya anlamsÄ±z belirteÃ§ler Ã¼retme riskini azaltÄ±r, ancak Ã§Ä±ktÄ±nÄ±n Ã§eÅŸitliliÄŸini de sÄ±nÄ±rlayabilir.\n\nTop-k deÄŸeri ne kadar yÃ¼ksek olursa (Ã¶rneÄŸin, 100), o kadar fazla belirteÃ§ dikkate alÄ±nÄ±r ve daha Ã§eÅŸitli bir metin Ã¼retilir. Daha dÃ¼ÅŸÃ¼k bir deÄŸer (Ã¶rneÄŸin, 10), en olasÄ± belirteÃ§lere odaklanÄ±r ve daha temkinli bir metin oluÅŸturur.\n\nâ€¢ VarsayÄ±lan deÄŸer <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU Ä°ÅŸ ParÃ§acÄ±klarÄ±",
  "llm.prediction.llama.cpuThreads/info": "Ä°ÅŸlem sÄ±rasÄ±nda kullanÄ±lacak iÅŸ parÃ§acÄ±ÄŸÄ± sayÄ±sÄ±. Ä°ÅŸ parÃ§acÄ±ÄŸÄ± sayÄ±sÄ±nÄ± artÄ±rmak her zaman daha iyi performansla iliÅŸkili olmayabilir. VarsayÄ±lan deÄŸer <{{dynamicValue}}>'dir.",
  "llm.prediction.maxPredictedTokens/title": "YanÄ±t UzunluÄŸunu SÄ±nÄ±rla",
  "llm.prediction.maxPredictedTokens/subTitle": "Ä°steÄŸe baÄŸlÄ± olarak yapay zekanÄ±n yanÄ±tÄ±nÄ±n uzunluÄŸunu sÄ±nÄ±rlayÄ±n",
  "llm.prediction.maxPredictedTokens/info": "Chatbot'un yanÄ±tÄ±nÄ±n maksimum uzunluÄŸunu kontrol edin. YanÄ±tÄ±n maksimum uzunluÄŸunu sÄ±nÄ±rlamak iÃ§in aÃ§Ä±n veya chatbot'un ne zaman duracaÄŸÄ±na karar vermesi iÃ§in kapalÄ± bÄ±rakÄ±n.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maksimum yanÄ±t uzunluÄŸu (tokenler)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "YaklaÅŸÄ±k {{maxWords}} kelime",
  "llm.prediction.llama.repeatPenalty/title": "Tekrar CezasÄ±",
  "llm.prediction.llama.repeatPenalty/info": "Modelin tekrarlayan veya monoton metinler Ã¼retmesini engellemeye yardÄ±mcÄ± olur.\n\nDaha yÃ¼ksek bir deÄŸer (Ã¶rneÄŸin, 1.5) tekrarlarÄ± daha gÃ¼Ã§lÃ¼ bir ÅŸekilde cezalandÄ±rÄ±r, daha dÃ¼ÅŸÃ¼k bir deÄŸer (Ã¶rneÄŸin, 0.9) ise daha hoÅŸgÃ¶rÃ¼lÃ¼ olur.\" â€¢ VarsayÄ±lan deÄŸer <{{dynamicValue}}>'dir",
  "llm.prediction.llama.minPSampling/title": "Min P Ã–rnekleme",
  "llm.prediction.llama.minPSampling/info": "Bir tokenin dikkate alÄ±nabilmesi iÃ§in gereken minimum olasÄ±lÄ±k, en olasÄ± token olasÄ±lÄ±ÄŸÄ±na gÃ¶re belirlenir. [0, 1] aralÄ±ÄŸÄ±nda olmalÄ±dÄ±r.\n\nâ€¢ VarsayÄ±lan deÄŸer <{{dynamicValue}}>'dir",
  "llm.prediction.llama.topPSampling/title": "Top P Ã–rnekleme",
  "llm.prediction.llama.topPSampling/info": "Top-p Ã¶rnekleme, aynÄ± zamanda Ã§ekirdek Ã¶rnekleme olarak da bilinir, bir metin oluÅŸturma yÃ¶ntemidir ve bir sonraki tokeni, birlikte en az p kÃ¼mÃ¼latif olasÄ±lÄ±ÄŸa sahip tokenler alt kÃ¼mesinden seÃ§er.\n\nBu yÃ¶ntem, hem tokenlerin olasÄ±lÄ±klarÄ±nÄ± hem de Ã¶rnekleme yapÄ±lacak token sayÄ±sÄ±nÄ± dikkate alarak Ã§eÅŸitlilik ve kalite arasÄ±nda bir denge saÄŸlar.\n\nDaha yÃ¼ksek bir top-p deÄŸeri (Ã¶rneÄŸin, 0.95) daha Ã§eÅŸitli bir metin Ã¼retirken, daha dÃ¼ÅŸÃ¼k bir deÄŸer (Ã¶rneÄŸin, 0.5) daha odaklanmÄ±ÅŸ ve temkinli bir metin oluÅŸturur. (0, 1] aralÄ±ÄŸÄ±nda olmalÄ±dÄ±r.\n\nâ€¢ VarsayÄ±lan deÄŸer <{{dynamicValue}}>'dir",
  "llm.prediction.stopStrings/title": "SonlandÄ±rma Metni",
  "llm.prediction.stopStrings/subTitle": "Modelin daha fazla jeton Ã¼retmesini durdurmasÄ± gereken metinler",
  "llm.prediction.stopStrings/info": "KarÅŸÄ±laÅŸÄ±ldÄ±ÄŸÄ±nda modelin daha fazla jeton Ã¼retmesini durduracak Ã¶zel metinler",
  "llm.prediction.stopStrings/placeholder": "Bir metin girin ve â tuÅŸuna basÄ±n",
  "llm.prediction.contextOverflowPolicy/title": "KonuÅŸma TaÅŸmasÄ±",
  "llm.prediction.contextOverflowPolicy/info": "KonuÅŸma modelin Ã§alÄ±ÅŸma belleÄŸi ('context') boyutunu aÅŸtÄ±ÄŸÄ±nda ne yapÄ±lacaÄŸÄ±na karar verin",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Limitte Dur",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Modelin belleÄŸi dolduÄŸunda Ã¼retmeyi durdurun",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "OrtayÄ± KÄ±salt",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Yeni mesajlar iÃ§in yer aÃ§mak amacÄ±yla konuÅŸmanÄ±n ortasÄ±ndan mesajlar kaldÄ±rÄ±lÄ±r. Model, konuÅŸmanÄ±n baÅŸÄ±nÄ± hÃ¢lÃ¢ hatÄ±rlayacaktÄ±r",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "DÃ¶nÃ¼ÅŸÃ¼mlÃ¼ Pencere",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Model her zaman en son birkaÃ§ mesajÄ± alÄ±r, ancak konuÅŸmanÄ±n baÅŸÄ±nÄ± unutabilir",
  "llm.prediction.llama.frequencyPenalty/title": "Frekans CezasÄ±",
  "llm.prediction.llama.presencePenalty/title": "VarlÄ±k CezasÄ±",
  "llm.prediction.llama.tailFreeSampling/title": "Kuyruksuz Ã–rnekleme",
  "llm.prediction.llama.locallyTypicalSampling/title": "Yerel Tipik Ã–rnekleme",
  "llm.prediction.mlx.repeatPenalty/title": "Tekrar CezasÄ±",
  "llm.prediction.mlx.repeatPenalty/info": "Daha yÃ¼ksek bir deÄŸer, modelin kendini tekrar etmesini engeller",
  "llm.prediction.onnx.topKSampling/title": "Top K Ã–rnekleme",
  "llm.prediction.onnx.topKSampling/info": "Top-k filtrelemesi iÃ§in saklanacak en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip kelime daÄŸarcÄ±ÄŸÄ± jetonlarÄ±nÄ±n sayÄ±sÄ±\n\nâ€¢ Bu filtre varsayÄ±lan olarak kapalÄ±dÄ±r",
  "llm.prediction.onnx.repeatPenalty/title": "Tekrar CezasÄ±",
  "llm.prediction.onnx.repeatPenalty/info": "Daha yÃ¼ksek bir deÄŸer, modelin kendini tekrar etmesini engeller",
  "llm.prediction.onnx.topPSampling/title": "Top P Ã–rnekleme",
  "llm.prediction.onnx.topPSampling/info": "YalnÄ±zca TopP veya daha yÃ¼ksek olasÄ±lÄ±klara sahip en olasÄ± jetonlar, Ã¼retim iÃ§in saklanÄ±r\n\nâ€¢ Bu filtre varsayÄ±lan olarak kapalÄ±dÄ±r",
  "llm.prediction.seed/title": "Tohum(Seed)",
  "llm.prediction.structured/title": "YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã‡Ä±ktÄ±",
  "llm.prediction.structured/info": "YapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã‡Ä±ktÄ±",
  "llm.prediction.promptTemplate/title": "Komut Åablonu",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Jinja ÅŸablonu ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manuel",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Sistemden Ã–nce",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "Sistem Ã¶nekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Sistem SonrasÄ±",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "Sistem sonekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "KullanÄ±cÄ±dan Ã–nce",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "KullanÄ±cÄ± Ã¶nekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "KullanÄ±cÄ± SonrasÄ±",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "KullanÄ±cÄ± sonekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Asistandan Ã–nce",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "Asistan Ã¶nekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Asistan SonrasÄ±",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "Asistan sonekini girin...",
  "llm.prediction.promptTemplate.stopStrings/label": "Ek Durdurma Metni",
  "llm.prediction.promptTemplate.stopStrings/hint": "Åablona Ã¶zgÃ¼ durma metni, kullanÄ±cÄ± tarafÄ±ndan belirtilen durma metinlerine ek olarak kullanÄ±lacaktÄ±r.",

  "llm.load.contextLength/title": "Context Length",
  "llm.load.contextLength/info": "Modelin bir kerede dikkate alabileceÄŸi maksimum token sayÄ±sÄ±nÄ± belirtir ve iÅŸleme sÄ±rasÄ±nda ne kadar context'i koruduÄŸunu etkiler",
  "llm.load.seed/title": "Tohum(Seed)",
  "llm.load.seed/info": "Rastgele Tohum: Tekrarlanabilir sonuÃ§lar saÄŸlamak amacÄ±yla rastgele sayÄ± Ã¼retimi iÃ§in tohumu ayarlar",

  "llm.load.llama.evalBatchSize/title": "DeÄŸerlendirme Paketi Boyutu",
  "llm.load.llama.evalBatchSize/info": "DeÄŸerlendirme sÄ±rasÄ±nda tek bir grupta birlikte iÅŸlenen Ã¶rnek sayÄ±sÄ±nÄ± ayarlar, hÄ±zÄ± ve bellek kullanÄ±mÄ±nÄ± etkiler",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frekans TabanÄ±",
  "llm.load.llama.ropeFrequencyBase/info": "[GeliÅŸmiÅŸ] Rotary Pozisyonel Kodlama iÃ§in temel frekansÄ± ayarlar, bu da pozisyonel bilgilerin nasÄ±l gÃ¶mÃ¼ldÃ¼ÄŸÃ¼nÃ¼ etkiler",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frekans Ã–lÃ§eÄŸi",
  "llm.load.llama.ropeFrequencyScale/info": "[GeliÅŸmiÅŸ] Rotary Pozisyonel Kodlama iÃ§in frekans Ã¶lÃ§eklemesini deÄŸiÅŸtirir, bu da pozisyonel kodlama ayrÄ±ntÄ± dÃ¼zeyini kontrol eder",
  "llm.load.llama.gpuOffload/title": "GPU BoÅŸaltma",
  "llm.load.llama.gpuOffload/info": "GPU'ya yÃ¼klenecek hesaplama oranÄ±nÄ± ayarlayÄ±n. GPU boÅŸaltmayÄ± devre dÄ±ÅŸÄ± bÄ±rakmak iÃ§in kapalÄ± olarak veya modelin karar vermesine izin vermek iÃ§in otomatik olarak ayarlayÄ±n.",
  "llm.load.llama.flashAttention/title": "Flash Dikkat",
  "llm.load.llama.flashAttention/info": "Dikkat mekanizmalarÄ±nÄ± daha hÄ±zlÄ± ve verimli iÅŸlem iÃ§in hÄ±zlandÄ±rÄ±r",
  "llm.load.llama.keepModelInMemory/title": "Modeli HafÄ±zada Tutun",
  "llm.load.llama.keepModelInMemory/info": "Modelin diske takas edilmesini Ã¶nleyerek daha yÃ¼ksek RAM kullanÄ±mÄ± pahasÄ±na daha hÄ±zlÄ± eriÅŸim saÄŸlar",
  "llm.load.llama.useFp16ForKVCache/title": "KV Ã–nbellek Ä°Ã§in FP16 KullanÄ±n",
  "llm.load.llama.useFp16ForKVCache/info": "Ã–nbelleÄŸi yarÄ±m hassasiyette (FP16) depolayarak bellek kullanÄ±mÄ±nÄ± azaltÄ±r",
  "llm.load.llama.tryMmap/title": "mmap() iÅŸlevini deneyin",
  "llm.load.llama.tryMmap/info": "Model dosyalarÄ±nÄ± doÄŸrudan diskten belleÄŸe yÃ¼kleme",

  "embedding.load.contextLength/title": "Context UzunluÄŸu",
  "embedding.load.contextLength/info": "Modelin bir kerede dikkate alabileceÄŸi maksimum token sayÄ±sÄ±nÄ± belirtir ve iÅŸleme sÄ±rasÄ±nda ne kadar context'i koruduÄŸunu etkiler",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frekans TabanÄ±",
  "embedding.load.llama.ropeFrequencyBase/info": "[GeliÅŸmiÅŸ] DÃ¶ner Konumsal Kodlama iÃ§in temel frekansÄ± ayarlayarak konum bilgisinin nasÄ±l gÃ¶mÃ¼leceÄŸini etkiler",
  "embedding.load.llama.evalBatchSize/title": "DeÄŸerlendirme Paket BÃ¼yÃ¼klÃ¼ÄŸÃ¼",
  "embedding.load.llama.evalBatchSize/info": "DeÄŸerlendirme sÄ±rasÄ±nda bir grupta birlikte iÅŸlenen token sayÄ±sÄ±nÄ± ayarlar",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frekans Ã–lÃ§eÄŸi",
  "embedding.load.llama.ropeFrequencyScale/info": "[GeliÅŸmiÅŸ] Rotary Pozisyonel Kodlama iÃ§in frekans Ã¶lÃ§eklemesini deÄŸiÅŸtirir, bu da pozisyonel kodlama ayrÄ±ntÄ± dÃ¼zeyini kontrol eder",
  "embedding.load.llama.gpuOffload/title": "GPU BoÅŸaltma",
  "embedding.load.llama.gpuOffload/info": "GPU'ya yÃ¼klenecek hesaplama oranÄ±nÄ± ayarlayÄ±n. GPU boÅŸaltmayÄ± devre dÄ±ÅŸÄ± bÄ±rakmak iÃ§in kapalÄ± olarak veya modelin karar vermesine izin vermek iÃ§in otomatik olarak ayarlayÄ±n.",
  "embedding.load.llama.keepModelInMemory/title": "Modeli HafÄ±zada Tutun",
  "embedding.load.llama.keepModelInMemory/info": "Modelin diske takas edilmesini Ã¶nleyerek daha yÃ¼ksek RAM kullanÄ±mÄ± pahasÄ±na daha hÄ±zlÄ± eriÅŸim saÄŸlar",
  "embedding.load.llama.tryMmap/title": "mmap() iÅŸlevini deneyin",
  "embedding.load.llama.tryMmap/info": "Model dosyalarÄ±nÄ± doÄŸrudan diskten belleÄŸe yÃ¼kleme",
  "embedding.load.seed/title": "Tohum(Seed)",
  "embedding.load.seed/info": "Rastgele Tohum: Tekrarlanabilir sonuÃ§lar saÄŸlamak amacÄ±yla rastgele sayÄ± Ã¼retimi iÃ§in tohumu ayarlar"
}
