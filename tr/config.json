{
  "noInstanceSelected": "Model örneği seçilmedi",
  "resetToDefault": "Varsayılana sıfırla",
  "showAdvancedSettings": "Gelişmiş ayarları göster",
  "showAll": "Hepsini göster",
  "basicSettings": "Temel",
  "configSubtitle": "Ön ayarları yükleyin veya kaydedin ve model parametre geçersiz kılmaları ile deney yapın",
  "inferenceParameters/title": "Tahmin Parametreleri",
  "inferenceParameters/info": "Tahmini etkileyen parametrelerle deney yapın.",
  "generalParameters/title": "Genel",
  "samplingParameters/title": "Örnekleme",
  "basicTab": "Temel",
  "advancedTab": "Gelişmiş",
  "advancedTab/title": "🧪 Gelişmiş Yapılandırma",
  "advancedTab/expandAll": "Hepsini genişlet",
  "advancedTab/overridesTitle": "Yapılandırma Geçersiz Kılmaları",
  "advancedTab/noConfigsText": "Kaydedilmemiş değişiklikleriniz yok - geçersiz kılmaları burada görmek için yukarıdaki değerleri düzenleyin.",
  "loadInstanceFirst": "Yapılandırılabilir parametreleri görmek için bir model yükleyin",
  "noListedConfigs": "Yapılandırılabilir parametre yok",
  "generationParameters/info": "Metin üretimini etkileyen temel parametrelerle deney yapın.",
  "loadParameters/title": "Yükleme Parametreleri",
  "loadParameters/description": "Bu parametreleri değiştirmek modelin yeniden yüklenmesini gerektirir",
  "loadParameters/reload": "Yükleme parametre değişikliklerini uygulamak için yeniden yükle",
  "discardChanges": "Değişiklikleri at",
  "llm.prediction.systemPrompt/title": "Sistem İstemi",
  "llm.prediction.systemPrompt/description": "Model için bir dizi kural, kısıtlama veya genel gereksinim gibi arka plan talimatları sağlamak için bu alanı kullanın. Bu alan genellikle \"sistem istemi\" olarak da adlandırılır.",
  "llm.prediction.systemPrompt/subTitle": "Yapay Zeka için Yönergeler",
  "llm.prediction.temperature/title": "Sıcaklık",
  "llm.prediction.temperature/info": "llama.cpp yardım belgelerinden: \"Varsayılan değer <{{dynamicValue}}> olup, rastgelelik ve determinizm arasında bir denge sağlar. Aşırı durumda, 0 sıcaklığı her zaman en olası sonraki belirteci seçer ve her çalışmada aynı çıktılara yol açar\"",
  "llm.prediction.topKSampling/title": "Top K Örnekleme",
  "llm.prediction.topKSampling/info": "llama.cpp yardım belgelerinden:\n\nTop-k örnekleme, model tarafından tahmin edilen en olası k belirtecinden yalnızca bir sonraki belirteci seçen bir metin üretim yöntemidir.\n\nDüşük olasılıklı veya anlamsız belirteçler üretme riskini azaltır, ancak aynı zamanda çıktının çeşitliliğini de sınırlayabilir.\n\nDaha yüksek bir top-k değeri (örneğin, 100) daha fazla belirteci dikkate alır ve daha çeşitli metinlere yol açar, daha düşük bir değer (örneğin, 10) en olası belirteçlere odaklanır ve daha muhafazakar metinler üretir.\n\n• Varsayılan değer <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU İş Parçacıkları",
  "llm.prediction.llama.cpuThreads/info": "Hesaplama sırasında kullanılacak iş parçacığı sayısı. İş parçacığı sayısının artırılması her zaman daha iyi performansla ilişkilendirilmez. Varsayılan <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Yanıt Uzunluğunu Sınırla",
  "llm.prediction.maxPredictedTokens/subTitle": "Yapay Zeka'nın yanıtının uzunluğunu isteğe bağlı olarak sınırla",
  "llm.prediction.maxPredictedTokens/info": "Sohbet botunun yanıtının maksimum uzunluğunu kontrol edin. Bir yanıtın maksimum uzunluğuna bir sınır koymak için açın veya sohbet botunun ne zaman duracağına karar vermesine izin vermek için kapatın.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maksimum yanıt uzunluğu (belirteçler)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Yaklaşık {{maxWords}} kelime",
  "llm.prediction.repeatPenalty/title": "Tekrar Cezası",
  "llm.prediction.repeatPenalty/info": "llama.cpp yardım belgelerinden: \"Modelin tekrarlayan veya monoton metinler üretmesini önlemeye yardımcı olur.\n\nDaha yüksek bir değer (örneğin, 1.5) tekrarları daha güçlü bir şekilde cezalandırır, daha düşük bir değer (örneğin, 0.9) daha hoşgörülüdür.\" • Varsayılan değer <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P Örnekleme",
  "llm.prediction.minPSampling/info": "llama.cpp yardım belgelerinden:\n\nEn olası belirtecin olasılığına göre bir belirtecin dikkate alınması için minimum olasılık. [0, 1] aralığında olmalıdır.\n\n• Varsayılan değer <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P Örnekleme",
  "llm.prediction.topPSampling/info": "llama.cpp yardım belgelerinden:\n\nTop-p örnekleme, aynı zamanda çekirdek örnekleme olarak da bilinir, bir sonraki belirteci, birlikte en az p olasılığına sahip bir belirteç alt kümesinden seçen bir metin üretim yöntemidir.\n\nBu yöntem, belirteçlerin olasılıklarını ve örneklenecek belirteç sayısını dikkate alarak çeşitlilik ve kalite arasında bir denge sağlar.\n\nDaha yüksek bir top-p değeri (örneğin, 0.95) daha çeşitli metinlere yol açar, daha düşük bir değer (örneğin, 0.5) daha odaklanmış ve muhafazakar metinler üretir. (0, 1] aralığında olmalıdır.\n\n• Varsayılan değer <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Durdurma Dizeleri",
  "llm.prediction.stopStrings/subTitle": "Modelin daha fazla belirteç üretmesini durduracak dizeler",
  "llm.prediction.stopStrings/info": "Karşılaşıldığında modelin daha fazla belirteç üretmesini durduracak belirli dizeler",
  "llm.prediction.stopStrings/placeholder": "Bir dize girin ve ⏎ tuşuna basın",
  "llm.prediction.contextOverflowPolicy/title": "Konuşma Taşması",
  "llm.prediction.contextOverflowPolicy/info": "Konuşma modelin çalışma belleğinin ('bağlam') boyutunu aştığında ne yapılacağını belirleyin",
  "customInputs.contextOverflowPolicy.stopAtLimit": "Sınırda Durdur",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "Modelin belleği dolduğunda üretimi durdur",
  "customInputs.contextOverflowPolicy.truncateMiddle": "Ortayı Kısalt",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "Yeni mesajlar için yer açmak için konuşmanın ortasındaki mesajları kaldırır. Model hala konuşmanın başlangıcını hatırlayacaktır",
  "customInputs.contextOverflowPolicy.rollingWindow": "Kaydırma Penceresi",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "Model her zaman en son birkaç mesajı alacak, ancak konuşmanın başlangıcını unutabilir",
  "llm.prediction.llama.frequencyPenalty/title": "Frekans Cezası",
  "llm.prediction.llama.presencePenalty/title": "Varlık Cezası",
  "llm.prediction.llama.tailFreeSampling/title": "Kuyruksuz Örnekleme",
  "llm.prediction.llama.locallyTypicalSampling/title": "Yerel Tipik Örnekleme",
  "llm.prediction.onnx.topKSampling/title": "Top K Örnekleme",
  "llm.prediction.onnx.topKSampling/info": "ONNX belgelerinden:\n\nTop-k filtreleme için tutulacak en yüksek olasılıklı kelime dağarcığı belirteçlerinin sayısı\n\n• Bu filtre varsayılan olarak kapalıdır",
  "llm.prediction.onnx.repeatPenalty/title": "Tekrar Cezası",
  "llm.prediction.onnx.repeatPenalty/info": "Daha yüksek bir değer modelin kendini tekrar etmesini caydırır",
  "llm.prediction.onnx.topPSampling/title": "Top P Örnekleme",
  "llm.prediction.onnx.topPSampling/info": "ONNX belgelerinden:\n\nTopP veya daha yüksek olasılıklara sahip en olası belirteçler yalnızca üretim için tutulur\n\n• Bu filtre varsayılan olarak kapalıdır",
  "llm.prediction.seed/title": "Tohum",
  "llm.prediction.structured/title": "Yapılandırılmış Çıktı",
  "llm.prediction.structured/info": "Yapılandırılmış Çıktı",
  "llm.prediction.promptTemplate/title": "İstem Şablonu",
  "customInputs.llmPromptTemplate.types.jinja/label": "Jinja",
  "customInputs.llmPromptTemplate.jinja/error": "Jinja şablonu ayrıştırılamadı: {{error}}",
  "customInputs.llmPromptTemplate.types.manual/label": "Manuel",
  "customInputs.llmPromptTemplate.manual.subfield.beforeSystem/label": "Sistem İsteminden Önce",
  "customInputs.llmPromptTemplate.manual.subfield.afterSystem/label": "Sistem İsteminden Sonra",
  "customInputs.llmPromptTemplate.manual.subfield.beforeUser/label": "Her Kullanıcı Mesajından Önce",
  "customInputs.llmPromptTemplate.manual.subfield.afterUser/label": "Her Kullanıcı Mesajından Sonra",
  "customInputs.llmPromptTemplate.manual.subfield.beforeAssistant/label": "Her AI Mesajından Önce",
  "customInputs.llmPromptTemplate.manual.subfield.afterAssistant/label": "Her AI Mesajından Sonra",
  "llm.load.contextLength/title": "Bağlam Uzunluğu",
  "llm.load.contextLength/info": "Modelin işlem sırasında bir kerede dikkate alabileceği maksimum belirteç sayısını belirler, bu da modelin işlem sırasında ne kadar bağlamı koruduğunu etkiler",
  "llm.load.seed/title": "Tohum",
  "llm.load.seed/info": "Rastgele Tohum: Rastgele sayı üretimi için tohum ayarlar, tekrarlanabilir sonuçlar sağlar",
  "llm.load.llama.evalBatchSize/title": "Değerlendirme Yığın Boyutu",
  "llm.load.llama.evalBatchSize/info": "Değerlendirme sırasında bir yığın halinde işlenen örnek sayısını ayarlar, hız ve bellek kullanımını etkiler",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frekans Temeli",
  "llm.load.llama.ropeFrequencyBase/info": "[Gelişmiş] Döner Konumsal Kodlama için temel frekansı ayarlar, konumsal bilginin nasıl gömüldüğünü etkiler",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frekans Ölçeği",
  "llm.load.llama.ropeFrequencyScale/info": "[Gelişmiş] Döner Konumsal Kodlama frekansının ölçeklendirilmesini değiştirir, konumsal kodlama ayrıntılarını kontrol eder",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Yükleme",
  "llm.load.llama.acceleration.offloadRatio/info": "Hesaplamanın GPU'ya aktarılma oranını ayarlayın. GPU yüklemeyi devre dışı bırakmak için kapalı olarak ayarlayın veya modelin karar vermesine izin vermek için otomatik olarak ayarlayın.",
  "llm.load.llama.flashAttention/title": "Flash Dikkat",
  "llm.load.llama.flashAttention/info": "Dikkat mekanizmalarını hızlandırarak daha hızlı ve verimli işlem sağlar",
  "llm.load.llama.keepModelInMemory/title": "Modeli Bellekte Tut",
  "llm.load.llama.keepModelInMemory/info": "Modelin diske takas edilmesini önler, daha yüksek RAM kullanımı pahasına daha hızlı erişim sağlar",
  "llm.load.llama.useFp16ForKVCache/title": "KV Önbelleği İçin FP16 Kullan",
  "llm.load.llama.useFp16ForKVCache/info": "Önbelleği yarı hassasiyette (FP16) depolayarak bellek kullanımını azaltır",
  "llm.load.llama.tryMmap/title": "mmap() Deneyin",
  "llm.load.llama.tryMmap/info": "Model dosyalarını doğrudan diskten belleğe yükleyin",
  "embedding.load.contextLength/title": "Bağlam Uzunluğu",
  "embedding.load.contextLength/info": "Modelin bir kerede dikkate alabileceği maksimum belirteç sayısını belirler, bu da modelin işlem sırasında ne kadar bağlamı koruduğunu etkiler",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frekans Temeli",
  "embedding.load.llama.ropeFrequencyBase/info": "[Gelişmiş] Döner Konumsal Kodlama için temel frekansı ayarlar, konumsal bilginin nasıl gömüldüğünü etkiler",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frekans Ölçeği",
  "embedding.load.llama.ropeFrequencyScale/info": "[Gelişmiş] Döner Konumsal Kodlama frekansının ölçeklendirilmesini değiştirir, konumsal kodlama ayrıntılarını kontrol eder",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Yükleme",
  "embedding.load.llama.acceleration.offloadRatio/info": "Hesaplamanın GPU'ya aktarılma oranını ayarlayın. GPU yüklemeyi devre dışı bırakmak için kapalı olarak ayarlayın veya modelin karar vermesine izin vermek için otomatik olarak ayarlayın.",
  "embedding.load.llama.keepModelInMemory/title": "Modeli Bellekte Tut",
  "embedding.load.llama.keepModelInMemory/info": "Modelin diske takas edilmesini önler, daha yüksek RAM kullanımı pahasına daha hızlı erişim sağlar",
  "embedding.load.llama.tryMmap/title": "mmap() Deneyin",
  "embedding.load.llama.tryMmap/info": "Model dosyalarını doğrudan diskten belleğe yükleyin",
  "embedding.load.seed/title": "Tohum",
  "embedding.load.seed/info": "Rastgele Tohum: Rastgele sayı üretimi için tohum ayarlar, tekrarlanabilir sonuçlar sağlar"
}
