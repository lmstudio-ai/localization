{
  "noInstanceSelected": "Model örneği seçilmedi",
  "resetToDefault": "Sıfırla",
  "showAdvancedSettings": "Gelişmiş ayarları göster",
  "showAll": "Hepsini göster",
  "basicSettings": "Temel",
  "configSubtitle": "Ön ayarları yükleyin veya kaydedin ve model parametre geçersiz kılmalarını deneyin",
  "inferenceParameters/title": "Tahmin Parametreleri",
  "inferenceParameters/info": "Tahmini etkileyen parametrelerle deneyler yapın.",
  "generalParameters/title": "Genel",
  "samplingParameters/title": "Örnekleme",
  "basicTab": "Temel",
  "advancedTab": "Gelişmiş",
  "advancedTab/title": "🧪 Gelişmiş Yapılandırma",
  "advancedTab/expandAll": "Tümünü genişlet",
  "advancedTab/overridesTitle": "Yapılandırma Geçersiz Kılmaları",
  "advancedTab/noConfigsText": "Kaydedilmemiş değişikliğiniz yok - buradaki geçersiz kılmaları görmek için yukarıdaki değerleri düzenleyin.",
  "loadInstanceFirst": "Yapılandırılabilir parametreleri görüntülemek için bir model yükleyin",
  "noListedConfigs": "Yapılandırılabilir parametre yok",
  "generationParameters/info": "Metin oluşturmayı etkileyen temel parametreleri deneyin.",
  "loadParameters/title": "Parametreleri Yükle",
  "loadParameters/description": "Modelin başlatılma ve belleğe yüklenme şeklini kontrol etmek için ayarlar.",
  "loadParameters/reload": "Değişiklikleri uygulamak için yeniden yükle",
  "discardChanges": "Değişiklikleri iptal et",
  "llm.prediction.systemPrompt/title": "Sistem Komutu",
  "llm.prediction.systemPrompt/description": "Bu alanı modele bir dizi kural, kısıtlama veya genel gereklilik gibi arka plan talimatları sağlamak için kullanın. Bu alan genellikle “sistem komutu” olarak da adlandırılır.",
  "llm.prediction.systemPrompt/subTitle": "Yapay Zeka Yönergeleri",
  "llm.prediction.temperature/title": "Temperature",
  "llm.prediction.temperature/info": "Varsayılan değer <{{dynamicValue}}> olup rastgelelik ile deterministiklik arasında bir denge sağlar. Aşırı durumda, 0 tempature her zaman en olası sonraki belirteci seçecek ve her çalıştırmada aynı çıktılara yol açacaktır",
  "llm.prediction.llama.topKSampling/title": "Top K Örnekleme",
  "llm.prediction.llama.topKSampling/info": "Top-k örnekleme, model tarafından tahmin edilen en olası k belirtecinden bir sonraki belirteci seçen bir metin oluşturma yöntemidir.\n\nBu yöntem, düşük olasılıklı veya anlamsız belirteçler üretme riskini azaltır, ancak çıktının çeşitliliğini de sınırlayabilir.\n\nTop-k değeri ne kadar yüksek olursa (örneğin, 100), o kadar fazla belirteç dikkate alınır ve daha çeşitli bir metin üretilir. Daha düşük bir değer (örneğin, 10), en olası belirteçlere odaklanır ve daha temkinli bir metin oluşturur.\n\n• Varsayılan değer <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU İş Parçacıkları",
  "llm.prediction.llama.cpuThreads/info": "İşlem sırasında kullanılacak iş parçacığı sayısı. İş parçacığı sayısını artırmak her zaman daha iyi performansla ilişkili olmayabilir. Varsayılan değer <{{dynamicValue}}>'dir.",
  "llm.prediction.maxPredictedTokens/title": "Yanıt Uzunluğunu Sınırla",
  "llm.prediction.maxPredictedTokens/subTitle": "İsteğe bağlı olarak yapay zekanın yanıtının uzunluğunu sınırlayın",
  "llm.prediction.maxPredictedTokens/info": "Chatbot'un yanıtının maksimum uzunluğunu kontrol edin. Yanıtın maksimum uzunluğunu sınırlamak için açın veya chatbot'un ne zaman duracağına karar vermesi için kapalı bırakın.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maksimum yanıt uzunluğu (tokenler)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Yaklaşık {{maxWords}} kelime",
  "llm.prediction.llama.repeatPenalty/title": "Tekrar Cezası",
  "llm.prediction.llama.repeatPenalty/info": "Modelin tekrarlayan veya monoton metinler üretmesini engellemeye yardımcı olur.\n\nDaha yüksek bir değer (örneğin, 1.5) tekrarları daha güçlü bir şekilde cezalandırır, daha düşük bir değer (örneğin, 0.9) ise daha hoşgörülü olur.\" • Varsayılan değer <{{dynamicValue}}>'dir",
  "llm.prediction.llama.minPSampling/title": "Min P Örnekleme",
  "llm.prediction.llama.minPSampling/info": "Bir tokenin dikkate alınabilmesi için gereken minimum olasılık, en olası token olasılığına göre belirlenir. [0, 1] aralığında olmalıdır.\n\n• Varsayılan değer <{{dynamicValue}}>'dir",
  "llm.prediction.llama.topPSampling/title": "Top P Örnekleme",
  "llm.prediction.llama.topPSampling/info": "Top-p örnekleme, aynı zamanda çekirdek örnekleme olarak da bilinir, bir metin oluşturma yöntemidir ve bir sonraki tokeni, birlikte en az p kümülatif olasılığa sahip tokenler alt kümesinden seçer.\n\nBu yöntem, hem tokenlerin olasılıklarını hem de örnekleme yapılacak token sayısını dikkate alarak çeşitlilik ve kalite arasında bir denge sağlar.\n\nDaha yüksek bir top-p değeri (örneğin, 0.95) daha çeşitli bir metin üretirken, daha düşük bir değer (örneğin, 0.5) daha odaklanmış ve temkinli bir metin oluşturur. (0, 1] aralığında olmalıdır.\n\n• Varsayılan değer <{{dynamicValue}}>'dir",
  "llm.prediction.stopStrings/title": "Sonlandırma Metni",
  "llm.prediction.stopStrings/subTitle": "Modelin daha fazla jeton üretmesini durdurması gereken metinler",
  "llm.prediction.stopStrings/info": "Karşılaşıldığında modelin daha fazla jeton üretmesini durduracak özel metinler",
  "llm.prediction.stopStrings/placeholder": "Bir metin girin ve ⏎ tuşuna basın",
  "llm.prediction.contextOverflowPolicy/title": "Konuşma Taşması",
  "llm.prediction.contextOverflowPolicy/info": "Konuşma modelin çalışma belleği ('context') boyutunu aştığında ne yapılacağına karar verin",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Limitte Dur",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Modelin belleği dolduğunda üretmeyi durdurun",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Ortayı Kısalt",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Yeni mesajlar için yer açmak amacıyla konuşmanın ortasından mesajlar kaldırılır. Model, konuşmanın başını hâlâ hatırlayacaktır",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Dönüşümlü Pencere",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Model her zaman en son birkaç mesajı alır, ancak konuşmanın başını unutabilir",
  "llm.prediction.llama.frequencyPenalty/title": "Frekans Cezası",
  "llm.prediction.llama.presencePenalty/title": "Varlık Cezası",
  "llm.prediction.llama.tailFreeSampling/title": "Kuyruksuz Örnekleme",
  "llm.prediction.llama.locallyTypicalSampling/title": "Yerel Tipik Örnekleme",
  "llm.prediction.mlx.repeatPenalty/title": "Tekrar Cezası",
  "llm.prediction.mlx.repeatPenalty/info": "Daha yüksek bir değer, modelin kendini tekrar etmesini engeller",
  "llm.prediction.onnx.topKSampling/title": "Top K Örnekleme",
  "llm.prediction.onnx.topKSampling/info": "Top-k filtrelemesi için saklanacak en yüksek olasılığa sahip kelime dağarcığı jetonlarının sayısı\n\n• Bu filtre varsayılan olarak kapalıdır",
  "llm.prediction.onnx.repeatPenalty/title": "Tekrar Cezası",
  "llm.prediction.onnx.repeatPenalty/info": "Daha yüksek bir değer, modelin kendini tekrar etmesini engeller",
  "llm.prediction.onnx.topPSampling/title": "Top P Örnekleme",
  "llm.prediction.onnx.topPSampling/info": "Yalnızca TopP veya daha yüksek olasılıklara sahip en olası jetonlar, üretim için saklanır\n\n• Bu filtre varsayılan olarak kapalıdır",
  "llm.prediction.seed/title": "Tohum(Seed)",
  "llm.prediction.structured/title": "Yapılandırılmış Çıktı",
  "llm.prediction.structured/info": "Yapılandırılmış Çıktı",
  "llm.prediction.promptTemplate/title": "Komut Şablonu",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Jinja şablonu ayrıştırılamadı: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manuel",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Sistemden Önce",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "Sistem önekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Sistem Sonrası",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "Sistem sonekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Kullanıcıdan Önce",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "Kullanıcı önekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "Kullanıcı Sonrası",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "Kullanıcı sonekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Asistandan Önce",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "Asistan önekini girin...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Asistan Sonrası",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "Asistan sonekini girin...",
  "llm.prediction.promptTemplate.stopStrings/label": "Ek Durdurma Metni",
  "llm.prediction.promptTemplate.stopStrings/hint": "Şablona özgü durma metni, kullanıcı tarafından belirtilen durma metinlerine ek olarak kullanılacaktır.",

  "llm.load.contextLength/title": "Context Length",
  "llm.load.contextLength/info": "Modelin bir kerede dikkate alabileceği maksimum token sayısını belirtir ve işleme sırasında ne kadar context'i koruduğunu etkiler",
  "llm.load.seed/title": "Tohum(Seed)",
  "llm.load.seed/info": "Rastgele Tohum: Tekrarlanabilir sonuçlar sağlamak amacıyla rastgele sayı üretimi için tohumu ayarlar",

  "llm.load.llama.evalBatchSize/title": "Değerlendirme Paketi Boyutu",
  "llm.load.llama.evalBatchSize/info": "Değerlendirme sırasında tek bir grupta birlikte işlenen örnek sayısını ayarlar, hızı ve bellek kullanımını etkiler",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frekans Tabanı",
  "llm.load.llama.ropeFrequencyBase/info": "[Gelişmiş] Rotary Pozisyonel Kodlama için temel frekansı ayarlar, bu da pozisyonel bilgilerin nasıl gömüldüğünü etkiler",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frekans Ölçeği",
  "llm.load.llama.ropeFrequencyScale/info": "[Gelişmiş] Rotary Pozisyonel Kodlama için frekans ölçeklemesini değiştirir, bu da pozisyonel kodlama ayrıntı düzeyini kontrol eder",
  "llm.load.llama.gpuOffload/title": "GPU Boşaltma",
  "llm.load.llama.gpuOffload/info": "GPU'ya yüklenecek hesaplama oranını ayarlayın. GPU boşaltmayı devre dışı bırakmak için kapalı olarak veya modelin karar vermesine izin vermek için otomatik olarak ayarlayın.",
  "llm.load.llama.flashAttention/title": "Flash Dikkat",
  "llm.load.llama.flashAttention/info": "Dikkat mekanizmalarını daha hızlı ve verimli işlem için hızlandırır",
  "llm.load.llama.keepModelInMemory/title": "Modeli Hafızada Tutun",
  "llm.load.llama.keepModelInMemory/info": "Modelin diske takas edilmesini önleyerek daha yüksek RAM kullanımı pahasına daha hızlı erişim sağlar",
  "llm.load.llama.useFp16ForKVCache/title": "KV Önbellek İçin FP16 Kullanın",
  "llm.load.llama.useFp16ForKVCache/info": "Önbelleği yarım hassasiyette (FP16) depolayarak bellek kullanımını azaltır",
  "llm.load.llama.tryMmap/title": "mmap() işlevini deneyin",
  "llm.load.llama.tryMmap/info": "Model dosyalarını doğrudan diskten belleğe yükleme",

  "embedding.load.contextLength/title": "Context Uzunluğu",
  "embedding.load.contextLength/info": "Modelin bir kerede dikkate alabileceği maksimum token sayısını belirtir ve işleme sırasında ne kadar context'i koruduğunu etkiler",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frekans Tabanı",
  "embedding.load.llama.ropeFrequencyBase/info": "[Gelişmiş] Döner Konumsal Kodlama için temel frekansı ayarlayarak konum bilgisinin nasıl gömüleceğini etkiler",
  "embedding.load.llama.evalBatchSize/title": "Değerlendirme Paket Büyüklüğü",
  "embedding.load.llama.evalBatchSize/info": "Değerlendirme sırasında bir grupta birlikte işlenen token sayısını ayarlar",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frekans Ölçeği",
  "embedding.load.llama.ropeFrequencyScale/info": "[Gelişmiş] Rotary Pozisyonel Kodlama için frekans ölçeklemesini değiştirir, bu da pozisyonel kodlama ayrıntı düzeyini kontrol eder",
  "embedding.load.llama.gpuOffload/title": "GPU Boşaltma",
  "embedding.load.llama.gpuOffload/info": "GPU'ya yüklenecek hesaplama oranını ayarlayın. GPU boşaltmayı devre dışı bırakmak için kapalı olarak veya modelin karar vermesine izin vermek için otomatik olarak ayarlayın.",
  "embedding.load.llama.keepModelInMemory/title": "Modeli Hafızada Tutun",
  "embedding.load.llama.keepModelInMemory/info": "Modelin diske takas edilmesini önleyerek daha yüksek RAM kullanımı pahasına daha hızlı erişim sağlar",
  "embedding.load.llama.tryMmap/title": "mmap() işlevini deneyin",
  "embedding.load.llama.tryMmap/info": "Model dosyalarını doğrudan diskten belleğe yükleme",
  "embedding.load.seed/title": "Tohum(Seed)",
  "embedding.load.seed/info": "Rastgele Tohum: Tekrarlanabilir sonuçlar sağlamak amacıyla rastgele sayı üretimi için tohumu ayarlar"
}
