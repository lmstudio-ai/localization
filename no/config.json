{
  "noInstanceSelected": "Ingen modellinstans valgt",
  "resetToDefault": "Tilbakestill",
  "showAdvancedSettings": "Vis avanserte innstillinger",
  "showAll": "Vis alle",
  "basicSettings": "Grunnleggende",
  "configSubtitle": "Last inn eller lagre forhåndsinnstillinger og eksperimenter med modellparameteroverstyringer",
  "inferenceParameters/title": "Prediksjonsparametere",
  "inferenceParameters/info": "Eksperimenter med parametere som påvirker prediksjonen.",
  "generationParameters/title": "Tekst",
  "samplingParameters/title": "Utvalg",
  "otherParameters/title": "Annet",
  "basicTab": "Grunnleggende",
  "advancedTab": "Avansert",
  "loadInstanceFirst": "Last inn en modell for å se konfigurerbare parametere",
  "generationParameters/info": "Eksperimenter med grunnleggende parametere som påvirker tekstgenerering.",
  "loadParameters/title": "Last inn parametere",
  "loadParameters/description": "Å endre disse parameterne krever at en må laste inn modellen på nytt",
  "loadParameters/reload": "Last inn på nytt for å bruke endringer i lastparametere",
  "discardChanges": "Forkast endringer",
  "llm.prediction.systemPrompt/title": "Retningslinjer for AI",
  "llm.prediction.systemPrompt/description": "Bruk dette feltet for å gi bakgrunnsinstruksjoner til modellen, for eksempel et sett med regler, begrensninger eller generelle krav. Dette feltet kalles ofte \"systemprompt\".",
  "llm.prediction.temperature/title": "Temperatur",
  "llm.prediction.temperature/info": "Fra llama.cpp hjelpefiler: \"Standardverdien er <{{dynamicValue}}>, som gir en balanse mellom tilfeldighet og forutsigbarhet. I ekstreme tilfeller vil en temperatur på 0 alltid velge det mest sannsynlige neste tegnet, noe som fører til identiske utdata i hver kjøring\"",
  "llm.prediction.topKSampling/title": "Topp K-utvalg",
  "llm.prediction.topKSampling/info": "Fra llama.cpp hjelpefiler:\n\nTopp-k-utvalg er en tekstgenereringsmetode som velger det neste tegnet bare fra de topp-k mest sannsynlige tegnene som er forutsagt av modellen.\n\nDet bidrar til å redusere risikoen for å generere tegn med lav sannsynlighet eller som ikke gir mening, men det kan også begrense mangfoldet i utdataene.\n\nEn høyere verdi for topp-k (f.eks. 100) vil vurdere flere tegn og føre til mer mangfoldig tekst, mens en lavere verdi (f.eks. 10) vil fokusere på de mest sannsynlige tegnene og generere mer konservativ tekst.\n\n• Standardverdien er <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU-tråder",
  "llm.prediction.llama.cpuThreads/info": "Antall tråder som skal brukes under beregningen. Å øke antall tråder vil ikke alltid bety bedre ytelse. Standardverdien er <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Begrens responslengde",
  "llm.prediction.maxPredictedTokens/info": "Kontroller den maksimale lengden på chatbotens svar. Slå på for å sette en grense for den maksimale lengden på et svar, eller slå av for å la chatboten bestemme når den skal stoppe.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Maksimal responslengde (tegn)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Omtrent {{maxWords}} ord",
  "llm.prediction.repeatPenalty/title": "Gjentakelsesstraff",
  "llm.prediction.repeatPenalty/info": "Fra llama.cpp hjelpefiler: \"Hjelper til å forhindre at modellen genererer repeterende eller monoton tekst.\n\nEn høyere verdi (f.eks. 1,5) vil straffe gjentakelser sterkere, mens en lavere verdi (f.eks. 0,9) vil være mer tolerant.\" • Standardverdien er <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P-utvalg",
  "llm.prediction.minPSampling/info": "Fra llama.cpp hjelpefiler:\n\nDen minimale sannsynligheten for at et tegn skal vurderes, i forhold til sannsynligheten for det mest sannsynlige tegnet. Må være i [0, 1].\n\n• Standardverdien er <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Topp P-utvalg",
  "llm.prediction.topPSampling/info": "Fra llama.cpp hjelpefiler:\n\nTopp-p-utvalg, også kjent som kjerneutvalg, er en annen tekstgenereringsmetode som velger det neste tegnet fra et delsett av tegn som til sammen har en kumulativ sannsynlighet på minst p.\n\nDenne metoden gir en balanse mellom mangfold og kvalitet ved å vurdere både sannsynligheten for tegn og antall tegn som skal tas utvalg fra.\n\nEn høyere verdi for topp-p (f.eks. 0,95) vil føre til mer mangfoldig tekst, mens en lavere verdi (f.eks. 0,5) vil generere mer fokusert og konservativ tekst. Må være i (0, 1].\n\n• Standardverdien er <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Stoppstrenger",
  "llm.prediction.stopStrings/info": "Spesifikke tegnstrenger som, når de oppdages, vil stoppe modellen fra å generere flere tegn",
  "llm.prediction.stopStrings/placeholder": "Skriv inn en streng og trykk ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Kontekst overbelastning",
  "llm.prediction.contextOverflowPolicy/info": "Bestem hva du skal gjøre når samtalen overstiger størrelsen på modellens arbeidsminne ('kontekst')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Stopp ved grensen",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Stopp genereringen når modellens minne blir fullt",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Avkort midt i",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Fjerner meldinger fra midten av samtalen for å gi plass til nyere meldinger. Modellen vil fortsatt huske begynnelsen av samtalen",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Glidende vindu",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Modellen vil alltid få de siste få meldingene, men kan glemme begynnelsen av samtalen",
  "llm.prediction.llama.frequencyPenalty/title": "Hyppighetsstraff",
  "llm.prediction.llama.presencePenalty/title": "Tilstedeværelsesstraff",
  "llm.prediction.llama.tailFreeSampling/title": "Sampling med fokus på variasjon",
  "llm.prediction.llama.locallyTypicalSampling/title": "Lokalt typisk utvalg",
  "llm.prediction.seed/title": "Verdi",
  "llm.prediction.structured/title": "Strukturert utdata",
  "llm.prediction.structured/info": "Strukturert utdata",
  "llm.load.contextLength/title": "Kontekstlengde",
  "llm.load.contextLength/info": "Angir det maksimale antall tegn modellen kan vurdere samtidig, noe som påvirker hvor mye kontekst den beholder under behandling",
  "llm.load.seed/title": "verdi",
  "llm.load.seed/info": "Startverdi: Angir startverdien for generering av tilfeldige tall, slik at resultatene blir de samme hver gang.",
  "llm.load.llama.evalBatchSize/title": "Evalueringsbatchstørrelse",
  "llm.load.llama.evalBatchSize/info": "Angir antall eksempler som behandles sammen i en batch under evaluering, noe som påvirker hastighet og minnebruk",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE-frekvensbase",
  "llm.load.llama.ropeFrequencyBase/info": "[Avansert] Justerer basisfrekvensen for rotasjonsposisjonell koding, noe som påvirker hvordan posisjonsinformasjon er innebygd",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE-frekvensskala",
  "llm.load.llama.ropeFrequencyScale/info": "[Avansert] Endrer skaleringen av frekvensen for rotasjonsposisjonell koding for å kontrollere posisjonskodingsgranularitet",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU-avlasting",
  "llm.load.llama.acceleration.offloadRatio/info": "Angi forholdet mellom beregning som skal avlastes til GPU. Sett til av for å deaktivere GPU-avlasting, eller auto for å la modellen bestemme.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Akselererer oppmerksomhetsmekanismer for raskere og mer effektiv behandling",
  "llm.load.llama.keepModelInMemory/title": "Hold modellen i minnet",
  "llm.load.llama.keepModelInMemory/info": "Forhindrer at modellen byttes ut til disk, noe som sikrer raskere tilgang på bekostning av høyere RAM-bruk",
  "llm.load.llama.useFp16ForKVCache/title": "Bruk FP16 for KV-buffer",
  "llm.load.llama.useFp16ForKVCache/info": "Reduserer minnebruk ved å lagre bufferen i halvpresisjon (FP16)",
  "llm.load.llama.tryMmap/title": "Prøv mmap()",
  "llm.load.llama.tryMmap/info": "Last inn modellfiler direkte fra disk til minne"
}
