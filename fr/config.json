{
  "noInstanceSelected": "Aucune instance de mod√®le s√©lectionn√©e",
  "resetToDefault": "R√©initialiser",
  "showAdvancedSettings": "Afficher les param√®tres avanc√©s",
  "showAll": "Tout",
  "basicSettings": "Basic",
  "configSubtitle": "Charger ou sauvegarder des pr√©r√©glages et exp√©rimenter avec les param√®tres du mod√®le",
  "inferenceParameters/title": "Param√®tres de pr√©diction",
  "inferenceParameters/info": "Exp√©rimentez avec les param√®tres qui influencent la pr√©diction.",
  "generalParameters/title": "G√©n√©ral",
  "samplingParameters/title": "√âchantillonnage",
  "basicTab": "Base",
  "advancedTab": "Avanc√©",
  "advancedTab/title": "üß™ Configuration avanc√©e",
  "advancedTab/expandAll": "Tout d√©velopper",
  "advancedTab/overridesTitle": "Remplacements de configuration",
  "advancedTab/noConfigsText": "Vous n'avez aucun changement non sauvegard√© - modifiez les valeurs ci-dessus pour voir les remplacements ici.",
  "loadInstanceFirst": "Chargez un mod√®le pour voir les param√®tres configurables",
  "noListedConfigs": "Aucun param√®tre configurable",
  "generationParameters/info": "Exp√©rimentez avec les param√®tres de base qui influencent la g√©n√©ration de texte.",
  "loadParameters/title": "Param√®tres de chargement",
  "loadParameters/description": "Param√®tres pour contr√¥ler la fa√ßon dont le mod√®le est initialis√© et charg√© en m√©moire.",
  "loadParameters/reload": "Recharger pour appliquer les changements",
  "discardChanges": "Annuler les modifications",
  "loadModelToSeeOptions": "Chargez un mod√®le pour voir les options",
  "llm.prediction.systemPrompt/title": "Message syst√®me",
  "llm.prediction.systemPrompt/description": "Utilisez ce champ pour fournir des instructions de base au mod√®le, comme un ensemble de r√®gles, de contraintes ou d'exigences g√©n√©rales.",
  "llm.prediction.systemPrompt/subTitle": "Directives pour l'IA",
  "llm.prediction.temperature/title": "Temp√©rature",
  "llm.prediction.temperature/subTitle": "Quantit√© d'al√©atoire √† introduire. 0 donnera le m√™me r√©sultat √† chaque fois, tandis que des valeurs plus √©lev√©es augmenteront la cr√©ativit√© et la variance",
  "llm.prediction.temperature/info": "D'apr√®s la documentation llama.cpp : \"La valeur par d√©faut est <{{dynamicValue}}>, qui offre un √©quilibre entre al√©atoire et d√©terminisme. √Ä l'extr√™me, une temp√©rature de 0 choisira toujours le token le plus probable, conduisant √† des sorties identiques √† chaque ex√©cution\"",
  "llm.prediction.llama.sampling/title": "√âchantillonnage",
  "llm.prediction.topKSampling/title": "√âchantillonnage Top K",
  "llm.prediction.topKSampling/subTitle": "Limite le prochain token √† l'un des k tokens les plus probables. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.topKSampling/info": "D'apr√®s la documentation llama.cpp :\n\nL'√©chantillonnage top-k est une m√©thode de g√©n√©ration de texte qui s√©lectionne le prochain token uniquement parmi les k tokens les plus probables pr√©dits par le mod√®le.\n\nCela aide √† r√©duire le risque de g√©n√©rer des tokens peu probables ou incoh√©rents, mais peut aussi limiter la diversit√© de la sortie.\n\nUne valeur plus √©lev√©e pour le top-k (par exemple, 100) prendra en compte plus de tokens et conduira √† un texte plus diversifi√©, tandis qu'une valeur plus basse (par exemple, 10) se concentrera sur les tokens les plus probables et g√©n√©rera un texte plus conservateur.\n\n‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Nombre de threads CPU √† utiliser pendant l'inf√©rence",
  "llm.prediction.llama.cpuThreads/info": "Le nombre de threads √† utiliser pendant le calcul. Augmenter le nombre de threads ne correspond pas toujours √† de meilleures performances. La valeur par d√©faut est <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limiter la longueur de r√©ponse",
  "llm.prediction.maxPredictedTokens/subTitle": "Limiter optionnellement la longueur de la r√©ponse de l'IA",
  "llm.prediction.maxPredictedTokens/info": "Contr√¥le la longueur maximale de la r√©ponse du chatbot. Activez pour d√©finir une limite sur la longueur maximale d'une r√©ponse, ou d√©sactivez pour laisser le chatbot d√©cider quand s'arr√™ter.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longueur maximale de r√©ponse (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Environ {{maxWords}} mots",
  "llm.prediction.repeatPenalty/title": "P√©nalit√© de r√©p√©tition",
  "llm.prediction.repeatPenalty/subTitle": "√Ä quel point d√©courager la r√©p√©tition du m√™me token",
  "llm.prediction.repeatPenalty/info": "D'apr√®s la documentation llama.cpp : \"Aide √† emp√™cher le mod√®le de g√©n√©rer du texte r√©p√©titif ou monotone.\n\nUne valeur plus √©lev√©e (par exemple, 1.5) p√©nalisera plus fortement les r√©p√©titions, tandis qu'une valeur plus basse (par exemple, 0.9) sera plus tol√©rante.\" ‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "√âchantillonnage Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilit√© de base minimum pour qu'un token soit s√©lectionn√© pour la sortie",
  "llm.prediction.minPSampling/info": "D'apr√®s la documentation llama.cpp :\n\nLa probabilit√© minimum pour qu'un token soit consid√©r√©, par rapport √† la probabilit√© du token le plus probable. Doit √™tre dans [0, 1].\n\n‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "√âchantillonnage Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilit√© cumulative minimum pour les tokens suivants possibles. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.topPSampling/info": "D'apr√®s la documentation llama.cpp :\n\nL'√©chantillonnage top-p, aussi connu sous le nom d'√©chantillonnage nucleus, est une autre m√©thode de g√©n√©ration de texte qui s√©lectionne le prochain token parmi un sous-ensemble de tokens qui ont ensemble une probabilit√© cumulative d'au moins p.\n\nCette m√©thode offre un √©quilibre entre diversit√© et qualit√© en consid√©rant √† la fois les probabilit√©s des tokens et le nombre de tokens √† √©chantillonner.\n\nUne valeur plus √©lev√©e pour le top-p (par exemple, 0.95) conduira √† un texte plus diversifi√©, tandis qu'une valeur plus basse (par exemple, 0.5) g√©n√©rera un texte plus concentr√© et conservateur. Doit √™tre dans (0, 1].\n\n‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cha√Ænes d'arr√™t",
  "llm.prediction.stopStrings/subTitle": "Cha√Ænes qui devraient arr√™ter le mod√®le de g√©n√©rer plus de tokens",
  "llm.prediction.stopStrings/info": "Cha√Ænes sp√©cifiques qui, lorsqu'elles sont rencontr√©es, arr√™teront le mod√®le de g√©n√©rer plus de tokens",
  "llm.prediction.stopStrings/placeholder": "Entrez une cha√Æne et appuyez sur ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "D√©bordement de contexte",
  "llm.prediction.contextOverflowPolicy/subTitle": "Comment le mod√®le doit se comporter quand la conversation devient trop grande pour √™tre g√©r√©e",
  "llm.prediction.contextOverflowPolicy/info": "D√©cidez quoi faire quand la conversation d√©passe la taille de la m√©moire de travail du mod√®le ('contexte')",
  "llm.prediction.llama.frequencyPenalty/title": "P√©nalit√© de fr√©quence",
  "llm.prediction.llama.presencePenalty/title": "P√©nalit√© de pr√©sence",
  "llm.prediction.llama.tailFreeSampling/title": "√âchantillonnage sans queue",
  "llm.prediction.llama.locallyTypicalSampling/title": "√âchantillonnage localement typique",
  "llm.prediction.onnx.topKSampling/title": "√âchantillonnage Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limite le prochain token √† l'un des k tokens les plus probables. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.onnx.topKSampling/info": "De la documentation ONNX :\n\nNombre de tokens de vocabulaire de plus haute probabilit√© √† conserver pour le filtrage top-k\n\n‚Ä¢ Ce filtre est d√©sactiv√© par d√©faut",
  "llm.prediction.onnx.repeatPenalty/title": "P√©nalit√© de r√©p√©tition",
  "llm.prediction.onnx.repeatPenalty/subTitle": "√Ä quel point d√©courager la r√©p√©tition du m√™me token",
  "llm.prediction.onnx.repeatPenalty/info": "Une valeur plus √©lev√©e d√©courage le mod√®le de se r√©p√©ter",
  "llm.prediction.onnx.topPSampling/title": "√âchantillonnage Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilit√© cumulative minimum pour les tokens suivants possibles. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.onnx.topPSampling/info": "De la documentation ONNX :\n\nSeuls les tokens les plus probables avec des probabilit√©s qui s'additionnent jusqu'√† TopP ou plus sont conserv√©s pour la g√©n√©ration\n\n‚Ä¢ Ce filtre est d√©sactiv√© par d√©faut",
  "llm.prediction.seed/title": "Graine",
  "llm.prediction.structured/title": "Sortie structur√©e",
  "llm.prediction.structured/info": "Sortie structur√©e",
  "llm.prediction.structured/description": "Avanc√© : vous pouvez fournir un sch√©ma JSON pour imposer un format de sortie particulier au mod√®le. Lisez la [documentation](https://lmstudio.ai/docs/advanced/structured-output) pour en savoir plus",
  "llm.prediction.promptTemplate/title": "Mod√®le de prompt",
  "llm.prediction.promptTemplate/subTitle": "Le format dans lequel les messages de chat sont envoy√©s au mod√®le. Modifier ceci peut introduire un comportement inattendu - assurez-vous de savoir ce que vous faites !",

  "llm.load.contextLength/title": "Longueur de contexte",
  "llm.load.contextLength/subTitle": "Le nombre maximum de tokens que le mod√®le peut prendre en compte dans un prompt. Voir les options de d√©bordement de conversation sous \"Param√®tres d'inf√©rence\" pour plus de fa√ßons de g√©rer cela",
  "llm.load.contextLength/info": "Sp√©cifie le nombre maximum de tokens que le mod√®le peut consid√©rer √† la fois, impactant la quantit√© de contexte qu'il retient pendant le traitement",
  "llm.load.contextLength/warning": "D√©finir une valeur √©lev√©e pour la longueur de contexte peut significativement impacter l'utilisation de la m√©moire",
  "llm.load.seed/title": "Graine",
  "llm.load.seed/subTitle": "La graine pour le g√©n√©rateur de nombres al√©atoires utilis√© dans la g√©n√©ration de texte. -1 est al√©atoire",
  "llm.load.seed/info": "Graine al√©atoire : D√©finit la graine pour la g√©n√©ration de nombres al√©atoires pour assurer des r√©sultats reproductibles",

  "llm.load.llama.evalBatchSize/title": "Taille du lot d'√©valuation",
  "llm.load.llama.evalBatchSize/subTitle": "Nombre de tokens d'entr√©e √† traiter √† la fois. Augmenter ceci augmente les performances au co√ªt de l'utilisation de la m√©moire",
  "llm.load.llama.evalBatchSize/info": "D√©finit le nombre d'exemples trait√©s ensemble dans un lot pendant l'√©valuation, affectant la vitesse et l'utilisation de la m√©moire",
  "llm.load.llama.ropeFrequencyBase/title": "Base de fr√©quence RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Base de fr√©quence personnalis√©e pour les embeddings positionnels rotatifs (RoPE). Augmenter ceci peut permettre de meilleures performances avec des contextes longs",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanc√©] Ajuste la fr√©quence de base pour l'encodage positionnel rotatif, affectant la fa√ßon dont l'information positionnelle est int√©gr√©e",
  "llm.load.llama.ropeFrequencyScale/title": "√âchelle de fr√©quence RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "La longueur de contexte est mise √† l'√©chelle par ce facteur pour √©tendre le contexte effectif en utilisant RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanc√©] Modifie la mise √† l'√©chelle de la fr√©quence pour l'encodage positionnel rotatif pour contr√¥ler la granularit√© de l'encodage positionnel",
  "llm.load.llama.acceleration.offloadRatio/title": "D√©chargement GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Nombre de couches discr√®tes du mod√®le √† calculer sur le GPU pour l'acc√©l√©ration GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "D√©finir le nombre de couches √† d√©charger vers le GPU.",
  "llm.load.llama.flashAttention/title": "Attention Flash",
  "llm.load.llama.flashAttention/subTitle": "Diminue l'utilisation de la m√©moire et le temps de g√©n√©ration sur certains mod√®les",
  "llm.load.llama.flashAttention/info": "Acc√©l√®re les m√©canismes d'attention pour un traitement plus rapide et plus efficace",
  "llm.load.numExperts/title": "Nombre d'experts",
  "llm.load.numExperts/subTitle": "Nombre d'experts √† utiliser dans le mod√®le",
  "llm.load.numExperts/info": "Le nombre d'experts √† utiliser dans le mod√®le",
  "llm.load.llama.keepModelInMemory/title": "Garder le mod√®le en m√©moire",
  "llm.load.llama.keepModelInMemory/subTitle": "R√©server la m√©moire syst√®me pour le mod√®le, m√™me quand il est d√©charg√© sur le GPU. Am√©liore les performances mais n√©cessite plus de RAM syst√®me",
  "llm.load.llama.keepModelInMemory/info": "Emp√™che le mod√®le d'√™tre d√©charg√© sur le disque, assurant un acc√®s plus rapide au co√ªt d'une utilisation plus √©lev√©e de la RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Utiliser FP16 pour le cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "R√©duit l'utilisation de la m√©moire en stockant le cache en demi-pr√©cision (FP16)",
  "llm.load.llama.tryMmap/title": "Essayer mmap()",
  "llm.load.llama.tryMmap/subTitle": "Am√©liore le temps de chargement du mod√®le. D√©sactiver ceci peut am√©liorer les performances quand le mod√®le est plus grand que la RAM syst√®me disponible",
  "llm.load.llama.tryMmap/info": "Charger les fichiers du mod√®le directement du disque vers la m√©moire",

  "embedding.load.contextLength/title": "Longueur de contexte",
  "embedding.load.contextLength/subTitle": "Le nombre maximum de tokens que le mod√®le peut prendre en compte dans un prompt. Voir les options de d√©bordement de conversation sous \"Param√®tres d'inf√©rence\" pour plus de fa√ßons de g√©rer cela",
  "embedding.load.contextLength/info": "Sp√©cifie le nombre maximum de tokens que le mod√®le peut consid√©rer √† la fois, impactant la quantit√© de contexte qu'il retient pendant le traitement",
  "embedding.load.llama.ropeFrequencyBase/title": "Base de fr√©quence RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Base de fr√©quence personnalis√©e pour les embeddings positionnels rotatifs (RoPE). Augmenter ceci peut permettre de meilleures performances avec des contextes longs",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avanc√©] Ajuste la fr√©quence de base pour l'encodage positionnel rotatif, affectant la fa√ßon dont l'information positionnelle est int√©gr√©e",
  "embedding.load.llama.evalBatchSize/title": "Taille du lot d'√©valuation",
  "embedding.load.llama.evalBatchSize/subTitle": "Nombre de tokens d'entr√©e √† traiter √† la fois. Augmenter ceci augmente les performances au co√ªt de l'utilisation de la m√©moire",
  "embedding.load.llama.evalBatchSize/info": "D√©finit le nombre de tokens trait√©s ensemble dans un lot pendant l'√©valuation",
  "embedding.load.llama.ropeFrequencyScale/title": "√âchelle de fr√©quence RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "La longueur de contexte est mise √† l'√©chelle par ce facteur pour √©tendre le contexte effectif en utilisant RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avanc√©] Modifie la mise √† l'√©chelle de la fr√©quence pour l'encodage positionnel rotatif pour contr√¥ler la granularit√© de l'encodage positionnel",
  "embedding.load.llama.acceleration.offloadRatio/title": "D√©chargement GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Nombre de couches discr√®tes du mod√®le √† calculer sur le GPU pour l'acc√©l√©ration GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "D√©finir le nombre de couches √† d√©charger vers le GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Garder le mod√®le en m√©moire",
  "embedding.load.llama.keepModelInMemory/subTitle": "R√©server la m√©moire syst√®me pour le mod√®le, m√™me quand il est d√©charg√© sur le GPU. Am√©liore les performances mais n√©cessite plus de RAM syst√®me",
  "embedding.load.llama.keepModelInMemory/info": "Emp√™che le mod√®le d'√™tre d√©charg√© sur le disque, assurant un acc√®s plus rapide au co√ªt d'une utilisation plus √©lev√©e de la RAM",
  "embedding.load.llama.tryMmap/title": "Essayer mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Am√©liore le temps de chargement du mod√®le. D√©sactiver ceci peut am√©liorer les performances quand le mod√®le est plus grand que la RAM syst√®me disponible",
  "embedding.load.llama.tryMmap/info": "Charger les fichiers du mod√®le directement du disque vers la m√©moire",
  "embedding.load.seed/title": "Graine",
  "embedding.load.seed/subTitle": "La graine pour le g√©n√©rateur de nombres al√©atoires utilis√© dans la g√©n√©ration de texte. -1 est une graine al√©atoire",
  "embedding.load.seed/info": "Graine al√©atoire : D√©finit la graine pour la g√©n√©ration de nombres al√©atoires pour assurer des r√©sultats reproductibles",

  "presetTooltip": {
    "included/title": "Valeurs pr√©d√©finies",
    "included/description": "Les champs suivants seront appliqu√©s",
    "included/empty": "Aucun champ de ce pr√©r√©glage ne s'applique dans ce contexte.",
    "included/conflict": "Vous devrez choisir si vous souhaitez appliquer cette valeur",
    "separateLoad/title": "Configuration au chargement",
    "separateLoad/description.1": "Le pr√©r√©glage inclut √©galement la configuration de chargement suivante. La configuration au chargement s'applique √† l'ensemble du mod√®le et n√©cessite un rechargement du mod√®le pour prendre effet. Maintenez",
    "separateLoad/description.2": "pour appliquer √†",
    "separateLoad/description.3": ".",
    "excluded/title": "Peut ne pas s'appliquer",
    "excluded/description": "Les champs suivants sont inclus dans le pr√©r√©glage mais ne s'appliquent pas dans le contexte actuel.",
    "legacy/title": "Pr√©r√©glage h√©rit√©",
    "legacy/description": "Ce pr√©r√©glage est un pr√©r√©glage h√©rit√©. Il inclut les champs suivants qui sont soit g√©r√©s automatiquement maintenant, soit ne sont plus applicables."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Vide>"
    },
    "checkboxNumeric": {
      "off": "D√âSACTIV√â"
    },
    "stringArray": {
      "empty": "<Vide>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Mod√®le (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Mod√®le",
      "jinja/error": "√âchec de l'analyse du mod√®le Jinja : {{error}}",
      "jinja/empty": "Veuillez entrer un mod√®le Jinja ci-dessus.",
      "jinja/unlikelyToWork": "Le mod√®le Jinja que vous avez fourni ci-dessus est peu susceptible de fonctionner car il ne fait pas r√©f√©rence √† la variable \"messages\". Veuillez v√©rifier si vous avez entr√© un mod√®le correct.",
      "types.manual/label": "Manuel",
      "manual.subfield.beforeSystem/label": "Avant Syst√®me",
      "manual.subfield.beforeSystem/placeholder": "Entrez le pr√©fixe Syst√®me...",
      "manual.subfield.afterSystem/label": "Apr√®s Syst√®me",
      "manual.subfield.afterSystem/placeholder": "Entrez le suffixe Syst√®me...",
      "manual.subfield.beforeUser/label": "Avant Utilisateur",
      "manual.subfield.beforeUser/placeholder": "Entrez le pr√©fixe Utilisateur...",
      "manual.subfield.afterUser/label": "Apr√®s Utilisateur",
      "manual.subfield.afterUser/placeholder": "Entrez le suffixe Utilisateur...",
      "manual.subfield.beforeAssistant/label": "Avant Assistant",
      "manual.subfield.beforeAssistant/placeholder": "Entrez le pr√©fixe Assistant...",
      "manual.subfield.afterAssistant/label": "Apr√®s Assistant",
      "manual.subfield.afterAssistant/placeholder": "Entrez le suffixe Assistant...",
      "stopStrings/label": "Cha√Ænes d'arr√™t suppl√©mentaires",
      "stopStrings/subTitle": "Cha√Ænes d'arr√™t sp√©cifiques au mod√®le qui seront utilis√©es en plus des cha√Ænes d'arr√™t sp√©cifi√©es par l'utilisateur."
    },
    "contextLength": {
      "maxValueTooltip": "C'est le nombre maximum de tokens pour lequel le mod√®le a √©t√© entra√Æn√©. Cliquez pour d√©finir le contexte √† cette valeur",
      "maxValueTextStart": "Le mod√®le prend en charge jusqu'√†",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "Bien qu'un mod√®le puisse prendre en charge jusqu'√† un certain nombre de tokens, les performances peuvent se d√©grader si les ressources de votre machine ne peuvent pas g√©rer la charge - soyez prudent lorsque vous augmentez cette valeur"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Arr√™ter √† la limite",
      "stopAtLimitSub": "Arr√™ter la g√©n√©ration une fois que la m√©moire du mod√®le est pleine",
      "truncateMiddle": "Tronquer le milieu",
      "truncateMiddleSub": "Supprime les messages du milieu de la conversation pour faire de la place aux nouveaux. Le mod√®le se souviendra toujours du d√©but de la conversation",
      "rollingWindow": "Fen√™tre glissante",
      "rollingWindowSub": "Le mod√®le obtiendra toujours les messages les plus r√©cents mais peut oublier le d√©but de la conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "D√âSACTIV√â"
    }
  },
  "saveConflictResolution": {
    "title": "Choisissez quelles valeurs inclure dans le pr√©r√©glage",
    "description": "Choisissez les valeurs √† conserver",
    "instructions": "Cliquez sur une valeur pour l'inclure",
    "userValues": "Valeur pr√©c√©dente",
    "presetValues": "Nouvelle valeur",
    "confirm": "Confirmer",
    "cancel": "Annuler"
  },
  "applyConflictResolution": {
    "title": "Quelles valeurs conserver ?",
    "description": "Vous avez des changements non valid√©s qui se chevauchent avec le pr√©r√©glage entrant",
    "instructions": "Cliquez sur une valeur pour la conserver",
    "userValues": "Valeur actuelle",
    "presetValues": "Valeur du pr√©r√©glage entrant",
    "confirm": "Confirmer",
    "cancel": "Annuler"
  },
  "empty": "<Vide>",
  "presets": {
    "title": "Pr√©r√©glage",
    "commitChanges": "Valider les modifications",
    "commitChanges/description": "Valider vos modifications dans le pr√©r√©glage.",
    "commitChanges.manual": "Nouveaux champs d√©tect√©s. Vous pourrez choisir quelles modifications inclure dans le pr√©r√©glage.",
    "commitChanges.manual.hold.0": "Maintenez",
    "commitChanges.manual.hold.1": "pour choisir quelles modifications valider dans le pr√©r√©glage.",
    "commitChanges.saveAll.hold.0": "Maintenez",
    "commitChanges.saveAll.hold.1": "pour sauvegarder toutes les modifications.",
    "commitChanges.saveInPreset.hold.0": "Maintenez",
    "commitChanges.saveInPreset.hold.1": "pour ne sauvegarder que les modifications des champs d√©j√† inclus dans le pr√©r√©glage.",
    "commitChanges/error": "√âchec de la validation des modifications dans le pr√©r√©glage.",
    "commitChanges.manual/description": "Choisissez quelles modifications inclure dans le pr√©r√©glage.",
    "saveAs": "Enregistrer sous...",
    "presetNamePlaceholder": "Entrez un nom pour le pr√©r√©glage...",
    "cannotCommitChangesLegacy": "Ceci est un pr√©r√©glage h√©rit√© et ne peut pas √™tre modifi√©. Vous pouvez cr√©er une copie en utilisant \"Enregistrer sous...\".",
    "cannotCommitChangesNoChanges": "Aucune modification √† valider.",
    "emptyNoUnsaved": "S√©lectionnez un pr√©r√©glage...",
    "emptyWithUnsaved": "Pr√©r√©glage non sauvegard√©",
    "saveEmptyWithUnsaved": "Enregistrer le pr√©r√©glage sous...",
    "saveConfirm": "Enregistrer",
    "saveCancel": "Annuler",
    "saving": "Enregistrement...",
    "save/error": "√âchec de l'enregistrement du pr√©r√©glage.",
    "deselect": "D√©s√©lectionner le pr√©r√©glage",
    "deselect/error": "√âchec de la d√©s√©lection du pr√©r√©glage.",
    "select/error": "√âchec de la s√©lection du pr√©r√©glage.",
    "delete/error": "√âchec de la suppression du pr√©r√©glage.",
    "discardChanges": "Abandonner les modifications non sauvegard√©es",
    "discardChanges/info": "Abandonner toutes les modifications non valid√©es et restaurer le pr√©r√©glage √† son √©tat d'origine",
    "newEmptyPreset": "Cr√©er un nouveau pr√©r√©glage vide...",
    "contextMenuSelect": "S√©lectionner le pr√©r√©glage",
    "contextMenuDelete": "Supprimer"
  },

  "flashAttentionWarning": "L'attention Flash est une fonctionnalit√© exp√©rimentale qui peut causer des probl√®mes avec certains mod√®les. Sivous rencontrez des probl√®mes, essayez de la d√©sactiver.",

  "seedUncheckedHint": "Graine al√©atoire",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}