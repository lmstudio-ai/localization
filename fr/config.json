{
  "noInstanceSelected": "Aucune instance de modèle sélectionnée",
  "resetToDefault": "Réinitialiser",
  "showAdvancedSettings": "Afficher les paramètres avancés",
  "showAll": "Afficher tout",
  "basicSettings": "De base",
  "configSubtitle": "Charger ou enregistrer des préréglages et expérimenter avec les paramètres de modèle",
  "inferenceParameters/title": "Paramètres de prédiction",
  "inferenceParameters/info": "Expérimentez avec des paramètres qui influencent la prédiction.",
  "generalParameters/title": "Général",
  "samplingParameters/title": "Échantillonnage",
  "basicTab": "De base",
  "advancedTab": "Avancé",
  "loadInstanceFirst": "Chargez un modèle pour voir les paramètres configurables",
  "noListedConfigs": "Aucun paramètre configurable",
  "generationParameters/info": "Expérimentez avec les paramètres de base qui influencent la génération de texte.",
  "loadParameters/title": "Charger les paramètres",
  "loadParameters/description": "Modifier ces paramètres nécessite un rechargement du modèle",
  "loadParameters/reload": "Recharger pour appliquer les modifications des paramètres de chargement",
  "discardChanges": "Annuler les modifications",
  "llm.prediction.systemPrompt/title": "Directives pour l'IA",
  "llm.prediction.systemPrompt/description": "Utilisez ce champ pour fournir des instructions de base au modèle, telles qu'un ensemble de règles, de contraintes ou d'exigences générales. Ce champ est également souvent appelé « invite système ».",
  "llm.prediction.temperature/title": "Température",
  "llm.prediction.temperature/info": "D'après les documents d'aide de llama.cpp : « La valeur par défaut est <{{dynamicValue}}>, qui offre un équilibre entre aléatoire et déterminisme. À l'extrême, une température de 0 choisira toujours le token le plus probable, ce qui conduira à des sorties identiques à chaque exécution »",
  "llm.prediction.topKSampling/title": "Échantillonnage Top K",
  "llm.prediction.topKSampling/info": "D'après les documents d'aide de llama.cpp :\n\nL'échantillonnage top-k est une méthode de génération de texte qui sélectionne le prochain token uniquement parmi les k tokens les plus probables prédits par le modèle.\n\nCela aide à réduire le risque de générer des tokens de faible probabilité ou absurdes, mais peut également limiter la diversité des sorties.\n\nUne valeur plus élevée pour top-k (par exemple, 100) prendra en compte plus de tokens et conduira à un texte plus diversifié, tandis qu'une valeur plus faible (par exemple, 10) se concentrera sur les tokens les plus probables et générera un texte plus conservateur.\n\n• La valeur par défaut est <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads CPU",
  "llm.prediction.llama.cpuThreads/info": "Le nombre de threads à utiliser pendant le calcul. Augmenter le nombre de threads ne corrèle pas toujours avec de meilleures performances. La valeur par défaut est <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limiter la longueur de réponse",
  "llm.prediction.maxPredictedTokens/info": "Contrôlez la longueur maximale de la réponse du chatbot. Activez pour définir une limite à la longueur maximale d'une réponse, ou désactivez pour laisser le chatbot décider quand s'arrêter.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longueur maximale de la réponse (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Environ {{maxWords}} mots",
  "llm.prediction.repeatPenalty/title": "Pénalité de répétition",
  "llm.prediction.repeatPenalty/info": "D'après les documents d'aide de llama.cpp : « Aide à empêcher le modèle de générer du texte répétitif ou monotone.\n\nUne valeur plus élevée (par exemple, 1,5) pénalisera plus fortement les répétitions, tandis qu'une valeur plus faible (par exemple, 0,9) sera plus indulgente. » • La valeur par défaut est <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Échantillonnage Min P",
  "llm.prediction.minPSampling/info": "D'après les documents d'aide de llama.cpp :\n\nLa probabilité minimale pour qu'un token soit considéré, relative à la probabilité du token le plus probable. Doit être dans [0, 1].\n\n• La valeur par défaut est <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Échantillonnage Top P",
  "llm.prediction.topPSampling/info": "D'après les documents d'aide de llama.cpp :\n\nL'échantillonnage top-p, également connu sous le nom d'échantillonnage de nucleus, est une autre méthode de génération de texte qui sélectionne le prochain token parmi un sous-ensemble de tokens dont la probabilité cumulative est d'au moins p.\n\nCette méthode offre un équilibre entre diversité et qualité en considérant à la fois les probabilités des tokens et le nombre de tokens à échantillonner.\n\nUne valeur plus élevée pour top-p (par exemple, 0,95) conduira à un texte plus diversifié, tandis qu'une valeur plus faible (par exemple, 0,5) générera un texte plus focalisé et conservateur. Doit être dans (0, 1].\n\n• La valeur par défaut est <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Chaînes d'arrêt",
  "llm.prediction.stopStrings/info": "Chaînes spécifiques qui, lorsqu'elles sont rencontrées, arrêteront le modèle de générer plus de tokens",
  "llm.prediction.stopStrings/placeholder": "Entrez une chaîne et appuyez sur ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Débordement de conversation",
  "llm.prediction.contextOverflowPolicy/info": "Décidez quoi faire lorsque la conversation dépasse la taille de la mémoire de travail du modèle ('contexte')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "S'arrêter à la limite",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Arrêtez de générer une fois que la mémoire du modèle est pleine",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Tronquer le milieu",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Supprime les messages du milieu de la conversation pour faire de la place aux plus récents. Le modèle se souviendra toujours du début de la conversation",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Fenêtre coulissante",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "Le modèle recevra toujours les quelques messages les plus récents mais pourra oublier le début de la conversation",
  "llm.prediction.llama.frequencyPenalty/title": "Pénalité de fréquence",
  "llm.prediction.llama.presencePenalty/title": "Pénalité de présence",
  "llm.prediction.llama.tailFreeSampling/title": "Échantillonnage sans queue",
  "llm.prediction.llama.locallyTypicalSampling/title": "Échantillonnage typique local",
  "llm.prediction.onnx.topKSampling/title": "Échantillonnage Top K",
  "llm.prediction.onnx.topKSampling/info": "D'après la documentation ONNX :\n\nNombre de tokens de vocabulaire avec la probabilité la plus élevée à conserver pour le filtrage top-k\n\n• Ce filtre est désactivé par défaut",
  "llm.prediction.onnx.repeatPenalty/title": "Pénalité de répétition",
  "llm.prediction.onnx.repeatPenalty/info": "Une valeur plus élevée décourage le modèle de se répéter",
  "llm.prediction.onnx.topPSampling/title": "Échantillonnage Top P",
  "llm.prediction.onnx.topPSampling/info": "D'après la documentation ONNX :\n\nSeuls les tokens les plus probables dont les probabilités s'additionnent à TopP ou plus sont conservés pour la génération\n\n• Ce filtre est désactivé par défaut",
  "llm.prediction.seed/title": "Graine",
  "llm.prediction.structured/title": "Sortie structurée",
  "llm.prediction.structured/info": "Sortie structurée",
  "llm.load.contextLength/title": "Longueur du contexte",
  "llm.load.contextLength/info": "Spécifie le nombre maximal de tokens que le modèle peut considérer à la fois, influençant la quantité de contexte qu'il retient pendant le traitement",
  "llm.load.seed/title": "Graine",
  "llm.load.seed/info": "Graine aléatoire : Définit la graine pour la génération de nombres aléatoires afin d'assurer des résultats reproductibles",
  "llm.load.llama.evalBatchSize/title": "Taille du lot d'évaluation",
  "llm.load.llama.evalBatchSize/info": "Définit le nombre d'exemples traités ensemble dans un lot pendant l'évaluation, affectant la vitesse et l'utilisation de la mémoire",
  "llm.load.llama.ropeFrequencyBase/title": "Base de fréquence RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Avancé] Ajuste la fréquence de base pour le codage positionnel rotatif, affectant la manière dont l'information positionnelle est intégrée",
  "llm.load.llama.ropeFrequencyScale/title": "Échelle de fréquence RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avancé] Modifie l'échelle de fréquence pour le codage positionnel rotatif pour contrôler la granularité du codage positionnel",
  "llm.load.llama.acceleration.offloadRatio/title": "Déchargement GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Définissez le ratio de calcul à décharger sur le GPU. Désactivez pour désactiver le déchargement GPU, ou auto pour laisser le modèle décider.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/info": "Accélère les mécanismes d'attention pour un traitement plus rapide et plus efficace",
  "llm.load.llama.keepModelInMemory/title": "Garder le modèle en mémoire",
  "llm.load.llama.keepModelInMemory/info": "Empêche le modèle d'être échangé sur disque, assurant un accès plus rapide au coût d'une utilisation accrue de la RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Utiliser FP16 pour le cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Réduit l'utilisation de la mémoire en stockant le cache en demi-précision (FP16)",
  "llm.load.llama.tryMmap/title": "Essayer mmap()",
  "llm.load.llama.tryMmap/info": "Chargez les fichiers de modèle directement du disque vers la mémoire"
}
