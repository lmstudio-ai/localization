{
  "tabs/server": "Serveur Local",
  "tabs/extensions": "Environnements d'exécution LM",
  "loadSettings/title": "Charger les paramètres",
  "modelSettings/placeholder": "Sélectionnez un modèle pour le configurer",

  "loadedModels/noModels": "Aucun modèle chargé",

  "serverOptions/title": "Options du serveur",
  "serverOptions/configurableTitle": "Options configurables",
  "serverOptions/port/hint": "Définissez le port réseau que le serveur local utilisera. Par défaut, LM Studio utilise le port 1234. Vous devrez peut-être le modifier si le port est déjà utilisé.",
  "serverOptions/port/subtitle": "Le port d'écoute",
  "serverOptions/autostart/title": "Démarrage automatique du serveur",
  "serverOptions/autostart/hint": "Démarrer le serveur local automatiquement lorsqu'un modèle est chargé",
  "serverOptions/port/integerWarning": "Le numéro de port doit être un nombre entier",
  "serverOptions/port/invalidPortWarning": "Le port doit être compris entre 1 et 65535",
  "serverOptions/cors/title": "Activer CORS",
  "serverOptions/cors/hint1": "L'activation de CORS (Cross-origin Resource Sharing) permettrait aux sites web que vous visitez de faire des requêtes au serveur LM Studio.",
  "serverOptions/cors/hint2": "CORS peut être nécessaire lors de requêtes depuis une page web ou VS Code / autre extension.",
  "serverOptions/cors/subtitle": "Autoriser les requêtes cross-origin",
  "serverOptions/network/title": "Servir sur le réseau local",
  "serverOptions/network/subtitle": "Exposer le serveur aux appareils sur le réseau",
  "serverOptions/network/hint1": "Autoriser ou non les connexions depuis d'autres appareils sur le réseau.",
  "serverOptions/network/hint2": "Si non coché, le serveur n'écoutera que sur localhost.",
  "serverOptions/verboseLogging/title": "Journalisation détaillée",
  "serverOptions/verboseLogging/subtitle": "Activer la journalisation détaillée pour le serveur local",
  "serverOptions/contentLogging/title": "Journaliser les requêtes et réponses",
  "serverOptions/contentLogging/subtitle": "Paramètres de journalisation des requêtes/réponses locales",
  "serverOptions/contentLogging/hint": "Détermine si les requêtes et/ou les réponses doivent être enregistrées dans le fichier journal du serveur local.",
  "serverOptions/jitModelLoading/title": "Chargement de modèle à la demande",
  "serverOptions/jitModelLoading/hint": "Lorsque activé, si une requête spécifie un modèle qui n'est pas chargé, il sera automatiquement chargé et utilisé. De plus, l'endpoint '/v1/models' inclura également les modèles qui ne sont pas encore chargés.",
  "serverOptions/loadModel/error": "Échec du chargement du modèle",

  "serverLogs/scrollToBottom": "Aller en bas",
  "serverLogs/clearLogs": "Effacer les journaux ({{shortcut}})",
  "serverLogs/openLogsFolder": "Ouvrir le dossier des journaux du serveur",

  "runtimeSettings/title": "Paramètres d'environnement d'exécution",
  "runtimeSettings/chooseRuntime/title": "Configurer les environnements d'exécution",
  "runtimeSettings/chooseRuntime/description": "Sélectionnez un environnement d'exécution pour chaque format de modèle",
  "runtimeSettings/chooseRuntime/showAllVersions/label": "Afficher tous les environnements d'exécution",
  "runtimeSettings/chooseRuntime/showAllVersions/hint": "Par défaut, LM Studio n'affiche que la dernière version de chaque environnement d'exécution compatible. Activez cette option pour voir tous les environnements disponibles.",
  "runtimeSettings/chooseRuntime/select/placeholder": "Sélectionner un environnement d'exécution",

  "runtimeOptions/uninstall": "Désinstaller",
  "runtimeOptions/uninstallDialog/title": "Désinstaller {{runtimeName}} ?",
  "runtimeOptions/uninstallDialog/body": "La désinstallation de cet environnement d'exécution le supprimera du système. Cette action est irréversible.",
  "runtimeOptions/uninstallDialog/body/caveats": "Certains fichiers ne pourront être supprimés qu'après le redémarrage de LM Studio.",
  "runtimeOptions/uninstallDialog/error": "Échec de la désinstallation de l'environnement d'exécution",
  "runtimeOptions/uninstallDialog/confirm": "Continuer et désinstaller",
  "runtimeOptions/uninstallDialog/cancel": "Annuler",
  "runtimeOptions/noCompatibleRuntimes": "Aucun environnement d'exécution compatible trouvé",
  "runtimeOptions/downloadIncompatibleRuntime": "Cet environnement d'exécution a été jugé incompatible avec votre machine. Il ne fonctionnera probablement pas.",
  "runtimeOptions/noRuntimes": "Aucun environnement d'exécution trouvé",

  "inferenceParams/noParams": "Aucun paramètre d'inférence configurable disponible pour ce type de modèle",

  "endpoints/openaiCompatRest/title": "Points d'accès pris en charge (type OpenAI)",
  "endpoints/openaiCompatRest/getModels": "Lister les modèles actuellement chargés",
  "endpoints/openaiCompatRest/postCompletions": "Mode Complétion de texte. Prédit le(s) prochain(s) token(s) étant donné une invite. Note : OpenAI considère ce point d'accès comme 'déprécié'.",
  "endpoints/openaiCompatRest/postChatCompletions": "Complétion de chat. Envoie un historique de chat au modèle pour prédire la prochaine réponse de l'assistant",
  "endpoints/openaiCompatRest/postEmbeddings": "Embedding de texte. Génère des embeddings de texte pour une entrée donnée. Accepte une chaîne ou un tableau de chaînes.",

  "model.createVirtualModelFromInstance": "Enregistrer les paramètres comme nouveau modèle virtuel",
  "model.createVirtualModelFromInstance/error": "Échec de l'enregistrement des paramètres comme nouveau modèle virtuel",

  "apiConfigOptions/title": "Configuration de l'API"
}