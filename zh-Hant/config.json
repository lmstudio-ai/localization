{
    "noInstanceSelected": "未選擇模型實例",
    "resetToDefault": "重置",
    "showAdvancedSettings": "顯示進階設定",
    "showAll": "全部",
    "basicSettings": "基本",
    "configSubtitle": "載入或儲存預設值，並試用模型參數覆蓋",
    "inferenceParameters/title": "預測參數",
    "inferenceParameters/info": "試用影響預測的參數。",
    "generalParameters/title": "一般",
    "samplingParameters/title": "抽樣",
    "basicTab": "基本",
    "advancedTab": "進階",
    "advancedTab/title": "🧪 高級配置",
    "advancedTab/expandAll": "全部展開",
    "advancedTab/overridesTitle": "配置覆蓋",
    "advancedTab/noConfigsText": "您沒有未儲存的變更 - 修改上方值以查看這裡的覆蓋設定。",
    "loadInstanceFirst": "載入模型以查看可配置參數",
    "noListedConfigs": "無可配置參數",
    "generationParameters/info": "試用影響文本生成的基本參數。",
    "loadParameters/title": "載入參數",
    "loadParameters/description": "控制模型初始化和載入記憶的設定。",
    "loadParameters/reload": "重新載入以應用變更",
    "loadParameters/reload/error": "無法重新載入模型",
    "discardChanges": "放棄變更",
    "loadModelToSeeOptions": "載入模型以查看選項",
    "schematicsError.title": "配置圖示包含以下欄位的錯誤：",
    "manifestSections": {
        "structuredOutput/title": "結構化輸出",
        "speculativeDecoding/title": "預測解碼",
        "sampling/title": "抽樣",
        "settings/title": "設定",
        "toolUse/title": "工具使用",
        "promptTemplate/title": "提示模板"
    },
    "llm.prediction.systemPrompt/title": "系統提示",
    "llm.prediction.systemPrompt/description": "請在這個欄位提供背景說明給模型，例如一組規則、限制或一般要求。",
    "llm.prediction.systemPrompt/subTitle": "人工智能的指導原則",
    "llm.prediction.temperature/title": "溫度",
    "llm.prediction.temperature/subTitle": "要引入多少隨機性。0會每次產生相同結果，而較高的值會增加創造性與變異性",
    "llm.prediction.temperature/info": "從 llama.cpp 助手文件說明：「預設值為 <{{dynamicValue}}>，在隨機性與確定性之間取得平衡。在極端情況下，溫度為 0 時會總是選擇最可能的下個 token，導致每次執行的結果都相同」",
    "llm.prediction.llama.sampling/title": "取樣",
    "llm.prediction.topKSampling/title": "Top K 取樣",
    "llm.prediction.topKSampling/subTitle": "限制下個 token 只在前 k 個最可能的 tokens 中選擇。效果類似於溫度參數",
    "llm.prediction.topKSampling/info": "9.，來自llama.cpp幫助文件：\n8.，\n7. Top-k 搜尋是文本生成的技術，會從模型預測的最可能 top k 個標記中選擇下個標記。\n6.，\n5. 有助於降低生成低機率或不合邏輯的標記的風險，但會限制輸出的樣本多樣性。\n4.，\n3. Top-k 的值越高（例如 100），考慮的標記越多，產生的文本越多元；值越低（例如 10），會聚焦最可能的標記，生成更保守的文本。\n2.，\n1. • 預設值為 <{{dynamicValue}}>",
    "llm.prediction.llama.cpuThreads/title": "CPU線數",
    "llm.prediction.llama.cpuThreads/subTitle": "推論期間使用的CPU線數",
    "llm.prediction.llama.cpuThreads/info": "計算期間使用的線數。增加線數不 всегда 會提升效能。默認值為 <{{dynamicValue}}>.",
    "llm.prediction.maxPredictedTokens/title": "限制回應長度",
    "llm.prediction.maxPredictedTokens/subTitle": "可選擇限制AI的回應長度",
    "llm.prediction.maxPredictedTokens/info": "控制聊天機器人的回應最大長度。啟用後設定回應長度限制，關閉則由聊天機器人決定停止時機。",
    "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（記憶體元）",
    "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 個字",
    "llm.prediction.repeatPenalty/title": "重複懲罰",
    "llm.prediction.repeatPenalty/subTitle": "不應重複相同記憶體元的程度",
    "llm.prediction.repeatPenalty/info": "從 llama.cpp 資料說明：「幫助防止模型生成重複或單調的文本。」\n\n較高的值（例如 1.5）會對重複強烈懲罰，較低的值（例如 0.9）則較寬鬆。• 默認值為 <{{dynamicValue}}>",
    "llm.prediction.minPSampling/title": "最小 P採樣",
    "llm.prediction.minPSampling/subTitle": "令牌被選為輸出的最低基礎機率",
    "llm.prediction.minPSampling/info": "從 llama.cpp 資料說明：\n\n相對於最可能令牌的機率，令牌被考慮的最低機率。必須在 [0, 1] 內。\n\n• 默認值為 <{{dynamicValue}}>",
    "llm.prediction.topPSampling/title": "Top P採樣",
    "llm.prediction.topPSampling/subTitle": "可能的下一個令牌的最小累計機率。作用類似溫度",
    "llm.prediction.topPSampling/info": "9.，來自llama.cpp幫助文件：\n8.，\n7.，Top-p抽樣，也稱為核心抽樣，是另一種文本生成方法，會從總概率至少為p的token子集中小概率選擇下一個token。\n6.，\n5.，此方法透過考量token概率與抽取token數量，在多樣性與品質之間取得平衡。\n4.，\n3.，Top-p值越高（例如0.95），生成的文本越多元；值越低（例如0.5），則會產生更聚焦且保守的文本。必須在(0, 1]之間。\n2.，\n1.，• 預設值為<{{dynamicValue}}>",
    "llm.prediction.stopStrings/title": "停止字符串",
    "llm.prediction.stopStrings/subTitle": "需要讓模型停止生成更多標點的字符串",
    "llm.prediction.stopStrings/info": "當遇到特定字符串時，應讓模型停止生成更多標點",
    "llm.prediction.stopStrings/placeholder": "輸入一個字符串並按下 Enter 鍵",
    "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
    "llm.prediction.contextOverflowPolicy/subTitle": "當對話內容超出模型處理能力時，模型應採取的行爲",
    "llm.prediction.contextOverflowPolicy/info": "設定當對話長度超出模型工作記憶範圍（'上下文'）時的處理方式",
    "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
    "llm.prediction.llama.presencePenalty/title": "存在懲罰",
    "llm.prediction.llama.tailFreeSampling/title": "尾部無效採樣",
    "llm.prediction.llama.locallyTypicalSampling/title": "局部典型採樣",
    "llm.prediction.llama.xtcProbability/title": "XTC採樣概率",
    "llm.prediction.llama.xtcProbability/subTitle": "XTC（排除最優選）採樣器在每個生成的_token_時，只會在特定概率下啟動。XTC採樣能提升創造力，並減少陳詞濫調",
    "llm.prediction.llama.xtcProbability/info": "XTC（排除最優選）採樣器在每個生成的_token_時，只會在特定概率下啟動。XTC採樣通常能提升創造力，並減少陳詞濫調",
    "llm.prediction.llama.xtcThreshold/title": "XTC採樣門檻",
    "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除最優選）門檻。在`xtc-probability`的機率下，搜尋概率在`xtc-threshold`與0.5之間的_token_，並移除除了最不可能的那個外的所有這種_token_",
    "llm.prediction.llama.xtcThreshold/info": "XTC（排除最佳選擇）閾值。當有 `xtc-probability` 機率時，搜尋概率在 `xtc-threshold` 與 0.5 之間的 token，並只保留最不可能的那個 token",
    "llm.prediction.mlx.topKSampling/title": "第2. 頂部K抽樣",
    "llm.prediction.mlx.topKSampling/subTitle": "限制下個標記只能是前k個最可能的標記中的一種。類似於溫度參數",
    "llm.prediction.mlx.topKSampling/info": "限制下個標記只能是前k個最可能的標記中的一種。類似於溫度參數",
    "llm.prediction.onnx.topKSampling/title": "Top K 抽樣",
    "llm.prediction.onnx.topKSampling/subTitle": "限制下個標記只能是前k個最可能的標記中的一種。類似於溫度參數",
    "llm.prediction.onnx.topKSampling/info": "來自ONNX文檔：\n\n保留給Top K過濾的最高概率詞彙標記數\n\n• 此過濾器預設為關閉",
    "llm.prediction.onnx.repeatPenalty/title": "重複懲罰係數",
    "llm.prediction.onnx.repeatPenalty/subTitle": "多少程度地 discourages 模型重複相同標記",
    "llm.prediction.onnx.repeatPenalty/info": "數值越高，會減少模型重複自身",
    "llm.prediction.onnx.topPSampling/title": "Top P 抽樣",
    "llm.prediction.onnx.topPSampling/subTitle": "可能的下個標記的最小累計概率。類似於溫度參數",
    "llm.prediction.onnx.topPSampling/info": "來自ONNX文檔：\n\n只保留概率總和達到TopP或以上的最可能標記\n\n• 此過濾器預設為關閉",
    "llm.prediction.seed/title": "種子",
    "llm.prediction.structured/title": "結構化輸出",
    "llm.prediction.structured/info": "結構化輸出",
    "llm.prediction.structured/description": "進階：您可提供[JSON Schema](https://json-schema.org/learn/miscellaneous-examples)來強制模型遵循特定輸出格式。閱讀[文檔](https://lmstudio.ai/docs/advanced/structured-output)瞭解更多",
    "llm.prediction.tools/title": "工具使用",
    "llm.prediction.tools/description": "進階：您可提供符合JSON格式的工具清單，讓模型請求調用。閱讀[文檔](https://lmstudio.ai/docs/advanced/tool-use)瞭解更多",
    "llm.prediction.tools/serverPageDescriptionAddon": "在使用伺服器API時，以`tools`標籤傳遞此資料",
    "llm.prediction.promptTemplate/title": "提示模板",
    "llm.prediction.promptTemplate/subTitle": "聊天訊息傳達給模型的格式。更改此設定可能導致不預期的行為，請確認您知道在做什麼！",
    "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "草稿生成標記數",
    "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "每個主模型標記生成草稿標記的數量。找到計算與獎勵的平衡點",
    "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿概率閥值",
    "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "繼續草稿直到某個標記的機率低於此閥值。數值越高，風險越低，獎勵越低",
    "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿大小",
    "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "草稿小於此值將被主模型忽略。數值越高，風險越低，獎勵越低",
    "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿大小",
    "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "設定草案允許的最大標記數。若所有標記概率均高於門檻值，則會觸發上限。較低的值通常代表較低風險，但獲利也相對減少",
    "llm.prediction.speculativeDecoding.draftModel/title": "草稿模型",
    "llm.prediction.reasoning.parsing/title": "推理段解析",
    "llm.prediction.reasoning.parsing/subTitle": "設定模型輸出中推理段的解析方式",
    "llm.load.contextLength/title": "上下文長度",
    "llm.load.contextLength/subTitle": "模型可同時處理的標記數最大值。請參閱「推理參數」下的「對話溢出」選項以瞭解更管理方式",
    "llm.load.contextLength/info": "指定模型可同時考量的標記數最大值，影響處理過程中能保留的上下文量",
    "llm.load.contextLength/warning": "設定過高的上下文長度會大幅影響記憶體使用",
    "llm.load.seed/title": "種子",
    "llm.load.seed/subTitle": "文本生成所使用的隨機數發生器種子。-1 表示隨機",
    "llm.load.seed/info": "隨機種子：設定隨機數生成的種子以確保結果可重現",
    "llm.load.llama.evalBatchSize/title": "驗證批次大小",
    "llm.load.llama.evalBatchSize/subTitle": "每次處理的輸入標記數。增加此值有助提升效能，但會增加記憶體使用",
    "llm.load.llama.evalBatchSize/info": "設定評估時每批次同時處理的範例數，影響運算速度與記憶體使用",
    "llm.load.llama.ropeFrequencyBase/title": "RoPE 頻率基礎",
    "llm.load.llama.ropeFrequencyBase/subTitle": "自訂 RoPE 頻率基礎。增加此值可能有助於在高上下文長度下取得更好表現",
    "llm.load.llama.ropeFrequencyBase/info": "[進階] 調整 RoPE 的基頻，影響位置資訊嵌入方式",
    "llm.load.llama.ropeFrequencyScale/title": "RoPE 頻率縮放",
    "llm.load.llama.ropeFrequencyScale/subTitle": "上下文長度會乘以此比例因子，以 RoPE 方式延長有效上下文",
    "llm.load.llama.ropeFrequencyScale/info": "[進階] 調整 RoPE 頻率縮放，以控制位置嵌入細節",
    "llm.load.llama.acceleration.offloadRatio/title": "GPU 註冊",
    "llm.load.llama.acceleration.offloadRatio/subTitle": "指定在 GPU 上計算的離散模型層數，以提升 GPU 加速效能",
    "llm.load.llama.acceleration.offloadRatio/info": "設定要移至 GPU 的層數",
    "llm.load.llama.flashAttention/title": "Flash 注意力",
    "llm.load.llama.flashAttention/subTitle": "可減少部分模型的記憶體使用與生成時間",
    "llm.load.llama.flashAttention/info": "加快注意力機制，以達成更快更有效的處理",
    "llm.load.numExperts/title": "專家數",
    "llm.load.numExperts/subTitle": "模型使用的專家數",
    "llm.load.numExperts/info": "模型使用的專家數",
    "llm.load.llama.keepModelInMemory/title": "保留模型記憶體",
    "llm.load.llama.keepModelInMemory/subTitle": "即使移至 GPU，仍保留系統記憶體供模型使用。有助提升效能，但需要更多系統 RAM",
    "llm.load.llama.keepModelInMemory/info": "避免模型被換出至硬碟，確保更快的存取速度，但會增加 RAM 使用",
    "llm.load.llama.useFp16ForKVCache/title": "使用FP16存儲KV緩衝區",
    "llm.load.llama.useFp16ForKVCache/info": "以半精度浮點數（FP16）儲存緩衝區可減少記憶體使用",
    "llm.load.llama.tryMmap/title": "尋求mmap()功能",
    "llm.load.llama.tryMmap/subTitle": "改善模型載入時間。當模型大小超出系統記憶體時，禁用此功能可能提升運算效能",
    "llm.load.llama.tryMmap/info": "直接從硬碟載入模型檔案至記憶體",
    "llm.load.llama.cpuThreadPoolSize/title": "CPU執行緒池大小",
    "llm.load.llama.cpuThreadPoolSize/subTitle": "配置用於模型計算的執行緒池的CPU執行緒數量",
    "llm.load.llama.cpuThreadPoolSize/info": "配置用於模型計算的執行緣池的CPU執行緣數量。增加執行緣數未必能提升效能。預設值為<{{dynamicValue}}>",
    "llm.load.llama.kCacheQuantizationType/title": "KV緩衝區量化類型",
    "llm.load.llama.kCacheQuantizationType/subTitle": "值越小記憶體使用越低，但可能降低輸出品質。效果因模型而異",
    "llm.load.llama.vCacheQuantizationType/title": "V緩衝區量化類型",
    "llm.load.llama.vCacheQuantizationType/subTitle": "值越小記憶體使用越低，但可能降低輸出品質。效果因模型而異",
    "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 若未啟用Flash Attention，必須禁用此設定",
    "llm.load.llama.vCacheQuantizationType/disabledMessage": "只有在啟用Flash Attention時才能啟動此功能",
    "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 使用F32時必須禁用Flash Attention",
    "llm.load.mlx.kvCacheBits/title": "KV緩衝區量化",
    "llm.load.mlx.kvCacheBits/subTitle": "KV緩衝區應量化為多少位元",
    "llm.load.mlx.kvCacheBits/info": "KV緩衝區應量化為多少位元",
    "llm.load.mlx.kvCacheBits/turnedOnWarning": "使用KV緩衝區量化時會忽略上下文長度設定",
    "llm.load.mlx.kvCacheGroupSize/title": "KV緩衝區量化：組大小",
    "llm.load.mlx.kvCacheGroupSize/subTitle": "在量化運算時用於KV緩衝區的組大小。組大小越大記憶體使用越低，但可能降低輸出品質",
    "llm.load.mlx.kvCacheGroupSize/info": "KV緩衝區應量化為多少位元",
    "llm.load.mlx.kvCacheQuantizationStart/title": "KV緩衝ity量化：當上下文長度超過此值時開始量化",
    "llm.load.mlx.kvCacheQuantizationStart/subTitle": "開始量化KV緩衝區的上下文長度閾值",
    "llm.load.mlx.kvCacheQuantizationStart/info": "開始量化KV緩衝區的上下文長度閾值",
    "llm.load.mlx.kvCacheQuantization/title": "KV緩衝區量化",
    "llm.load.mlx.kvCacheQuantization/subTitle": "量化模型的KV緩衝區。這可能會使產生速度更快且記憶體佔用更低，\n但以輸出品質的降低為代價。",
    "llm.load.mlx.kvCacheQuantization/bits/title": "KV緩衝區量化位元數",
    "llm.load.mlx.kvCacheQuantization/bits/tooltip": "量化KV緩衝區的位元數",
    "llm.load.mlx.kvCacheQuantization/bits/bits": "位元",
    "llm.load.mlx.kvCacheQuantization/groupSize/title": "組大小策略",
    "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "精確度",
    "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "平衡",
    "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "速度優先",
    "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "高級：量化「矩陣乘法羣大小」配置\n\n• 精度 = 羣大小 32\n• 平衡 = 羣大小 64\n• 速度 = 羣大小 128\n",
    "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "當上下文長度達到這個值時開始量化",
    "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "當上下文長度達到這個 token 數量時，\n開始量化 KV 緩存",
    "embedding.load.contextLength/title": "上下文長度",
    "embedding.load.contextLength/subTitle": "模型在一個提示中能處理的最大 token 數量。請參閱「推理參數」下的對話溢出選項，瞭解更多管理方式",
    "embedding.load.contextLength/info": "指定模型一次能考慮的最大 token 數量，影響處理時會保留多少上下文",
    "embedding.load.llama.ropeFrequencyBase/title": "RoPE 頻率基數",
    "embedding.load.llama.ropeFrequencyBase/subTitle": "為旋轉位置嵌入（RoPE）自訂的頻率基數。提升這個值或許能在高上下文長度下取得更好的表現",
    "embedding.load.llama.ropeFrequencyBase/info": "[高階] 調整旋轉位置編碼的基礎頻率，影響位置資訊的嵌入方式",
    "embedding.load.llama.evalBatchSize/title": "評估批次大小",
    "embedding.load.llama.evalBatchSize/subTitle": "每次處理的輸入記號數。增加此值可提升效能，但會增加記憶體使用",
    "embedding.load.llama.evalBatchSize/info": "設定評估時每批次同時處理的記號數",
    "embedding.load.llama.ropeFrequencyScale/title": "旋轉位置編碼頻率縮放",
    "embedding.load.llama.ropeFrequencyScale/subTitle": "透過此係數縮放上下文長度，以延長使用旋轉位置編碼的有效上下文",
    "embedding.load.llama.ropeFrequencyScale/info": "[高階] 修改旋轉位置編碼的頻率縮放，以控制位置編碼的精細度",
    "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
    "embedding.load.llama.acceleration.offloadRatio/subTitle": "在 GPU 上計算的離散模型層數。用於 GPU 加速",
    "embedding.load.llama.acceleration.offloadRatio/info": "設定卸載至 GPU 的層數",
    "embedding.load.llama.keepModelInMemory/title": "保持模型在記憶體中",
    "embedding.load.llama.keepModelInMemory/subTitle": "即使卸載至 GPU，也保留系統記憶體給模型。改善效能但需要更多系統RAM",
    "embedding.load.llama.keepModelInMemory/info": "防止模型被換出到磁碟，確保快速訪問，但會增加RAM使用",
    "embedding.load.llama.tryMmap/title": "嘗試使用mmap()",
    "embedding.load.llama.tryMmap/subTitle": "提高模型的載入時間。如模型大於可用系統記憶體，關閉此功能可能提升效能",
    "embedding.load.llama.tryMmap/info": "直接從硬碟載入模型檔案到記憶體",
    "embedding.load.seed/title": "種子",
    "embedding.load.seed/subTitle": "用於文字生成的隨機數生成器的種子。-1 表示隨機種子",
    "embedding.load.seed/info": "隨機種子：設定隨機數生成的種子以確保結果可重現",
    "presetTooltip": {
        "included/title": "預設值",
        "included/description": "以下欄位將會被應用",
        "included/empty": "這個預設設定在當前情境下不適用。",
        "included/conflict": "您將被要求選擇是否適用此值。",
        "separateLoad/title": "載入時配置",
        "separateLoad/description.1": "此預設設定還包含以下載入時配置。載入時間配置是模型範圍的，需重新載入模型才能生效。請保持",
        "separateLoad/description.2": "應用於",
        "separateLoad/description.3": ".",
        "excluded/title": "可能不適用",
        "excluded/description": "下列欄位屬於此預設設定，但在當前情境下不適用。",
        "legacy/title": "傳統預設",
        "legacy/description": "此預設設定為傳統預設。它包含的下列欄位，現已自動處理，或不再適用。",
        "button/publish": "上傳至 Hub",
        "button/pushUpdate": "將變更推送到 Hub",
        "button/export": "匯出"
    },
    "customInputs": {
        "string": {
            "emptyParagraph": "<空>"
        },
        "checkboxNumeric": {
            "off": "關閉"
        },
        "llamaCacheQuantizationType": {
            "off": "關閉"
        },
        "mlxKvCacheBits": {
            "off": "關閉"
        },
        "stringArray": {
            "empty": "<空>"
        },
        "llmPromptTemplate": {
            "type": "類型",
            "types.jinja/label": "模版（Jinja）",
            "jinja.bosToken/label": "BOS 標記",
            "jinja.eosToken/label": "EOS 標記",
            "jinja.template/label": "模版",
            "jinja/error": "無法解析 Jinja 模板：{{錯誤}}",
            "jinja/empty": "請在上方輸入一個 Jinja 模板。",
            "jinja/unlikelyToWork": "您提供的上方 Jinja 模板可能無法正常運作，因為它未參考變數「messages」。請確認是否正確輸入了模板。",
            "types.manual/label": "手動",
            "manual.subfield.beforeSystem/label": "系統前",
            "manual.subfield.beforeSystem/placeholder": "輸入系統前綴…",
            "manual.subfield.afterSystem/label": "系統後",
            "manual.subfield.afterSystem/placeholder": "輸入系統後綴…",
            "manual.subfield.beforeUser/label": "使用者前",
            "manual.subfield.beforeUser/placeholder": "輸入使用者前綴…",
            "manual.subfield.afterUser/label": "使用者後",
            "manual.subfield.afterUser/placeholder": "輸入使用者後綴…",
            "manual.subfield.beforeAssistant/label": "助理前",
            "manual.subfield.beforeAssistant/placeholder": "輸入助理前綴…",
            "manual.subfield.afterAssistant/label": "助理後",
            "manual.subfield.afterAssistant/placeholder": "輸入助理後綴…",
            "stopStrings/label": "額外停止字串",
            "stopStrings/subTitle": "將會用在附加於使用者指定停止字串上的模板特定停止字串。"
        },
        "contextLength": {
            "maxValueTooltip": "這是模型訓練時最大處理的標記數。點擊即可設定語境為此值",
            "maxValueTextStart": "模型最高支援",
            "maxValueTextEnd": "標記",
            "tooltipHint": "雖然模型可能支援至某個標記數，但若您的硬體無法承載，表現可能遞減 - 增加此值時請謹慎"
        },
        "contextOverflowPolicy": {
            "stopAtLimit": "到達極限時停止",
            "stopAtLimitSub": "一旦模型記憶體填滿，就停止生成",
            "truncateMiddle": "截斷中間",
            "truncateMiddleSub": "移除對話中間的訊息以騰出空間給較新的訊息。模型仍會記得對話開始的部分",
            "rollingWindow": "滾動窗口",
            "rollingWindowSub": "模型會總是以最近的幾條訊息為據，但可能遺忘對話開始的部分"
        },
        "llamaAccelerationOffloadRatio": {
            "max": "MAX",
            "off": "關閉"
        },
        "llamaAccelerationSplitStrategy": {
            "evenly": "均勻",
            "favorMainGpu": "偏好主 GPU"
        },
        "speculativeDecodingDraftModel": {
            "readMore": "閱讀它的工作方式",
            "placeholder": "選擇一個相容的草稿模型",
            "noCompatible": "未找到與您當前模型選項相容的草案模型",
            "stillLoading": "尋找相容的草案模型…",
            "notCompatible": "您選擇的草案模型 (<draft/>) 與當前模型選項 (<current/>) 不相容。",
            "off": "關閉",
            "loadModelToSeeOptions": "載入模型 <keyboard-shortcut /> 以查看相容選項",
            "compatibleWithNumberOfModels": "建議適用於至少 {{dynamicValue}} 的模型",
            "recommendedForSomeModels": "建議適用於部分模型",
            "recommendedForLlamaModels": "建議適用於Llama模型",
            "recommendedForQwenModels": "建議適用於Qwen模型",
            "onboardingModal": {
                "introducing": "介紹",
                "speculativeDecoding": "猜測解碼",
                "firstStepBody": "針對 <custom-span>llama.cpp</custom-span> 與 <custom-span>MLX</custom-span> 模型的推論速度提升",
                "secondStepTitle": "搭配猜測解碼的推論速度提升",
                "secondStepBody": "猜測解碼是兩個模型協作的技術：\n - 一個較大的「主」模型\n - 一個較小的「草案」模型\n\n在生成過程中，草案模型會快速提出 token 供較大的主模型驗證。驗證 token 的過程比實際生成更快，這就是速度提升的來源。**通常，主模型與草案模型之間的大小差距越大，速度提升越明顯**。\n\n為維持品質，主模型只接受與自己生成一致的 token，使大模型在更快的推論速度下仍能保持回應品質。兩模型必須共用相同的詞彙。",
                "draftModelRecommendationsTitle": "草案模型建議",
                "basedOnCurrentModels": "基於您目前的模型",
                "close": "關閉",
                "next": "下一個",
                "done": "完成"
            },
            "speculativeDecodingLoadModelToSeeOptions": "請先載入一個模型 <model-badge /> ",
            "errorEngineNotSupported": "猜測解碼需至少 {{minVersion}} 版本的引擎 {{engineName}}。請更新引擎 (<key/>) 並重新載入模型才能使用此功能。",
            "errorEngineNotSupported/noKey": "猜測解碼需至少 {{minVersion}} 版本的引擎 {{engineName}}。請更新引擎並重新載入模型才能使用此功能。"
        },
        "llmReasoningParsing": {
            "startString/label": "起始字符串",
            "startString/placeholder": "輸入起始字符串…",
            "endString/label": "結束字符串",
            "endString/placeholder": "輸入結束字符串…"
        }
    },
    "saveConflictResolution": {
        "title": "選擇要在預設值中包含的值",
        "description": "選擇保留哪些值",
        "instructions": "點擊要包含的值",
        "userValues": "上一值",
        "presetValues": "新值",
        "confirm": "確認",
        "cancel": "取消"
    },
    "applyConflictResolution": {
        "title": "要保留哪些值？",
        "description": "您有未提交的變更與 incoming Preset 有重疊",
        "instructions": "請點擊您想要保留的值",
        "userValues": "當前值",
        "presetValues": "incoming Preset 值",
        "confirm": "確認",
        "cancel": "取消"
    },
    "empty": "<空白>",
    "noModelSelected": "沒有選取模型",
    "apiIdentifier.label": "API 識別碼",
    "apiIdentifier.hint": "選項提供此模型的識別碼。此識別碼將用於 API 請求。留空則使用預設識別碼。",
    "idleTTL.label": "若閒置即卸載 (TTL)",
    "idleTTL.hint": "若設定，模型在閒置指定時間後將自動卸載。",
    "idleTTL.mins": "分鐘",
    "presets": {
        "title": "預設",
        "commitChanges": "提交變更",
        "commitChanges/description": "將您的變更提交至預設值。",
        "commitChanges.manual": "新字段偵測到。您將能選擇哪些變更包含在預設中。",
        "commitChanges.manual.hold.0": "保持",
        "commitChanges.manual.hold.1": "用以選擇哪些變更提交至預設。",
        "commitChanges.saveAll.hold.0": "保持",
        "commitChanges.saveAll.hold.1": "用以保存所有變更。",
        "commitChanges.saveInPreset.hold.0": "保持",
        "commitChanges.saveInPreset.hold.1": "僅保存已包含在預設中的字段變更。",
        "commitChanges/error": "無法提交變更至預設。",
        "commitChanges.manual/description": "選擇哪些變更包含在預設中。",
        "saveAs": "另存為新…",
        "presetNamePlaceholder": "輸入預設名稱…",
        "cannotCommitChangesLegacy": "這是一個遺留預設，無法修改。您可使用「另存為新…」建立副本。",
        "cannotCommitChangesNoChanges": "無變更可提交。",
        "emptyNoUnsaved": "選擇預設…",
        "emptyWithUnsaved": "未保存預設",
        "saveEmptyWithUnsaved": "保存預設為…",
        "saveConfirm": "保存",
        "saveCancel": "取消",
        "saving": "保存中…",
        "save/error": "無法保存預設。",
        "deselect": "取消選擇預設",
        "deselect/error": "無法取消選擇預設。",
        "select/error": "無法選擇預設。",
        "delete/error": "無法刪除預設。",
        "discardChanges": "清除未保存",
        "discardChanges/info": "清除所有未提交變更並恢復預設至原狀態",
        "newEmptyPreset": "+ 新預設",
        "importPreset": "導入",
        "contextMenuSelect": "應用預設",
        "contextMenuDelete": "刪除…",
        "contextMenuShare": "發布…",
        "contextMenuOpenInHub": "在中心觀看",
        "contextMenuPushChanges": "將變更推送到中心",
        "contextMenuPushingChanges": "推送中…",
        "contextMenuPushedChanges": "變更已推送",
        "contextMenuExport": "導出檔案",
        "contextMenuRevealInExplorer": "在檔案管理員中顯示",
        "contextMenuRevealInFinder": "在搜尋器中顯示",
        "share": {
            "title": "發布預設",
            "action": "與其他人分享您的預設，以便他們下載、點讚與分叉",
            "presetOwnerLabel": "擁有者",
            "uploadAs": "您的預設將被建立為{{name}}",
            "presetNameLabel": "預設名稱",
            "descriptionLabel": "描述（可選）",
            "loading": "上傳中…",
            "success": "預設已成功上傳",
            "presetIsLive": "<預設名稱 /> 現在已在站上線！",
            "close": "關閉",
            "confirmViewOnWeb": "在網頁觀看",
            "confirmCopy": "複製網址",
            "confirmCopied": "已複製！",
            "pushedToHub": "您的預設已上傳至站",
            "descriptionPlaceholder": "輸入描述…",
            "willBePublic": "發布您的預設將使其公開",
            "publicSubtitle": "您的預設是 <custom-bold>公共</custom-bold>。其他人可以在 lmstudio.ai 下載和分叉它",
            "confirmShareButton": "發布",
            "error": "失敗於發佈預設",
            "createFreeAccount": "在中心註冊免費帳號以發佈預設"
        },
        "update": {
            "title": "將變更推送到中心",
            "title/success": "預設更新成功",
            "subtitle": "編輯 <custom-preset-name /> 並推送到中心",
            "descriptionLabel": "說明",
            "descriptionPlaceholder": "輸入說明…",
            "loading": "推送中…",
            "cancel": "取消",
            "createFreeAccount": "在中心註冊免費帳號以發佈預設",
            "error": "失敗於推送更新",
            "confirmUpdateButton": "推送"
        },
        "import": {
            "title": "從文件導入預設",
            "dragPrompt": "拖放預設 JSON 檔案或 <custom-link>從您的電腦選擇</custom-link>",
            "remove": "刪除",
            "cancel": "取消",
            "importPreset_zero": "導入預設",
            "importPreset_one": "導入預設",
            "importPreset_other": "導入 {{count}} 個預設",
            "selectDialog": {
                "title": "選擇預設檔 (.json)",
                "button": "導入"
            },
            "error": "導入預設失敗",
            "resultsModal": {
                "titleSuccessSection_one": "成功導入 1 個預設",
                "titleSuccessSection_other": "成功導入 {{count}} 個預設",
                "titleFailSection_zero": "",
                "titleFailSection_one": "（{{count}} 個失敗）",
                "titleFailSection_other": "（{{count}} 個失敗）",
                "titleAllFailed": "預設導入失敗",
                "importMore": "導入更多",
                "close": "完成",
                "successBadge": "成功",
                "alreadyExistsBadge": "預設已存在",
                "errorBadge": "錯誤",
                "invalidFileBadge": "無效檔案",
                "otherErrorBadge": "預設導入失敗",
                "errorViewDetailsButton": "查看細節",
                "seeError": "查看錯誤",
                "noName": "無預設名稱",
                "useInChat": "用於對話"
            },
            "importFromUrl": {
                "button": "從 URL 優化…",
                "title": "從 URL 優化",
                "back": "從文件優化…",
                "action": "請在下方貼上您想導入的預設在 LM Studio Hub 的 URL",
                "invalidUrl": "無效 URL。請確認您貼的是正確的 LM Studio Hub URL。",
                "tip": "您可以在 LM Studio Hub 中使用 {{buttonName}} 按鈕直接安裝此預設",
                "confirm": "導入",
                "cancel": "取消",
                "loading": "導入中…",
                "error": "預設下載失敗。"
            }
        },
        "download": {
            "title": "從 LM Studio Hub 取得 <preset-name /> 預設",
            "subtitle": "將 <custom-name /> 儲存至您的預設。這樣您就可以在應用程式中使用此預設",
            "button": "取得",
            "button/loading": "取得中…",
            "cancel": "取消",
            "error": "預設下載失敗。"
        },
        "inclusiveness": {
            "speculativeDecoding": "包含於預設"
        }
    },
    "flashAttentionWarning": "閃光注意力是實驗性功能，可能導致某些模型出現問題。如果遇到問題，試著關閉它。",
    "llamaKvCacheQuantizationWarning": "KV 緩存量化是實驗性功能，可能導致某些模型出現問題。閃光注意必須啟用才能使用 V 緩存量化。如果遇到問題，請還原為預設的 \"F16\"。",
    "seedUncheckedHint": "隨機種子",
    "ropeFrequencyBaseUncheckedHint": "自動",
    "ropeFrequencyScaleUncheckedHint": "自動",
    "hardware": {
        "advancedGpuSettings": "進階 GPU 設定",
        "advancedGpuSettings.info": "若不確定，請保留這些預設值",
        "advancedGpuSettings.reset": "重置為預設",
        "environmentVariables": {
            "title": "環境變數",
            "description": "模型生命週期中活躍的環境變數。",
            "key.placeholder": "選擇變數…",
            "value.placeholder": "值"
        },
        "mainGpu": {
            "title": "主要GPU",
            "description": "為模型計算優先選擇的GPU。",
            "placeholder": "選擇主要GPU…"
        },
        "splitStrategy": {
            "title": "分割策略",
            "description": "如何在多張GPU上分割模型計算。",
            "placeholder": "選擇分割策略…"
        }
    }
}