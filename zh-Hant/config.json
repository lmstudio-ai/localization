{
  "noInstanceSelected": "未選擇模型實例",
  "resetToDefault": "重置",
  "showAdvancedSettings": "顯示高級設定",
  "showAll": "全部",
  "basicSettings": "基本設定",
  "configSubtitle": "載入或儲存預設，並試驗覆蓋的模型參數",
  "inferenceParameters/title": "推理參數",
  "inferenceParameters/info": "試驗影響推理的參數。",
  "generalParameters/title": "通用",
  "samplingParameters/title": "取樣",
  "basicTab": "基本",
  "advancedTab": "高級",
  "advancedTab/title": "🧪 高級配置",
  "advancedTab/expandAll": "展開全部",
  "advancedTab/overridesTitle": "配置覆蓋",
  "advancedTab/noConfigsText": "您沒有未保存的更改 — 編輯上方的值以在此處查看覆蓋的參數。",
  "loadInstanceFirst": "載入模型以查看可配置參數",
  "noListedConfigs": "無可配置參數",
  "generationParameters/info": "試驗影響文本生成的基本參數。",
  "loadParameters/title": "載入參數",
  "loadParameters/description": "控制模型初始化和載入到記憶體中方式的設定。",
  "loadParameters/reload": "重新載入以應用更改",
  "discardChanges": "放棄更改",
  "llm.prediction.systemPrompt/title": "系統提示語",
  "llm.prediction.systemPrompt/description": "使用此欄位為模型提供背景指令，例如一組規則、約束或一般需求。此欄位也常被稱為「系統提示語」。",
  "llm.prediction.systemPrompt/subTitle": "AI 指南",
  "llm.prediction.temperature/title": "溫度",
  "llm.prediction.temperature/subTitle": "引入多少隨機性。0 意味著每次都產生相同的結果，較高的值將增加創造力和變化性",
  "llm.prediction.temperature/info": "來自 llama.cpp 幫助文檔：「預設值為 <{{dynamicValue}}>，在隨機性和確定性之間提供平衡。在極端情況下，溫度為 0 將始終選擇最可能的下一個 token，導致每次運行的輸出相同。」",
  "llm.prediction.llama.sampling/title": "取樣",
  "llm.prediction.llama.topKSampling/title": "Top K 取樣",
  "llm.prediction.llama.topKSampling/subTitle": "限制下一個 token 為幾率最高的 top-k 個 tokens 之一。作用類似於溫度",
  "llm.prediction.llama.topKSampling/info": "來自 llama.cpp 幫助文檔：\n\nTop-k 取樣是一種文本生成方法，僅從模型預測的機率最高的 top-k 個 tokens 中選擇下一個 token。\n\n它有助於降低生成低機率或無意義 token 的風險，但也可能限制輸出的多樣性。\n\n較高的 top-k 值（例如 100）將考慮更多的 token 並生成更多樣化的文本，而較低的值（例如 10）將關注機率最高的幾個 token 並生成更保守的文本。\n\n• 預設值為 <{{dynamicValue}}>。",
  "llm.prediction.llama.cpuThreads/title": "CPU 執行緒數量",
  "llm.prediction.llama.cpuThreads/subTitle": "推理期間使用的 CPU 執行緒數量",
  "llm.prediction.llama.cpuThreads/info": "推理期間使用的執行緒數量。增加執行緒數量並不總能提供更好的性能。預設為 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制回應長度",
  "llm.prediction.maxPredictedTokens/subTitle": "可選地限制 AI 的回應長度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天機器人回應的最大長度。開啟以設定回應的最大長度，或關閉以由聊天機器人自己決定何時停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（token 數）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 個詞",
  "llm.prediction.llama.repeatPenalty/title": "重複懲罰",
  "llm.prediction.llama.repeatPenalty/subTitle": "在多大程度上抑制重複相同的 token",
  "llm.prediction.llama.repeatPenalty/info": "來自 llama.cpp 幫助文檔：「有助於防止模型生成重複或單調的文本。\n\n較高的值（例如 1.5）具有更強烈地重複懲罰，而較低的值（例如 0.9）則更寬鬆。」 • 預設值為 <{{dynamicValue}}>。",
  "llm.prediction.llama.minPSampling/title": "最小 P 取樣",
  "llm.prediction.llama.minPSampling/subTitle": "被考慮的 token 的最小基礎機率",
  "llm.prediction.llama.minPSampling/info": "來自 llama.cpp 幫助文檔：\n\ntoken 被考慮的最小機率，相對於最可能 token 的機率。必須在 [0, 1] 範圍內。\n\n• 預設值為 <{{dynamicValue}}>。",
  "llm.prediction.llama.topPSampling/title": "Top P 取樣",
  "llm.prediction.llama.topPSampling/subTitle": "可能的下一個 token 的最小累積機率。作用類似於溫度",
  "llm.prediction.llama.topPSampling/info": "來自 llama.cpp 幫助文檔：\n\nTop-p 取樣，也稱為累積機率門閾值或核心取樣，是另一種文本生成方法，從累積機率至少為 p 的 tokens 子集中選擇下一個 token。\n\n此方法通過同時考慮了 tokens 的機率和要取樣的 tokens 數量，提供了多樣性和品質的平衡。\n\n較高的 top-p 值（例如 0.95）將生成更具多樣性的文本，而較低的值（例如 0.5）將生成更集中和保守的文本。必須在 (0, 1] 範圍內。\n\n• 預設值為 <{{dynamicValue}}>。",
  "llm.prediction.stopStrings/title": "停止字串",
  "llm.prediction.stopStrings/subTitle": "應該使模型停止生成更多 token 的字串",
  "llm.prediction.stopStrings/info": "當遇到特定字串時，將停止模型生成更多 token",
  "llm.prediction.stopStrings/placeholder": "輸入字串並按 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "對話溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "當對話過大而無法處理時，模型應該如何表現",
  "llm.prediction.contextOverflowPolicy/info": "決定當對話超過模型的工作記憶（'上下文'）大小時該怎麼做",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "達到限制時停止",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "一旦模型的記憶已滿，停止生成",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "從中間截斷",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "從對話中間刪除訊息，以便為新訊息騰出空間。但模型仍會記住對話的開頭",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "滾動窗口",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "模型將始終獲取最近的幾條訊息，但可能會忘記對話的開頭",
  "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
  "llm.prediction.llama.presencePenalty/title": "存在懲罰",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free 取樣",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical 取樣",
  "llm.prediction.mlx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.mlx.repeatPenalty/subTitle": "在多大程度上抑制重複相同的 token",
  "llm.prediction.mlx.repeatPenalty/info": "較高的值具有更強烈地重複懲罰",
  "llm.prediction.onnx.topKSampling/title": "Top K 取樣",
  "llm.prediction.onnx.topKSampling/subTitle": "將下一個 token 限制為機率最高的 top-k 個 tokens 之一。作用類似於溫度",
  "llm.prediction.onnx.topKSampling/info": "來自 ONNX 文檔：\n\n保留機率最高的 top-k 個 tokens 進行取樣\n\n• 此過濾器默認關閉",
  "llm.prediction.onnx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.onnx.repeatPenalty/subTitle": "在多大程度上抑制重複相同的 token",
  "llm.prediction.onnx.repeatPenalty/info": "較高的值具有更強烈地重複懲罰",
  "llm.prediction.onnx.topPSampling/title": "Top P 取樣",
  "llm.prediction.onnx.topPSampling/subTitle": "下一個可能的 token 的最小累積機率。作用類似於溫度",
  "llm.prediction.onnx.topPSampling/info": "來自 ONNX 文檔：\n\n僅保留累積機率達到 TopP 或更高的機率的 tokens 進行生成\n\n• 此過濾器默認關閉",
  "llm.prediction.seed/title": "種子",
  "llm.prediction.structured/title": "結構化輸出",
  "llm.prediction.structured/info": "結構化輸出",
  "llm.prediction.promptTemplate/title": "提示語模板",
  "llm.prediction.promptTemplate/subTitle": "對話中發送給模型的聊天訊息格式。更改此項可能會引入意外行為 - 請確保您知道自己在做什麼！",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "解析 Jinja 模板失敗：{{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "手動",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "系統之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "輸入系統前綴…",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "系統之後",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "輸入系統後綴…",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "用戶之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "輸入用戶前綴…",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "用戶之後",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "輸入用戶後綴…",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "助理之前",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "輸入助理前綴…",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "助理之後",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "輸入助理後綴…",
  "llm.prediction.promptTemplate.stopStrings/label": "附加停止字串",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "除了用戶指定的停止字串之外，模板還會使用特定的停止字符串。",
  
  "llm.load.contextLength/title": "上下文長度",
  "llm.load.contextLength/subTitle": "模型在一段提示語中可以關注的最大 token 數。查看「推理參數」下的對話溢出選項，以了解更多管理方法",
  "llm.load.contextLength/info": "指定模型一次可以考慮的最大 token 數，將影響處理過程中保留的上下文數量",
  "llm.load.seed/title": "種子",
  "llm.load.seed/subTitle": "用於文本生成中隨機數生成器使用的種子。-1 表示隨機",
  "llm.load.seed/info": "隨機種子：設定隨機數生成的種子，以確保結果可重現",
  
  "llm.load.llama.evalBatchSize/title": "批次評估大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次處理的輸入 token 數。增加這個值會提高性能，亦會增加記憶體使用",
  "llm.load.llama.evalBatchSize/info": "設置評估期間在一個批次中一起處理的 token 數量，影響速度和記憶體使用",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 頻率基數",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋轉位置嵌入（RoPE）的自定義基頻。增加此值可能會提升長上下文長度下的性能",
  "llm.load.llama.ropeFrequencyBase/info": "[高級] 調整旋轉位置編碼的基頻，影響位置信息的嵌入",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 頻率縮放",
  "llm.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此比例縮放，以使用 RoPE 擴展有效上下文",
  "llm.load.llama.ropeFrequencyScale/info": "[高級] 修改旋轉位置編碼頻率的縮放，以控制位置編碼的細粒度",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 加速 （GPU Offload）",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "在 GPU 上計算的離散模型層數，用於 GPU Offload。越多層將使用越多的 VRAM。",
  "llm.load.llama.acceleration.offloadRatio/info": "設置 Offload 到 GPU 的模型層數。",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "在某些模型上能夠減少記憶體使用和生成時間",
  "llm.load.llama.flashAttention/info": "加速注意力機制，以更快、更高效地處理",
  "llm.load.llama.keepModelInMemory/title": "將模型保留在記憶體中",
  "llm.load.llama.keepModelInMemory/subTitle": "將模型保留在系統記憶體，即便啓用了 GPU Offload。可以提高性能但需要更多的系統 RAM",
  "llm.load.llama.keepModelInMemory/info": "防止模型被交換到磁盤，確保更快的訪問速度，但會消耗更多的 RAM",
  "llm.load.llama.useFp16ForKVCache/title": "KV 快取使用 FP16",
  "llm.load.llama.useFp16ForKVCache/info": "通過以半精度（FP16）存儲快取來減少記憶體使用",
  "llm.load.llama.tryMmap/title": "嘗試 mmap()",
  "llm.load.llama.tryMmap/subTitle": "改善模型載入時間。禁用此選項可能在模型大小超過系統可用記憶體時提升性能",

  "llm.load.llama.tryMmap/info": "直接從磁盤載入模型文件到記憶體",
  
  "embedding.load.contextLength/title": "上下文長度",
  "embedding.load.contextLength/subTitle": "模型在一個提示中可以關注的最大 token 數。查看「推理參數」下的對話溢出選項，以了解更多管理方法",
  "embedding.load.contextLength/info": "指定模型一次可以考慮的最大 token 數，影響處理過程中保留的上下文數量",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 頻率基數",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋轉位置嵌入（RoPE）的自定義基頻。增加此值可能會提高高上下文長度下的性能",
  "embedding.load.llama.ropeFrequencyBase/info": "[高級] 調整旋轉位置編碼的基頻，影響位置信息的嵌入",
  "embedding.load.llama.evalBatchSize/title": "評估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次處理的輸入 token 數。增加這個值會提高性能，但會增加記憶體使用",
  "embedding.load.llama.evalBatchSize/info": "設置評估期間在一個批次中一起處理的示例數量",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 頻率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文長度按此比例縮放，以使用 RoPE 擴展有效上下文",
  "embedding.load.llama.ropeFrequencyScale/info": "[高級] 修改旋轉位置編碼頻率的縮放，以控制位置編碼的細粒度",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "在 GPU 上計算的離散模型層數，以加速 GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "設置卸載到 GPU 的層數。",
  "embedding.load.llama.keepModelInMemory/title": "保留模型在記憶體中",
  "embedding.load.llama.keepModelInMemory/subTitle": "為模型保留系統記憶體，即使卸載到 GPU。提高性能但需要更多的系統 RAM",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交換到磁盤，確保更快的訪問速度，但會消耗更多的 RAM",
  "embedding.load.llama.tryMmap/title": "嘗試 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "改善模型載入時間。禁用此選項可能在模型大小超過系統可用記憶體時提升性能",
  "embedding.load.llama.tryMmap/info": "將模型文件直接從磁盤載入到記憶體",
  "embedding.load.seed/title": "種子",
  "embedding.load.seed/subTitle": "文本生成中隨機數生成器使用的種子。-1 表示使用隨機生成的種子",
  "embedding.load.seed/info": "隨機種子：設置隨機數生成的種子，以確保結果可重現"
}
