{
  "noInstanceSelected": "Tidak ada instance model yang dipilih",
  "resetToDefault": "Atur Ulang",
  "showAdvancedSettings": "Tampilkan pengaturan lanjutan",
  "showAll": "Semua",
  "basicSettings": "Dasar",
  "configSubtitle": "Muat atau simpan preset dan bereksperimen dengan pengaturan parameter model",
  "inferenceParameters/title": "Parameter Prediksi",
  "inferenceParameters/info": "Bereksperimen dengan parameter yang mempengaruhi prediksi.",
  "generalParameters/title": "Umum",
  "samplingParameters/title": "Sampling",
  "basicTab": "Dasar",
  "advancedTab": "Lanjutan",
  "advancedTab/title": "üß™ Konfigurasi Lanjutan",
  "advancedTab/expandAll": "Perluas semua",
  "advancedTab/overridesTitle": "Pengaturan Override",
  "advancedTab/noConfigsText": "Anda tidak memiliki perubahan yang belum disimpan - edit nilai di atas untuk melihat override di sini.",
  "loadInstanceFirst": "Muat model untuk melihat parameter yang dapat dikonfigurasi",
  "noListedConfigs": "Tidak ada parameter yang dapat dikonfigurasi",
  "generationParameters/info": "Bereksperimen dengan parameter dasar yang mempengaruhi pembuatan teks.",
  "loadParameters/title": "Parameter Muat",
  "loadParameters/description": "Pengaturan untuk mengontrol cara model diinisialisasi dan dimuat ke dalam memori.",
  "loadParameters/reload": "Muat ulang untuk menerapkan perubahan",
  "discardChanges": "Buang perubahan",
  "loadModelToSeeOptions": "Muat model untuk melihat opsi",
  "llm.prediction.systemPrompt/title": "Prompt Sistem",
  "llm.prediction.systemPrompt/description": "Gunakan bidang ini untuk memberikan instruksi latar belakang kepada model, seperti seperangkat aturan, batasan, atau persyaratan umum.",
  "llm.prediction.systemPrompt/subTitle": "Panduan untuk AI",
  "llm.prediction.temperature/title": "Temperatur",
  "llm.prediction.temperature/subTitle": "Seberapa banyak keacakan yang akan diperkenalkan. 0 akan menghasilkan hasil yang sama setiap kali, sementara nilai yang lebih tinggi akan meningkatkan kreativitas dan variasi",
  "llm.prediction.temperature/info": "Dari dokumen bantuan llama.cpp: \"Nilai default adalah <{{dynamicValue}}>, yang memberikan keseimbangan antara keacakan dan determinisme. Pada ekstremnya, temperatur 0 akan selalu memilih token berikutnya yang paling mungkin, menghasilkan output yang identik di setiap run\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top K Sampling",
  "llm.prediction.topKSampling/subTitle": "Membatasi token berikutnya ke salah satu dari token paling mungkin di top-k. Bertindak mirip dengan temperatur",
  "llm.prediction.topKSampling/info": "Dari dokumen bantuan llama.cpp:\n\nTop-k sampling adalah metode pembuatan teks yang memilih token berikutnya hanya dari token paling mungkin di top k yang diprediksi oleh model.\n\nIni membantu mengurangi risiko menghasilkan token dengan probabilitas rendah atau tidak masuk akal, tetapi juga dapat membatasi keragaman output.\n\nNilai yang lebih tinggi untuk top-k (misalnya, 100) akan mempertimbangkan lebih banyak token dan menghasilkan teks yang lebih beragam, sementara nilai yang lebih rendah (misalnya, 10) akan fokus pada token yang paling mungkin dan menghasilkan teks yang lebih konservatif.\n\n‚Ä¢ Nilai default adalah <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU Threads",
  "llm.prediction.llama.cpuThreads/subTitle": "Jumlah thread CPU yang digunakan selama inferensi",
  "llm.prediction.llama.cpuThreads/info": "Jumlah thread yang digunakan selama komputasi. Meningkatkan jumlah thread tidak selalu berkorelasi dengan kinerja yang lebih baik. Default adalah <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Batas Panjang Respons",
  "llm.prediction.maxPredictedTokens/subTitle": "Opsional membatasi panjang respons AI",
  "llm.prediction.maxPredictedTokens/info": "Kontrol panjang maksimum respons chatbot. Aktifkan untuk menetapkan batas panjang maksimum respons, atau nonaktifkan untuk membiarkan chatbot memutuskan kapan harus berhenti.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Panjang respons maksimum (token)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Sekitar {{maxWords}} kata",
  "llm.prediction.repeatPenalty/title": "Penalti Pengulangan",
  "llm.prediction.repeatPenalty/subTitle": "Seberapa banyak untuk mencegah pengulangan token yang sama",
  "llm.prediction.repeatPenalty/info": "Dari dokumen bantuan llama.cpp: \"Membantu mencegah model menghasilkan teks yang berulang atau monoton.\n\nNilai yang lebih tinggi (misalnya, 1.5) akan lebih kuat menghukum pengulangan, sementara nilai yang lebih rendah (misalnya, 0.9) akan lebih lunak.\" ‚Ä¢ Nilai default adalah <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P Sampling",
  "llm.prediction.minPSampling/subTitle": "Probabilitas dasar minimum untuk token yang dipilih untuk output",
  "llm.prediction.minPSampling/info": "Dari dokumen bantuan llama.cpp:\n\nProbabilitas minimum untuk token yang dipertimbangkan, relatif terhadap probabilitas token yang paling mungkin. Harus dalam [0, 1].\n\n‚Ä¢ Nilai default adalah <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P Sampling",
  "llm.prediction.topPSampling/subTitle": "Probabilitas kumulatif minimum untuk token berikutnya yang mungkin. Bertindak mirip dengan temperatur",
  "llm.prediction.topPSampling/info": "Dari dokumen bantuan llama.cpp:\n\nTop-p sampling, juga dikenal sebagai nucleus sampling, adalah metode pembuatan teks lain yang memilih token berikutnya dari subset token yang bersama-sama memiliki probabilitas kumulatif setidaknya p.\n\nMetode ini memberikan keseimbangan antara keragaman dan kualitas dengan mempertimbangkan baik probabilitas token maupun jumlah token yang diambil sampelnya.\n\nNilai yang lebih tinggi untuk top-p (misalnya, 0.95) akan menghasilkan teks yang lebih beragam, sementara nilai yang lebih rendah (misalnya, 0.5) akan menghasilkan teks yang lebih fokus dan konservatif. Harus dalam (0, 1].\n\n‚Ä¢ Nilai default adalah <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "String Penghenti",
  "llm.prediction.stopStrings/subTitle": "String yang harus menghentikan model dari menghasilkan lebih banyak token",
  "llm.prediction.stopStrings/info": "String tertentu yang ketika ditemui akan menghentikan model dari menghasilkan lebih banyak token",
  "llm.prediction.stopStrings/placeholder": "Masukkan string dan tekan ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Kelebihan Konteks",
  "llm.prediction.contextOverflowPolicy/subTitle": "Bagaimana model harus berperilaku ketika percakapan menjadi terlalu besar untuk ditangani",
  "llm.prediction.contextOverflowPolicy/info": "Putuskan apa yang harus dilakukan ketika percakapan melebihi ukuran memori kerja model ('konteks')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalti Frekuensi",
  "llm.prediction.llama.presencePenalty/title": "Penalti Kehadiran",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Membatasi token berikutnya ke salah satu dari token paling mungkin di top-k. Bertindak mirip dengan temperatur",
  "llm.prediction.onnx.topKSampling/info": "Dari dokumentasi ONNX:\n\nJumlah token kosakata dengan probabilitas tertinggi untuk disimpan untuk top-k-filtering\n\n‚Ä¢ Filter ini dimatikan secara default",
  "llm.prediction.onnx.repeatPenalty/title": "Penalti Pengulangan",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Seberapa banyak untuk mencegah pengulangan token yang sama",
  "llm.prediction.onnx.repeatPenalty/info": "Nilai yang lebih tinggi mencegah model mengulangi dirinya sendiri",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilitas kumulatif minimum untuk token berikutnya yang mungkin. Bertindak mirip dengan temperatur",
  "llm.prediction.onnx.topPSampling/info": "Dari dokumentasi ONNX:\n\nHanya token yang paling mungkin dengan probabilitas yang menambah hingga TopP atau lebih tinggi yang disimpan untuk pembuatan\n\n‚Ä¢ Filter ini dimatikan secara default",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Output Terstruktur",
  "llm.prediction.structured/info": "Output Terstruktur",
  "llm.prediction.promptTemplate/title": "Template Prompt",
  "llm.prediction.promptTemplate/subTitle": "Format di mana pesan dalam obrolan dikirim ke model. Mengubah ini dapat memperkenalkan perilaku yang tidak terduga - pastikan Anda tahu apa yang Anda lakukan!",

  "llm.load.contextLength/title": "Panjang Konteks",
  "llm.load.contextLength/subTitle": "Jumlah maksimum token yang dapat diperhatikan model dalam satu prompt. Lihat opsi Overflow Percakapan di bawah \"Parameter Inferensi\" untuk lebih banyak cara mengelola ini",
  "llm.load.contextLength/info": "Menentukan jumlah maksimum token yang dapat dipertimbangkan model sekaligus, mempengaruhi seberapa banyak konteks yang dipertahankan selama pemrosesan",
  "llm.load.contextLength/warning": "Menetapkan nilai tinggi untuk panjang konteks dapat secara signifikan mempengaruhi penggunaan memori",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "Seed untuk generator angka acak yang digunakan dalam pembuatan teks. -1 adalah acak",
  "llm.load.seed/info": "Seed Acak: Menetapkan seed untuk pembuatan angka acak untuk memastikan hasil yang dapat direproduksi",

  "llm.load.llama.evalBatchSize/title": "Ukuran Batch Evaluasi",
  "llm.load.llama.evalBatchSize/subTitle": "Jumlah token input yang diproses sekaligus. Meningkatkan ini meningkatkan kinerja dengan biaya penggunaan memori",
  "llm.load.llama.evalBatchSize/info": "Menetapkan jumlah contoh yang diproses bersama dalam satu batch selama evaluasi, mempengaruhi kecepatan dan penggunaan memori",
  "llm.load.llama.ropeFrequencyBase/title": "Frekuensi Dasar RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frekuensi dasar khusus untuk embedding posisi rotary (RoPE). Meningkatkan ini dapat memungkinkan kinerja yang lebih baik pada panjang konteks yang tinggi",
  "llm.load.llama.ropeFrequencyBase/info": "[Lanjutan] Menyesuaikan frekuensi dasar untuk Rotary Positional Encoding, mempengaruhi bagaimana informasi posisi diembed",
  "llm.load.llama.ropeFrequencyScale/title": "Skala Frekuensi RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Panjang konteks diskalakan oleh faktor ini untuk memperpanjang konteks efektif menggunakan RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Lanjutan] Memodifikasi skala frekuensi untuk Rotary Positional Encoding untuk mengontrol granularitas encoding posisi",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Jumlah lapisan model diskrit yang dihitung pada GPU untuk akselerasi GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Tetapkan jumlah lapisan yang di-offload ke GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Mengurangi penggunaan memori dan waktu pembuatan pada beberapa model",
  "llm.load.llama.flashAttention/info": "Mempercepat mekanisme perhatian untuk pemrosesan yang lebih cepat dan lebih efisien",
  "llm.load.numExperts/title": "Jumlah Ahli",
  "llm.load.numExperts/subTitle": "Jumlah ahli yang digunakan dalam model",
  "llm.load.numExperts/info": "Jumlah ahli yang digunakan dalam model",
  "llm.load.llama.keepModelInMemory/title": "Simpan Model di Memori",
  "llm.load.llama.keepModelInMemory/subTitle": "Cadangkan memori sistem untuk model, bahkan saat di-offload ke GPU. Meningkatkan kinerja tetapi membutuhkan lebih banyak RAM sistem",
  "llm.load.llama.keepModelInMemory/info": "Mencegah model dari ditukar ke disk, memastikan akses lebih cepat dengan biaya penggunaan RAM yang lebih tinggi",
  "llm.load.llama.useFp16ForKVCache/title": "Gunakan FP16 Untuk KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Mengurangi penggunaan memori dengan menyimpan cache dalam presisi setengah (FP16)",
  "llm.load.llama.tryMmap/title": "Coba mmap()",
  "llm.load.llama.tryMmap/subTitle": "Meningkatkan waktu muat untuk model. Menonaktifkan ini dapat meningkatkan kinerja saat model lebih besar dari RAM sistem yang tersedia",
  "llm.load.llama.tryMmap/info": "Muat file model langsung dari disk ke memori",

  "embedding.load.contextLength/title": "Panjang Konteks",
  "embedding.load.contextLength/subTitle": "Jumlah maksimum token yang dapat diperhatikan model dalam satu prompt. Lihat opsi Overflow Percakapan di bawah \"Parameter Inferensi\" untuk lebih banyak cara mengelola ini",
  "embedding.load.contextLength/info": "Menentukan jumlah maksimum token yang dapat dipertimbangkan model sekaligus, mempengaruhi seberapa banyak konteks yang dipertahankan selama pemrosesan",
  "embedding.load.llama.ropeFrequencyBase/title": "Frekuensi Dasar RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frekuensi dasar khusus untuk embedding posisi rotary (RoPE). Meningkatkan ini dapat memungkinkan kinerja yang lebih baik pada panjang konteks yang tinggi",
  "embedding.load.llama.ropeFrequencyBase/info": "[Lanjutan] Menyesuaikan frekuensi dasar untuk Rotary Positional Encoding, mempengaruhi bagaimana informasi posisi diembed",
  "embedding.load.llama.evalBatchSize/title": "Ukuran Batch Evaluasi",
  "embedding.load.llama.evalBatchSize/subTitle": "Jumlah token input yang diproses sekaligus. Meningkatkan ini meningkatkan kinerja dengan biaya penggunaan memori",
  "embedding.load.llama.evalBatchSize/info": "Menetapkan jumlah token yang diproses bersama dalam satu batch selama evaluasi",
  "embedding.load.llama.ropeFrequencyScale/title": "Skala Frekuensi RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Panjang konteks diskalakan oleh faktor ini untuk memperpanjang konteks efektif menggunakan RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Lanjutan] Memodifikasi skala frekuensi untuk Rotary Positional Encoding untuk mengontrol granularitas encoding posisi",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Jumlah lapisan model diskrit yang dihitung pada GPU untuk akselerasi GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Tetapkan jumlah lapisan yang di-offload ke GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Simpan Model di Memori",
  "embedding.load.llama.keepModelInMemory/subTitle": "Cadangkan memori sistem untuk model, bahkan saat di-offload ke GPU. Meningkatkan kinerja tetapi membutuhkan lebih banyak RAM sistem",
  "embedding.load.llama.keepModelInMemory/info": "Mencegah model dari ditukar ke disk, memastikan akses lebih cepat dengan biaya penggunaan RAM yang lebih tinggi",
  "embedding.load.llama.tryMmap/title": "Coba mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Meningkatkan waktu muat untuk model. Menonaktifkan ini dapat meningkatkan kinerja saat model lebih besar dari RAM sistem yang tersedia",
  "embedding.load.llama.tryMmap/info": "Muat file model langsung dari disk ke memori",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "Seed untuk generator angka acak yang digunakan dalam pembuatan teks. -1 adalah seed acak",

  "embedding.load.seed/info": "Seed Acak: Menetapkan seed untuk pembuatan angka acak untuk memastikan hasil yang dapat direproduksi",

  "presetTooltip": {
    "included/title": "Nilai Preset",
    "included/description": "Bidang berikut akan diterapkan",
    "included/empty": "Tidak ada bidang dari preset ini yang berlaku dalam konteks ini.",
    "included/conflict": "Anda akan diminta untuk memilih apakah akan menerapkan nilai ini",
    "separateLoad/title": "Konfigurasi Waktu Muat",
    "separateLoad/description.1": "Preset juga mencakup konfigurasi waktu muat berikut. Konfigurasi waktu muat bersifat model-wide dan memerlukan pemuatan ulang model untuk berlaku. Tahan",
    "separateLoad/description.2": "untuk menerapkan ke",
    "separateLoad/description.3": ".",
    "excluded/title": "Mungkin tidak berlaku",
    "excluded/description": "Bidang berikut termasuk dalam preset tetapi tidak berlaku dalam konteks saat ini.",
    "legacy/title": "Preset Warisan",
    "legacy/description": "Preset ini adalah preset warisan. Ini mencakup bidang-bidang berikut yang sekarang ditangani secara otomatis, atau tidak lagi berlaku."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Kosong>"
    },
    "checkboxNumeric": {
      "off": "MATI"
    },
    "stringArray": {
      "empty": "<Kosong>"
    },
    "llmPromptTemplate": {
      "type": "Tipe",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Template",
      "jinja/error": "Gagal memparsing template Jinja: {{error}}",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Sebelum Sistem",
      "manual.subfield.beforeSystem/placeholder": "Masukkan awalan Sistem...",
      "manual.subfield.afterSystem/label": "Setelah Sistem",
      "manual.subfield.afterSystem/placeholder": "Masukkan akhiran Sistem...",
      "manual.subfield.beforeUser/label": "Sebelum Pengguna",
      "manual.subfield.beforeUser/placeholder": "Masukkan awalan Pengguna...",
      "manual.subfield.afterUser/label": "Setelah Pengguna",
      "manual.subfield.afterUser/placeholder": "Masukkan akhiran Pengguna...",
      "manual.subfield.beforeAssistant/label": "Sebelum Asisten",
      "manual.subfield.beforeAssistant/placeholder": "Masukkan awalan Asisten...",
      "manual.subfield.afterAssistant/label": "Setelah Asisten",
      "manual.subfield.afterAssistant/placeholder": "Masukkan akhiran Asisten...",
      "stopStrings/label": "String Penghenti Tambahan",
      "stopStrings/subTitle": "String penghenti khusus template yang akan digunakan selain string penghenti yang ditentukan pengguna."
    },
    "contextLength": {
      "maxValueTooltip": "Ini adalah jumlah maksimum token yang dapat ditangani model. Klik untuk mengatur konteks ke nilai ini",
      "maxValueTextStart": "Model mendukung hingga",
      "maxValueTextEnd": "token",
      "tooltipHint": "Meskipun model dapat mendukung hingga sejumlah token tertentu, kinerja dapat menurun jika sumber daya mesin Anda tidak dapat menangani beban - gunakan hati-hati saat meningkatkan nilai ini"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Berhenti di Batas",
      "stopAtLimitSub": "Berhenti menghasilkan setelah memori model penuh",
      "truncateMiddle": "Potong Tengah",
      "truncateMiddleSub": "Menghapus pesan dari tengah percakapan untuk memberi ruang bagi yang lebih baru. Model masih akan mengingat awal percakapan",
      "rollingWindow": "Jendela Bergulir",
      "rollingWindowSub": "Model akan selalu mendapatkan beberapa pesan terbaru tetapi mungkin melupakan awal percakapan"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAKS",
      "off": "MATI"
    }
  },
  "saveConflictResolution": {
    "title": "Pilih nilai mana yang akan disertakan dalam Preset",
    "description": "Pilih dan pilih nilai mana yang akan disimpan",
    "instructions": "Klik pada nilai untuk menyertakannya",
    "userValues": "Nilai Sebelumnya",
    "presetValues": "Nilai Baru",
    "confirm": "Konfirmasi",
    "cancel": "Batal"
  },
  "applyConflictResolution": {
    "title": "Nilai mana yang akan disimpan?",
    "description": "Anda memiliki perubahan yang belum dikomit yang tumpang tindih dengan Preset yang masuk",
    "instructions": "Klik pada nilai untuk menyimpannya",
    "userValues": "Nilai Saat Ini",
    "presetValues": "Nilai Preset yang Masuk",
    "confirm": "Konfirmasi",
    "cancel": "Batal"
  },
  "empty": "<Kosong>",
  "presets": {
    "title": "Preset",
    "commitChanges": "Komit Perubahan",
    "commitChanges/description": "Komit perubahan Anda ke preset.",
    "commitChanges.manual": "Bidang baru terdeteksi. Anda akan dapat memilih perubahan mana yang akan disertakan dalam preset.",
    "commitChanges.manual.hold.0": "Tahan",
    "commitChanges.manual.hold.1": "untuk memilih perubahan mana yang akan dikomit ke preset.",
    "commitChanges.saveAll.hold.0": "Tahan",
    "commitChanges.saveAll.hold.1": "untuk menyimpan semua perubahan.",
    "commitChanges.saveInPreset.hold.0": "Tahan",
    "commitChanges.saveInPreset.hold.1": "untuk hanya menyimpan perubahan pada bidang yang sudah termasuk dalam preset.",
    "commitChanges/error": "Gagal mengkomit perubahan ke preset.",
    "commitChanges.manual/description": "Pilih perubahan mana yang akan disertakan dalam preset.",
    "saveAs": "Simpan Sebagai Baru...",
    "presetNamePlaceholder": "Masukkan nama untuk preset...",
    "cannotCommitChangesLegacy": "Ini adalah preset warisan dan tidak dapat dimodifikasi. Anda dapat membuat salinan dengan menggunakan \"Simpan Sebagai Baru...\".",
    "cannotCommitChangesNoChanges": "Tidak ada perubahan untuk dikomit.",
    "emptyNoUnsaved": "Pilih Preset...",
    "emptyWithUnsaved": "Preset Belum Tersimpan",
    "saveEmptyWithUnsaved": "Simpan Preset Sebagai...",
    "saveConfirm": "Simpan",
    "saveCancel": "Batal",
    "saving": "Menyimpan...",
    "save/error": "Gagal menyimpan preset.",
    "deselect": "Batalkan Pilihan Preset",
    "deselect/error": "Gagal membatalkan pilihan preset.",
    "select/error": "Gagal memilih preset.",
    "delete/error": "Gagal menghapus preset.",
    "discardChanges": "Buang Perubahan yang Belum Tersimpan",
    "discardChanges/info": "Buang semua perubahan yang belum dikomit dan kembalikan preset ke keadaan aslinya",
    "newEmptyPreset": "Buat preset kosong baru...",
    "contextMenuSelect": "Pilih Preset",
    "contextMenuDelete": "Hapus"
  },

  "flashAttentionWarning": "Flash Attention adalah fitur eksperimental yang dapat menyebabkan masalah dengan beberapa model. Jika Anda mengalami masalah, coba nonaktifkan.",

  "seedUncheckedHint": "Seed Acak",
  "ropeFrequencyBaseUncheckedHint": "Otomatis",
  "ropeFrequencyScaleUncheckedHint": "Otomatis"
}
