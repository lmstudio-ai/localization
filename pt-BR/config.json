{
  "noInstanceSelected": "Nenhuma inst√¢ncia de modelo selecionada",
  "resetToDefault": "Redefinir",
  "showAdvancedSettings": "Mostrar configura√ß√µes avan√ßadas",
  "showAll": "Tudo",
  "basicSettings": "B√°sico",
  "configSubtitle": "Carregue ou salve predefini√ß√µes e experimente com substitui√ß√µes de par√¢metros do modelo",
  "inferenceParameters/title": "Par√¢metros de Previs√£o",
  "inferenceParameters/info": "Experimente par√¢metros que impactam a previs√£o.",
  "generalParameters/title": "Geral",
  "samplingParameters/title": "Amostragem",
  "basicTab": "B√°sico",
  "advancedTab": "Avan√ßado",
  "advancedTab/title": "üß™ Configura√ß√£o Avan√ßada",
  "advancedTab/expandAll": "Expandir tudo",
  "advancedTab/overridesTitle": "Substitui√ß√µes de Configura√ß√£o",
  "advancedTab/noConfigsText": "Voc√™ n√£o possui altera√ß√µes n√£o salvas. Edite os valores acima para ver as substitui√ß√µes aqui.",
  "loadInstanceFirst": "Carregue um modelo para visualizar os par√¢metros configur√°veis",
  "noListedConfigs": "Nenhum par√¢metro configur√°vel",
  "generationParameters/info": "Experimente com par√¢metros b√°sicos que impactam a gera√ß√£o de texto.",
  "loadParameters/title": "Par√¢metros de Carregamento",
  "loadParameters/description": "Configura√ß√µes que controlam como o modelo √© inicializado e carregado na mem√≥ria.",
  "loadParameters/reload": "Recarregar para aplicar altera√ß√µes",
  "loadParameters/reload/error": "Falha ao recarregar o modelo",
  "discardChanges": "Descartar altera√ß√µes",
  "loadModelToSeeOptions": "Carregue um modelo para ver as op√ß√µes",
  "schematicsError.title": "Os esquemas de configura√ß√£o cont√™m erros nos seguintes campos:",
  "manifestSections": {
    "structuredOutput/title": "Sa√≠da Estruturada",
    "speculativeDecoding/title": "Decodifica√ß√£o Especulativa",
    "sampling/title": "Amostragem",
    "settings/title": "Configura√ß√µes",
    "toolUse/title": "Uso de Ferramentas",
    "promptTemplate/title": "Modelo de Prompt"
  },

  "llm.prediction.systemPrompt/title": "Prompt de Sistema",
  "llm.prediction.systemPrompt/description": "Use este campo para fornecer instru√ß√µes de base ao modelo, como regras, restri√ß√µes ou requisitos gerais.",
  "llm.prediction.systemPrompt/subTitle": "Diretrizes para a IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Define o n√≠vel de aleatoriedade. 0 sempre produzir√° o mesmo resultado, enquanto valores mais altos aumentam a criatividade e a varia√ß√£o.",
  "llm.prediction.temperature/info": "Da documenta√ß√£o do llama.cpp: \"O valor padr√£o √© <{{dynamicValue}}>, que oferece um equil√≠brio entre aleatoriedade e determinismo. No extremo, uma temperatura de 0 sempre escolher√° o token mais prov√°vel, resultando em sa√≠das id√™nticas a cada execu√ß√£o.\"",
  "llm.prediction.llama.sampling/title": "Amostragem",
  "llm.prediction.topKSampling/title": "Amostragem Top-K",
  "llm.prediction.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos 'k' tokens mais prov√°veis. Atua de forma semelhante √† temperatura.",
  "llm.prediction.topKSampling/info": "Da documenta√ß√£o do llama.cpp:\n\nA amostragem Top-K √© um m√©todo que seleciona o pr√≥ximo token apenas entre os 'k' tokens mais prov√°veis previstos pelo modelo.\n\nAjuda a reduzir o risco de gerar tokens de baixa probabilidade ou sem sentido, mas tamb√©m pode limitar a diversidade da sa√≠da.\n\nUm valor maior para 'k' (ex: 100) considerar√° mais tokens, gerando textos mais diversos, enquanto um valor menor (ex: 10) focar√° nos mais prov√°veis, gerando textos mais conservadores.\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads da CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "N√∫mero de threads da CPU a serem usados durante a infer√™ncia",
  "llm.prediction.llama.cpuThreads/info": "O n√∫mero de threads a serem usados durante o processamento. Aumentar o n√∫mero de threads nem sempre melhora o desempenho. O padr√£o √© <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitar Tamanho da Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Define um limite opcional para o tamanho da resposta da IA.",
  "llm.prediction.maxPredictedTokens/info": "Controle o tamanho m√°ximo da resposta do chatbot. Ative para definir um limite ou desative para permitir que o chatbot decida quando parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Tamanho m√°ximo da resposta (em tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadamente {{maxWords}} palavras",
  "llm.prediction.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
  "llm.prediction.repeatPenalty/subTitle": "Define uma penalidade para desencorajar a repeti√ß√£o do mesmo token.",
  "llm.prediction.repeatPenalty/info": "Da documenta√ß√£o do llama.cpp: \"Ajuda a evitar que o modelo gere textos repetitivos ou mon√≥tonos.\n\nUm valor mais alto (ex: 1.5) penalizar√° repeti√ß√µes com mais for√ßa, enquanto um valor menor (ex: 0.9) ser√° mais tolerante.\" ‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Amostragem Min-P",
  "llm.prediction.minPSampling/subTitle": "Probabilidade m√≠nima base para um token ser selecionado.",
  "llm.prediction.minPSampling/info": "Da documenta√ß√£o do llama.cpp:\n\nA probabilidade m√≠nima para um token ser considerado, em rela√ß√£o √† probabilidade do token mais prov√°vel. Deve estar entre [0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Amostragem Top-P",
  "llm.prediction.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os pr√≥ximos tokens poss√≠veis. Atua de forma semelhante √† temperatura.",
  "llm.prediction.topPSampling/info": "Da documenta√ß√£o do llama.cpp:\n\nA amostragem Top-P (ou amostragem de n√∫cleo) seleciona o pr√≥ximo token de um subconjunto de tokens cuja probabilidade cumulativa seja pelo menos 'p'.\n\nEste m√©todo equilibra diversidade e qualidade. Um valor maior para 'p' (ex: 0.95) gera textos mais diversos, enquanto um valor menor (ex: 0.5) gera textos mais focados e conservadores. Deve estar entre (0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Strings de Parada",
  "llm.prediction.stopStrings/subTitle": "Textos que, ao serem gerados, interrompem a resposta do modelo.",
  "llm.prediction.stopStrings/info": "Strings espec√≠ficas que, quando encontradas, far√£o o modelo parar de gerar mais tokens.",
  "llm.prediction.stopStrings/placeholder": "Digite um texto e pressione ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Excesso de Contexto",
  "llm.prediction.contextOverflowPolicy/subTitle": "Como o modelo deve agir quando a conversa excede seu limite de contexto.",
  "llm.prediction.contextOverflowPolicy/info": "Decida o que fazer quando a conversa exceder o tamanho da mem√≥ria de trabalho do modelo (o 'contexto').",
  "llm.prediction.llama.frequencyPenalty/title": "Penalidade de Frequ√™ncia",
  "llm.prediction.llama.presencePenalty/title": "Penalidade de Presen√ßa",
  "llm.prediction.llama.tailFreeSampling/title": "Amostragem Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem Localmente T√≠pica",
  "llm.prediction.llama.xtcProbability/title": "Probabilidade da Amostragem XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "O amostrador XTC (Exclude Top Choices) ser√° ativado com esta probabilidade a cada token gerado. Pode aumentar a criatividade e reduzir clich√™s.",
  "llm.prediction.llama.xtcProbability/info": "A amostragem XTC (Exclude Top Choices) s√≥ ser√° ativada com esta probabilidade, por token gerado. A amostragem XTC geralmente aumenta a criatividade e reduz clich√™s.",
  "llm.prediction.llama.xtcThreshold/title": "Limiar da Amostragem XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Limiar XTC. Com uma chance de 'xtc-probability', busca tokens com probabilidades entre 'xtc-threshold' e 0.5, removendo todos, exceto o menos prov√°vel.",
  "llm.prediction.llama.xtcThreshold/info": "Limiar XTC (Exclude Top Choices). Com uma chance de 'xtc-probability', busca tokens com probabilidades entre 'xtc-threshold' e 0.5, removendo todos, exceto o menos prov√°vel.",
  "llm.prediction.mlx.topKSampling/title": "Amostragem Top-K",
  "llm.prediction.mlx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos 'k' tokens mais prov√°veis. Atua de forma semelhante √† temperatura.",
  "llm.prediction.mlx.topKSampling/info": "Limita o pr√≥ximo token a um dos 'k' tokens mais prov√°veis. Atua de forma semelhante √† temperatura.",
  "llm.prediction.onnx.topKSampling/title": "Amostragem Top-K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos 'k' tokens mais prov√°veis. Atua de forma semelhante √† temperatura.",
  "llm.prediction.onnx.topKSampling/info": "Da documenta√ß√£o ONNX:\n\nN√∫mero de tokens de maior probabilidade a manter para a filtragem Top-K.\n\n‚Ä¢ Este filtro est√° desativado por padr√£o.",
  "llm.prediction.onnx.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Define uma penalidade para desencorajar a repeti√ß√£o do mesmo token.",
  "llm.prediction.onnx.repeatPenalty/info": "Um valor mais alto desencoraja o modelo de se repetir.",
  "llm.prediction.onnx.topPSampling/title": "Amostragem Top-P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os pr√≥ximos tokens poss√≠veis. Atua de forma semelhante √† temperatura.",
  "llm.prediction.onnx.topPSampling/info": "Da documenta√ß√£o ONNX:\n\nApenas os tokens mais prov√°veis com probabilidades que somam Top-P ou mais s√£o mantidos para gera√ß√£o.\n\n‚Ä¢ Este filtro est√° desativado por padr√£o.",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Sa√≠da Estruturada",
  "llm.prediction.structured/info": "Sa√≠da Estruturada",
  "llm.prediction.structured/description": "Avan√ßado: voc√™ pode fornecer um [Esquema JSON](https://json-schema.org/learn/miscellaneous-examples) para for√ßar um formato de sa√≠da espec√≠fico do modelo. Leia a [documenta√ß√£o](https://lmstudio.ai/docs/advanced/structured-output) para saber mais.",
  "llm.prediction.tools/title": "Uso de Ferramentas",
  "llm.prediction.tools/description": "Avan√ßado: voc√™ pode fornecer uma lista de ferramentas em formato JSON para que o modelo possa solicitar chamadas. Leia a [documenta√ß√£o](https://lmstudio.ai/docs/advanced/tool-use) para saber mais.",
  "llm.prediction.tools/serverPageDescriptionAddon": "Passe isso no corpo da requisi√ß√£o como `tools` ao usar a API do servidor.",
  "llm.prediction.promptTemplate/title": "Modelo de Prompt",
  "llm.prediction.promptTemplate/subTitle": "O formato no qual as mensagens da conversa s√£o enviadas ao modelo. Alterar isso pode causar comportamentos inesperados. Tenha certeza do que est√° fazendo!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Tokens de Rascunho a Gerar",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "O n√∫mero de tokens a gerar com o modelo de rascunho por token do modelo principal. Encontre o equil√≠brio ideal entre custo computacional e benef√≠cio.",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Limiar de Probabilidade do Rascunho",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue rascunhando at√© que a probabilidade de um token caia abaixo deste limiar. Valores mais altos geralmente significam menor risco e menor recompensa.",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Tamanho M√≠nimo do Rascunho",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Rascunhos menores que este ser√£o ignorados pelo modelo principal. Valores mais altos geralmente significam menor risco e menor recompensa.",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Tamanho M√°ximo do Rascunho",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "N√∫mero m√°ximo de tokens permitidos em um rascunho. √â o teto se todas as probabilidades forem maiores que o limiar. Valores mais baixos geralmente significam menor risco e menor recompensa.",
  "llm.prediction.speculativeDecoding.draftModel/title": "Modelo de Rascunho",
  "llm.prediction.reasoning.parsing/title": "An√°lise da Se√ß√£o de Racioc√≠nio",
  "llm.prediction.reasoning.parsing/subTitle": "Como analisar se√ß√µes de racioc√≠nio na sa√≠da do modelo.",

  "llm.load.contextLength/title": "Tamanho do Contexto",
  "llm.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode processar de uma vez. Veja as op√ß√µes de Excesso de Contexto para mais formas de gerenciar isso.",
  "llm.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando quanto contexto ele ret√©m durante o processamento.",
  "llm.load.contextLength/warning": "Definir um valor alto para o tamanho do contexto pode impactar significativamente o uso de mem√≥ria.",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "A 'seed' para o gerador de n√∫meros aleat√≥rios. Use -1 para aleat√≥rio.",
  "llm.load.seed/info": "Seed Aleat√≥ria: Define a 'seed' para a gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis.",

  "llm.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
  "llm.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a serem processados por vez. Aumentar este valor melhora o desempenho, mas consome mais mem√≥ria.",
  "llm.load.llama.evalBatchSize/info": "Define o n√∫mero de exemplos processados juntos em um lote durante a avalia√ß√£o, afetando a velocidade e o uso de mem√≥ria.",
  "llm.load.llama.ropeFrequencyBase/title": "Frequ√™ncia Base do RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base para embeddings posicionais rotativos (RoPE). Aumentar este valor pode melhorar o desempenho em contextos longos.",
  "llm.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa (RoPE), afetando como a informa√ß√£o posicional √© codificada.",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia do RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "O tamanho do contexto √© escalonado por este fator para estender o contexto efetivo usando RoPE.",
  "llm.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa (RoPE) para controlar a granularidade da codifica√ß√£o posicional.",
  "llm.load.llama.acceleration.offloadRatio/title": "Descarregar para GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas do modelo a serem processadas na GPU para acelerar o desempenho.",
  "llm.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para descarregar na GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Reduz o uso de mem√≥ria e o tempo de gera√ß√£o em alguns modelos.",
  "llm.load.llama.flashAttention/info": "Acelera os mecanismos de aten√ß√£o para um processamento mais r√°pido e eficiente.",
  "llm.load.numExperts/title": "N√∫mero de Especialistas",
  "llm.load.numExperts/subTitle": "N√∫mero de especialistas (experts) a serem usados no modelo.",
  "llm.load.numExperts/info": "O n√∫mero de especialistas (experts) a serem usados no modelo.",
  "llm.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserva mem√≥ria RAM para o modelo, mesmo quando descarregado para a GPU. Melhora o desempenho, mas exige mais RAM.",
  "llm.load.llama.keepModelInMemory/info": "Evita que o modelo seja movido para o disco, garantindo acesso mais r√°pido ao custo de maior uso de RAM.",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para o Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de mem√≥ria armazenando o cache em meia precis√£o (FP16).",
  "llm.load.llama.tryMmap/title": "Tentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desativar pode melhorar o desempenho se o modelo for maior que a RAM dispon√≠vel.",
  "llm.load.llama.tryMmap/info": "Carrega os arquivos do modelo diretamente do disco para a mem√≥ria.",
  "llm.load.llama.cpuThreadPoolSize/title": "Tamanho do Pool de Threads da CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "N√∫mero de threads da CPU a alocar para o pool de threads usado para computa√ß√£o do modelo.",
  "llm.load.llama.cpuThreadPoolSize/info": "O n√∫mero de threads da CPU a alocar para o pool de threads usado na computa√ß√£o. Aumentar o n√∫mero nem sempre melhora o desempenho. O padr√£o √© <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Tipo de Quantiza√ß√£o do Cache K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Valores menores reduzem o uso de mem√≥ria, mas podem diminuir a qualidade. O efeito varia entre os modelos.",
  "llm.load.llama.vCacheQuantizationType/title": "Tipo de Quantiza√ß√£o do Cache V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Valores menores reduzem o uso de mem√≥ria, mas podem diminuir a qualidade. O efeito varia entre os modelos.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Voc√™ deve desativar este valor se o Flash Attention n√£o estiver habilitado.",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "S√≥ pode ser ativado quando o Flash Attention estiver habilitado.",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Voc√™ deve desativar o Flash Attention ao usar F32.",
  "llm.load.mlx.kvCacheBits/title": "Quantiza√ß√£o do Cache KV",
  "llm.load.mlx.kvCacheBits/subTitle": "N√∫mero de bits para quantizar o cache KV.",
  "llm.load.mlx.kvCacheBits/info": "N√∫mero de bits para o qual o cache KV deve ser quantizado.",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "A configura√ß√£o de Tamanho de Contexto √© ignorada ao usar Quantiza√ß√£o do Cache KV.",
  "llm.load.mlx.kvCacheGroupSize/title": "Quantiza√ß√£o do Cache KV: Tamanho do Grupo",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Tamanho do grupo na quantiza√ß√£o do cache KV. Um grupo maior reduz o uso de mem√≥ria, mas pode diminuir a qualidade.",
  "llm.load.mlx.kvCacheGroupSize/info": "N√∫mero de bits para o qual o cache KV deve ser quantizado.",
  "llm.load.mlx.kvCacheQuantizationStart/title": "Quantiza√ß√£o do Cache KV: Iniciar quando o contexto atingir este tamanho",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Limiar de tamanho de contexto para iniciar a quantiza√ß√£o do cache KV.",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Limiar de tamanho de contexto para iniciar a quantiza√ß√£o do cache KV.",
  "llm.load.mlx.kvCacheQuantization/title": "Quantiza√ß√£o do Cache KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantiza o cache KV do modelo. Isso pode resultar em gera√ß√£o mais r√°pida e menor uso de mem√≥ria, ao custo da qualidade da sa√≠da.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "Bits de quantiza√ß√£o do cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "N√∫mero de bits para quantizar o cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Estrat√©gia de tamanho do grupo",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Precis√£o",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanceado",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "R√°pido",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Avan√ßado: Configura√ß√£o do 'tamanho do grupo matmul' quantizado\n\n‚Ä¢ Precis√£o = tamanho 32\n‚Ä¢ Balanceado = tamanho 64\n‚Ä¢ R√°pido = tamanho 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Come√ßar a quantizar quando o contexto atingir este tamanho",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Quando o contexto atingir esta quantidade de tokens,\ncome√ßar a quantizar o cache KV.",

  "embedding.load.contextLength/title": "Tamanho do Contexto",
  "embedding.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode processar de uma vez. Veja as op√ß√µes de Excesso de Contexto para mais formas de gerenciar isso.",
  "embedding.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando quanto contexto ele ret√©m durante o processamento.",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequ√™ncia Base do RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base para embeddings posicionais rotativos (RoPE). Aumentar este valor pode melhorar o desempenho em contextos longos.",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa (RoPE), afetando como a informa√ß√£o posicional √© codificada.",
  "embedding.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
  "embedding.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a serem processados por vez. Aumentar este valor melhora o desempenho, mas consome mais mem√≥ria.",
  "embedding.load.llama.evalBatchSize/info": "Define o n√∫mero de tokens processados juntos em um lote durante a avalia√ß√£o.",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia do RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "O tamanho do contexto √© escalonado por este fator para estender o contexto efetivo usando RoPE.",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa (RoPE) para controlar a granularidade da codifica√ß√£o posicional.",
  "embedding.load.llama.acceleration.offloadRatio/title": "Descarregado para GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas do modelo a serem processadas na GPU para acelerar o desempenho.",
  "embedding.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para descarregar na GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserva mem√≥ria RAM para o modelo, mesmo quando descarregado para a GPU. Melhora o desempenho, mas exige mais RAM.",
  "embedding.load.llama.keepModelInMemory/info": "Evita que o modelo seja movido para o disco, garantindo acesso mais r√°pido ao custo de maior uso de RAM.",
  "embedding.load.llama.tryMmap/title": "Tentar mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desativar pode melhorar o desempenho se o modelo for maior que a RAM dispon√≠vel.",
  "embedding.load.llama.tryMmap/info": "Carrega os arquivos do modelo diretamente do disco para a mem√≥ria.",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "A 'seed' para o gerador de n√∫meros aleat√≥rios. Use -1 para aleat√≥rio.",

  "embedding.load.seed/info": "Seed Aleat√≥ria: Define a 'seed' para a gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis.",

  "presetTooltip": {
    "included/title": "Valores da Predefini√ß√£o",
    "included/description": "Os seguintes campos ser√£o aplicados",
    "included/empty": "Nenhum campo desta predefini√ß√£o se aplica neste contexto.",
    "included/conflict": "Voc√™ precisar√° escolher se deseja aplicar este valor.",
    "separateLoad/title": "Configura√ß√£o de Carregamento",
    "separateLoad/description.1": "A predefini√ß√£o tamb√©m inclui a seguinte configura√ß√£o de carregamento. Esta configura√ß√£o afeta o modelo todo e requer que ele seja recarregado para ter efeito. Segure",
    "separateLoad/description.2": "para aplicar a",
    "separateLoad/description.3": ".",
    "excluded/title": "Pode n√£o se aplicar",
    "excluded/description": "Os seguintes campos est√£o inclu√≠dos na predefini√ß√£o, mas n√£o se aplicam no contexto atual.",
    "legacy/title": "Predefini√ß√£o Legada",
    "legacy/description": "Esta √© uma predefini√ß√£o legada. Ela inclui campos que agora s√£o gerenciados automaticamente ou n√£o s√£o mais aplic√°veis.",
    "button/publish": "Publicar no Hub",
    "button/pushUpdate": "Enviar Altera√ß√µes para o Hub",
    "button/export": "Exportar"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Vazio>"
    },
    "checkboxNumeric": {
      "off": "DESL."
    },
    "llamaCacheQuantizationType": {
      "off": "DESL."
    },
    "mlxKvCacheBits": {
      "off": "DESL."
    },
    "stringArray": {
      "empty": "<Vazio>"
    },
    "llmPromptTemplate": {
      "type": "Tipo",
      "types.jinja/label": "Modelo (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Modelo",
      "jinja/error": "Falha ao analisar o modelo Jinja: {{error}}",
      "jinja/empty": "Por favor, insira um modelo Jinja acima.",
      "jinja/unlikelyToWork": "O modelo Jinja que voc√™ forneceu provavelmente n√£o funcionar√°, pois n√£o faz refer√™ncia √† vari√°vel \"messages\". Verifique se o modelo est√° correto.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Antes do Sistema",
      "manual.subfield.beforeSystem/placeholder": "Insira o prefixo do Sistema...",
      "manual.subfield.afterSystem/label": "Depois do Sistema",
      "manual.subfield.afterSystem/placeholder": "Insira o sufixo do Sistema...",
      "manual.subfield.beforeUser/label": "Antes do Usu√°rio",
      "manual.subfield.beforeUser/placeholder": "Insira o prefixo do Usu√°rio...",
      "manual.subfield.afterUser/label": "Depois do Usu√°rio",
      "manual.subfield.afterUser/placeholder": "Insira o sufixo do Usu√°rio...",
      "manual.subfield.beforeAssistant/label": "Antes do Assistente",
      "manual.subfield.beforeAssistant/placeholder": "Insira o prefixo do Assistente...",
      "manual.subfield.afterAssistant/label": "Depois do Assistente",
      "manual.subfield.afterAssistant/placeholder": "Insira o sufixo do Assistente...",
      "stopStrings/label": "Strings de Parada Adicionais",
      "stopStrings/subTitle": "Strings de parada espec√≠ficas do modelo que ser√£o usadas em adi√ß√£o √†s definidas pelo usu√°rio."
    },
    "contextLength": {
      "maxValueTooltip": "Este √© o n√∫mero m√°ximo de tokens que o modelo foi treinado para suportar. Clique para definir o contexto com este valor.",
      "maxValueTextStart": "O modelo suporta at√©",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "Embora um modelo suporte um certo n√∫mero de tokens, o desempenho pode ser afetado se os recursos da sua m√°quina n√£o forem suficientes. Tenha cuidado ao aumentar este valor."
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Parar no Limite",
      "stopAtLimitSub": "Para de gerar respostas quando a mem√≥ria do modelo estiver cheia.",
      "truncateMiddle": "Truncar o Meio",
      "truncateMiddleSub": "Remove mensagens do meio da conversa para abrir espa√ßo. O modelo ainda lembrar√° o in√≠cio.",
      "rollingWindow": "Janela Deslizante",
      "rollingWindowSub": "O modelo sempre ter√° as mensagens mais recentes, mas poder√° esquecer o in√≠cio da conversa."
    },
    "llamaAccelerationOffloadRatio": {
      "max": "M√ÅX.",
      "off": "DESL."
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Dividir Igualmente",
      "favorMainGpu": "Priorizar GPU Principal"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Entenda como funciona",
      "placeholder": "Selecione um modelo de rascunho compat√≠vel",
      "noCompatible": "Nenhum modelo de rascunho compat√≠vel encontrado para o modelo principal selecionado.",
      "stillLoading": "Identificando modelos de rascunho compat√≠veis...",
      "notCompatible": "O modelo de rascunho selecionado (<draft/>) n√£o √© compat√≠vel com o modelo principal (<current/>).",
      "off": "DESL.",
      "loadModelToSeeOptions": "Carregue o modelo <keyboard-shortcut /> para ver op√ß√µes compat√≠veis.",
      "compatibleWithNumberOfModels": "Recomendado para pelo menos {{dynamicValue}} dos seus modelos",
      "recommendedForSomeModels": "Recomendado para alguns modelos",
      "recommendedForLlamaModels": "Recomendado para modelos Llama",
      "recommendedForQwenModels": "Recomendado para modelos Qwen",
      "onboardingModal": {
        "introducing": "Apresentando",
        "speculativeDecoding": "Decodifica√ß√£o Especulativa",
        "firstStepBody": "Acelera√ß√£o de infer√™ncia para modelos <custom-span>llama.cpp</custom-span> e <custom-span>MLX</custom-span>.",
        "secondStepTitle": "Acelera√ß√£o de Infer√™ncia com Decodifica√ß√£o Especulativa",
        "secondStepBody": "Decodifica√ß√£o Especulativa √© uma t√©cnica que usa dois modelos:\n- Um modelo \"principal\" maior\n- Um modelo \"de rascunho\" menor\n\nDurante a gera√ß√£o, o modelo de rascunho prop√µe tokens rapidamente, e o modelo principal os verifica. Verificar √© muito mais r√°pido que gerar, o que acelera o processo. **Geralmente, quanto maior a diferen√ßa de tamanho entre os modelos, maior a acelera√ß√£o**.\n\nPara manter a qualidade, o modelo principal s√≥ aceita tokens que ele mesmo geraria, garantindo a qualidade da resposta do modelo maior com maior velocidade. Ambos os modelos devem compartilhar o mesmo vocabul√°rio.",
        "draftModelRecommendationsTitle": "Recomenda√ß√µes de modelo de rascunho",
        "basedOnCurrentModels": "Com base nos seus modelos atuais",
        "close": "Fechar",
        "next": "Pr√≥ximo",
        "done": "Concluir"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Por favor, carregue um modelo primeiro <model-badge /> ",
      "errorEngineNotSupported": "A decodifica√ß√£o especulativa requer no m√≠nimo a vers√£o {{minVersion}} do motor {{engineName}}. Por favor, atualize o motor (<key/>) e recarregue o modelo.",
      "errorEngineNotSupported/noKey": "A decodifica√ß√£o especulativa requer no m√≠nimo a vers√£o {{minVersion}} do motor {{engineName}}. Por favor, atualize o motor e recarregue o modelo."
    },
    "llmReasoningParsing": {
      "startString/label": "String de In√≠cio",
      "startString/placeholder": "Insira a string de in√≠cio...",
      "endString/label": "String de Fim",
      "endString/placeholder": "Insira a string de fim..."
    }
  },
  "saveConflictResolution": {
    "title": "Escolha quais valores incluir na Predefini√ß√£o",
    "description": "Escolha quais valores manter",
    "instructions": "Clique em um valor para inclu√≠-lo",
    "userValues": "Valor Anterior",
    "presetValues": "Novo Valor",
    "confirm": "Confirmar",
    "cancel": "Cancelar"
  },
  "applyConflictResolution": {
    "title": "Quais valores manter?",
    "description": "Voc√™ possui altera√ß√µes n√£o salvas que conflitam com a Predefini√ß√£o recebida.",
    "instructions": "Clique em um valor para mant√™-lo.",
    "userValues": "Valor Atual",
    "presetValues": "Valor da Predefini√ß√£o Recebida",
    "confirm": "Confirmar",
    "cancel": "Cancelar"
  },
  "empty": "<Vazio>",
  "noModelSelected": "Nenhum modelo selecionado",
  "apiIdentifier.label": "Identificador da API",
  "apiIdentifier.hint": "Opcional. Forne√ßa um identificador para este modelo, que ser√° usado em requisi√ß√µes de API. Deixe em branco para usar o padr√£o.",
  "idleTTL.label": "Descarregar Automaticamente por Inatividade (TTL)",
  "idleTTL.hint": "Se definido, o modelo ser√° descarregado automaticamente ap√≥s o tempo de inatividade especificado.",
  "idleTTL.mins": "min",

  "presets": {
    "title": "Predefini√ß√£o",
    "commitChanges": "Confirmar Altera√ß√µes",
    "commitChanges/description": "Confirme suas altera√ß√µes na predefini√ß√£o.",
    "commitChanges.manual": "Novos campos detectados. Voc√™ poder√° escolher quais altera√ß√µes incluir.",
    "commitChanges.manual.hold.0": "Segure",
    "commitChanges.manual.hold.1": "para escolher quais altera√ß√µes confirmar na predefini√ß√£o.",
    "commitChanges.saveAll.hold.0": "Segure",
    "commitChanges.saveAll.hold.1": "para salvar todas as altera√ß√µes.",
    "commitChanges.saveInPreset.hold.0": "Segure",
    "commitChanges.saveInPreset.hold.1": "para salvar apenas as altera√ß√µes nos campos que j√° est√£o na predefini√ß√£o.",
    "commitChanges/error": "Falha ao confirmar altera√ß√µes na predefini√ß√£o.",
    "commitChanges.manual/description": "Escolha quais altera√ß√µes incluir na predefini√ß√£o.",
    "saveAs": "Salvar Como Nova...",
    "presetNamePlaceholder": "Digite um nome para a predefini√ß√£o...",
    "cannotCommitChangesLegacy": "Esta √© uma predefini√ß√£o legada e n√£o pode ser modificada. Voc√™ pode criar uma c√≥pia usando \"Salvar Como Nova...\".",
    "cannotCommitChangesNoChanges": "Nenhuma altera√ß√£o a ser confirmada.",
    "emptyNoUnsaved": "Selecione uma Predefini√ß√£o...",
    "emptyWithUnsaved": "Predefini√ß√£o N√£o Salva",
    "saveEmptyWithUnsaved": "Salvar Predefini√ß√£o Como...",
    "saveConfirm": "Salvar",
    "saveCancel": "Cancelar",
    "saving": "Salvando...",
    "save/error": "Falha ao salvar predefini√ß√£o.",
    "deselect": "Desselecionar Predefini√ß√£o",
    "deselect/error": "Falha ao desselecionar predefini√ß√£o.",
    "select/error": "Falha ao selecionar predefini√ß√£o.",
    "delete/error": "Falha ao excluir predefini√ß√£o.",
    "discardChanges": "Descartar N√£o Salvas",
    "discardChanges/info": "Descarta todas as altera√ß√µes n√£o salvas e restaura a predefini√ß√£o ao seu estado original.",
    "newEmptyPreset": "+ Nova Predefini√ß√£o",
    "importPreset": "Importar",
    "contextMenuSelect": "Aplicar Predefini√ß√£o",
    "contextMenuDelete": "Excluir...",
    "contextMenuShare": "Publicar...",
    "contextMenuOpenInHub": "Ver no Hub",
    "contextMenuPushChanges": "Enviar altera√ß√µes para o Hub",
    "contextMenuPushingChanges": "Enviando...",
    "contextMenuPushedChanges": "Altera√ß√µes enviadas",
    "contextMenuExport": "Exportar Arquivo",
    "contextMenuRevealInExplorer": "Revelar no Explorador de Arquivos",
    "contextMenuRevealInFinder": "Revelar no Finder",
    "share": {
      "title": "Publicar Predefini√ß√£o",
      "action": "Compartilhe sua predefini√ß√£o para que outros possam baixar, curtir e fazer fork.",
      "presetOwnerLabel": "Propriet√°rio",
      "uploadAs": "Sua predefini√ß√£o ser√° criada como {{name}}",
      "presetNameLabel": "Nome da Predefini√ß√£o",
      "descriptionLabel": "Descri√ß√£o (opcional)",
      "loading": "Publicando...",
      "success": "Predefini√ß√£o Enviada com Sucesso",
      "presetIsLive": "<preset-name /> est√° agora dispon√≠vel no Hub!",
      "close": "Fechar",
      "confirmViewOnWeb": "Ver na web",
      "confirmCopy": "Copiar URL",
      "confirmCopied": "Copiado!",
      "pushedToHub": "Sua predefini√ß√£o foi enviada para o Hub",
      "descriptionPlaceholder": "Digite uma descri√ß√£o...",
      "willBePublic": "Publicar sua predefini√ß√£o a tornar√° p√∫blica.",
      "publicSubtitle": "Sua predefini√ß√£o √© <custom-bold>P√∫blica</custom-bold>. Outros poder√£o baix√°-la e fazer fork em lmstudio.ai.",
      "confirmShareButton": "Publicar",
      "error": "Falha ao publicar predefini√ß√£o.",
      "createFreeAccount": "Crie uma conta gratuita no Hub para publicar predefini√ß√µes."
    },
    "update": {
      "title": "Enviar Altera√ß√µes para o Hub",
      "title/success": "Predefini√ß√£o Atualizada com Sucesso",
      "subtitle": "Fa√ßa altera√ß√µes em <custom-preset-name /> e envie-as para o Hub.",
      "descriptionLabel": "Descri√ß√£o",
      "descriptionPlaceholder": "Digite uma descri√ß√£o...",
      "loading": "Enviando...",
      "cancel": "Cancelar",
      "createFreeAccount": "Crie uma conta gratuita no Hub para publicar predefini√ß√µes.",
      "error": "Falha ao enviar atualiza√ß√£o.",
      "confirmUpdateButton": "Enviar"
    },
    "import": {
      "title": "Importar Predefini√ß√£o de um Arquivo",
      "dragPrompt": "Arraste e solte arquivos JSON de predefini√ß√£o ou <custom-link>selecione do seu computador</custom-link>",
      "remove": "Remover",
      "cancel": "Cancelar",
      "importPreset_zero": "Importar Predefini√ß√£o",
      "importPreset_one": "Importar Predefini√ß√£o",
      "importPreset_other": "Importar {{count}} Predefini√ß√µes",
      "selectDialog": {
        "title": "Selecionar Arquivo de Predefini√ß√£o (.json)",
        "button": "Importar"
      },
      "error": "Falha ao importar predefini√ß√£o.",
      "resultsModal": {
        "titleSuccessSection_one": "1 predefini√ß√£o importada com sucesso",
        "titleSuccessSection_other": "{{count}} predefini√ß√µes importadas com sucesso",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} falhou)",
        "titleFailSection_other": "({{count}} falharam)",
        "titleAllFailed": "Falha ao importar predefini√ß√µes.",
        "importMore": "Importar Mais",
        "close": "Conclu√≠do",
        "successBadge": "Sucesso",
        "alreadyExistsBadge": "Predefini√ß√£o j√° existe",
        "errorBadge": "Erro",
        "invalidFileBadge": "Arquivo inv√°lido",
        "otherErrorBadge": "Falha ao importar predefini√ß√£o",
        "errorViewDetailsButton": "Ver Detalhes",
        "seeError": "Ver Erro",
        "noName": "Sem nome de predefini√ß√£o",
        "useInChat": "Usar no Chat"
      },
      "importFromUrl": {
        "button": "Importar de URL...",
        "title": "Importar de URL",
        "back": "Importar de Arquivo...",
        "action": "Cole abaixo a URL do LM Studio Hub da predefini√ß√£o que voc√™ deseja importar.",
        "invalidUrl": "URL inv√°lida. Por favor, certifique-se de que est√° colando uma URL correta do LM Studio Hub.",
        "tip": "Voc√™ pode instalar a predefini√ß√£o diretamente com o bot√£o {{buttonName}} no LM Studio Hub.",
        "confirm": "Importar",
        "cancel": "Cancelar",
        "loading": "Importando...",
        "error": "Falha ao baixar predefini√ß√£o."
      }
    },
    "download": {
      "title": "Baixar <preset-name /> do LM Studio Hub",
      "subtitle": "Salve <custom-name /> em suas predefini√ß√µes. Ao fazer isso, voc√™ poder√° usar esta predefini√ß√£o no aplicativo.",
      "button": "Baixar",
      "button/loading": "Baixando...",
      "cancel": "Cancelar",
      "error": "Falha ao baixar predefini√ß√£o."
    },
    "inclusiveness": {
      "speculativeDecoding": "Incluir na Predefini√ß√£o"
    }
  },

  "flashAttentionWarning": "O Flash Attention √© um recurso experimental que pode causar problemas com alguns modelos. Se encontrar problemas, tente desativ√°-lo.",
  "llamaKvCacheQuantizationWarning": "A Quantiza√ß√£o do Cache KV √© um recurso experimental que pode causar problemas com alguns modelos. O Flash Attention deve estar habilitado para a quantiza√ß√£o do cache V. Se encontrar problemas, redefina para o padr√£o \"F16\".",

  "seedUncheckedHint": "Seed Aleat√≥ria",
  "ropeFrequencyBaseUncheckedHint": "Autom√°tico",
  "ropeFrequencyScaleUncheckedHint": "Autom√°tico",

  "hardware": {
    "advancedGpuSettings": "Configura√ß√µes Avan√ßadas de GPU",
    "advancedGpuSettings.info": "Se n√£o tiver certeza, mantenha os valores padr√£o.",
    "advancedGpuSettings.reset": "Redefinir para Padr√£o",
    "environmentVariables": {
      "title": "Vari√°veis de Ambiente",
      "description": "Vari√°veis de ambiente ativas durante a vida √∫til do modelo.",
      "key.placeholder": "Selecionar var...",
      "value.placeholder": "Valor"
    },
    "mainGpu": {
      "title": "GPU Principal",
      "description": "A GPU a ser priorizada para o processamento do modelo.",
      "placeholder": "Selecionar GPU principal..."
    },
    "splitStrategy": {
      "title": "Estrat√©gia de Divis√£o",
      "description": "Como dividir o processamento do modelo entre as GPUs.",
      "placeholder": "Selecionar estrat√©gia de divis√£o..."
    }
  }
}
