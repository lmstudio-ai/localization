{
  "noInstanceSelected": "Nenhuma inst√¢ncia do modelo selecionada",
  "resetToDefault": "Redefinir",
  "showAdvancedSettings": "Mostrar configura√ß√µes avan√ßadas",
  "showAll": "Tudo",
  "basicSettings": "B√°sico",
  "configSubtitle": "Carregue ou salve predefini√ß√µes e experimente reescritas de par√¢metros do modelo",
  "inferenceParameters/title": "Revis√£o de Par√¢metros",
  "inferenceParameters/info": "Experimente par√¢metros que impactam a previs√£o.",
  "generalParameters/title": "Geral",
  "samplingParameters/title": "Amostragem",
  "basicTab": "B√°sico",
  "advancedTab": "Avan√ßado",
  "advancedTab/title": "üß™ Configura√ß√£o Avan√ßada",
  "advancedTab/expandAll": "Expandir tudo",
  "advancedTab/overridesTitle": "Substitui√ß√µes de Configura√ß√£o",
  "advancedTab/noConfigsText": "Voc√™ n√£o tem altera√ß√µes n√£o salvas - edite valores acima para ver mudan√ßas aqui.",
  "loadInstanceFirst": "Carregue um modelo para visualizar os par√¢metros configur√°veis",
  "noListedConfigs": "Nenhum par√¢metro configur√°vel",
  "generationParameters/info": "Experimente par√¢metros b√°sicos que impactam a gera√ß√£o de texto.",
  "loadParameters/title": "Carregar Par√¢metros",
  "loadParameters/description": "Configura√ß√µes para controlar a forma como o modelo √© inicializado e carregado na mem√≥ria.",
  "loadParameters/reload": "Recarregar para aplicar altera√ß√µes",
  "discardChanges": "Descartar altera√ß√µes",
  "llm.prediction.systemPrompt/title": "Prompt do Sistema",
  "llm.prediction.systemPrompt/description": "Use este campo para fornecer instru√ß√µes de fundo ao modelo, como um conjunto de regras, restri√ß√µes ou requisitos gerais. Este campo tamb√©m √© frequentemente referido como \"prompt do sistema\".",
  "llm.prediction.systemPrompt/subTitle": "Diretrizes para a IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quanta aleatoriedade introduzir. 0 produzir√° o mesmo resultado toda vez, enquanto valores mais altos aumentar√£o a criatividade e a varia√ß√£o",
  "llm.prediction.temperature/info": "Dos documentos de ajuda do llama.cpp: \"O valor padr√£o √© <{{dynamicValue}}>, que proporciona um equil√≠brio entre aleatoriedade e determinismo. No extremo, uma temperatura de 0 sempre escolher√° o pr√≥ximo token mais prov√°vel, resultando em sa√≠das id√™nticas em cada execu√ß√£o\"",
  "llm.prediction.llama.sampling/title": "Amostragem(Sampling)",
  "llm.prediction.topKSampling/title": "Amostragem Top K",
  "llm.prediction.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos tokens mais prov√°veis do top-k. Atua de maneira semelhante √† temperatura",
  "llm.prediction.topKSampling/info": "Dos documentos de ajuda do llama.cpp:\n\nA amostragem top-k √© um m√©todo de gera√ß√£o de texto que seleciona o pr√≥ximo token apenas dos tokens mais prov√°veis do top k previstos pelo modelo.\n\nAjuda a reduzir o risco de gerar tokens de baixa probabilidade ou sem sentido, mas tamb√©m pode limitar a diversidade da sa√≠da.\n\nUm valor mais alto para top-k (ex: 100) considerar√° mais tokens e levar√° a um texto mais diversificado, enquanto um valor mais baixo (ex: 10) focar√° nos tokens mais prov√°veis e gerar√° um texto mais conservador.\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads de CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "N√∫mero de threads de CPU a serem usados durante a infer√™ncia",
  "llm.prediction.llama.cpuThreads/info": "O n√∫mero de threads a serem usados durante a computa√ß√£o. Aumentar o n√∫mero de threads nem sempre est√° correlacionado com um melhor desempenho. O padr√£o √© <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitar Comprimento da Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente, limite o comprimento da resposta da IA",
  "llm.prediction.maxPredictedTokens/info": "Controle o comprimento m√°ximo da resposta do chatbot. Ative para definir um limite no comprimento m√°ximo de uma resposta ou desative para permitir que o chatbot decida quando parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Comprimento m√°ximo da resposta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Cerca de {{maxWords}} palavras",
  "llm.prediction.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
  "llm.prediction.repeatPenalty/subTitle": "Quanta penaliza√ß√£o para desencorajar a repeti√ß√£o do mesmo token",
  "llm.prediction.repeatPenalty/info": "Dos documentos de ajuda do llama.cpp: \"Ajuda a impedir que o modelo gere texto repetitivo ou mon√≥tono.\n\nUm valor mais alto (ex: 1.5) penalizar√° as repeti√ß√µes com mais for√ßa, enquanto um valor mais baixo (ex: 0.9) ser√° mais indulgente.\" ‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Amostragem Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilidade base m√≠nima para um token ser selecionado para sa√≠da",
  "llm.prediction.minPSampling/info": "Dos documentos de ajuda do llama.cpp:\n\nA probabilidade m√≠nima para um token ser considerado, relativa √† probabilidade do token mais prov√°vel. Deve estar entre [0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Amostragem Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os poss√≠veis pr√≥ximos tokens. Atua de maneira semelhante √† temperatura",
  "llm.prediction.topPSampling/info": "Dos documentos de ajuda do llama.cpp:\n\nA amostragem top-p, tamb√©m conhecida como amostragem de n√∫cleo, √© outro m√©todo de gera√ß√£o de texto que seleciona o pr√≥ximo token de um subconjunto de tokens que juntos t√™m uma probabilidade cumulativa de pelo menos p.\n\nEste m√©todo oferece um equil√≠brio entre diversidade e qualidade, considerando tanto as probabilidades dos tokens quanto o n√∫mero de tokens para amostrar.\n\nUm valor mais alto para top-p (ex: 0.95) levar√° a um texto mais diversificado, enquanto um valor mais baixo (ex: 0.5) gerar√° um texto mais focado e conservador. Deve estar entre (0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Strings de Parada",
  "llm.prediction.stopStrings/subTitle": "Strings que devem parar o modelo de gerar mais tokens",
  "llm.prediction.stopStrings/info": "Strings espec√≠ficas que, quando encontradas, far√£o o modelo parar de gerar mais tokens",
  "llm.prediction.stopStrings/placeholder": "Digite uma string e pressione ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Excesso de Conversa",
  "llm.prediction.contextOverflowPolicy/subTitle": "Como o modelo deve se comportar quando a conversa crescer demais para ele lidar",
  "llm.prediction.contextOverflowPolicy/info": "Decida o que fazer quando a conversa exceder o tamanho da mem√≥ria de trabalho do modelo ('contexto')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Parar no Limite",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Parar de gerar uma vez que a mem√≥ria do modelo estiver cheia",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Truncar Meio",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Remove mensagens do meio da conversa para abrir espa√ßo para novas. O modelo ainda se lembrar√° do in√≠cio da conversa",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Janela Rolante",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "O modelo sempre ter√° as mensagens mais recentes, mas pode esquecer o in√≠cio da conversa",
  "llm.prediction.llama.frequencyPenalty/title": "Penalidade de Frequ√™ncia",
  "llm.prediction.llama.presencePenalty/title": "Penalidade de Presen√ßa",
  "llm.prediction.llama.tailFreeSampling/title": "Amostragem Livre de Cauda",
  "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem Localmente T√≠pica",
  "llm.prediction.onnx.topKSampling/title": "Amostragem Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos tokens mais prov√°veis do top-k. Atua de maneira semelhante √† temperatura",
  "llm.prediction.onnx.topKSampling/info": "Da documenta√ß√£o ONNX:\n\nN√∫mero de tokens do vocabul√°rio de maior probabilidade a manter para filtragem top-k\n\n‚Ä¢ Este filtro est√° desligado por padr√£o",
  "llm.prediction.onnx.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Quanta penaliza√ß√£o para desencorajar a repeti√ß√£o do mesmo token",
  "llm.prediction.onnx.repeatPenalty/info": "Um valor mais alto desencoraja o modelo de se repetir",
  "llm.prediction.onnx.topPSampling/title": "Amostragem Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os poss√≠veis pr√≥ximos tokens. Atua de maneira semelhante √† temperatura",
  "llm.prediction.onnx.topPSampling/info": "Da documenta√ß√£o ONNX:\n\nSomente os tokens mais prov√°veis com probabilidades que somam TopP ou mais s√£o mantidos para gera√ß√£o\n\n‚Ä¢ Este filtro est√° desligado por padr√£o",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Sa√≠da Estruturada",
  "llm.prediction.structured/info": "Sa√≠da Estruturada",
  "llm.prediction.promptTemplate/title": "Modelo de Prompt",
  "llm.prediction.promptTemplate/subTitle": "O formato em que as mensagens no chat s√£o enviadas ao modelo. Alterar isso pode introduzir um comportamento inesperado - certifique-se de saber o que est√° fazendo!",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Falha ao analisar o modelo Jinja: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manual",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Antes do Sistema",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "Insira o prefixo do Sistema...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Ap√≥s o Sistema",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "Insira o sufixo do Sistema...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Antes do Usu√°rio",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "Insira o prefixo do Usu√°rio...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "Ap√≥s o Usu√°rio",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "Insira o sufixo do Usu√°rio...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Antes do Assistente",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "Insira o prefixo do Assistente...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Ap√≥s o Assistente",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "Insira o sufixo do Assistente...",
  "llm.prediction.promptTemplate.stopStrings/label": "Strings de Parada Adicionais",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "Strings de parada espec√≠ficas do modelo que ser√£o usadas al√©m das strings de parada especificadas pelo usu√°rio.",

  "llm.load.contextLength/title": "Comprimento do Contexto",
  "llm.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode atender em um prompt. Veja as op√ß√µes de Excesso de Conversa em \"Par√¢metros de Infer√™ncia\" para mais maneiras de gerenciar isso",
  "llm.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando a quantidade de contexto que ele ret√©m durante o processamento",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "A seed para o gerador de n√∫meros aleat√≥rios usado na gera√ß√£o de texto. -1 √© aleat√≥rio",
  "llm.load.seed/info": "Seed Aleat√≥ria: Define a seed para gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis",

  "llm.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
  "llm.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a serem processados de uma vez. Aumentar isso aumenta o desempenho ao custo de uso de mem√≥ria",
  "llm.load.llama.evalBatchSize/info": "Define o n√∫mero de exemplos processados juntos em um lote durante a avalia√ß√£o, afetando a velocidade e o uso de mem√≥ria",
  "llm.load.llama.ropeFrequencyBase/title": "Frequ√™ncia Base do RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base personalizada para embeddings posicionais rotativos (RoPE). Aumentar isso pode permitir um melhor desempenho em comprimentos de contexto altos",
  "llm.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa, afetando como a informa√ß√£o posicional √© embutida",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia do RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "O comprimento do contexto √© escalado por este fator para estender o contexto efetivo usando RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa para controlar a granularidade da codifica√ß√£o posicional",
  "llm.load.llama.acceleration.offloadRatio/title": "Offload da GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas discretas do modelo para calcular na GPU para acelera√ß√£o de GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para descarregar na GPU.",
  "llm.load.llama.flashAttention/title": "Aten√ß√£o Flash",
  "llm.load.llama.flashAttention/subTitle": "Reduz o uso de mem√≥ria e o tempo de gera√ß√£o em alguns modelos",
  "llm.load.llama.flashAttention/info": "Acelera os mecanismos de aten√ß√£o para processamento mais r√°pido e eficiente",
  "llm.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve mem√≥ria do sistema para o modelo, mesmo quando descarregado para a GPU. Melhora o desempenho, mas exige mais RAM do sistema",
  "llm.load.llama.keepModelInMemory/info": "Impede que o modelo seja trocado para o disco, garantindo acesso mais r√°pido ao custo de maior uso de RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de mem√≥ria armazenando o cache em meia precis√£o (FP16)",
  "llm.load.llama.tryMmap/title": "Tentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desativar isso pode melhorar o desempenho quando o modelo √© maior que a RAM dispon√≠vel do sistema",
  "llm.load.llama.tryMmap/info": "Carregar arquivos do modelo diretamente do disco para a mem√≥ria",

  "embedding.load.contextLength/title": "Comprimento do Contexto",
  "embedding.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode atender em um prompt. Veja as op√ß√µes de Excesso de Conversa em \"Par√¢metros de Infer√™ncia\" para mais maneiras de gerenciar isso",
  "embedding.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando a quantidade de contexto que ele ret√©m durante o processamento",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequ√™ncia Base do RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base personalizada para embeddings posicionais rotativos (RoPE). Aumentar isso pode permitir um melhor desempenho em comprimentos de contexto altos",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa, afetando como a informa√ß√£o posicional √© embutida",
  "embedding.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
  "embedding.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a serem processados de uma vez. Aumentar isso aumenta o desempenho ao custo de uso de mem√≥ria",
  "embedding.load.llama.evalBatchSize/info": "Define o n√∫mero de tokens processados juntos em um lote durante a avalia√ß√£o",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia do RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "O comprimento do contexto √© escalado por este fator para estender o contexto efetivo usando RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa para controlar a granularidade da codifica√ß√£o posicional",
  "embedding.load.llama.acceleration.offloadRatio/title": "Offload da GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas discretas do modelo para calcular na GPU para acelera√ß√£o de GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para descarregar na GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve mem√≥ria do sistema para o modelo, mesmo quando descarregado para a GPU. Melhora o desempenho, mas exige mais RAM do sistema",
  "embedding.load.llama.keepModelInMemory/info": "Impede que o modelo seja trocado para o disco, garantindo acesso mais r√°pido ao custo de maior uso de RAM",
  "embedding.load.llama.tryMmap/title": "Tentar mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desativar isso pode melhorar o desempenho quando o modelo √© maior que a RAM dispon√≠vel do sistema",
  "embedding.load.llama.tryMmap/info": "Carregar arquivos do modelo diretamente do disco para a mem√≥ria",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "A seed para o gerador de n√∫meros aleat√≥rios usado na gera√ß√£o de texto. -1 √© seed aleat√≥ria",
  "embedding.load.seed/info": "Seed Aleat√≥ria: Define a seed para gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis"
}
