{
  "noInstanceSelected": "Nenhuma inst√¢ncia do modelo selecionada",
  "resetToDefault": "Redefinir",
  "showAdvancedSettings": "Mostrar configura√ß√µes avan√ßadas",
  "showAll": "Tudo",
  "basicSettings": "B√°sico",
  "configSubtitle": "Carregue ou salve predefini√ß√µes e experimente reescritas de par√¢metros do modelo",
  "inferenceParameters/title": "Revis√£o de Par√¢metros",
  "inferenceParameters/info": "Experimente par√¢metros que impactam a previs√£o.",
  "generalParameters/title": "Geral",
  "samplingParameters/title": "Amostragem",
  "basicTab": "B√°sico",
  "advancedTab": "Avan√ßado",
  "advancedTab/title": "üß™ Configura√ß√£o Avan√ßada",
  "advancedTab/expandAll": "Expandir tudo",
  "advancedTab/overridesTitle": "Substitui√ß√µes de Configura√ß√£o",
  "advancedTab/noConfigsText": "Voc√™ n√£o tem altera√ß√µes n√£o salvas - edite valores acima para ver mudan√ßas aqui.",
  "loadInstanceFirst": "Carregue um modelo para visualizar os par√¢metros configur√°veis",
  "noListedConfigs": "Nenhum par√¢metro configur√°vel",
  "generationParameters/info": "Experimente par√¢metros b√°sicos que impactam a gera√ß√£o de texto.",
  "loadParameters/title": "Carregar Par√¢metros",
  "loadParameters/description": "Configura√ß√µes para controlar a forma como o modelo √© inicializado e carregado na mem√≥ria.",
  "loadParameters/reload": "Recarregar para aplicar altera√ß√µes",
  "loadParameters/reload/error": "Falha ao recarregar o modelo",
  "discardChanges": "Descartar altera√ß√µes",
  "loadModelToSeeOptions": "Carregue um modelo para ver as op√ß√µes",
  "schematicsError.title": "Os esquemas de configura√ß√£o cont√™m erros nos seguintes campos:",
  "manifestSections": {
    "structuredOutput/title": "Sa√≠da Estruturada",
    "speculativeDecoding/title": "Decodifica√ß√£o Especulativa",
    "sampling/title": "Amostragem",
    "settings/title": "Configura√ß√µes",
    "toolUse/title": "Uso de Ferramentas",
    "promptTemplate/title": "Modelo de Prompt"
  },

  "llm.prediction.systemPrompt/title": "Prompt do Sistema",
  "llm.prediction.systemPrompt/description": "Use este campo para fornecer instru√ß√µes de fundo ao modelo, como um conjunto de regras, restri√ß√µes ou requisitos gerais.",
  "llm.prediction.systemPrompt/subTitle": "Diretrizes para a IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quanta aleatoriedade introduzir. 0 produzir√° o mesmo resultado toda vez, enquanto valores mais altos aumentar√£o a criatividade e a varia√ß√£o",
  "llm.prediction.temperature/info": "Dos documentos de ajuda do llama.cpp: \"O valor padr√£o √© <{{dynamicValue}}>, que proporciona um equil√≠brio entre aleatoriedade e determinismo. No extremo, uma temperatura de 0 sempre escolher√° o pr√≥ximo token mais prov√°vel, resultando em sa√≠das id√™nticas em cada execu√ß√£o\"",
  "llm.prediction.llama.sampling/title": "Amostragem",
  "llm.prediction.topKSampling/title": "Amostragem Top K",
  "llm.prediction.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos tokens mais prov√°veis do top-k. Atua de maneira semelhante √† temperatura",
  "llm.prediction.topKSampling/info": "Dos documentos de ajuda do llama.cpp:\n\nA amostragem top-k √© um m√©todo de gera√ß√£o de texto que seleciona o pr√≥ximo token apenas dos tokens mais prov√°veis do top k previstos pelo modelo.\n\nAjuda a reduzir o risco de gerar tokens de baixa probabilidade ou sem sentido, mas tamb√©m pode limitar a diversidade da sa√≠da.\n\nUm valor mais alto para top-k (ex: 100) considerar√° mais tokens e levar√° a um texto mais diversificado, enquanto um valor mais baixo (ex: 10) focar√° nos tokens mais prov√°veis e gerar√° um texto mais conservador.\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads da CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "N√∫mero de threads da CPU a serem usados durante a infer√™ncia",
  "llm.prediction.llama.cpuThreads/info": "O n√∫mero de threads a serem usados durante a computa√ß√£o. Aumentar o n√∫mero de threads nem sempre est√° correlacionado com um melhor desempenho. O padr√£o √© <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limitar Comprimento da Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente, limite o comprimento da resposta da IA",
  "llm.prediction.maxPredictedTokens/info": "Controle o comprimento m√°ximo da resposta do chatbot. Ative para definir um limite no comprimento m√°ximo de uma resposta ou desative para permitir que o chatbot decida quando parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Comprimento m√°ximo da resposta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Cerca de {{maxWords}} palavras",
  "llm.prediction.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
  "llm.prediction.repeatPenalty/subTitle": "Quanta penaliza√ß√£o para desencorajar a repeti√ß√£o do mesmo token",
  "llm.prediction.repeatPenalty/info": "Dos documentos de ajuda do llama.cpp: \"Ajuda a impedir que o modelo gere texto repetitivo ou mon√≥tono.\n\nUm valor mais alto (ex: 1.5) penalizar√° as repeti√ß√µes com mais for√ßa, enquanto um valor mais baixo (ex: 0.9) ser√° mais indulgente.\" ‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Amostragem Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilidade base m√≠nima para um token ser selecionado para sa√≠da",
  "llm.prediction.minPSampling/info": "Dos documentos de ajuda do llama.cpp:\n\nA probabilidade m√≠nima para um token ser considerado, relativa √† probabilidade do token mais prov√°vel. Deve estar entre [0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Amostragem Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os poss√≠veis pr√≥ximos tokens. Atua de maneira semelhante √† temperatura",
  "llm.prediction.topPSampling/info": "Dos documentos de ajuda do llama.cpp:\n\nA amostragem top-p, tamb√©m conhecida como amostragem de n√∫cleo, √© outro m√©todo de gera√ß√£o de texto que seleciona o pr√≥ximo token de um subconjunto de tokens que juntos t√™m uma probabilidade cumulativa de pelo menos p.\n\nEste m√©todo oferece um equil√≠brio entre diversidade e qualidade, considerando tanto as probabilidades dos tokens quanto o n√∫mero de tokens para amostrar.\n\nUm valor mais alto para top-p (ex: 0.95) levar√° a um texto mais diversificado, enquanto um valor mais baixo (ex: 0.5) gerar√° um texto mais focado e conservador. Deve estar entre (0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Strings de Parada",
  "llm.prediction.stopStrings/subTitle": "Strings que devem parar o modelo de gerar mais tokens",
  "llm.prediction.stopStrings/info": "Strings espec√≠ficas que, quando encontradas, far√£o o modelo parar de gerar mais tokens",
  "llm.prediction.stopStrings/placeholder": "Digite uma string e pressione ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Excesso de Contexto",
  "llm.prediction.contextOverflowPolicy/subTitle": "Como o modelo deve se comportar quando a conversa crescer demais para ele lidar",
  "llm.prediction.contextOverflowPolicy/info": "Decida o que fazer quando a conversa exceder o tamanho da mem√≥ria de trabalho do modelo ('contexto')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalidade de Frequ√™ncia",
  "llm.prediction.llama.presencePenalty/title": "Penalidade de Presen√ßa",
  "llm.prediction.llama.tailFreeSampling/title": "Amostragem Livre de Cauda",
  "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem Localmente T√≠pica",
  "llm.prediction.llama.xtcProbability/title": "Probabilidade de Amostragem XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "O amostrador XTC (Exclude Top Choices - Excluir Principais Escolhas) ser√° ativado apenas com esta probabilidade por token gerado. A amostragem XTC pode aumentar a criatividade e reduzir clich√™s",
  "llm.prediction.llama.xtcProbability/info": "A amostragem XTC (Exclude Top Choices - Excluir Principais Escolhas) ser√° ativada apenas com esta probabilidade, por token gerado. A amostragem XTC geralmente aumenta a criatividade e reduz clich√™s",
  "llm.prediction.llama.xtcThreshold/title": "Limiar de Amostragem XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Limiar XTC (Exclude Top Choices - Excluir Principais Escolhas). Com uma chance de `xtc-probability`, procure por tokens com probabilidades entre `xtc-threshold` e 0.5, e remova todos esses tokens, exceto o menos prov√°vel",
  "llm.prediction.llama.xtcThreshold/info": "Limiar XTC (Exclude Top Choices - Excluir Principais Escolhas). Com uma chance de `xtc-probability`, procure por tokens com probabilidades entre `xtc-threshold` e 0.5, e remova todos esses tokens, exceto o menos prov√°vel",
  "llm.prediction.mlx.topKSampling/title": "Amostragem Top K",
  "llm.prediction.mlx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos tokens mais prov√°veis do top-k. Atua de maneira semelhante √† temperatura",
  "llm.prediction.mlx.topKSampling/info": "Limita o pr√≥ximo token a um dos tokens mais prov√°veis do top-k. Atua de maneira semelhante √† temperatura",
  "llm.prediction.onnx.topKSampling/title": "Amostragem Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos tokens mais prov√°veis do top-k. Atua de maneira semelhante √† temperatura",
  "llm.prediction.onnx.topKSampling/info": "Da documenta√ß√£o ONNX:\n\nN√∫mero de tokens do vocabul√°rio de maior probabilidade a manter para filtragem top-k\n\n‚Ä¢ Este filtro est√° desligado por padr√£o",
  "llm.prediction.onnx.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Quanta penaliza√ß√£o para desencorajar a repeti√ß√£o do mesmo token",
  "llm.prediction.onnx.repeatPenalty/info": "Um valor mais alto desencoraja o modelo de se repetir",
  "llm.prediction.onnx.topPSampling/title": "Amostragem Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os poss√≠veis pr√≥ximos tokens. Atua de maneira semelhante √† temperatura",
  "llm.prediction.onnx.topPSampling/info": "Da documenta√ß√£o ONNX:\n\nSomente os tokens mais prov√°veis com probabilidades que somam TopP ou mais s√£o mantidos para gera√ß√£o\n\n‚Ä¢ Este filtro est√° desligado por padr√£o",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Sa√≠da Estruturada",
  "llm.prediction.structured/info": "Sa√≠da Estruturada",
  "llm.prediction.structured/description": "Avan√ßado: voc√™ pode fornecer um [Esquema JSON](https://json-schema.org/learn/miscellaneous-examples) para impor um formato de sa√≠da espec√≠fico do modelo. Leia a [documenta√ß√£o](https://lmstudio.ai/docs/advanced/structured-output) para saber mais",
  "llm.prediction.tools/title": "Uso de Ferramentas",
  "llm.prediction.tools/description": "Avan√ßado: voc√™ pode fornecer uma lista de ferramentas compat√≠vel com JSON para o modelo solicitar chamadas. Leia a [documenta√ß√£o](https://lmstudio.ai/docs/advanced/tool-use) para saber mais",
  "llm.prediction.tools/serverPageDescriptionAddon": "Passe isso pelo corpo da requisi√ß√£o como `tools` ao usar a API do servidor",
  "llm.prediction.promptTemplate/title": "Modelo de Prompt",
  "llm.prediction.promptTemplate/subTitle": "O formato em que as mensagens no chat s√£o enviadas ao modelo. Alterar isso pode introduzir um comportamento inesperado - certifique-se de saber o que est√° fazendo!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Tokens de Rascunho a Gerar",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "O n√∫mero de tokens a gerar com o modelo de rascunho por token do modelo principal. Encontre o ponto ideal entre computa√ß√£o e recompensa",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Corte de Probabilidade de Rascunho",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue rascunhando at√© que a probabilidade de um token caia abaixo deste limiar. Valores mais altos geralmente significam menor risco, menor recompensa",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Tamanho M√≠nimo do Rascunho",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Rascunhos menores que este ser√£o ignorados pelo modelo principal. Valores mais altos geralmente significam menor risco, menor recompensa",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Tamanho M√°ximo do Rascunho",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "N√∫mero m√°ximo de tokens permitidos em um rascunho. Teto se todas as probabilidades de token forem > que o corte. Valores mais baixos geralmente significam menor risco, menor recompensa",
  "llm.prediction.speculativeDecoding.draftModel/title": "Modelo de Rascunho",
  "llm.prediction.reasoning.parsing/title": "An√°lise de Se√ß√£o de Racioc√≠nio",
  "llm.prediction.reasoning.parsing/subTitle": "Como analisar se√ß√µes de racioc√≠nio na sa√≠da do modelo",

  "llm.load.contextLength/title": "Comprimento do Contexto",
  "llm.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode atender em um prompt. Veja as op√ß√µes de Excesso de Conversa em \"Par√¢metros de Infer√™ncia\" para mais maneiras de gerenciar isso",
  "llm.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando a quantidade de contexto que ele ret√©m durante o processamento",
  "llm.load.contextLength/warning": "Definir um valor alto para o comprimento do contexto pode impactar significativamente o uso de mem√≥ria",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "A seed para o gerador de n√∫meros aleat√≥rios usado na gera√ß√£o de texto. -1 √© aleat√≥rio",
  "llm.load.seed/info": "Seed Aleat√≥ria: Define a seed para gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis",

  "llm.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
  "llm.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a serem processados de uma vez. Aumentar isso aumenta o desempenho ao custo de uso de mem√≥ria",
  "llm.load.llama.evalBatchSize/info": "Define o n√∫mero de exemplos processados juntos em um lote durante a avalia√ß√£o, afetando a velocidade e o uso de mem√≥ria",
  "llm.load.llama.ropeFrequencyBase/title": "Frequ√™ncia Base do RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base personalizada para embeddings posicionais rotativos (RoPE). Aumentar isso pode permitir um melhor desempenho em comprimentos de contexto altos",
  "llm.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa, afetando como a informa√ß√£o posicional √© embutida",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia do RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "O comprimento do contexto √© escalado por este fator para estender o contexto efetivo usando RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa para controlar a granularidade da codifica√ß√£o posicional",
  "llm.load.llama.acceleration.offloadRatio/title": "Offload da GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas discretas do modelo para calcular na GPU para acelera√ß√£o de GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para descarregar na GPU.",
  "llm.load.llama.flashAttention/title": "Aten√ß√£o Flash",
  "llm.load.llama.flashAttention/subTitle": "Reduz o uso de mem√≥ria e o tempo de gera√ß√£o em alguns modelos",
  "llm.load.llama.flashAttention/info": "Acelera os mecanismos de aten√ß√£o para processamento mais r√°pido e eficiente",
  "llm.load.numExperts/title": "N√∫mero de Experts",
  "llm.load.numExperts/subTitle": "N√∫mero de experts a serem usados no modelo",
  "llm.load.numExperts/info": "O n√∫mero de experts a serem usados no modelo",
  "llm.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve mem√≥ria do sistema para o modelo, mesmo quando descarregado para a GPU. Melhora o desempenho, mas exige mais RAM do sistema",
  "llm.load.llama.keepModelInMemory/info": "Impede que o modelo seja trocado para o disco, garantindo acesso mais r√°pido ao custo de maior uso de RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de mem√≥ria armazenando o cache em meia precis√£o (FP16)",
  "llm.load.llama.tryMmap/title": "Tentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desativar isso pode melhorar o desempenho quando o modelo √© maior que a RAM dispon√≠vel do sistema",
  "llm.load.llama.tryMmap/info": "Carregar arquivos do modelo diretamente do disco para a mem√≥ria",
  "llm.load.llama.cpuThreadPoolSize/title": "Tamanho do Pool de Threads da CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "N√∫mero de threads da CPU a alocar para o pool de threads usado para computa√ß√£o do modelo",
  "llm.load.llama.cpuThreadPoolSize/info": "O n√∫mero de threads da CPU a alocar para o pool de threads usado para computa√ß√£o do modelo. Aumentar o n√∫mero de threads nem sempre est√° correlacionado com um melhor desempenho. O padr√£o √© <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Tipo de Quantiza√ß√£o do Cache K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Valores mais baixos reduzem o uso de mem√≥ria, mas podem diminuir a qualidade. O efeito varia significativamente entre os modelos.",
  "llm.load.llama.vCacheQuantizationType/title": "Tipo de Quantiza√ß√£o do Cache V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Valores mais baixos reduzem o uso de mem√≥ria, mas podem diminuir a qualidade. O efeito varia significativamente entre os modelos.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Voc√™ deve desabilitar este valor se a Flash Attention n√£o estiver habilitada",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "S√≥ pode ser ativado quando a Flash Attention estiver habilitada",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Voc√™ deve desabilitar a flash attention ao usar F32",
  "llm.load.mlx.kvCacheBits/title": "Quantiza√ß√£o do Cache KV",
  "llm.load.mlx.kvCacheBits/subTitle": "N√∫mero de bits para os quais o cache KV deve ser quantizado",
  "llm.load.mlx.kvCacheBits/info": "N√∫mero de bits para os quais o cache KV deve ser quantizado",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "A configura√ß√£o de Comprimento do Contexto √© ignorada ao usar Quantiza√ß√£o do Cache KV",
  "llm.load.mlx.kvCacheGroupSize/title": "Quantiza√ß√£o do Cache KV: Tamanho do Grupo",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Tamanho do grupo durante a opera√ß√£o de quantiza√ß√£o para o cache KV. Um tamanho de grupo maior reduz o uso de mem√≥ria, mas pode diminuir a qualidade",
  "llm.load.mlx.kvCacheGroupSize/info": "N√∫mero de bits para os quais o cache KV deve ser quantizado",
  "llm.load.mlx.kvCacheQuantizationStart/title": "Quantiza√ß√£o do Cache KV: Come√ßar a quantizar quando o contexto cruzar este comprimento",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Limiar de comprimento do contexto para come√ßar a quantizar o cache KV",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Limiar de comprimento do contexto para come√ßar a quantizar o cache KV",
  "llm.load.mlx.kvCacheQuantization/title": "Quantiza√ß√£o do Cache KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize o cache KV do modelo. Isso pode resultar em gera√ß√£o mais r√°pida e menor uso de mem√≥ria,\n√† custa da qualidade da sa√≠da do modelo.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "Bits de quantiza√ß√£o do cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "N√∫mero de bits para quantizar o cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Estrat√©gia de tamanho do grupo",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Precis√£o",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanceado",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "R√°pido",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Avan√ßado: Configura√ß√£o 'tamanho do grupo matmul' quantizado\n\n‚Ä¢ Precis√£o = tamanho do grupo 32\n‚Ä¢ Balanceado = tamanho do grupo 64\n‚Ä¢ R√°pido = tamanho do grupo 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Come√ßar a quantizar quando o contexto atingir este comprimento",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Quando o contexto atingir esta quantidade de tokens,\ncomece a quantizar o cache KV",

  "embedding.load.contextLength/title": "Comprimento do Contexto",
  "embedding.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode atender em um prompt. Veja as op√ß√µes de Excesso de Conversa em \"Par√¢metros de Infer√™ncia\" para mais maneiras de gerenciar isso",
  "embedding.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando a quantidade de contexto que ele ret√©m durante o processamento",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequ√™ncia Base do RoPE",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base personalizada para embeddings posicionais rotativos (RoPE). Aumentar isso pode permitir um melhor desempenho em comprimentos de contexto altos",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa, afetando como a informa√ß√£o posicional √© embutida",
  "embedding.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
  "embedding.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada a serem processados de uma vez. Aumentar isso aumenta o desempenho ao custo de uso de mem√≥ria",
  "embedding.load.llama.evalBatchSize/info": "Define o n√∫mero de tokens processados juntos em um lote durante a avalia√ß√£o",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia do RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "O comprimento do contexto √© escalado por este fator para estender o contexto efetivo usando RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa para controlar a granularidade da codifica√ß√£o posicional",
  "embedding.load.llama.acceleration.offloadRatio/title": "Offload da GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas discretas do modelo para calcular na GPU para acelera√ß√£o de GPU",
  "embedding.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para descarregar na GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve mem√≥ria do sistema para o modelo, mesmo quando descarregado para a GPU. Melhora o desempenho, mas exige mais RAM do sistema",
  "embedding.load.llama.keepModelInMemory/info": "Impede que o modelo seja trocado para o disco, garantindo acesso mais r√°pido ao custo de maior uso de RAM",
  "embedding.load.llama.tryMmap/title": "Tentar mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Melhora o tempo de carregamento do modelo. Desativar isso pode melhorar o desempenho quando o modelo √© maior que a RAM dispon√≠vel do sistema",
  "embedding.load.llama.tryMmap/info": "Carregar arquivos do modelo diretamente do disco para a mem√≥ria",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "A seed para o gerador de n√∫meros aleat√≥rios usado na gera√ß√£o de texto. -1 √© seed aleat√≥ria",
  "embedding.load.seed/info": "Seed Aleat√≥ria: Define a seed para gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis",

  "presetTooltip": {
    "included/title": "Valores da Predefini√ß√£o",
    "included/description": "Os seguintes campos ser√£o aplicados",
    "included/empty": "Nenhum campo desta predefini√ß√£o se aplica neste contexto.",
    "included/conflict": "Voc√™ ser√° solicitado a escolher se deseja aplicar este valor",
    "separateLoad/title": "Configura√ß√£o em Tempo de Carregamento",
    "separateLoad/description.1": "A predefini√ß√£o tamb√©m inclui a seguinte configura√ß√£o em tempo de carregamento. A configura√ß√£o em tempo de carregamento √© para todo o modelo e requer recarregar o modelo para ter efeito. Segure",
    "separateLoad/description.2": "para aplicar a",
    "separateLoad/description.3": ".",
    "excluded/title": "Pode n√£o se aplicar",
    "excluded/description": "Os seguintes campos est√£o inclu√≠dos na predefini√ß√£o, mas n√£o se aplicam no contexto atual.",
    "legacy/title": "Predefini√ß√£o Legada",
    "legacy/description": "Esta √© uma predefini√ß√£o legada. Ela inclui os seguintes campos que agora s√£o tratados automaticamente ou n√£o s√£o mais aplic√°veis.",
    "button/publish": "Publicar no Hub",
    "button/pushUpdate": "Enviar Altera√ß√µes para o Hub",
    "button/export": "Exportar"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Vazio>"
    },
    "checkboxNumeric": {
      "off": "DESLIGADO"
    },
    "llamaCacheQuantizationType": {
      "off": "DESLIGADO"
    },
    "mlxKvCacheBits": {
      "off": "DESLIGADO"
    },
    "stringArray": {
      "empty": "<Vazio>"
    },
    "llmPromptTemplate": {
      "type": "Tipo",
      "types.jinja/label": "Modelo (Jinja)",
      "jinja.bosToken/label": "Token BOS",
      "jinja.eosToken/label": "Token EOS",
      "jinja.template/label": "Modelo",
      "jinja/error": "Falha ao analisar o modelo Jinja: {{error}}",
      "jinja/empty": "Por favor, insira um modelo Jinja acima.",
      "jinja/unlikelyToWork": "O modelo Jinja que voc√™ forneceu acima provavelmente n√£o funcionar√°, pois n√£o faz refer√™ncia √† vari√°vel \"messages\". Por favor, verifique novamente se voc√™ inseriu um modelo correto.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Antes do Sistema",
      "manual.subfield.beforeSystem/placeholder": "Insira o prefixo do Sistema...",
      "manual.subfield.afterSystem/label": "Ap√≥s o Sistema",
      "manual.subfield.afterSystem/placeholder": "Insira o sufixo do Sistema...",
      "manual.subfield.beforeUser/label": "Antes do Usu√°rio",
      "manual.subfield.beforeUser/placeholder": "Insira o prefixo do Usu√°rio...",
      "manual.subfield.afterUser/label": "Ap√≥s o Usu√°rio",
      "manual.subfield.afterUser/placeholder": "Insira o sufixo do Usu√°rio...",
      "manual.subfield.beforeAssistant/label": "Antes do Assistente",
      "manual.subfield.beforeAssistant/placeholder": "Insira o prefixo do Assistente...",
      "manual.subfield.afterAssistant/label": "Ap√≥s o Assistente",
      "manual.subfield.afterAssistant/placeholder": "Insira o sufixo do Assistente...",
      "stopStrings/label": "Strings de Parada Adicionais",
      "stopStrings/subTitle": "Strings de parada espec√≠ficas do modelo que ser√£o usadas al√©m das strings de parada especificadas pelo usu√°rio."
    },
    "contextLength": {
      "maxValueTooltip": "Este √© o n√∫mero m√°ximo de tokens que o modelo foi treinado para lidar. Clique para definir o contexto para este valor",
      "maxValueTextStart": "O modelo suporta at√©",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "Embora um modelo possa suportar at√© um certo n√∫mero de tokens, o desempenho pode deteriorar se os recursos da sua m√°quina n√£o conseguirem lidar com a carga - use com cautela ao aumentar este valor"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Parar no Limite",
      "stopAtLimitSub": "Parar de gerar assim que a mem√≥ria do modelo estiver cheia",
      "truncateMiddle": "Truncar Meio",
      "truncateMiddleSub": "Remove mensagens do meio da conversa para abrir espa√ßo para novas. O modelo ainda se lembrar√° do in√≠cio da conversa",
      "rollingWindow": "Janela Rolante",
      "rollingWindowSub": "O modelo sempre ter√° as mensagens mais recentes, mas pode esquecer o in√≠cio da conversa"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "M√ÅX",
      "off": "DESLIGADO"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Uniformemente",
      "favorMainGpu": "Priorizar GPU Principal"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Leia como funciona",
      "placeholder": "Selecione um modelo de rascunho compat√≠vel",
      "noCompatible": "Nenhum modelo de rascunho compat√≠vel encontrado para sua sele√ß√£o de modelo atual",
      "stillLoading": "Identificando modelos de rascunho compat√≠veis...",
      "notCompatible": "O modelo de rascunho selecionado (<draft/>) n√£o √© compat√≠vel com a sele√ß√£o de modelo atual (<current/>).",
      "off": "DESLIGADO",
      "loadModelToSeeOptions": "Carregue o modelo <keyboard-shortcut /> para ver op√ß√µes compat√≠veis",
      "compatibleWithNumberOfModels": "Recomendado para pelo menos {{dynamicValue}} dos seus modelos",
      "recommendedForSomeModels": "Recomendado para alguns modelos",
      "recommendedForLlamaModels": "Recomendado para modelos Llama",
      "recommendedForQwenModels": "Recomendado para modelos Qwen",
      "onboardingModal": {
        "introducing": "Apresentando",
        "speculativeDecoding": "Decodifica√ß√£o Especulativa",
        "firstStepBody": "Acelera√ß√£o de infer√™ncia para modelos <custom-span>llama.cpp</custom-span> e <custom-span>MLX</custom-span>",
        "secondStepTitle": "Acelera√ß√£o de Infer√™ncia com Decodifica√ß√£o Especulativa",
        "secondStepBody": "Decodifica√ß√£o Especulativa √© uma t√©cnica que envolve a colabora√ß√£o de dois modelos:\n - Um modelo \"principal\" maior\n - Um modelo \"de rascunho\" menor\n\nDurante a gera√ß√£o, o modelo de rascunho prop√µe rapidamente tokens para o modelo principal maior verificar. Verificar tokens √© um processo muito mais r√°pido do que realmente ger√°-los, que √© a fonte dos ganhos de velocidade. **Geralmente, quanto maior a diferen√ßa de tamanho entre o modelo principal e o modelo de rascunho, maior a acelera√ß√£o**.\n\nPara manter a qualidade, o modelo principal aceita apenas tokens que se alinham com o que ele mesmo teria gerado, permitindo a qualidade de resposta do modelo maior em velocidades de infer√™ncia mais r√°pidas. Ambos os modelos devem compartilhar o mesmo vocabul√°rio.",
        "draftModelRecommendationsTitle": "Recomenda√ß√µes de modelo de rascunho",
        "basedOnCurrentModels": "Com base nos seus modelos atuais",
        "close": "Fechar",
        "next": "Pr√≥ximo",
        "done": "Conclu√≠do"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Por favor, carregue um modelo primeiro <model-badge /> ",
      "errorEngineNotSupported": "A decodifica√ß√£o especulativa requer pelo menos a vers√£o {{minVersion}} do motor {{engineName}}. Por favor, atualize o motor (<key/>) e recarregue o modelo para usar este recurso.",
      "errorEngineNotSupported/noKey": "A decodifica√ß√£o especulativa requer pelo menos a vers√£o {{minVersion}} do motor {{engineName}}. Por favor, atualize o motor e recarregue o modelo para usar este recurso."
    },
    "llmReasoningParsing": {
      "startString/label": "String de In√≠cio",
      "startString/placeholder": "Insira a string de in√≠cio...",
      "endString/label": "String de Fim",
      "endString/placeholder": "Insira a string de fim..."
    }
  },
  "saveConflictResolution": {
    "title": "Escolha quais valores incluir na Predefini√ß√£o",
    "description": "Escolha quais valores manter",
    "instructions": "Clique em um valor para inclu√≠-lo",
    "userValues": "Valor Anterior",
    "presetValues": "Novo Valor",
    "confirm": "Confirmar",
    "cancel": "Cancelar"
  },
  "applyConflictResolution": {
    "title": "Quais valores manter?",
    "description": "Voc√™ tem altera√ß√µes n√£o salvas que se sobrep√µem √† Predefini√ß√£o recebida",
    "instructions": "Clique em um valor para mant√™-lo",
    "userValues": "Valor Atual",
    "presetValues": "Valor da Predefini√ß√£o Recebida",
    "confirm": "Confirmar",
    "cancel": "Cancelar"
  },
  "empty": "<Vazio>",
  "noModelSelected": "Nenhum modelo selecionado",
  "apiIdentifier.label": "Identificador da API",
  "apiIdentifier.hint": "Opcionalmente, forne√ßa um identificador para este modelo. Isso ser√° usado em requisi√ß√µes de API. Deixe em branco para usar o identificador padr√£o.",
  "idleTTL.label": "Descarregar Automaticamente se Inativo (TTL)",
  "idleTTL.hint": "Se definido, o modelo ser√° descarregado automaticamente ap√≥s ficar inativo pelo tempo especificado.",
  "idleTTL.mins": "min",

  "presets": {
    "title": "Predefini√ß√£o",
    "commitChanges": "Salvar Altera√ß√µes",
    "commitChanges/description": "Salve suas altera√ß√µes na predefini√ß√£o.",
    "commitChanges.manual": "Novos campos detectados. Voc√™ poder√° escolher quais altera√ß√µes incluir na predefini√ß√£o.",
    "commitChanges.manual.hold.0": "Segure",
    "commitChanges.manual.hold.1": "para escolher quais altera√ß√µes salvar na predefini√ß√£o.",
    "commitChanges.saveAll.hold.0": "Segure",
    "commitChanges.saveAll.hold.1": "para salvar todas as altera√ß√µes.",
    "commitChanges.saveInPreset.hold.0": "Segure",
    "commitChanges.saveInPreset.hold.1": "para salvar apenas as altera√ß√µes nos campos que j√° est√£o inclu√≠dos na predefini√ß√£o.",
    "commitChanges/error": "Falha ao salvar altera√ß√µes na predefini√ß√£o.",
    "commitChanges.manual/description": "Escolha quais altera√ß√µes incluir na predefini√ß√£o.",
    "saveAs": "Salvar Como Novo...",
    "presetNamePlaceholder": "Digite um nome para a predefini√ß√£o...",
    "cannotCommitChangesLegacy": "Esta √© uma predefini√ß√£o legada e n√£o pode ser modificada. Voc√™ pode criar uma c√≥pia usando \"Salvar Como Novo...\".",
    "cannotCommitChangesNoChanges": "Nenhuma altera√ß√£o para salvar.",
    "emptyNoUnsaved": "Selecione uma Predefini√ß√£o...",
    "emptyWithUnsaved": "Predefini√ß√£o N√£o Salva",
    "saveEmptyWithUnsaved": "Salvar Predefini√ß√£o Como...",
    "saveConfirm": "Salvar",
    "saveCancel": "Cancelar",
    "saving": "Salvando...",
    "save/error": "Falha ao salvar predefini√ß√£o.",
    "deselect": "Desselecionar Predefini√ß√£o",
    "deselect/error": "Falha ao desselecionar predefini√ß√£o.",
    "select/error": "Falha ao selecionar predefini√ß√£o.",
    "delete/error": "Falha ao excluir predefini√ß√£o.",
    "discardChanges": "Descartar N√£o Salvas",
    "discardChanges/info": "Descartar todas as altera√ß√µes n√£o salvas e restaurar a predefini√ß√£o ao seu estado original",
    "newEmptyPreset": "+ Nova Predefini√ß√£o",
    "importPreset": "Importar",
    "contextMenuSelect": "Aplicar Predefini√ß√£o",
    "contextMenuDelete": "Excluir...",
    "contextMenuShare": "Publicar...",
    "contextMenuOpenInHub": "Ver no Hub",
    "contextMenuPushChanges": "Enviar altera√ß√µes para o Hub",
    "contextMenuPushingChanges": "Enviando...",
    "contextMenuPushedChanges": "Altera√ß√µes enviadas",
    "contextMenuExport": "Exportar Arquivo",
    "contextMenuRevealInExplorer": "Revelar no Explorador de Arquivos",
    "contextMenuRevealInFinder": "Revelar no Finder",
    "share": {
      "title": "Publicar Predefini√ß√£o",
      "action": "Compartilhe sua predefini√ß√£o para que outros possam baixar, curtir e fazer fork",
      "presetOwnerLabel": "Propriet√°rio",
      "uploadAs": "Sua predefini√ß√£o ser√° criada como {{name}}",
      "presetNameLabel": "Nome da Predefini√ß√£o",
      "descriptionLabel": "Descri√ß√£o (opcional)",
      "loading": "Publicando...",
      "success": "Predefini√ß√£o Enviada com Sucesso",
      "presetIsLive": "<preset-name /> est√° agora ativa no Hub!",
      "close": "Fechar",
      "confirmViewOnWeb": "Ver na web",
      "confirmCopy": "Copiar URL",
      "confirmCopied": "Copiado!",
      "pushedToHub": "Sua predefini√ß√£o foi enviada para o Hub",
      "descriptionPlaceholder": "Digite uma descri√ß√£o...",
      "willBePublic": "Publicar sua predefini√ß√£o a tornar√° p√∫blica",
      "publicSubtitle": "Sua predefini√ß√£o √© <custom-bold>P√∫blica</custom-bold>. Outros podem baix√°-la e fazer fork em lmstudio.ai",
      "confirmShareButton": "Publicar",
      "error": "Falha ao publicar predefini√ß√£o",
      "createFreeAccount": "Crie uma conta gratuita no Hub para publicar predefini√ß√µes"
    },
    "update": {
      "title": "Enviar Altera√ß√µes para o Hub",
      "title/success": "Predefini√ß√£o Atualizada com Sucesso",
      "subtitle": "Fa√ßa altera√ß√µes em <custom-preset-name /> e envie-as para o Hub",
      "descriptionLabel": "Descri√ß√£o",
      "descriptionPlaceholder": "Digite uma descri√ß√£o...",
      "loading": "Enviando...",
      "cancel": "Cancelar",
      "createFreeAccount": "Crie uma conta gratuita no Hub para publicar predefini√ß√µes",
      "error": "Falha ao enviar atualiza√ß√£o",
      "confirmUpdateButton": "Enviar"
    },
    "import": {
      "title": "Importar uma Predefini√ß√£o de Arquivo",
      "dragPrompt": "Arraste e solte arquivos JSON de predefini√ß√£o ou <custom-link>selecione do seu computador</custom-link>",
      "remove": "Remover",
      "cancel": "Cancelar",
      "importPreset_zero": "Importar Predefini√ß√£o",
      "importPreset_one": "Importar Predefini√ß√£o",
      "importPreset_other": "Importar {{count}} Predefini√ß√µes",
      "selectDialog": {
        "title": "Selecionar Arquivo de Predefini√ß√£o (.json)",
        "button": "Importar"
      },
      "error": "Falha ao importar predefini√ß√£o",
      "resultsModal": {
        "titleSuccessSection_one": "1 predefini√ß√£o importada com sucesso",
        "titleSuccessSection_other": "{{count}} predefini√ß√µes importadas com sucesso",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} falhou)",
        "titleFailSection_other": "({{count}} falharam)",
        "titleAllFailed": "Falha ao importar predefini√ß√µes",
        "importMore": "Importar Mais",
        "close": "Conclu√≠do",
        "successBadge": "Sucesso",
        "alreadyExistsBadge": "Predefini√ß√£o j√° existe",
        "errorBadge": "Erro",
        "invalidFileBadge": "Arquivo inv√°lido",
        "otherErrorBadge": "Falha ao importar predefini√ß√£o",
        "errorViewDetailsButton": "Ver Detalhes",
        "seeError": "Ver Erro",
        "noName": "Sem nome de predefini√ß√£o",
        "useInChat": "Usar no Chat"
      },
      "importFromUrl": {
        "button": "Importar de URL...",
        "title": "Importar de URL",
        "back": "Importar de Arquivo...",
        "action": "Cole a URL do LM Studio Hub da predefini√ß√£o que voc√™ deseja importar abaixo",
        "invalidUrl": "URL inv√°lida. Por favor, certifique-se de que est√° colando uma URL correta do LM Studio Hub.",
        "tip": "Voc√™ pode instalar a predefini√ß√£o diretamente com o bot√£o {{buttonName}} no LM Studio Hub",
        "confirm": "Importar",
        "cancel": "Cancelar",
        "loading": "Importando...",
        "error": "Falha ao baixar predefini√ß√£o."
      }
    },
    "download": {
      "title": "Baixar <preset-name /> do LM Studio Hub",
      "subtitle": "Salve <custom-name /> em suas predefini√ß√µes. Ao fazer isso, voc√™ poder√° usar esta predefini√ß√£o no aplicativo",
      "button": "Baixar",
      "button/loading": "Baixando...",
      "cancel": "Cancelar",
      "error": "Falha ao baixar predefini√ß√£o."
    },
    "inclusiveness": {
      "speculativeDecoding": "Incluir na Predefini√ß√£o"
    }
  },

  "flashAttentionWarning": "Flash Attention √© um recurso experimental que pode causar problemas com alguns modelos. Se encontrar problemas, tente desativ√°-lo.",
  "llamaKvCacheQuantizationWarning": "A Quantiza√ß√£o do Cache KV √© um recurso experimental que pode causar problemas com alguns modelos. A Flash Attention deve estar habilitada para a quantiza√ß√£o do cache V. Se encontrar problemas, redefina para o padr√£o \"F16\".",

  "seedUncheckedHint": "Seed Aleat√≥ria",
  "ropeFrequencyBaseUncheckedHint": "Autom√°tico",
  "ropeFrequencyScaleUncheckedHint": "Autom√°tico",

  "hardware": {
    "advancedGpuSettings": "Configura√ß√µes Avan√ßadas de GPU",
    "advancedGpuSettings.info": "Se n√£o tiver certeza, deixe estes valores nos padr√µes",
    "advancedGpuSettings.reset": "Redefinir para o padr√£o",
    "environmentVariables": {
      "title": "Vari√°veis de Ambiente",
      "description": "Vari√°veis de ambiente ativas durante o tempo de vida do modelo.",
      "key.placeholder": "Selecionar var...",
      "value.placeholder": "Valor"
    },
    "mainGpu": {
      "title": "GPU Principal",
      "description": "A GPU a ser priorizada para a computa√ß√£o do modelo.",
      "placeholder": "Selecionar GPU principal..."
    },
    "splitStrategy": {
      "title": "Estrat√©gia de Divis√£o",
      "description": "Como dividir a computa√ß√£o do modelo entre GPUs.",
      "placeholder": "Selecionar estrat√©gia de divis√£o..."
    }
  }
}
