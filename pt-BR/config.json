{
    "noInstanceSelected": "Nenhuma inst√¢ncia de modelo selecionada",
    "resetToDefault": "Redefinir",
    "showAdvancedSettings": "Mostrar configura√ß√µes avan√ßadas",
    "showAll": "Tudo",
    "basicSettings": "B√°sico",
    "configSubtitle": "Carregue ou salve predefini√ß√µes e experimente com substitui√ß√µes de par√¢metros do modelo",
    "inferenceParameters/title": "Par√¢metros de Predi√ß√£o",
    "inferenceParameters/info": "Experimente com par√¢metros que impactam a predi√ß√£o.",
    "generalParameters/title": "Geral",
    "samplingParameters/title": "Amostragem",
    "basicTab": "B√°sico",
    "advancedTab": "Avan√ßado",
    "advancedTab/title": "üß™ Configura√ß√£o Avan√ßada",
    "advancedTab/expandAll": "Expandir tudo",
    "advancedTab/overridesTitle": "Substitui√ß√µes de Configura√ß√£o",
    "advancedTab/noConfigsText": "Voc√™ n√£o tem altera√ß√µes n√£o salvas - edite valores acima para ver substitui√ß√µes aqui.",
    "loadInstanceFirst": "Carregue um modelo para ver par√¢metros configur√°veis",
    "noListedConfigs": "Sem par√¢metros configur√°veis",
    "generationParameters/info": "Experimente com par√¢metros b√°sicos que impactam a gera√ß√£o de texto.",
    "loadParameters/title": "Par√¢metros de Carregamento",
    "loadParameters/description": "Configura√ß√µes para controlar como o modelo √© inicializado e carregado na mem√≥ria.",
    "loadParameters/reload": "Recarregar para aplicar altera√ß√µes",
    "loadParameters/reload/error": "Falha ao recarregar o modelo",
    "discardChanges": "Descartar altera√ß√µes",
    "loadModelToSeeOptions": "Carregue um modelo para ver op√ß√µes",
    "schematicsError.title": "O esquema de configura√ß√£o cont√©m erros nos seguintes campos:",
    "manifestSections": {
        "structuredOutput/title": "Sa√≠da Estruturada",
        "speculativeDecoding/title": "Decodifica√ß√£o Especulativa",
        "sampling/title": "Amostragem",
        "settings/title": "Configura√ß√µes",
        "toolUse/title": "Uso de Ferramentas",
        "promptTemplate/title": "Modelo de Prompt",
        "customFields/title": "Campos Personalizados"
    },

    "llm.prediction.systemPrompt/title": "Prompt do Sistema",
    "llm.prediction.systemPrompt/description": "Use este campo para fornecer instru√ß√µes de contexto ao modelo, como um conjunto de regras, restri√ß√µes ou requisitos gerais.",
    "llm.prediction.systemPrompt/subTitle": "Diretrizes para a IA",
    "llm.prediction.systemPrompt/openEditor": "Editor",
    "llm.prediction.systemPrompt/closeEditor": "Fechar Editor",
    "llm.prediction.systemPrompt/openedEditor": "Aberto no Editor...",
    "llm.prediction.systemPrompt/edit": "Editar Prompt do Sistema...",
    "llm.prediction.systemPrompt/addInstructionsWithMore": "Adicionar instru√ß√µes...",
    "llm.prediction.systemPrompt/addInstructions": "Adicionar instru√ß√µes",
    "llm.prediction.temperature/title": "Temperatura",
    "llm.prediction.temperature/subTitle": "Quanta aleatoriedade introduzir. 0 produzir√° o mesmo resultado sempre, enquanto valores mais altos aumentar√£o a criatividade e varia√ß√£o",
    "llm.prediction.temperature/info": "Da documenta√ß√£o do llama.cpp: \"O valor padr√£o √© <{{dynamicValue}}>, que fornece um equil√≠brio entre aleatoriedade e determinismo. No extremo, uma temperatura de 0 sempre escolher√° o pr√≥ximo token mais prov√°vel, levando a sa√≠das id√™nticas em cada execu√ß√£o\"",
    "llm.prediction.llama.sampling/title": "Amostragem",
    "llm.prediction.topKSampling/title": "Amostragem Top K",
    "llm.prediction.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos top-k tokens mais prov√°veis. Age de forma similar √† temperatura",
    "llm.prediction.topKSampling/info": "Da documenta√ß√£o do llama.cpp:\n\nAmostragem Top-k √© um m√©todo de gera√ß√£o de texto que seleciona o pr√≥ximo token apenas dos top k tokens mais prov√°veis previstos pelo modelo.\n\nIsso ajuda a reduzir o risco de gerar tokens sem sentido ou de baixa probabilidade, mas tamb√©m pode limitar a diversidade da sa√≠da.\n\nUm valor mais alto para top-k (ex: 100) considerar√° mais tokens e levar√° a textos mais diversos, enquanto um valor mais baixo (ex: 10) focar√° nos tokens mais prov√°veis e gerar√° texto mais conservador.\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
    "llm.prediction.llama.cpuThreads/title": "Threads de CPU",
    "llm.prediction.llama.cpuThreads/subTitle": "N√∫mero de threads de CPU para usar durante a infer√™ncia",
    "llm.prediction.llama.cpuThreads/info": "O n√∫mero de threads para usar durante a computa√ß√£o. Aumentar o n√∫mero de threads nem sempre correla com melhor desempenho. O padr√£o √© <{{dynamicValue}}>.",
    "llm.prediction.maxPredictedTokens/title": "Limitar Comprimento da Resposta",
    "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente limitar o comprimento da resposta da IA",
    "llm.prediction.maxPredictedTokens/info": "Controle o comprimento m√°ximo da resposta do chatbot. Ligue para definir um limite no comprimento m√°ximo, ou desligue para deixar o chatbot decidir quando parar.",
    "llm.prediction.maxPredictedTokens/inputLabel": "Comprimento m√°ximo da resposta (tokens)",
    "llm.prediction.maxPredictedTokens/wordEstimate": "Cerca de {{maxWords}} palavras",
    "llm.prediction.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
    "llm.prediction.repeatPenalty/subTitle": "Quanto desencorajar a repeti√ß√£o do mesmo token",
    "llm.prediction.repeatPenalty/info": "Da documenta√ß√£o do llama.cpp: \"Ajuda a prevenir que o modelo gere texto repetitivo ou mon√≥tono.\n\nUm valor mais alto (ex: 1.5) penalizar√° repeti√ß√µes mais fortemente, enquanto um valor mais baixo (ex: 0.9) ser√° mais tolerante.\" ‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
    "llm.prediction.minPSampling/title": "Amostragem Min P",
    "llm.prediction.minPSampling/subTitle": "Probabilidade base m√≠nima para um token ser selecionado para sa√≠da",
    "llm.prediction.minPSampling/info": "Da documenta√ß√£o do llama.cpp:\n\nA probabilidade m√≠nima para um token ser considerado, relativa √† probabilidade do token mais prov√°vel. Deve estar entre [0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
    "llm.prediction.topPSampling/title": "Amostragem Top P",
    "llm.prediction.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os poss√≠veis pr√≥ximos tokens. Age similar √† temperatura",
    "llm.prediction.topPSampling/info": "Da documenta√ß√£o do llama.cpp:\n\nAmostragem Top-p, tamb√©m conhecida como amostragem de n√∫cleo (nucleus), √© outro m√©todo de gera√ß√£o de texto que seleciona o pr√≥ximo token de um subconjunto de tokens que juntos t√™m uma probabilidade cumulativa de pelo menos p.\n\nEste m√©todo fornece um equil√≠brio entre diversidade e qualidade ao considerar tanto as probabilidades dos tokens quanto o n√∫mero de tokens para amostrar.\n\nUm valor mais alto para top-p (ex: 0.95) levar√° a textos mais diversos, enquanto um valor mais baixo (ex: 0.5) gerar√° texto mais focado e conservador. Deve estar entre (0, 1].\n\n‚Ä¢ O valor padr√£o √© <{{dynamicValue}}>",
    "llm.prediction.stopStrings/title": "Strings de Parada",
    "llm.prediction.stopStrings/subTitle": "Strings que devem fazer o modelo parar de gerar mais tokens",
    "llm.prediction.stopStrings/info": "Strings espec√≠ficas que, quando encontradas, far√£o o modelo parar de gerar mais tokens",
    "llm.prediction.stopStrings/placeholder": "Digite uma string e pressione ‚èé",
    "llm.prediction.contextOverflowPolicy/title": "Estouro de Contexto",
    "llm.prediction.contextOverflowPolicy/subTitle": "Como o modelo deve se comportar quando a conversa cresce demais para ele lidar",
    "llm.prediction.contextOverflowPolicy/info": "Decida o que fazer quando a conversa exceder o tamanho da mem√≥ria de trabalho ('contexto') do modelo",
    "llm.prediction.llama.frequencyPenalty/title": "Penalidade de Frequ√™ncia",
    "llm.prediction.llama.presencePenalty/title": "Penalidade de Presen√ßa",
    "llm.prediction.llama.tailFreeSampling/title": "Amostragem Tail-Free",
    "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem Tipicamente Local",
    "llm.prediction.llama.xtcProbability/title": "Probabilidade de Amostragem XTC",
    "llm.prediction.llama.xtcProbability/subTitle": "O amostrador XTC (Exclude Top Choices) s√≥ ser√° ativado com esta probabilidade por token gerado. A amostragem XTC pode aumentar a criatividade e reduzir clich√™s",
    "llm.prediction.llama.xtcProbability/info": "Amostragem XTC (Exclude Top Choices) s√≥ ser√° ativada com esta probabilidade, por token gerado. A amostragem XTC geralmente aumenta a criatividade e reduz clich√™s",
    "llm.prediction.llama.xtcThreshold/title": "Limiar de Amostragem XTC",
    "llm.prediction.llama.xtcThreshold/subTitle": "Limiar XTC. Com uma chance de `xtc-probability`, busca por tokens com probabilidades entre `xtc-threshold` e 0.5, e remove todos esses tokens exceto o menos prov√°vel",
    "llm.prediction.llama.xtcThreshold/info": "Limiar XTC (Exclude Top Choices). Com uma chance de `xtc-probability`, busca por tokens com probabilidades entre `xtc-threshold` e 0.5, e remove todos esses tokens exceto o menos prov√°vel",
    "llm.prediction.mlx.topKSampling/title": "Amostragem Top K",
    "llm.prediction.mlx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos top-k tokens mais prov√°veis. Age similar √† temperatura",
    "llm.prediction.mlx.topKSampling/info": "Limita o pr√≥ximo token a um dos top-k tokens mais prov√°veis. Age similar √† temperatura",
    "llm.prediction.onnx.topKSampling/title": "Amostragem Top K",
    "llm.prediction.onnx.topKSampling/subTitle": "Limita o pr√≥ximo token a um dos top-k tokens mais prov√°veis. Age similar √† temperatura",
    "llm.prediction.onnx.topKSampling/info": "Da documenta√ß√£o ONNX:\n\nN√∫mero de tokens de vocabul√°rio de maior probabilidade para manter no filtro top-k\n\n‚Ä¢ Este filtro est√° desligado por padr√£o",
    "llm.prediction.onnx.repeatPenalty/title": "Penalidade de Repeti√ß√£o",
    "llm.prediction.onnx.repeatPenalty/subTitle": "Quanto desencorajar a repeti√ß√£o do mesmo token",
    "llm.prediction.onnx.repeatPenalty/info": "Um valor mais alto desencoraja o modelo de se repetir",
    "llm.prediction.onnx.topPSampling/title": "Amostragem Top P",
    "llm.prediction.onnx.topPSampling/subTitle": "Probabilidade cumulativa m√≠nima para os poss√≠veis pr√≥ximos tokens. Age similar √† temperatura",
    "llm.prediction.onnx.topPSampling/info": "Da documenta√ß√£o ONNX:\n\nApenas os tokens mais prov√°veis com probabilidades que somam at√© TopP ou mais s√£o mantidos para gera√ß√£o\n\n‚Ä¢ Este filtro est√° desligado por padr√£o",
    "llm.prediction.seed/title": "Semente",
    "llm.prediction.structured/title": "Sa√≠da Estruturada",
    "llm.prediction.structured/info": "Sa√≠da Estruturada",
    "llm.prediction.structured/description": "Avan√ßado: voc√™ pode fornecer um [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) para for√ßar um formato de sa√≠da particular do modelo. Leia a [documenta√ß√£o](https://lmstudio.ai/docs/advanced/structured-output) para saber mais",
    "llm.prediction.tools/title": "Uso de Ferramentas",
    "llm.prediction.tools/description": "Avan√ßado: voc√™ pode fornecer uma lista de ferramentas compat√≠vel com JSON para o modelo solicitar chamadas. Leia a [documenta√ß√£o](https://lmstudio.ai/docs/advanced/tool-use) para saber mais",
    "llm.prediction.tools/serverPageDescriptionAddon": "Passe isso atrav√©s do corpo da requisi√ß√£o como `tools` ao usar a API do servidor",
    "llm.prediction.promptTemplate/title": "Modelo de Prompt",
    "llm.prediction.promptTemplate/subTitle": "O formato no qual mensagens na conversa s√£o enviadas ao modelo. Mudar isso pode introduzir comportamento inesperado - certifique-se de saber o que est√° fazendo!",
    "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Tokens de Rascunho a Gerar",
    "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "O n√∫mero de tokens para gerar com o modelo de rascunho por token do modelo principal. Encontre o ponto ideal de computa√ß√£o vs. recompensa",
    "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Corte de Probabilidade de Rascunho",
    "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continuar rascunhando at√© a probabilidade de um token cair abaixo deste limiar. Valores mais altos geralmente significam menor risco, menor recompensa",
    "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Tamanho M√≠n. do Rascunho",
    "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Rascunhos menores que isso ser√£o ignorados pelo modelo principal. Valores mais altos geralmente significam menor risco, menor recompensa",
    "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Tamanho M√°x. do Rascunho",
    "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "N√∫mero m√°x. de tokens permitidos em um rascunho. Teto se todas as probs de token forem > que o corte. Valores mais baixos geralmente significam menor risco, menor recompensa",
    "llm.prediction.speculativeDecoding.draftModel/title": "Modelo de Rascunho",
    "llm.prediction.reasoning.parsing/title": "An√°lise da Se√ß√£o de Racioc√≠nio",
    "llm.prediction.reasoning.parsing/subTitle": "Como analisar se√ß√µes de racioc√≠nio na sa√≠da do modelo",

    "llm.load.mainGpu/title": "GPU Principal",
    "llm.load.mainGpu/subTitle": "A GPU para priorizar na computa√ß√£o do modelo",
    "llm.load.mainGpu/placeholder": "Selecione a GPU principal...",
    "llm.load.splitStrategy/title": "Estrat√©gia de Divis√£o",
    "llm.load.splitStrategy/subTitle": "Como dividir a computa√ß√£o do modelo entre GPUs",
    "llm.load.splitStrategy/placeholder": "Selecione a estrat√©gia de divis√£o...",
    "llm.load.offloadKVCacheToGpu/title": "Alocar Cache KV na Mem√≥ria da GPU",
    "llm.load.offloadKVCacheToGpu/subTitle": "Aloca o cache KV na mem√≥ria da GPU. Melhora o desempenho, mas requer mais mem√≥ria de GPU",
    "load.gpuStrictVramCap/title": "Limitar Aloca√ß√£o do Modelo √† Mem√≥ria Dedicada da GPU",
    "load.gpuStrictVramCap.customSubTitleOff": "DESLIGADO: Permitir que pesos do modelo sejam alocados na mem√≥ria compartilhada se a mem√≥ria dedicada da GPU estiver cheia",
    "load.gpuStrictVramCap.customSubTitleOn": "LIGADO: O sistema limitar√° a aloca√ß√£o de pesos do modelo apenas √† mem√≥ria dedicada da GPU e RAM. O contexto ainda pode usar mem√≥ria compartilhada",
    "load.gpuStrictVramCap.customGpuOffloadWarning": "Aloca√ß√£o do modelo limitada √† mem√≥ria dedicada da GPU. O n√∫mero real de camadas alocadas pode diferir",
    "load.allGpusDisabledWarning": "Todas as GPUs est√£o desativadas. Ative pelo menos uma para alocar",

    "llm.load.contextLength/title": "Tamanho do Contexto",
    "llm.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode atender em um prompt. Veja as op√ß√µes de Estouro de Conversa em \"Par√¢metros de Infer√™ncia\" para mais formas de gerenciar isso",
    "llm.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando quanto contexto ele ret√©m durante o processamento",
    "llm.load.contextLength/warning": "Definir um valor alto para o tamanho do contexto pode impactar significativamente o uso de mem√≥ria",
    "llm.load.seed/title": "Semente",
    "llm.load.seed/subTitle": "A semente para o gerador de n√∫meros aleat√≥rios usado na gera√ß√£o de texto. -1 √© aleat√≥rio",
    "llm.load.seed/info": "Semente Aleat√≥ria: Define a semente para gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis",
    "llm.load.numCpuExpertLayersRatio/title": "For√ßar Pesos de Especialistas do Modelo na CPU",
    "llm.load.numCpuExpertLayersRatio/subTitle": "Se deve for√ßar pesos de especialistas MoE na RAM da CPU. Economiza VRAM e pode ser mais r√°pido que aloca√ß√£o parcial na GPU. N√£o recomendado se o modelo couber inteiramente na VRAM.",
    "llm.load.numCpuExpertLayersRatio/info": "Especifica se deve colocar ou n√£o todas as camadas de especialistas MoE na RAM da CPU. Deixa camadas de aten√ß√£o na GPU, economizando VRAM enquanto mant√©m a infer√™ncia razoavelmente r√°pida",

    "llm.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
    "llm.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada para processar por vez. Aumentar isso aumenta o desempenho ao custo de uso de mem√≥ria",
    "llm.load.llama.evalBatchSize/info": "Define o n√∫mero de exemplos processados juntos em um lote durante a avalia√ß√£o, afetando velocidade e uso de mem√≥ria",
    "llm.load.llama.ropeFrequencyBase/title": "Base de Frequ√™ncia RoPE",
    "llm.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base personalizada para vetores posicionais rotativos (RoPE). Aumentar isso pode permitir melhor desempenho em contextos longos",
    "llm.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa, afetando como a informa√ß√£o posicional √© incorporada",
    "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia RoPE",
    "llm.load.llama.ropeFrequencyScale/subTitle": "Tamanho do contexto √© escalado por este fator para estender o contexto efetivo usando RoPE",
    "llm.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa para controlar a granularidade da codifica√ß√£o posicional",
    "llm.load.llama.acceleration.offloadRatio/title": "Uso da GPU (Offload)",
    "llm.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas discretas do modelo para computar na GPU para acelera√ß√£o",
    "llm.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para alocar na GPU.",
    "llm.load.llama.flashAttention/title": "Flash Attention",
    "llm.load.llama.flashAttention/subTitle": "Diminui o uso de mem√≥ria e tempo de gera√ß√£o em alguns modelos",
    "llm.load.llama.flashAttention/info": "Acelera mecanismos de aten√ß√£o para processamento mais r√°pido e eficiente",
    "llm.load.numExperts/title": "N√∫mero de Especialistas",
    "llm.load.numExperts/subTitle": "N√∫mero de especialistas para usar no modelo",
    "llm.load.numExperts/info": "O n√∫mero de especialistas para usar no modelo",
    "llm.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
    "llm.load.llama.keepModelInMemory/subTitle": "Reservar mem√≥ria do sistema para o modelo, mesmo quando alocado na GPU. Melhora desempenho mas requer mais RAM do sistema",
    "llm.load.llama.keepModelInMemory/info": "Previne que o modelo seja trocado para o disco (swap), garantindo acesso mais r√°pido ao custo de maior uso de RAM",
    "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Cache KV",
    "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de mem√≥ria armazenando o cache em meia-precis√£o (FP16)",
    "llm.load.llama.tryMmap/title": "Tentar mmap()",
    "llm.load.llama.tryMmap/subTitle": "Melhora tempo de carregamento do modelo. Desativar isso pode melhorar o desempenho quando o modelo √© maior que a RAM dispon√≠vel",
    "llm.load.llama.tryMmap/info": "Carregar arquivos de modelo diretamente do disco para a mem√≥ria",
    "llm.load.llama.cpuThreadPoolSize/title": "Tamanho do Pool de Threads da CPU",
    "llm.load.llama.cpuThreadPoolSize/subTitle": "N√∫mero de threads de CPU para alocar ao pool de threads usado para computa√ß√£o do modelo",
    "llm.load.llama.cpuThreadPoolSize/info": "O n√∫mero de threads de CPU para alocar ao pool de threads usado para computa√ß√£o do modelo. Aumentar o n√∫mero de threads nem sempre correla com melhor desempenho. O padr√£o √© <{{dynamicValue}}>.",
    "llm.load.llama.kCacheQuantizationType/title": "Tipo de Quantiza√ß√£o do Cache K",
    "llm.load.llama.kCacheQuantizationType/subTitle": "Valores mais baixos reduzem uso de mem√≥ria mas podem diminuir qualidade. O efeito varia significativamente entre modelos.",
    "llm.load.llama.vCacheQuantizationType/title": "Tipo de Quantiza√ß√£o do Cache V",
    "llm.load.llama.vCacheQuantizationType/subTitle": "Valores mais baixos reduzem uso de mem√≥ria mas podem diminuir qualidade. O efeito varia significativamente entre modelos.",
    "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Voc√™ deve desativar este valor se Flash Attention n√£o estiver ativado",
    "llm.load.llama.vCacheQuantizationType/disabledMessage": "S√≥ pode ser ligado quando Flash Attention est√° ativado",
    "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Voc√™ deve desativar Flash Attention ao usar F32",
    "llm.load.mlx.kvCacheBits/title": "Quantiza√ß√£o do Cache KV",
    "llm.load.mlx.kvCacheBits/subTitle": "N√∫mero de bits para os quais o cache KV deve ser quantizado",
    "llm.load.mlx.kvCacheBits/info": "N√∫mero de bits para os quais o cache KV deve ser quantizado",
    "llm.load.mlx.kvCacheBits/turnedOnWarning": "Configura√ß√£o de Tamanho do Contexto √© ignorada ao usar Quantiza√ß√£o do Cache KV",
    "llm.load.mlx.kvCacheGroupSize/title": "Quantiza√ß√£o do Cache KV: Tamanho do Grupo",
    "llm.load.mlx.kvCacheGroupSize/subTitle": "Tamanho do grupo durante opera√ß√£o de quantiza√ß√£o para o cache KV. Tamanho de grupo maior reduz uso de mem√≥ria mas pode diminuir qualidade",
    "llm.load.mlx.kvCacheGroupSize/info": "N√∫mero de bits para os quais o cache KV deve ser quantizado",
    "llm.load.mlx.kvCacheQuantizationStart/title": "Quantiza√ß√£o do Cache KV: Iniciar quantiza√ß√£o quando ctx cruzar este tamanho",
    "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Limiar de tamanho de contexto para iniciar a quantiza√ß√£o do cache KV",
    "llm.load.mlx.kvCacheQuantizationStart/info": "Limiar de tamanho de contexto para iniciar a quantiza√ß√£o do cache KV",
    "llm.load.mlx.kvCacheQuantization/title": "Quantiza√ß√£o do Cache KV",
    "llm.load.mlx.kvCacheQuantization/subTitle": "Quantizar o cache KV do modelo. Isso pode resultar em gera√ß√£o mais r√°pida e menor pegada de mem√≥ria,\n√†s custas da qualidade da sa√≠da do modelo.",
    "llm.load.mlx.kvCacheQuantization/bits/title": "Bits de quantiza√ß√£o do cache KV",
    "llm.load.mlx.kvCacheQuantization/bits/tooltip": "N√∫mero de bits para quantizar o cache KV",
    "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
    "llm.load.mlx.kvCacheQuantization/groupSize/title": "Estrat√©gia de tamanho do grupo",
    "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Precis√£o",
    "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Equilibrado",
    "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "R√°pido",
    "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Avan√ßado: Configura√ß√£o de 'tamanho de grupo matmul' quantizado\n\n‚Ä¢ Precis√£o = tamanho do grupo 32\n‚Ä¢ Equilibrado = tamanho do grupo 64\n‚Ä¢ R√°pido = tamanho do grupo 128\n",
    "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Iniciar quantiza√ß√£o quando ctx atingir este tamanho",
    "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Quando o contexto atingir esta quantidade de tokens,\ncome√ßar a quantizar o cache KV",

    "embedding.load.contextLength/title": "Tamanho do Contexto",
    "embedding.load.contextLength/subTitle": "O n√∫mero m√°ximo de tokens que o modelo pode atender em um prompt. Veja as op√ß√µes de Estouro de Conversa em \"Par√¢metros de Infer√™ncia\" para mais formas de gerenciar isso",
    "embedding.load.contextLength/info": "Especifica o n√∫mero m√°ximo de tokens que o modelo pode considerar de uma vez, impactando quanto contexto ele ret√©m durante o processamento",
    "embedding.load.llama.ropeFrequencyBase/title": "Base de Frequ√™ncia RoPE",
    "embedding.load.llama.ropeFrequencyBase/subTitle": "Frequ√™ncia base personalizada para vetores posicionais rotativos (RoPE). Aumentar isso pode permitir melhor desempenho em contextos longos",
    "embedding.load.llama.ropeFrequencyBase/info": "[Avan√ßado] Ajusta a frequ√™ncia base para Codifica√ß√£o Posicional Rotativa, afetando como a informa√ß√£o posicional √© incorporada",
    "embedding.load.llama.evalBatchSize/title": "Tamanho do Lote de Avalia√ß√£o",
    "embedding.load.llama.evalBatchSize/subTitle": "N√∫mero de tokens de entrada para processar por vez. Aumentar isso aumenta o desempenho ao custo de uso de mem√≥ria",
    "embedding.load.llama.evalBatchSize/info": "Define o n√∫mero de tokens processados juntos em um lote durante a avalia√ß√£o",
    "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequ√™ncia RoPE",
    "embedding.load.llama.ropeFrequencyScale/subTitle": "Tamanho do contexto √© escalado por este fator para estender o contexto efetivo usando RoPE",
    "embedding.load.llama.ropeFrequencyScale/info": "[Avan√ßado] Modifica a escala de frequ√™ncia para Codifica√ß√£o Posicional Rotativa para controlar a granularidade da codifica√ß√£o posicional",
    "embedding.load.llama.acceleration.offloadRatio/title": "Uso da GPU",
    "embedding.load.llama.acceleration.offloadRatio/subTitle": "N√∫mero de camadas discretas do modelo para computar na GPU para acelera√ß√£o",
    "embedding.load.llama.acceleration.offloadRatio/info": "Defina o n√∫mero de camadas para alocar na GPU.",
    "embedding.load.llama.keepModelInMemory/title": "Manter Modelo na Mem√≥ria",
    "embedding.load.llama.keepModelInMemory/subTitle": "Reservar mem√≥ria do sistema para o modelo, mesmo quando alocado na GPU. Melhora desempenho mas requer mais RAM do sistema",
    "embedding.load.llama.keepModelInMemory/info": "Previne que o modelo seja trocado para o disco (swap), garantindo acesso mais r√°pido ao custo de maior uso de RAM",
    "embedding.load.llama.tryMmap/title": "Tentar mmap()",
    "embedding.load.llama.tryMmap/subTitle": "Melhora tempo de carregamento do modelo. Desativar isso pode melhorar o desempenho quando o modelo √© maior que a RAM dispon√≠vel",
    "embedding.load.llama.tryMmap/info": "Carregar arquivos de modelo diretamente do disco para a mem√≥ria",
    "embedding.load.seed/title": "Semente",
    "embedding.load.seed/subTitle": "A semente para o gerador de n√∫meros aleat√≥rios usado na gera√ß√£o de texto. -1 √© semente aleat√≥ria",

    "embedding.load.seed/info": "Semente Aleat√≥ria: Define a semente para gera√ß√£o de n√∫meros aleat√≥rios para garantir resultados reproduz√≠veis",

    "presetTooltip": {
        "included/title": "Valores da Predefini√ß√£o",
        "included/description": "Os seguintes campos ser√£o aplicados",
        "included/empty": "Nenhum campo desta predefini√ß√£o se aplica neste contexto.",
        "included/conflict": "Voc√™ ser√° solicitado a escolher se deseja aplicar este valor",
        "separateLoad/title": "Configura√ß√£o de Tempo de Carregamento",
        "separateLoad/description.1": "A predefini√ß√£o tamb√©m inclui a seguinte configura√ß√£o de tempo de carregamento. Configs de carregamento s√£o aplicadas ao modelo todo e requerem recarregar o modelo para surtir efeito. Segure",
        "separateLoad/description.2": "para aplicar a",
        "separateLoad/description.3": ".",
        "excluded/title": "Pode n√£o se aplicar",
        "excluded/description": "Os seguintes campos est√£o inclu√≠dos na predefini√ß√£o mas n√£o se aplicam no contexto atual.",
        "legacy/title": "Predefini√ß√£o Antiga",
        "legacy/description": "Esta predefini√ß√£o √© legada. Ela inclui os seguintes campos que s√£o tratados automaticamente agora, ou n√£o s√£o mais aplic√°veis.",
        "button/publish": "Publicar no Hub",
        "button/pushUpdate": "Enviar Altera√ß√µes ao Hub",
        "button/noChangesToPush": "Sem altera√ß√µes para enviar",
        "button/export": "Exportar",
        "hubLabel": "Predefini√ß√£o do Hub por {{user}}",
        "ownHubLabel": "Sua predefini√ß√£o do Hub"
    },

    "customInputs": {
        "string": {
            "emptyParagraph": "<Vazio>"
        },
        "checkboxNumeric": {
            "off": "DESL."
        },
        "llamaCacheQuantizationType": {
            "off": "DESL."
        },
        "mlxKvCacheBits": {
            "off": "DESL."
        },
        "stringArray": {
            "empty": "<Vazio>"
        },
        "llmPromptTemplate": {
            "type": "Tipo",
            "types.jinja/label": "Template (Jinja)",
            "jinja.bosToken/label": "Token BOS",
            "jinja.eosToken/label": "Token EOS",
            "jinja.template/label": "Template",
            "jinja/error": "Falha ao analisar template Jinja: {{error}}",
            "jinja/empty": "Por favor insira um template Jinja acima.",
            "jinja/unlikelyToWork": "O template Jinja que voc√™ forneceu acima provavelmente n√£o funcionar√° pois n√£o referencia a vari√°vel \"messages\". Por favor verifique se voc√™ inseriu um template correto.",
            "types.manual/label": "Manual",
            "manual.subfield.beforeSystem/label": "Antes do Sistema",
            "manual.subfield.beforeSystem/placeholder": "Insira prefixo do Sistema...",
            "manual.subfield.afterSystem/label": "Depois do Sistema",
            "manual.subfield.afterSystem/placeholder": "Insira sufixo do Sistema...",
            "manual.subfield.beforeUser/label": "Antes do Usu√°rio",
            "manual.subfield.beforeUser/placeholder": "Insira prefixo do Usu√°rio...",
            "manual.subfield.afterUser/label": "Depois do Usu√°rio",
            "manual.subfield.afterUser/placeholder": "Insira sufixo do Usu√°rio...",
            "manual.subfield.beforeAssistant/label": "Antes do Assistente",
            "manual.subfield.beforeAssistant/placeholder": "Insira prefixo do Assistente...",
            "manual.subfield.afterAssistant/label": "Depois do Assistente",
            "manual.subfield.afterAssistant/placeholder": "Insira sufixo do Assistente...",
            "stopStrings/label": "Strings de Parada Adicionais",
            "stopStrings/subTitle": "Strings de parada espec√≠ficas do template que ser√£o usadas al√©m das strings de parada especificadas pelo usu√°rio."
        },
        "contextLength": {
            "maxValueTooltip": "Este √© o n√∫mero m√°ximo de tokens que o modelo foi treinado para lidar. Clique para definir o contexto para este valor",
            "maxValueTextStart": "Modelo suporta at√©",
            "maxValueTextEnd": "tokens",
            "tooltipHint": "Embora um modelo possa suportar at√© um certo n√∫mero de tokens, o desempenho pode deteriorar se os recursos da sua m√°quina n√£o puderem lidar com a carga - tenha cautela ao aumentar este valor"
        },
        "contextOverflowPolicy": {
            "stopAtLimit": "Parar no Limite",
            "stopAtLimitSub": "Parar de gerar assim que a mem√≥ria do modelo ficar cheia",
            "truncateMiddle": "Truncar Meio",
            "truncateMiddleSub": "Remove mensagens do meio da conversa para abrir espa√ßo para novas. O modelo ainda lembrar√° do in√≠cio da conversa",
            "rollingWindow": "Janela Deslizante",
            "rollingWindowSub": "O modelo sempre receber√° as mensagens mais recentes, mas pode esquecer o in√≠cio da conversa"
        },
        "llamaAccelerationOffloadRatio": {
            "max": "M√ÅX.",
            "off": "DESL."
        },
        "gpuSplitStrategy": {
            "evenly": "Igualmente",
            "favorMainGpu": "Favorecer GPU Principal"
        },
        "speculativeDecodingDraftModel": {
            "readMore": "Leia como funciona",
            "placeholder": "Selecione um modelo de rascunho compat√≠vel",
            "noCompatible": "Nenhum modelo de rascunho compat√≠vel encontrado para sua sele√ß√£o atual de modelo",
            "stillLoading": "Identificando modelos de rascunho compat√≠veis...",
            "notCompatible": "O modelo de rascunho selecionado (<draft/>) n√£o √© compat√≠vel com a sele√ß√£o atual de modelo (<current/>).",
            "off": "DESLIGADO",
            "loadModelToSeeOptions": "Carregue o modelo <keyboard-shortcut /> para ver op√ß√µes compat√≠veis",
            "compatibleWithNumberOfModels": "Recomendado para pelo menos {{dynamicValue}} dos seus modelos",
            "recommendedForSomeModels": "Recomendado para alguns modelos",
            "recommendedForLlamaModels": "Recomendado para modelos Llama",
            "recommendedForQwenModels": "Recomendado para modelos Qwen",
            "onboardingModal": {
                "introducing": "Apresentando",
                "speculativeDecoding": "Decodifica√ß√£o Especulativa",
                "firstStepBody": "Acelera√ß√£o de infer√™ncia para modelos <custom-span>llama.cpp</custom-span> e <custom-span>MLX</custom-span>",
                "secondStepTitle": "Acelera√ß√£o de Infer√™ncia com Decodifica√ß√£o Especulativa",
                "secondStepBody": "Decodifica√ß√£o Especulativa √© uma t√©cnica envolvendo a colabora√ß√£o de dois modelos:\n - Um modelo \"principal\" maior\n - Um modelo de \"rascunho\" menor\n\nDurante a gera√ß√£o, o modelo de rascunho prop√µe rapidamente tokens para o modelo principal verificar. Verificar tokens √© um processo muito mais r√°pido do que ger√°-los de fato, que √© a fonte dos ganhos de velocidade. **Geralmente, quanto maior a diferen√ßa de tamanho entre o modelo principal e o modelo de rascunho, maior a acelera√ß√£o**.\n\nPara manter a qualidade, o modelo principal aceita apenas tokens que se alinham com o que ele mesmo teria gerado, permitindo a qualidade de resposta do modelo maior em velocidades de infer√™ncia mais r√°pidas. Ambos os modelos devem compartilhar o mesmo vocabul√°rio.",
                "draftModelRecommendationsTitle": "Recomenda√ß√µes de modelo de rascunho",
                "basedOnCurrentModels": "Baseado em seus modelos atuais",
                "close": "Fechar",
                "next": "Pr√≥ximo",
                "done": "Concluir"
            },
            "speculativeDecodingLoadModelToSeeOptions": "Por favor carregue um modelo primeiro <model-badge /> ",
            "errorEngineNotSupported": "Decodifica√ß√£o especulativa requer pelo menos a vers√£o {{minVersion}} do motor {{engineName}}. Por favor atualize o motor (<key/>) e recarregue o modelo para usar este recurso.",
            "errorEngineNotSupported/noKey": "Decodifica√ß√£o especulativa requer pelo menos a vers√£o {{minVersion}} do motor {{engineName}}. Por favor atualize o motor e recarregue o modelo para usar este recurso."
        },
        "llmReasoningParsing": {
            "startString/label": "String de In√≠cio",
            "startString/placeholder": "Insira a string de in√≠cio...",
            "endString/label": "String de Fim",
            "endString/placeholder": "Insira a string de fim..."
        }
    },
    "saveConflictResolution": {
        "title": "Escolha quais valores incluir na Predefini√ß√£o",
        "description": "Escolha quais valores manter",
        "instructions": "Clique em um valor para inclu√≠-lo",
        "userValues": "Valor Anterior",
        "presetValues": "Novo Valor",
        "confirm": "Confirmar",
        "cancel": "Cancelar"
    },
    "applyConflictResolution": {
        "title": "Quais valores manter?",
        "description": "Voc√™ tem altera√ß√µes n√£o confirmadas que sobrep√µem a Predefini√ß√£o recebida",
        "instructions": "Clique em um valor para mant√™-lo",
        "userValues": "Valor Atual",
        "presetValues": "Valor da Predefini√ß√£o Recebida",
        "confirm": "Confirmar",
        "cancel": "Cancelar"
    },
    "empty": "<Vazio>",
    "noModelSelected": "Nenhum modelo selecionado",
    "apiIdentifier.label": "Identificador da API",
    "apiIdentifier.hint": "Opcionalmente forne√ßa um identificador para este modelo. Isso ser√° usado em requisi√ß√µes da API. Deixe em branco para usar o identificador padr√£o.",
    "idleTTL.label": "Auto Descarregar Se Ocioso (TTL)",
    "idleTTL.hint": "Se definido, o modelo ser√° automaticamente descarregado ap√≥s ficar ocioso pela quantidade de tempo especificada.",
    "idleTTL.mins": "mins",
    "presets": {
        "title": "Predefini√ß√£o",
        "saveChanges": "Salvar",
        "saveChanges/description": "Salvar suas altera√ß√µes na predefini√ß√£o.",
        "saveChanges.manual": "Novos campos detectados. Voc√™ poder√° escolher quais altera√ß√µes incluir na predefini√ß√£o.",
        "saveChanges.manual.hold.0": "Segure",
        "saveChanges.manual.hold.1": "para escolher quais altera√ß√µes salvar na predefini√ß√£o.",
        "saveChanges.saveAll.hold.0": "Segure",
        "saveChanges.saveAll.hold.1": "para salvar todas as altera√ß√µes.",
        "saveChanges.saveInPreset.hold.0": "Segure",
        "saveChanges.saveInPreset.hold.1": "para salvar apenas altera√ß√µes em campos que j√° est√£o inclu√≠dos na predefini√ß√£o.",
        "saveChanges/error": "Falha ao salvar altera√ß√µes na predefini√ß√£o.",
        "saveChanges.manual/description": "Escolha quais altera√ß√µes incluir na predefini√ß√£o.",
        "saveAs": "Salvar Como Nova...",
        "presetNamePlaceholder": "Insira um nome para a predefini√ß√£o...",
        "cannotCommitChangesLegacy": "Esta √© uma predefini√ß√£o legada e n√£o pode ser modificada. Voc√™ pode criar uma c√≥pia usando \"Salvar Como Nova...\".",
        "cannotSaveChangesNoChanges": "Sem altera√ß√µes para salvar.",
        "emptyNoUnsaved": "Selecione uma Predefini√ß√£o...",
        "emptyWithUnsaved": "Predefini√ß√£o N√£o Salva",
        "saveEmptyWithUnsaved": "Salvar Predefini√ß√£o Como...",
        "saveConfirm": "Salvar",
        "saveCancel": "Cancelar",
        "saving": "Salvando...",
        "save/error": "Falha ao salvar predefini√ß√£o.",
        "deselect": "Deselecionar Predefini√ß√£o",
        "deselect/error": "Falha ao deselecionar predefini√ß√£o.",
        "select/error": "Falha ao selecionar predefini√ß√£o.",
        "delete/error": "Falha ao excluir predefini√ß√£o.",
        "discardChanges": "Descartar N√£o Salvos",
        "discardChanges/info": "Descartar todas as altera√ß√µes n√£o salvas e restaurar a predefini√ß√£o ao seu estado original",
        "newEmptyPreset": "+ Nova Predefini√ß√£o",
        "importPreset": "Importar",
        "contextMenuCopyIdentifier": "Copiar Identificador da Predefini√ß√£o",
        "contextMenuSelect": "Aplicar Predefini√ß√£o",
        "contextMenuDelete": "Excluir...",
        "contextMenuShare": "Publicar...",
        "contextMenuOpenInHub": "Ver na Web",
        "contextMenuPullFromHub": "Baixar Mais Recente",
        "contextMenuPushChanges": "Enviar Altera√ß√µes ao Hub",
        "contextMenuPushingChanges": "Enviando...",
        "contextMenuPushedChanges": "Altera√ß√µes enviadas",
        "contextMenuExport": "Exportar Arquivo",
        "contextMenuRevealInExplorer": "Mostrar no Explorador de Arquivos",
        "contextMenuRevealInFinder": "Mostrar no Finder",
        "share": {
            "title": "Publicar Predefini√ß√£o",
            "action": "Compartilhe sua predefini√ß√£o para outros baixarem, curtirem e bifurcarem (fork)",
            "presetOwnerLabel": "Propriet√°rio",
            "uploadAs": "Sua predefini√ß√£o ser√° criada como {{name}}",
            "presetNameLabel": "Nome da Predefini√ß√£o",
            "descriptionLabel": "Descri√ß√£o (opcional)",
            "loading": "Publicando...",
            "success": "Predefini√ß√£o Enviada com Sucesso",
            "presetIsLive": "<preset-name /> est√° agora ao vivo no Hub!",
            "close": "Fechar",
            "confirmViewOnWeb": "Ver na web",
            "confirmCopy": "Copiar URL",
            "confirmCopied": "Copiado!",
            "pushedToHub": "Sua predefini√ß√£o foi enviada para o Hub",
            "descriptionPlaceholder": "Insira uma descri√ß√£o...",
            "willBePublic": "Esta predefini√ß√£o ser√° p√∫blica. Qualquer pessoa na internet poder√° v√™-la.",
            "willBePrivate": "Apenas voc√™ poder√° ver esta predefini√ß√£o",
            "willBeOrgVisible": "Esta predefini√ß√£o ser√° vis√≠vel para todos na organiza√ß√£o.",
            "publicSubtitle": "Sua predefini√ß√£o √© <custom-bold>P√∫blica</custom-bold>. Outros podem baix√°-la e bifurc√°-la no lmstudio.ai",
            "privateUsageReached": "Limite de n√∫mero de predefini√ß√µes privadas atingido.",
            "continueInBrowser": "Continuar no Navegador",
            "confirmShareButton": "Publicar",
            "error": "Falha ao publicar predefini√ß√£o",
            "createFreeAccount": "Crie uma conta gratuita no Hub para publicar predefini√ß√µes"
        },
        "update": {
            "title": "Enviar Altera√ß√µes ao Hub",
            "title/success": "Predefini√ß√£o Atualizada com Sucesso",
            "subtitle": "Fa√ßa altera√ß√µes em <custom-preset-name /> e envie-as para o Hub",
            "descriptionLabel": "Descri√ß√£o",
            "descriptionPlaceholder": "Insira uma descri√ß√£o...",
            "loading": "Enviando...",
            "cancel": "Cancelar",
            "createFreeAccount": "Crie uma conta gratuita no Hub para publicar predefini√ß√µes",
            "error": "Falha ao enviar atualiza√ß√£o",
            "confirmUpdateButton": "Enviar"
        },
        "resolve": {
            "title": "Resolver conflitos...",
            "tooltip": "Abrir um pop-up para resolver diferen√ßas com a vers√£o do Hub"
        },
        "loginToManage": {
            "title": "Login para gerenciar..."
        },
        "downloadFromHub": {
            "title": "Baixar",
            "downloading": "Baixando...",
            "success": "Baixado!",
            "error": "Falha ao baixar"
        },
        "push": {
            "title": "Enviar altera√ß√µes",
            "pushing": "Enviando...",
            "success": "Enviado",
            "tooltip": "Envie suas altera√ß√µes locais para a vers√£o remota hospedada no Hub",
            "error": "Falha ao enviar"
        },
        "saveAsNewModal": {
            "title": "Ops! N√£o encontramos a predefini√ß√£o no Hub",
            "confirmSaveAsNewDescription": "Deseja publicar esta predefini√ß√£o como uma nova?",
            "confirmButton": "Publicar como Nova"
        },
        "pull": {
            "title": "Baixar Mais Recente",
            "error": "Falha ao baixar",
            "contextMenuErrorMessage": "Falha ao baixar",
            "success": "Baixado",
            "pulling": "Baixando...",
            "upToDate": "Atualizado!",
            "unsavedChangesModal": {
                "title": "Voc√™ tem altera√ß√µes n√£o salvas.",
                "bodyContent": "Baixar do remoto sobrescrever√° suas altera√ß√µes n√£o salvas. Continuar?",
                "confirmButton": "Sobrescrever Altera√ß√µes N√£o Salvas"
            }
        },
        "import": {
            "title": "Importar uma Predefini√ß√£o de Arquivo",
            "dragPrompt": "Arraste e solte arquivos de predefini√ß√£o (.tar.gz ou preset.json) ou <custom-link>selecione do seu computador</custom-link>",
            "remove": "Remover",
            "cancel": "Cancelar",
            "importPreset_zero": "Importar Predefini√ß√£o",
            "importPreset_one": "Importar Predefini√ß√£o",
            "importPreset_other": "Importar {{count}} Predefini√ß√µes",
            "selectDialog": {
                "title": "Selecione Arquivo de Predefini√ß√£o (preset.json ou .tar.gz)",
                "button": "Importar"
            },
            "error": "Falha ao importar predefini√ß√£o",
            "resultsModal": {
                "titleSuccessSection_one": "Importou 1 predefini√ß√£o com sucesso",
                "titleSuccessSection_other": "Importou {{count}} predefini√ß√µes com sucesso",
                "titleFailSection_zero": "",
                "titleFailSection_one": "({{count}} falhou)",
                "titleFailSection_other": "({{count}} falharam)",
                "titleAllFailed": "Falha ao importar predefini√ß√µes",
                "importMore": "Importar Mais",
                "close": "Conclu√≠do",
                "successBadge": "Sucesso",
                "alreadyExistsBadge": "Predefini√ß√£o j√° existe",
                "errorBadge": "Erro",
                "invalidFileBadge": "Arquivo inv√°lido",
                "otherErrorBadge": "Falha ao importar predefini√ß√£o",
                "errorViewDetailsButton": "Ver Detalhes",
                "seeError": "Ver Erro",
                "noName": "Sem nome de predefini√ß√£o",
                "useInChat": "Usar na Conversa"
            },
            "importFromUrl": {
                "button": "Importar de URL...",
                "title": "Importar de URL",
                "back": "Importar de Arquivo...",
                "action": "Cole a URL do LM Studio Hub da predefini√ß√£o que deseja importar abaixo",
                "invalidUrl": "URL inv√°lida. Por favor certifique-se de que est√° colando uma URL correta do LM Studio Hub.",
                "tip": "Voc√™ pode instalar a predefini√ß√£o diretamente com o bot√£o {{buttonName}} no LM Studio Hub",
                "confirm": "Importar",
                "cancel": "Cancelar",
                "loading": "Importando...",
                "error": "Falha ao baixar predefini√ß√£o."
            }
        },
        "download": {
            "title": "Baixar <preset-name /> do LM Studio Hub",
            "subtitle": "Salvar <custom-name /> em suas predefini√ß√µes. Fazendo isso voc√™ poder√° usar esta predefini√ß√£o no aplicativo",
            "button": "Baixar",
            "button/loading": "Baixando...",
            "cancel": "Cancelar",
            "error": "Falha ao baixar predefini√ß√£o."
        },
        "inclusiveness": {
            "speculativeDecoding": "Incluir na Predefini√ß√£o"
        }
    },
    "flashAttentionWarning": "Flash Attention √© um recurso experimental que pode causar problemas com alguns modelos. Se encontrar problemas, tente desativ√°-lo.",
    "llamaKvCacheQuantizationWarning": "Quantiza√ß√£o de Cache KV √© um recurso experimental que pode causar problemas com alguns modelos. Flash Attention deve estar ativado para quantiza√ß√£o de cache V. Se encontrar problemas, redefina para o padr√£o \"F16\".",
    "seedUncheckedHint": "Semente Aleat√≥ria",
    "ropeFrequencyBaseUncheckedHint": "Auto",
    "ropeFrequencyScaleUncheckedHint": "Auto",
    "hardware": {
        "environmentVariables": "Vari√°veis de Ambiente",
        "environmentVariables.info": "Se n√£o tiver certeza, deixe estes valores padr√£o",
        "environmentVariables.reset": "Redefinir para padr√£o",
        "gpus.information": "Configure unidades de processamento gr√°fico (GPUs) detectadas em sua m√°quina",
        "gpuSettings": {
            "editMaxCapacity": "Editar Capacidade M√°x.",
            "hideEditMaxCapacity": "Ocultar Editar Capacidade M√°x.",
            "allOffWarning": "Todas as GPUs est√£o desligadas ou desativadas, certifique-se de que h√° alguma aloca√ß√£o de GPU para permitir o carregamento de modelos",
            "split": {
                "title": "Estrat√©gia",
                "placeholder": "Selecione uma aloca√ß√£o de mem√≥ria de GPU",
                "options": {
                    "generalDescription": "Configure como modelos ser√£o carregados em suas GPUs",
                    "evenly": {
                        "title": "Dividir igualmente",
                        "description": "Alocar mem√≥ria igualmente entre GPUs"
                    },
                    "priorityOrder": {
                        "title": "Ordem de prioridade",
                        "description": "Arraste para reordenar prioridade. O sistema tentar√° alocar mais nas GPUs listadas primeiro"
                    },
                    "custom": {
                        "title": "Personalizado",
                        "description": "Alocar mem√≥ria",
                        "maxAllocation": "Aloca√ß√£o M√°xima"
                    }
                }
            },
            "deviceId.info": "Identificador √∫nico para este dispositivo",
            "changesOnlyAffectNewlyLoadedModels": "Altera√ß√µes afetar√£o apenas modelos carregados recentemente",
            "toggleGpu": "Ativar/Desativar GPU"
        }
    },
    "load.gpuSplitConfig/title": "Configura√ß√£o de Divis√£o de GPU",
    "envVars/title": "Definir uma Vari√°vel de Ambiente",
    "envVars": {
        "select": {
            "placeholder": "Selecione uma vari√°vel de ambiente...",
            "noOptions": "N√£o h√° mais dispon√≠veis",
            "filter": {
                "placeholder": "Filtrar resultados de busca",
                "resultsFound_zero": "Nenhum resultado encontrado",
                "resultsFound_one": "1 resultado encontrado",
                "resultsFound_other": "{{count}} resultados encontrados"
            }
        },
        "inputValue": {
            "placeholder": "Insira um valor"
        },
        "values": {
            "title": "Valores Atuais"
        }
    }
}