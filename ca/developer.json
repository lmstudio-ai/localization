{
  "tabs/server": "Servidor Local",
  "tabs/extensions": "Entorns d'Execuci√≥ de LM",
  "loadSettings/title": "Carrega la configuraci√≥",
  "modelSettings/placeholder": "Selecciona un model per configurar-lo",

  "loadedModels/noModels": "Cap model carregat",

  "serverOptions/title": "Opcions del Servidor",
  "serverOptions/configurableTitle": "Opcions Configurables",
  "serverOptions/port/hint": "Selecciona el port local de la xarxa que utilitzar√† el servidor. Per defecte, LM Studio utilitza el port 1234. Potser has de canviar-lo si ja est√†s utilitzant el port.",
  "serverOptions/port/subtitle": "El port a escoltar",
  "serverOptions/autostart/title": "Iniciar autom√†ticament el servidor",
  "serverOptions/autostart/hint": "Enc√©n autom√†ticament el servidor local LLM de LM Studio en iniciar l'app o l'ordinador",
  "serverOptions/port/integerWarning": "El n√∫mero del port ha de ser un nombre sencer",
  "serverOptions/port/invalidPortWarning": "El port ha d'estar entre l'1 i el 65535",
  "serverOptions/cors/title": "Activa CORS",
  "serverOptions/cors/hint1": "Activar CORS (Cross-origin Resource Sharing) permet a les p√†gines web fer solicituds al teu servidor de LM Studio.",
  "serverOptions/cors/hint2": "Potser necessites CORS per fer solicituds des d'una p√†gina web o del VS Code / altres extensions.",
  "serverOptions/cors/subtitle": "Permet solicituds multiplataforma",
  "serverOptions/network/title": "Dona servei a la teva Xarxa Local",
  "serverOptions/network/subtitle": "Mostra el servidor als dispositius de la teva xarxa",
  "serverOptions/network/hint1": "Decideix si permetre les connexions des d'altres dispositius de la xarxa.",
  "serverOptions/network/hint2": "Si es desmarca, el servidor nom√©s far√† cas al localhost.",
  "serverOptions/verboseLogging/title": "Verbose Logging",
  "serverOptions/verboseLogging/subtitle": "Enable verbose logging for the local server",
  "serverOptions/contentLogging/title": "Log Prompts and Responses",
  "serverOptions/contentLogging/subtitle": "Local request / response logging settings",
  "serverOptions/contentLogging/hint": "Whether to log prompts and/or the response in the local server log file.",
  "serverOptions/fileLoggingMode/title": "File Logging Mode",
  "serverOptions/fileLoggingMode/off/title": "OFF",
  "serverOptions/fileLoggingMode/off/hint": "Don't create log files",
  "serverOptions/fileLoggingMode/succinct/title": "Succinct",
  "serverOptions/fileLoggingMode/succinct/hint": "Log the same content as in the console. Long requests will be truncated.",
  "serverOptions/fileLoggingMode/full/title": "Full",
  "serverOptions/fileLoggingMode/full/hint": "Don't truncate long requests.",
  "serverOptions/jitModelLoading/title": "Just-in-Time Model Loading",
  "serverOptions/jitModelLoading/hint": "When enabled, if a request specified a model that is not loaded, it will be automatically loaded and used. In addition, the \"/v1/models\" endpoint will also include models that are not yet loaded.",
  "serverOptions/loadModel/error": "Failed to load model",
  "serverOptions/jitModelLoadingTTL/title": "Auto unload unused JIT loaded models",
  "serverOptions/jitModelLoadingTTL/hint": "A model that was loaded Just-in-time (JIT) to serve an API request will be automatically unloaded after being unused for some duration (TTL).",
  "serverOptions/jitModelLoadingTTL/ttl/label": "Max idle TTL",
  "serverOptions/jitModelLoadingTTL/ttl/unit": "minutes",
  "serverOptions/unloadPreviousJITModelOnLoad/title": "Only Keep Last JIT Loaded Model",
  "serverOptions/unloadPreviousJITModelOnLoad/hint": "Ensure at most 1 model is loaded via JIT at any given time (unloads previous model)",

  "serverLogs/scrollToBottom": "Ves al final",
  "serverLogs/clearLogs": "Buida el registre ({{shortcut}})",
  "serverLogs/openLogsFolder": "Carpeta de registres del servidor",

  "runtimeSettings/title": "Configuraci√≥ d'entorns d'execuci√≥",
  "runtimeSettings/chooseRuntime/title": "Seleccions per Defecte",
  "runtimeSettings/chooseRuntime/description": "Selecciona un entorn d'execuci√≥ per defecte per a cada format de model",
  "runtimeSettings/chooseRuntime/showAllVersions/label": "Mostra tots els entorns d'execuci√≥",
  "runtimeSettings/chooseRuntime/showAllVersions/hint": "Per defecte, LM Studio nom√©s mostra l'√∫ltima versi√≥ de cada entorn d'execuci√≥ compatible. Activa aquesta opci√≥ per veure'ls tots.",
  "runtimeSettings/chooseRuntime/select/placeholder": "Selecciona un Entorn d'Execuci√≥",

  "runtimeOptions/uninstall": "Uninstall",
  "runtimeOptions/uninstallDialog/title": "Uninstall {{runtimeName}}?",
  "runtimeOptions/uninstallDialog/body": "Uninstalling this runtime will remove it from the system. This action is irreversible.",
  "runtimeOptions/uninstallDialog/body/caveats": "Some files may only be removed after LM Studio is restarted.",
  "runtimeOptions/uninstallDialog/error": "Failed to uninstall runtime",
  "runtimeOptions/uninstallDialog/confirm": "Continue and Uninstall",
  "runtimeOptions/uninstallDialog/cancel": "Cancel",
  "runtimeOptions/noCompatibleRuntimes": "No compatible runtimes found",
  "runtimeOptions/downloadIncompatibleRuntime": "This runtime was determined to be incompatible with your machine. It will most likely not work.",
  "runtimeOptions/noRuntimes": "No runtimes found",

  "runtimes": {
    "manageLMRuntimes": "Manage LM Runtimes",
    "includeOlderRuntimeVersions": "Include older runtime versions",
    "dismiss": "Dismiss",
    "updateAvailableToast": {
      "title": "LM Runtime Update Available!"
    },
    "updatedToast": {
      "title": " ‚úÖ LM Runtime Updated: {{runtime}} ‚Üí v{{version}}",
      "preferencesUpdated": "Newly loaded {{compatibilityTypes}} models will use the updated runtime."
    },
    "noAvx2ErrorMessage": "All LM Runtimes currently require a CPU with AVX2 support",
    "downloadableRuntimes": {
      "runtimeExtensionPacks": "Runtime Extension Packs",
      "refresh": "Refresh",
      "refreshing": "Refreshing...",
      "filterSegment": {
        "compatibleOnly": "Compatible Only",
        "all": "All"
      },
      "card": {
        "releaseNotes": "Release Notes",
        "latestVersionInstalled": "Latest Version Installed",
        "updateAvailable": "Update Available"
      }
    },
    "installedRuntimes": {
      "manage": {
        "title": "Manage Active Runtimes"
      },
      "dropdownOptions": {
        "installedVersions": "Manage Versions",
        "close": "Close"
      },
      "tabs": {
        "all": "All",
        "frameworks": "My Frameworks",
        "engines": "My Engines"
      },
      "detailsModal": {
        "installedVersions": "Installed versions for {{runtimeName}}",
        "manifestJsonTitle": "Manifest JSON (advanced)",
        "releaseNotesTitle": "Release Notes",
        "noReleaseNotes": "No release notes available for this version",
        "back": "Back",
        "close": "Close"
      },
      "noEngines": "No engines installed",
      "noFrameworks": "No frameworks installed"
    }
  },

  "inferenceParams/noParams": "No configurable inference parameters available for this model type",

  "quickDocs": {
    "tabChipTitle": "Quick Docs",
    "newToolUsePopover": "Code Snippets are now available here in \"Quick Docs\". Click here to get started with tool use!",
    "newToolUsePopoverTitle": "üìö Quick Docs",
    "learnMore": "‚ÑπÔ∏è üëæ To learn more about LM Studio local server endpoints, visit the [documentation](https://lmstudio.ai/docs).",
    "helloWorld": {
      "title": "Hello, World!"
    },
    "chat": {
      "title": "Chat"
    },
    "structuredOutput": {
      "title": "Structured Output"
    },
    "imageInput": {
      "title": "Image Input"
    },
    "embeddings": {
      "title": "Embeddings"
    },
    "toolUse": {
      "title": "Tool Use",
      "tab": {
        "saveAsPythonFile": "Save as Python file",
        "runTheScript": "Run the script:",
        "savePythonFileCopyPaste": "Save as Python file for a copy-and-paste command"
      }
    },
    "newBadge": "New"
  },

  "endpoints/openaiCompatRest/title": "Supported endpoints (OpenAI-like)",
  "endpoints/openaiCompatRest/getModels": "List the currently loaded models",
  "endpoints/openaiCompatRest/postCompletions": "Text Completions mode. Predict the next token(s) given a prompt. Note: OpenAI considers this endpoint 'deprecated'.",
  "endpoints/openaiCompatRest/postChatCompletions": "Chat completions. Send a chat history to the model to predict the next assistant response",
  "endpoints/openaiCompatRest/postEmbeddings": "Text Embedding. Generate text embeddings for a given text input. Takes a string or array of strings.",

  "model.createVirtualModelFromInstance": "Save Settings as a New Virtual Model",
  "model.createVirtualModelFromInstance/error": "Failed to save settings as a new virtual model",

  "model": {
    "toolUseSectionTitle": "Tool Use",
    "toolUseDescription": "This model is detected to have been trained for tool use\n\nOpen <custom-link>quick docs</custom-link> for more information"
  },

  "apiConfigOptions/title": "API Configuration"
}
