{
  "tabs/server": "Servidor Local",
  "tabs/extensions": "LM Runtimes",
  "loadSettings/title": "Càrrega Configuració",
  "modelSettings/placeholder": "Selecciona un model per configurar-lo",

  "loadedModels/noModels": "Cap model carregat",

  "serverOptions/title": "Opcions del Servidor",
  "serverOptions/configurableTitle": "Opcions Configurables",
  "serverOptions/port/hint": "Estableix el port de xarxa que el servidor local utilitzarà. Per defecte, LM Studio utilitza el port 1234. És possible que s'hagi de canviar si el port ja té un altre ús.",
  "serverOptions/port/subtitle": "The port to listen on",
  "serverOptions/autostart/title": "Auto-iniciar el Servidor",
  "serverOptions/autostart/hint": "Inicia automàticament el servidor local en carregar un model",
  "serverOptions/port/integerWarning": "El port ha de ser un nombre sencer",
  "serverOptions/port/invalidPortWarning": "El port ha d'estar entre 1 i 65535",
  "serverOptions/cors/title": "Activa CORS",
  "serverOptions/cors/hint1": "Activar CORS (Cross-origin Resource Sharing) permetria que els webs que visites enviessin sol·licituts al servidor de LM Studio.",
  "serverOptions/cors/hint2": "Pot ser necessari activar CORS quan VS Code (les seves extensions) o un web enviï una sol·licitud.",
  "serverOptions/cors/subtitle": "Permet cross-origin requests",
  "serverOptions/network/title": "Permet Dispositius en la Xarxa Local",
  "serverOptions/network/subtitle": "Mostra el servidor als dispositius de la xarxa",
  "serverOptions/network/hint1": "Decideix si altres dispositius en la teva xarxa es poden connectar al servidor.",
  "serverOptions/network/hint2": "Si no es marca, el servidor només escoltarà al localhost.",
  "serverOptions/verboseLogging/title": "Verbose Logging",
  "serverOptions/verboseLogging/subtitle": "Enable verbose logging for the local server",
  "serverOptions/contentLogging/title": "Log Prompts and Responses",
  "serverOptions/contentLogging/subtitle": "Local request / response logging settings",
  "serverOptions/contentLogging/hint": "Whether to log prompts and/or the response in the local server log file.",
  "serverOptions/jitModelLoading/title": "Just-in-Time Model Loading",
  "serverOptions/jitModelLoading/hint": "When enabled, if a request specified a model that is not loaded, it will be automatically loaded and used. In addition, the \"/v1/models\" endpoint will also include models that are not yet loaded.",
  "serverOptions/loadModel/error": "Error en carregar el model",

  "serverLogs/scrollToBottom": "Ves al final",
  "serverLogs/clearLogs": "Neteja els logs ({{shortcut}})",
  "serverLogs/openLogsFolder": "Obre la carpeta dels logs del servidor",

  "runtimeSettings/title": "Runtime settings",
  "runtimeSettings/chooseRuntime/title": "Configure Runtimes",
  "runtimeSettings/chooseRuntime/description": "Select a runtime for each model format",
  "runtimeSettings/chooseRuntime/showAllVersions/label": "Show all runtimes",
  "runtimeSettings/chooseRuntime/showAllVersions/hint": "By default, LM Studio only shows the latest version of each compatible runtime. Enable this option to see all available runtimes.",
  "runtimeSettings/chooseRuntime/select/placeholder": "Select a Runtime",

  "runtimeOptions/uninstall": "Desinstal·la",
  "runtimeOptions/uninstallDialog/title": "Desinstal·la {{runtimeName}}?",
  "runtimeOptions/uninstallDialog/body": "Desinstal·lar el runtime l'el·liminarà del sistema. L'acció és irreversible.",
  "runtimeOptions/uninstallDialog/body/caveats": "Alguns arxius no s'el·liminaran fins que no es reiniciï LM Studio.",
  "runtimeOptions/uninstallDialog/error": "Failed to uninstall runtime",
  "runtimeOptions/uninstallDialog/confirm": "Continua i Desinstal·la",
  "runtimeOptions/uninstallDialog/cancel": "Cancel·la",
  "runtimeOptions/noCompatibleRuntimes": "Cap runtime compatible trobat",
  "runtimeOptions/downloadIncompatibleRuntime": "Aquest runtime no és compatible amb el teu maquinari. Probablement no funcionarà.",
  "runtimeOptions/noRuntimes": "Cap runtime trobat",

  "inferenceParams/noParams": "No configurable inference parameters available for this model type",

  "endpoints/openaiCompatRest/title": "Supported endpoints (OpenAI-like)",
  "endpoints/openaiCompatRest/getModels": "Llistat dels models carregats",
  "endpoints/openaiCompatRest/postCompletions": "Text Completions mode. Predict the next token(s) given a prompt. Note: OpenAI considers this endpoint 'deprecated'.",
  "endpoints/openaiCompatRest/postChatCompletions": "Chat completions. Send a chat history to the model to predict the next assistant response",
  "endpoints/openaiCompatRest/postEmbeddings": "Text Embedding. Generate text embeddings for a given text input. Takes a string or array of strings.",

  "model.createVirtualModelFromInstance": "Guarda la configuració com un nou model virtual",
  "model.createVirtualModelFromInstance/error": "Error en guardar la configuració com un nou model virtual",

  "apiConfigOptions/title": "Configurar l'API"
}
