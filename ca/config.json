{
  "noInstanceSelected": "Cap model seleccionat",
  "resetToDefault": "Reinicia",
  "showAdvancedSettings": "Mostra configuraci√≥ avan√ßada",
  "showAll": "Tot",
  "basicSettings": "B√†sic",
  "configSubtitle": "Load or save presets and experiment with model parameter overrides [TODO]",
  "inferenceParameters/title": "Par√†metres de predicci√≥",
  "inferenceParameters/info": "Experimenta amb par√†metres que impacten a la predicci√≥.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Mostreig",
  "basicTab": "B√†sic",
  "advancedTab": "Avan√ßat",
  "advancedTab/title": "üß™ Configuraci√≥ Avan√ßada",
  "advancedTab/expandAll": "Mostra tot",
    "advancedTab/overridesTitle": "Config Overrides [TODO]",
  "advancedTab/noConfigsText": "You have no unsaved changes - edit values above to see overrides here. [TODO]",
  "loadInstanceFirst": "Carrega un model per veure'n els par√†metres configurables",
  "noListedConfigs": "Cap par√†metre configurable",
  "generationParameters/info": "Experimenta amb par√†mentres b√†sics que afectin a la generaci√≥ de text.",
  "loadParameters/title": "Carrega Par√†metres",
  "loadParameters/description": "La configuraci√≥ que controla com el model s'inicialitza i carrega en la mem√≤ria.",
  "loadParameters/reload": "Reinicia per aplicar els canvis",
  "loadParameters/reload/error": "No s'ha pogut reiniciar el model",
  "discardChanges": "Descarta els canvis",
  "loadModelToSeeOptions": "Carrega un model per veure'n opcions",
  "schematicsError.title": "L'esquema de configuraci√≥ cont√© errors als seg√ºents camps:",
  "manifestSections": {
    "structuredOutput/title": "Resposta Estructurada",
    "speculativeDecoding/title": "Decodificaci√≥ Especulativa",
    "sampling/title": "Mostreig",
    "settings/title": "Configuraci√≥",
    "toolUse/title": "√ös d'Eines",
    "promptTemplate/title": "Plantilla de Prompts",
    "customFields/title": "Camps Personalitzats"
  },

  "llm.prediction.systemPrompt/title": "Prompt del Sistema",
  "llm.prediction.systemPrompt/description": "Utilitza aquest camp per donar al model instruccions de fons, com un conjunt de regles, l√≠mits o instruccions en general.",
  "llm.prediction.systemPrompt/subTitle": "Indicacions per a la IA",
  "llm.prediction.systemPrompt/openEditor": "Editor",
  "llm.prediction.systemPrompt/closeEditor": "Tanca l'Editor",
  "llm.prediction.systemPrompt/openedEditor": "Obert a l'Editor...",
  "llm.prediction.systemPrompt/edit": "Edita Prompt del Sistema...",
  "llm.prediction.systemPrompt/addInstructionsWithMore": "Afegeix instruccions...",
  "llm.prediction.systemPrompt/addInstructions": "Afegeix Instruccions",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quant aleatori ha de ser. Amb 0 sempre obtindr√†s el mateix resultat, mentre que valors m√©s alts incrementen la creativitat i la varietat",
  "llm.prediction.temperature/info": "De la documentaci√≥ de llama.cpp: \"El valor per defecte √©s <{{dynamicValue}}>. Aix√≠ s'obt√© un equilibri entre aleatorietat i determinisme. Als extrems, una temperatura de 0 escollir√† sempre el token m√©s probable, resultant en outputs id√®ntics cada vegada que s'executi\"",
  "llm.prediction.llama.sampling/title": "Mostreig",
  "llm.prediction.topKSampling/title": "Mostreig Top K",
  "llm.prediction.topKSampling/subTitle": "Limita el pr√≤xim token a un dels top-k m√©s probables. √âs semblant a la temperatura",
  "llm.prediction.topKSampling/info": "De la documentaci√≥ de llama.cpp:\n\nEl mostreig top-k √©s un m√®tode de generaci√≥ de text que nom√©s selecciona el pr√≤xim token d'entre els top-k m√©s probables segons el model.\n\nAix√≤ ajuda a reduir el risc de generar tokens poc probables o sense sentit, per√≤ √©s possible que limiti la diversitat del resultat.\n\nUn valor m√©s alt pel top-k (p.ex., 100) far√† que consideri m√©s tokens i resultar√† en textos m√©s diversos, mentre que un valor m√©s baix (p.ex., 10) se centrar√† en el token m√©s probable i generar√† textos m√©s reservats.\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Fils de la CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Nombre de fils de la CPU a utilitzar durant el raonament",
  "llm.prediction.llama.cpuThreads/info": "El nombre de fils a utilitzar durant la computaci√≥. Augmentar el nombre de fils no sempre resultar√† amb un millor rendiment. El valor per defecte √©s <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Llargada M√†xima de la Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Talla volunt√†riament la llargada de les respostes de la IA",
  "llm.prediction.maxPredictedTokens/info": "Controla la llargada m√†xima de les respostes del xatbot. Activa-ho per establir un l√≠mit a la llargada m√†xima que pot tenir una resposta, o desactiva-ho per deixar al xatbot controlar la llargada de les respostes.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Llargada M√†xima de les Respostes (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadament {{maxWords}} paraules",
  "llm.prediction.repeatPenalty/title": "Penalitzaci√≥ per Repetici√≥",
  "llm.prediction.repeatPenalty/subTitle": "Quant s'ha de penalitzar la repetici√≥ del mateix token",
  "llm.prediction.repeatPenalty/info": "De la documentaci√≥ de llama.cpp: \"Ajuda a evitar que el model generi textos repetitius o mon√≤tons.\n\nUn valor m√©s alt (p.ex., 1.5) penalitzar√† les repeticions severament, mentre que un valor m√©s baix (p.ex., 0.9) ser√† m√©s permisiu.\" ‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Mostreig Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilitat base m√≠nima que un token necessita per escollir-lo",
  "llm.prediction.minPSampling/info": "De la documentaci√≥ de llama.cpp:\n\nLa probabilitat m√≠nima necessaria per considerar un token, relativa a la probabiliat m√©s alta. Ha d'estar entre [0, 1].\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Mostreig Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilitat acumulada m√≠nima que els pr√≤xims tokens necessiten. √âs semblant a la temperatura",
  "llm.prediction.topPSampling/info": "De la documetaci√≥ de llama.cpp:\n\nEl mostreig top-p, tamb√© conegut com nucleus sampling, √©s un altre m√®tode de generaci√≥ de text que selecciona el pr√≤xim token d'un subconjunt de tokens que junts tenen una probabilitat acumuluada de, com a m√≠nim, p.\n\nAquest m√®tode dona un equilibri entre diversitat i qualitat considerant tant les probabilitats dels tokens com el nombre de tokens a mostrejar.\n\nUn valor m√©s alt pel top-p (p.ex., 0.95) resultar√† en textos m√©s diversos, mentre que un valor m√©s baix (p.ex., 0.5) generar√† un text m√©s centrat i reservat. Ha d'estar entre (0, 1].\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenes d'Aturada",
  "llm.prediction.stopStrings/subTitle": "Cadenes de text que haurien d'aturar al model",
  "llm.prediction.stopStrings/info": "Cadenes de text espec√≠fiques que haurien d'aturar al model quan se les trobi",
  "llm.prediction.stopStrings/placeholder": "Insereix una cadena i pulsa ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Sobrec√†rrega de Context",
  "llm.prediction.contextOverflowPolicy/subTitle": "Com ha de reaccionar el model quan la conversa sigui massa gran perqu√® la pugui gestionar",
  "llm.prediction.contextOverflowPolicy/info": "Decideix qu√® fer quan la conversa excedeix la quantitat de mem√≤ria utilitzable pel model ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalitzaci√≥ per Freq√º√®ncia",
  "llm.prediction.llama.presencePenalty/title": "Penalitzaci√≥ per Pres√®ncia",
  "llm.prediction.llama.tailFreeSampling/title": "Mostreig Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Mostreig T√≠pic Localment",
  "llm.prediction.llama.xtcProbability/title": "Probabilitat del Mostreig XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "El mostreig XTC (Exclou Opcions Top) nom√©s s'activar√† quan un token generat tingui aquesta probabilitat. El mostreig XTC pot millorar la creativitat i reduir els clix√©s",
  "llm.prediction.llama.xtcProbability/info": "El mostreig XTC (Exclou Opcions Top) nom√©s s'activar√† quan un token generat tingui aquesta probabilitat. El mostreig XTC pot millorar la creativitat i reduir clix√©s",
  "llm.prediction.llama.xtcThreshold/title": "Llindar de Mostreig XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Un llindar pel mostreig XTC (Exclou Opcions Top). Donada una probabilitat `xtc`, cerca tokens dins el llindar entre la probabilitat 'xtc' i 0.5, i el¬∑limina'ls tots excepte el menys probable",
  "llm.prediction.llama.xtcThreshold/info": "Un llindar pel mostreig XTC (Exclou Opcions Top). Donada una probabilitat `xtc`, cerca tokens dins el llindar entre la probabilitat 'xtc' i 0.5, i el¬∑limina'ls tots excepte el menys probable",
  "llm.prediction.mlx.topKSampling/title": "Mostreig Top K",
  "llm.prediction.mlx.topKSampling/subTitle": "Limita el pr√≤xim token a un dels top-k m√©s probables. √âs semblant a la temperatura",
  "llm.prediction.mlx.topKSampling/info": "Limita el pr√≤xim token a un dels top-k m√©s probables. √âs semblant a la temperatura",
  "llm.prediction.onnx.topKSampling/title": "Mostreig Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limita el pr√≤xim token a un dels top-k m√©s probables. √âs semblant a la temperatura",
  "llm.prediction.onnx.topKSampling/info": "De la documentaci√≥ d'ONNX:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\n‚Ä¢ El filtre est√† apagat per defecte",
  "llm.prediction.onnx.repeatPenalty/title": "Penalitzaci√≥ per Repetici√≥",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Quant s'ha d'evitar la repetici√≥ de tokens",
  "llm.prediction.onnx.repeatPenalty/info": "Un valor m√©s alt penalitzar√† al model m√©s severament si es repeteix",
  "llm.prediction.onnx.topPSampling/title": "Mostreig Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilitat acumulada m√≠nima pels pr√≤xims tokens. √âs semblant a la temperatura",
  "llm.prediction.onnx.topPSampling/info": "De la documentaci√≥ d'ONNX:\n\nOnly the most probable tokens with probabilities that add up to TopP or higher are kept for generation\n\n‚Ä¢ Aquest filtre est√† apagat per defecte",
  "llm.prediction.seed/title": "Llavor",
  "llm.prediction.structured/title": "Resposta Estructurada",
  "llm.prediction.structured/info": "Resposta Estructurada",
  "llm.prediction.structured/description": "Avan√ßat: pots donar al model un [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) per obtenir un format definit a les respostes. Llegeix-te la [documentaci√≥](https://lmstudio.ai/docs/advanced/structured-output) per aprendre'n m√©s",
  "llm.prediction.tools/title": "√ös d'Eines",
  "llm.prediction.tools/description": "Avan√ßat: pots donar al model una llista tipus JSON amb eines perqu√® les utilitzi. Llegeix-te la [documentaci√≥](https://lmstudio.ai/docs/advanced/tool-use) per aprendre'n m√©s",
  "llm.prediction.tools/serverPageDescriptionAddon": "Envia-ho a trav√©s del cos de la solicitut com 'eines' quan s'utilitzi l'API del servidor",
  "llm.prediction.promptTemplate/title": "Plantilla de Prompt",
  "llm.prediction.promptTemplate/subTitle": "El format en qu√® els missatges del xat s'envien al model. Canviar aix√≤ pot provocar comportaments inesperats - sigues conscient de qu√® fas!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Draft Tokens to Generate [TODO]",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "The number of tokens to generate with the draft model per main model token. Find the sweet spot of compute vs. reward [TODO]",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Drafting Probability Cutoff [TODO]",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue drafting until a token's probability falls below this threshold. Higher values generally mean lower risk, lower reward [TODO]",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min Draft Size [TODO]",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Drafts smaller than this will be ignored by the main model. Higher values generally mean lower risk, lower reward [TODO]",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max Draft Size [TODO]",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Max number of tokens allowed in a draft. Ceiling if all token probs are > the cutoff. Lower values generally mean lower risk, lower reward [TODO]",
  "llm.prediction.speculativeDecoding.draftModel/title": "Draft Model [TODO]",
  "llm.prediction.reasoning.parsing/title": "Reasoning Section Parsing [TODO]",
  "llm.prediction.reasoning.parsing/subTitle": "How to parse reasoning sections in the model's output [TODO]",

  "llm.load.mainGpu/title": "GPU Principal",
  "llm.load.mainGpu/subTitle": "La GPU a prioritzar durant la computaci√≥ del model",
  "llm.load.mainGpu/placeholder": "Selecciona la GPU principal...",
  "llm.load.splitStrategy/title": "Estrat√®gia de Divisi√≥",
  "llm.load.splitStrategy/subTitle": "Com repartir-se la computaci√≥ del model entre diferents GPUs",
  "llm.load.splitStrategy/placeholder": "Selecciona estrat√®gia de divisi√≥...",
  "llm.load.offloadKVCacheToGpu/title": "Descarrega la Cache KV a la mem√≤ria de la GPU",
  "llm.load.offloadKVCacheToGpu/subTitle": "Descarrega la cache KV a la mem√≤ria de la GPU. Millora el rendiment per√≤ requereix m√©s mem√≤ria de GPU",
  "load.gpuStrictVramCap/title": "Limita la C√†rrega del Model a la Mem√≤ria de la GPU",
  "load.gpuStrictVramCap.customSubTitleOff": "APAGAT: Permet que la c√†rrega del model es comparteixi amb la mem√≤ria RAM si la mem√≤ria de la GPU √©s plena",
  "load.gpuStrictVramCap.customSubTitleOn": "ENC√àS: El sistema limitar√† la quantitat de c√†rrega a la mem√≤ria de la GPU o a la RAM. El context pot ser desat a la mem√≤ria compartida igualment",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "La c√†rrega del model est√† limitada a la mem√≤ria de la GPU. El nombre real de capes carregades podria ser diferent",
  "load.allGpusDisabledWarning": "Totes les GPUs estan desactivades. Activa'n al menys una per carregar-la",

  "llm.load.contextLength/title": "Llargada del Context",
  "llm.load.contextLength/subTitle": "El nombre m√†xim de tokens que el model pot assumir en un prompt. Revisa les opcions de Sobrec√†rrega de Converses a \"Par√†metres de Predicci√≥\" per trobar m√©s formes de gestionar-ho",
  "llm.load.contextLength/info": "Especifica el nombre m√†xim de tokens que el model pot considerar a la vegada, impactant en la quantitat de context que pot retenir durant el processat",
  "llm.load.contextLength/warning": "Establir un valor alt per la llargada del context pot impactar significativament en l'√∫s de mem√≤ria",
  "llm.load.seed/title": "Llavor",
  "llm.load.seed/subTitle": "La llavor que utilitza el generador de nombres aleatoris a la generaci√≥ de text. -1 √©s aleatori",
  "llm.load.seed/info": "Llavor aleat√≤ria: Estableix la llavor pel generador de nombres aleatoris per assegurar resultats reprodu√Øbles",
  "llm.load.numCpuExpertLayersRatio/title": "Force Model Expert Weights onto CPU",
  "llm.load.numCpuExpertLayersRatio/subTitle": "Whether to force MoE expert weights into CPU RAM. Saves VRAM and can be faster than partial GPU offload. Not recommended if the model fits entirely in VRAM.",
  "llm.load.numCpuExpertLayersRatio/info": "Specifies whether or not to put all MoE expert layers in to CPU RAM. Leaves attention layers on GPU, saving VRAM while keeping inference fairly fast",

  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "llm.load.llama.evalBatchSize/info": "Sets the number of examples processed together in one batch during evaluation, affecting speed and memory usage",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "llm.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Decreases memory usage and generation time on some models",
  "llm.load.llama.flashAttention/info": "Accelerates attention mechanisms for faster and more efficient processing",
  "llm.load.numExperts/title": "Number of Experts",
  "llm.load.numExperts/subTitle": "Number of experts to use in the model",
  "llm.load.numExperts/info": "The number of experts to use in the model",
  "llm.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "llm.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "llm.load.llama.useFp16ForKVCache/title": "Use FP16 For KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Reduces memory usage by storing cache in half-precision (FP16)",
  "llm.load.llama.tryMmap/title": "Try mmap()",
  "llm.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "llm.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU Thread Pool Size",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Number of CPU threads to allocate to the thread pool used for model computation",
  "llm.load.llama.cpuThreadPoolSize/info": "The number of CPU threads to allocate to the thread pool used for model computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K Cache Quantization Type",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/title": "V Cache Quantization Type",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è You must disable this value if Flash Attention is not enabled",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Can only be turned on when Flash Attention is enabled",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è You must disable flash attention when using F32",
  "llm.load.mlx.kvCacheBits/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheBits/subTitle": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Context Length setting is ignored when using KV Cache Quantization",
  "llm.load.mlx.kvCacheGroupSize/title": "KV Cache Quantization: Group Size",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Group size during quantization operation for the KV cache. Higher group size reduces memory usage but may decrease quality",
  "llm.load.mlx.kvCacheGroupSize/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV Cache Quantization: Start quantizing when ctx crosses this length",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantization/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize the model's KV cache. This may result in faster generation and lower memory footprint,\nat the expense of the quality of the model output.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV cache quantization bits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Number of bits to quantize the KV cache to",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Group size strategy",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Accuracy",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanced",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Speedy",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Advanced: Quantized 'matmul group size' configuration\n\n‚Ä¢ Accuracy = group size 32\n‚Ä¢ Balanced = group size 64\n‚Ä¢ Speedy = group size 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Start quantizing when ctx reaches this length",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "When the context reaches this amount of tokens,\nbegin quantizing the KV cache",

  "embedding.load.contextLength/title": "Context Length",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Try mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "embedding.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random seed",

  "embedding.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",

  "presetTooltip": {
    "included/title": "Preset Values",
    "included/description": "The following fields will be applied",
    "included/empty": "No fields of this preset apply in this context.",
    "included/conflict": "You will be asked to choose whether to apply this value",
    "separateLoad/title": "Load-time Configuration",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "to apply to",
    "separateLoad/description.3": ".",
    "excluded/title": "May not apply",
    "excluded/description": "The following fields are included in the preset but does not apply in the current context.",
    "legacy/title": "Legacy Preset",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable.",
    "button/publish": "Publish to Hub",
    "button/pushUpdate": "Push Changes to Hub",
    "button/noChangesToPush": "No changes to push",
    "button/export": "Export",
    "hubLabel": "Preset from the Hub by {{user}}",
    "ownHubLabel": "Your preset from the Hub"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "llamaCacheQuantizationType": {
      "off": "OFF"
    },
    "mlxKvCacheBits": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Template",
      "jinja/error": "Failed to parse Jinja template: {{error}}",
      "jinja/empty": "Please enter a Jinja template above.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Before System",
      "manual.subfield.beforeSystem/placeholder": "Enter System prefix...",
      "manual.subfield.afterSystem/label": "After System",
      "manual.subfield.afterSystem/placeholder": "Enter System suffix...",
      "manual.subfield.beforeUser/label": "Before User",
      "manual.subfield.beforeUser/placeholder": "Enter User prefix...",
      "manual.subfield.afterUser/label": "After User",
      "manual.subfield.afterUser/placeholder": "Enter User suffix...",
      "manual.subfield.beforeAssistant/label": "Before Assistant",
      "manual.subfield.beforeAssistant/placeholder": "Enter Assistant prefix...",
      "manual.subfield.afterAssistant/label": "After Assistant",
      "manual.subfield.afterAssistant/placeholder": "Enter Assistant suffix...",
      "stopStrings/label": "Additional Stop Strings",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "This is the maximum number of tokens the model was trained to handle. Click to set the context to this value",
      "maxValueTextStart": "Model supports up to",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "While a model may support up to a certain number of tokens, performance may deteriorate if your machine's resources cannot handle the load - use caution when increasing this value"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Stop at Limit",
      "stopAtLimitSub": "Stop generating once the model's memory gets full",
      "truncateMiddle": "Truncate Middle",
      "truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
      "rollingWindow": "Rolling Window",
      "rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    },
    "gpuSplitStrategy": {
      "evenly": "Evenly",
      "favorMainGpu": "Favor Main GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Read how it works",
      "placeholder": "Select a compatible draft model",
      "noCompatible": "No compatible draft models found for your current model selection",
      "stillLoading": "Identifying compatible draft models...",
      "notCompatible": "The selected draft model (<draft/>) is not compatible with the current model selection (<current/>).",
      "off": "OFF",
      "loadModelToSeeOptions": "Load model <keyboard-shortcut /> to see compatible options",
      "compatibleWithNumberOfModels": "Recommended for at least {{dynamicValue}} of your models",
      "recommendedForSomeModels": "Recommended for some models",
      "recommendedForLlamaModels": "Recommended for Llama models",
      "recommendedForQwenModels": "Recommended for Qwen models",
      "onboardingModal": {
        "introducing": "Introducing",
        "speculativeDecoding": "Speculative Decoding",
        "firstStepBody": "Inference speedup for <custom-span>llama.cpp</custom-span> and <custom-span>MLX</custom-span> models",
        "secondStepTitle": "Inference Speedup with Speculative Decoding",
        "secondStepBody": "Speculative Decoding is a technique involving the collaboration of two models:\n - A larger \"main\" model\n - A smaller \"draft\" model\n\nDuring generation, the draft model rapidly proposes tokens for the larger main model to verify. Verifying tokens is a much faster process than actually generating them, which is the source of the speed gains. **Generally, the larger the size difference between the main model and the draft model, the greater the speed-up**.\n\nTo maintain quality, the main model only accepts tokens that align with what it would have generated itself, enabling the response quality of the larger model at faster inference speeds. Both models must share the same vocabulary.",
        "draftModelRecommendationsTitle": "Draft model recommendations",
        "basedOnCurrentModels": "Based on your current models",
        "close": "Close",
        "next": "Next",
        "done": "Done"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Please load a model first <model-badge /> ",
      "errorEngineNotSupported": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine (<key/>) and reload the model to use this feature.",
      "errorEngineNotSupported/noKey": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine and reload the model to use this feature."
    },
    "llmReasoningParsing": {
      "startString/label": "Start String",
      "startString/placeholder": "Enter the start string...",
      "endString/label": "End String",
      "endString/placeholder": "Enter the end string..."
    }
  },
  "saveConflictResolution": {
    "title": "Choose which values to include in the Preset",
    "description": "Pick and choose which values to keep",
    "instructions": "Click on a value to include it",
    "userValues": "Previous Value",
    "presetValues": "New Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "applyConflictResolution": {
    "title": "Which values to keep?",
    "description": "You have uncommitted changes which overlap with the incoming Preset",
    "instructions": "Click on a value to keep it",
    "userValues": "Current Value",
    "presetValues": "Incoming Preset Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "empty": "<Empty>",
  "noModelSelected": "No models selected",
  "apiIdentifier.label": "API Identifier",
  "apiIdentifier.hint": "Optionally provide an identifier for this model. This will be used in API requests. Leave blank to use the default identifier.",
  "idleTTL.label": "Auto Unload If Idle (TTL)",
  "idleTTL.hint": "If set, the model will be automatically unloaded after being idle for the specified amount of time.",
  "idleTTL.mins": "mins",

  "presets": {
    "title": "Preset",
    "saveChanges": "Save",
    "saveChanges/description": "Save your changes to the preset.",
    "saveChanges.manual": "New fields detected. You will be able to choose which changes to include in the preset.",
    "saveChanges.manual.hold.0": "Hold",
    "saveChanges.manual.hold.1": "to choose which changes to save to the preset.",
    "saveChanges.saveAll.hold.0": "Hold",
    "saveChanges.saveAll.hold.1": "to save all changes.",
    "saveChanges.saveInPreset.hold.0": "Hold",
    "saveChanges.saveInPreset.hold.1": "to only save changes to fields that are already included in the preset.",
    "saveChanges/error": "Failed to save changes to the preset.",
    "saveChanges.manual/description": "Choose which changes to include in the preset.",
    "saveAs": "Save As New...",
    "presetNamePlaceholder": "Enter a name for the preset...",
    "cannotCommitChangesLegacy": "This is a legacy preset and cannot be modified. You can create a copy by using \"Save As New...\".",
    "cannotSaveChangesNoChanges": "No changes to save.",
    "emptyNoUnsaved": "Select a Preset...",
    "emptyWithUnsaved": "Unsaved Preset",
    "saveEmptyWithUnsaved": "Save Preset As...",
    "saveConfirm": "Save",
    "saveCancel": "Cancel",
    "saving": "Saving...",
    "save/error": "Failed to save preset.",
    "deselect": "Deselect Preset",
    "deselect/error": "Failed to deselect preset.",
    "select/error": "Failed to select preset.",
    "delete/error": "Failed to delete preset.",
    "discardChanges": "Discard Unsaved",
    "discardChanges/info": "Discard all unsaved changes and restore the preset to its original state",
    "newEmptyPreset": "+ New Preset",
    "importPreset": "Import",
    "contextMenuCopyIdentifier": "Copy Preset Identifier",
    "contextMenuSelect": "Apply Preset",
    "contextMenuDelete": "Delete...",
    "contextMenuShare": "Publish...",
    "contextMenuOpenInHub": "View on Web",
    "contextMenuPullFromHub": "Pull Latest",
    "contextMenuPushChanges": "Push Changes to Hub",
    "contextMenuPushingChanges": "Pushing...",
    "contextMenuPushedChanges": "Changes pushed",
    "contextMenuExport": "Export File",
    "contextMenuRevealInExplorer": "Reveal in File Explorer",
    "contextMenuRevealInFinder": "Reveal in Finder",
    "share": {
      "title": "Publish Preset",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "Owner",
      "uploadAs": "Your preset will be created as {{name}}",
      "presetNameLabel": "Preset Name",
      "descriptionLabel": "Description (optional)",
      "loading": "Publishing...",
      "success": "Preset Successfully Pushed",
      "presetIsLive": "<preset-name /> is now live on the Hub!",
      "close": "Close",
      "confirmViewOnWeb": "View on web",
      "confirmCopy": "Copy URL",
      "confirmCopied": "Copied!",
      "pushedToHub": "Your preset was pushed to the Hub",
      "descriptionPlaceholder": "Enter a description...",
      "willBePublic": "This preset will be public. Anyone on the internet will be able to see it.",
      "willBePrivate": "Only you will be able to see this preset",
      "willBeOrgVisible": "This preset will be visible to everyone in the organization.",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "privateUsageReached": "Private preset number limit reached.",
      "continueInBrowser": "Continue in Browser",
      "confirmShareButton": "Publish",
      "error": "Failed to publish preset",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "Push Changes to Hub",
      "title/success": "Preset Successfully Updated",
      "subtitle": "Make changes to <custom-preset-name /> and push them to the Hub",
      "descriptionLabel": "Description",
      "descriptionPlaceholder": "Enter a description...",
      "loading": "Pushing...",
      "cancel": "Cancel",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "Failed to push update",
      "confirmUpdateButton": "Push"
    },
    "resolve": {
      "title": "Resolve conflicts...",
      "tooltip": "Open a modal to resolve differences with the Hub version"
    },
    "loginToManage": {
      "title": "Login to manage..."
    },
    "downloadFromHub": {
      "title": "Download",
      "downloading": "Downloading...",
      "success": "Downloaded!",
      "error": "Failed to download"
    },
    "push": {
      "title": "Push changes",
      "pushing": "Pushing...",
      "success": "Pushed",
      "tooltip": "Push your local changes to the remote version hosted on the Hub",
      "error": "Failed to push"
    },
    "saveAsNewModal": {
      "title": "Oops! Did not find the preset on Hub",
      "confirmSaveAsNewDescription": "Do you want to publish this preset as a new one?",
      "confirmButton": "Publish as New"
    },
    "pull": {
      "title": "Pull Latest",
      "error": "Failed to pull",
      "contextMenuErrorMessage": "Failed to pull",
      "success": "Pulled",
      "pulling": "Pulling...",
      "upToDate": "Up to date!",
      "unsavedChangesModal": {
        "title": "You have unsaved changes.",
        "bodyContent": "Pulling from remote will overwrite your unsaved changes. Continue?",
        "confirmButton": "Overwrite Unsaved Changes"
      }
    },
    "import": {
      "title": "Import a Preset from File",
      "dragPrompt": "Drag and drop preset files (.tar.gz or preset.json) or <custom-link>select from your computer</custom-link>",
      "remove": "Remove",
      "cancel": "Cancel",
      "importPreset_zero": "Import Preset",
      "importPreset_one": "Import Preset",
      "importPreset_other": "Import {{count}} Presets",
      "selectDialog": {
        "title": "Select Preset File (preset.json or .tar.gz)",
        "button": "Import"
      },
      "error": "Failed to import preset",
      "resultsModal": {
        "titleSuccessSection_one": "Imported 1 preset successfully",
        "titleSuccessSection_other": "Imported {{count}} presets successfully",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} failed)",
        "titleFailSection_other": "({{count}} failed)",
        "titleAllFailed": "Failed to import presets",
        "importMore": "Import More",
        "close": "Done",
        "successBadge": "Success",
        "alreadyExistsBadge": "Preset already exists",
        "errorBadge": "Error",
        "invalidFileBadge": "Invalid file",
        "otherErrorBadge": "Failed to import preset",
        "errorViewDetailsButton": "View Details",
        "seeError": "See Error",
        "noName": "No preset name",
        "useInChat": "Use in Chat"
      },
      "importFromUrl": {
        "button": "Import from URL...",
        "title": "Import from URL",
        "back": "Import from File...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "Import",
        "cancel": "Cancel",
        "loading": "Importing...",
        "error": "Failed to download preset."
      }
    },
    "download": {
      "title": "Pull <preset-name /> from LM Studio Hub",
      "subtitle": "Save <custom-name /> to your presets. Doing so you will allow you to use this preset in the app",
      "button": "Pull",
      "button/loading": "Pulling...",
      "cancel": "Cancel",
      "error": "Failed to download preset."
    },
    "inclusiveness": {
      "speculativeDecoding": "Include in Preset"
    }
  },

  "flashAttentionWarning": "L'Atenci√≥ Flash √©s una caracter√≠stica experimental que pot causar problemes en certs models. Si trobes problemes, prova desactivant-la.",
  "llamaKvCacheQuantizationWarning": "La KV Cache Quantization √©s una caracter√≠stica experimental que pot causar problemes en certs models. L'Atenci√≥ Flash ha d'estar activada perqu√® funcioni. Si trobes problemes, estableix el valor per defecte \"F16\".",

  "seedUncheckedHint": "Llavor Aleat√≤ria",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",

  "hardware": {
    "environmentVariables": "Environment Variables",
    "environmentVariables.info": "Si no est√°s segur, deixa els valors per defecte",
    "environmentVariables.reset": "Torna als valors per defecte",

    "gpus.information": "Configura les unitats de processament gr√†fic (GPUs) trobades al teu dispositiu",
    "gpuSettings": {
      "editMaxCapacity": "Edita la Capacitat M√†xima",
      "hideEditMaxCapacity": "Amaga Edita la Capacitat M√†x",
      "allOffWarning": "Totes les GPUs estan apagades o desactivades, assegura't que hi ha alguna GPU per poder distribuir la c√†rrega dels models",
      "split": {
        "title": "Estrat√®gia",
        "placeholder": "Selecciona una distribuci√≥ de mem√≤ria de les GPUs",
        "options": {
          "generalDescription": "Configura com els models es carregaran a les GPUs",
          "evenly": {
            "title": "Distribuci√≥ Equitativa",
            "description": "Distribueix la mem√≤ria equitativament a les diferents GPUs"
          },
          "priorityOrder": {
            "title": "Ordre de prioritat",
            "description": "Arrastra per ordenar la prioritat. El sistema provar√† distribuir-ne m√©s a les primeres GPUs de la llista"
          },
          "custom": {
            "title": "Personalitzat",
            "description": "Destina la mem√≤ria",
            "maxAllocation": "M√†xima Distribuci√≥"
          }
        }
      },
      "deviceId.info": "Identificador √önic per a aquest dispositiu",
      "changesOnlyAffectNewlyLoadedModels": "Els canvis nom√©s afectaran als models carregats a partir d'ara",
      "toggleGpu": "Activa/Desactiva la GPU"
    }
  },

  "load.gpuSplitConfig/title": "Configuraci√≥ de GPU Dividida",
  "envVars/title": "Estableix una Environment Variable",
  "envVars": {
    "select": {
      "placeholder": "Selecciona una environment variable...",
      "noOptions": "Cap altra disponible",
      "filter": {
        "placeholder": "Filtra els resultats",
        "resultsFound_zero": "Cap resultat",
        "resultsFound_one": "1 resultat",
        "resultsFound_other": "{{count}} resultats trobats"
      }
    },
    "inputValue": {
      "placeholder": "Introdueix un valor"
    },
    "values": {
      "title": "Valors Actuals"
    }
  }
}
