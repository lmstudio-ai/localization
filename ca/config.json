{
  "noInstanceSelected": "Cap inst√†ncia del model seleccionada",
  "resetToDefault": "Reinicia",
  "showAdvancedSettings": "Mostra la configuraci√≥ avan√ßada",
  "showAll": "Tot",
  "basicSettings": "B√†sic",
  "configSubtitle": "Carrega o desa preestablerts i experimenta amb els par√†mentres de sobreescriptura del model",
  "inferenceParameters/title": "Par√†metres de Predicci√≥",
  "inferenceParameters/info": "Experimenta amb els par√†metres que afecten a la predicci√≥.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Sampling",
  "basicTab": "B√†sic",
  "advancedTab": "Avan√ßat",
  "advancedTab/title": "üß™ Configuraci√≥ Avan√ßada",
  "advancedTab/expandAll": "Mostra-ho tot",
  "advancedTab/overridesTitle": "Configura Sobreescriptures",
  "advancedTab/noConfigsText": "No tens cap canvi sense desar - edita valors a dalt per veure sobreescriptures aqu√≠.",
  "loadInstanceFirst": "Carrega un model per veure'n els par√†mentres configurables",
  "noListedConfigs": "Cap par√†metre configurableNo configurable parameters",
  "generationParameters/info": "Experimenta amb par√†metres b√†sics que afecten a la generaci√≥ de text.",
  "loadParameters/title": "Carrega Par√†metres",
  "loadParameters/description": "Opcions per controlar la forma en qu√® el model s'inicialitza i es carrega en la mem√≤ria.",
  "loadParameters/reload": "Recarrega per aplicar els canvis",
  "loadParameters/reload/error": "Error en recarregar el model",
  "discardChanges": "Descarta els canvis",
  "loadModelToSeeOptions": "Carrega un model per veure opcions",
  "schematicsError.title": "L'esquema de configuraci√≥ cont√© errors als seg√ºents camps:",
  "manifestSections": {
    "structuredOutput/title": "Resultat Esctructurat",
    "speculativeDecoding/title": "Decodificaci√≥ Especulativa",
    "sampling/title": "Sampling",
    "settings/title": "Configuraci√≥",
    "toolUse/title": "√ös d'Eina",
    "promptTemplate/title": "Plantilla de Prompt"
  },

  "llm.prediction.systemPrompt/title": "Prompt del Sistema",
  "llm.prediction.systemPrompt/description": "Utilitza aquest camp per donar instruccions amb context al model, com un conjunt de normes, impediments o peticions generals.",
  "llm.prediction.systemPrompt/subTitle": "Directrius per a la IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Qu√® tan aleat√≤ria ha de ser. 0 farp√† que sempre retorni la mateixa respost, mentre que valors m√©s alts faran que augmenti la seva creativitat",
  "llm.prediction.temperature/info": "De la documentaci√≥ de llama.cpp: \"El valor per defecte √©s <{{dynamicValue}}>, el qual dona un equilibri entre l'aleatorietat i la determinaci√≥. En el seu extrem, una temperatura de 0 sempre far√† que esculli el pr√≤xim token m√©s probable, provocant respostes id√®ntiques en cada execuci√≥\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top K Sampling",
  "llm.prediction.topKSampling/subTitle": "Limita el seg√ºent token a un del top-k amb m√©s probabilitat. Act√∫a similar a la temperatura",
  "llm.prediction.topKSampling/info": "De la documentaci√≥ de llama.cpp:\n\nEl sampling Top-k √©s un m√®tode de generaci√≥ de text que nom√©s selecciona el pr√≤xim token d'entre el top k tokens m√©s probables per predicci√≥ del model.\n\nAix√≤ ajuda a reduir el risc de generar tokens poc probables o sense sentit, per√≤ pot provocar respostes menys creatives.\n\nUn valor m√©s alt pel top-k (p.e., 100) considerar√† m√©s tokens i resultar√† en respostes m√©s variades, mentre que un valor m√©s baix (p.e., 10) far√† que se centri en els tokens m√©s probables i generar√† un text m√©s reservat.\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Fils de la CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Nombre de fils de la CPU a utilitzar durant la infer√®ncia",
  "llm.prediction.llama.cpuThreads/info": "El nombre de fils a utilitzar durant la computaci√≥. Augmentar el nombre de fils no sempre resulta amb un millor rendiment. El nombre per defecte √©s <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "L√≠mit de Llargada de Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Pots determinar la llargada m√†xima per a les respostes de la IA",
  "llm.prediction.maxPredictedTokens/info": "Controla la llargada m√†xima de les respostes del xatbot. Activa-ho per determinar un l√≠mit a la llargada m√†xima que pot tenir una resposta, o desactiva-ho per deixar decidir al xatbot quan s'ha d'aturar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Llargada M√†xima de Respostes (en tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadament {{maxWords}} paraules",
  "llm.prediction.repeatPenalty/title": "Penalitzaci√≥ per Repetici√≥",
  "llm.prediction.repeatPenalty/subTitle": "Qu√® tant s'ha d'evitar repetir el mateix token",
  "llm.prediction.repeatPenalty/info": "De la documentaci√≥ de llama.cpp: \"Ajuda a evitar que el model generi respostes repetitives o mon√≤tones.\n\nUn valor m√©s alt (p.e., 1.5) penalitzar√† les repeticions severament, mentre que un valor m√©s baix (p.e., 0.9) ser√† m√©s permisiu.\" ‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "P Sampling M√≠nim",
  "llm.prediction.minPSampling/subTitle": "Probabilitat base m√≠nima perqu√® un token sigui seleccionat per a la resposta",
  "llm.prediction.minPSampling/info": "De la documentaci√≥ de llama.cpp:\n\nLa probabilitat m√≠nima per considerar un token, respecte la probabilitat del token m√©s probable. Ha d'estar entre [0, 1].\n\n‚Ä¢ El valor predeterminat √©s <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "P Sampling M√†xim",
  "llm.prediction.topPSampling/subTitle": "Probabilitat acumulada m√≠nima per seleccionar el seg√ºent token. Funciona semblant a la temperatura",
  "llm.prediction.topPSampling/info": "De la documentaci√≥ de llama.cpp:\n\nEl p sampling m√†xim, tamb√© conegut com sampling de nucli, √©s un altre m√®tode de generaci√≥ que selecciona el seg√ºent token d'un grup de tokens que, en conjunt, tenen una probabilitat acumulada de, com a m√≠nim, p.\n\nAquest m√®tode dona un equilibri entre diversitat i qualitat considerant totes les probabilitats dels tokens i el nombre de tokens dels quals generar.\n\nUn valor m√©s alt pel p m√†xim (p.e., 0.95) resultar√† en un text m√©s divers, mentre que un valor m√©s baix (p.e., 0.5) generar√† una resposta m√©s centrada i conservadora. Ha d'estar entre (0, 1].\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenes per Aturar-se Strings",
  "llm.prediction.stopStrings/subTitle": "Cadenes que haurien d'evitar que el model continu√©s generant tokens",
  "llm.prediction.stopStrings/info": "Quan es troben aquestes cadenes espec√≠fiques el model deixar√† de generar tokens",
  "llm.prediction.stopStrings/placeholder": "Introdueix una cadena i prem ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Sobrec√†rrega del Context",
  "llm.prediction.contextOverflowPolicy/subTitle": "Com hauria de gestionar el model que una conversa s'ha fet massa llarga",
  "llm.prediction.contextOverflowPolicy/info": "Decideix qu√® fer quan una conversa excedeix la capacitat del model de gestionar-la ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalitzaci√≥ per Freq√º√®ncia",
  "llm.prediction.llama.presencePenalty/title": "Presence Penalty",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.llama.xtcProbability/title": "Probabilitat de Sampling XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "El sampler XTC (Exclude Top Choices) nom√©s s'activar√† amb aquesta probabilitat de token. El sampling XTC pot millorar la creativitat del model i reduir els clix√©s",
  "llm.prediction.llama.xtcProbability/info": "El sampling XTC (Exclude Top Choices) nom√©s s'activar√† amb aquesta probabilitat per token generat. El sampling XTC usualment millora la creativitat i redueix els clix√©s",
  "llm.prediction.llama.xtcThreshold/title": "XTC Sampling Threshold",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.llama.xtcThreshold/info": "XTC (Exclude Top Choices) threshold. With a chance of `xtc-probability`, search for tokens with probabilities between `xtc-threshold` and 0.5, and removes all such tokens except the least probable one",
  "llm.prediction.mlx.topKSampling/title": "Top K Sampling",
  "llm.prediction.mlx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.mlx.topKSampling/info": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/info": "From ONNX documentation:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.onnx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.onnx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.onnx.repeatPenalty/info": "A higher value discourages the model from repeating itself",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topPSampling/info": "From ONNX documentation:\n\nOnly the most probable tokens with probabilities that add up to TopP or higher are kept for generation\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Structured Output",
  "llm.prediction.structured/info": "Structured Output",
  "llm.prediction.structured/description": "Advanced: you can provide a [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) to enforce a particular output format from the model. Read the [documentation](https://lmstudio.ai/docs/advanced/structured-output) to learn more",
  "llm.prediction.tools/title": "Tool Use",
  "llm.prediction.tools/description": "Advanced: you can provide a JSON-compliant list of tools for the model to request calls to. Read the [documentation](https://lmstudio.ai/docs/advanced/tool-use) to learn more",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pass this through the request body as `tools` when using the server API",
  "llm.prediction.promptTemplate/title": "Prompt Template",
  "llm.prediction.promptTemplate/subTitle": "The format in which messages in chat are sent to the model. Changing this may introduce unexpected behavior - make sure you know what you're doing!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Draft Tokens to Generate",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "The number of tokens to generate with the draft model per main model token. Find the sweet spot of compute vs. reward",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Drafting Probability Cutoff",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue drafting until a token's probability falls below this threshold. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min Draft Size",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Drafts smaller than this will be ignored by the main model. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max Draft Size",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Max number of tokens allowed in a draft. Ceiling if all token probs are > the cutoff. Lower values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.draftModel/title": "Draft Model",
  "llm.prediction.reasoning.parsing/title": "Reasoning Section Parsing",
  "llm.prediction.reasoning.parsing/subTitle": "How to parse reasoning sections in the model's output",

  "llm.load.contextLength/title": "Llargada del Context",
  "llm.load.contextLength/subTitle": "El nombre m√†xim de tokens que el model pot gestionar per prompt. Mira les opcions de la Sobrec√†rrega de Converses sota \"Par√†metres d'Infer√®ncia\" per veure m√©s formes de gestionar-ho",
  "llm.load.contextLength/info": "Especif√≠ca el nombre m√†xim de tokens que el model pot considerar d'una sola vegada, impactant en quant context pot retenir durant el processament",
  "llm.load.contextLength/warning": "Establir un valor alt per la llargada del context pot impactar severament en l'√∫s de mem√≤ria",
  "llm.load.seed/title": "Llavor",
  "llm.load.seed/subTitle": "La llavor que el generador de nombres aleatoris (RNG) ha utilitzat en aquesta generaci√≥. -1 √©s aleatori",
  "llm.load.seed/info": "Llavor Aleat√≤ria: Estableix una llavor aleat√≤ria per al generador de nombres aleatoris (RNG) per assegurar resultats reprodu√Øbles",

  "llm.load.llama.evalBatchSize/title": "Evaluaci√≥ del Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Nombre de tokens a l'input a processar d'una vegada. Augmentar aix√≤ augmenta el rendiment a costa d'un major consum de mem√≤ria",
  "llm.load.llama.evalBatchSize/info": "Estableix el nombre d'exemples processats junts en un sol batch durant l'evaluaci√≥, afectant a la velocitat i el consum de mem√≤ria",
  "llm.load.llama.ropeFrequencyBase/title": "Freq√º√®ncia Base RoPE",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Freq√º√®ncia base personalitzada pel rotary positional embeddings (RoPE). Augmentar-ho pot resultar en un millor rendiment quan es tenen contexts molt llargs",
  "llm.load.llama.ropeFrequencyBase/info": "[Avan√ßat] Ajusta la freq√º√®ncia base del Rotary Positional Encoding, afectant a com la informaci√≥ posicional s'agrupa",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Freq√º√®ncia RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "La llargada del context s'escala per aquest factor per extendre el context efectiu utilitzant RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avan√ßat] Modifica la freq√º√®ncia d'escalat pel Rotary Positional Encoding per controlar la graduaci√≥ de la codificaci√≥ posicional",
  "llm.load.llama.acceleration.offloadRatio/title": "Desc√†rrega de la GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Nombre de capes discretes del model que s'han de computar a la GPU per usar acceleraci√≥ de GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "Estableix el nombre de capes del model per descarregar a la GPU.",
  "llm.load.llama.flashAttention/title": "Atenci√≥ Flash",
  "llm.load.llama.flashAttention/subTitle": "Disminueix l'√∫s de mem√≤ria i el temps de generaci√≥ en certs models",
  "llm.load.llama.flashAttention/info": "Accelera mecansimes d'atenci√≥ per obtenir un processament m√©s r√†pid i eficient",
  "llm.load.numExperts/title": "Nombre d'Experts",
  "llm.load.numExperts/subTitle": "Nombre d'experts a utilitzar en el model",
  "llm.load.numExperts/info": "El nombre d'experts a utilitzar en aquest model",
  "llm.load.llama.keepModelInMemory/title": "Mantenir Model a la Mem√≤ria",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserva mem√≤ria del sistema per aquest model, incl√∫s si est√† descarregada a la GPU. Millora el rendiment per√≤ requereix m√©s mem√≤ria RAM del sistema",
  "llm.load.llama.keepModelInMemory/info": "Impideix que el model s'expulsi al disc, assegurant-ne un acc√®s m√©s r√†pid a canvi d'un major consum de mem√≤ria RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Utilitza FP16 Per la Mem√≤ria Cau KV",
  "llm.load.llama.useFp16ForKVCache/info": "Redueix l'√∫s de mem√≤ria desant la mem√≤ria cau amb semi-precisi√≥ (FP16)",
  "llm.load.llama.tryMmap/title": "Prova mmap()",
  "llm.load.llama.tryMmap/subTitle": "Millora els temps de ca√†rrega dels models. Desactivar-ho pot millorar el rendiment quan el model √©s m√©s gran qua la mem√≤ria RAM disponible al sistema",
  "llm.load.llama.tryMmap/info": "Carrega els fitxers del model directament del disc a la mem√≤ria",
  "llm.load.llama.cpuThreadPoolSize/title": "Gr√†ndaria dels Fils de CPU Disponibles",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Nombre de fils de CPU dels quals dispondre per a la computaci√≥ del model",
  "llm.load.llama.cpuThreadPoolSize/info": "Nombre de fils de CPU dels quals dispondre per a la computaci√≥ del model. Augmentar el nombre de fils no sempre resulta en un millor rendiment. El valor per defecte √©s <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Tipus de Quantificaci√≥ de Mem√≤ria Cau K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Valors m√©s baixos poden reduir l'√∫s de mem√≤ria reduint la qualitat. L'efecte pot variar dr√†sticament entre models.",
  "llm.load.llama.vCacheQuantizationType/title": "Tipus de Quantificaci√≥ de Mem√≤ria Cau V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Valors m√©s baixos poden reduir l'√∫s de mem√≤ria reduint la qualitat. L'efecte pot variar dr√†sticament entre models.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Has de desactivar aix√≤ si l'Atenci√≥ Flash no est√† activada",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Nom√©s es pot activar si l'Atenci√≥ Flash est√† tamb√© activada",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Has de desactivar l'Atenci√≥ Flash si vols utilitzar F32",
  "llm.load.mlx.kvCacheBits/title": "Quantificaci√≥ de la Mem√≤ria Cau KV",
  "llm.load.mlx.kvCacheBits/subTitle": "Nombre de bits als que la mem√≤ria cau KV s'hauria de quantificar",
  "llm.load.mlx.kvCacheBits/info": "Nombre de bits als quals la mem√≤ria cau KV s'hauria de quantificar",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "La configuraci√≥ de Llargada de Context s'ignora quan s'utilitza la Quantificaci√≥ de Mem√≤ria Cau KV",
  "llm.load.mlx.kvCacheGroupSize/title": "Quantificaci√≥ de la Mem√≤ria Cau KV: Grand√†ria de Grup",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Gr√†ndaria d'agrupament durant les operacions amb mem√≤ria cau KV quantificada. Valors m√©s alts redueixen l'√∫s de mem√≤ria reduint la qualitat",
  "llm.load.mlx.kvCacheGroupSize/info": "Nombre de buts als quals la mem√≤ria cau KV s'hauria de quantificar",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV Cache Quantization: Start quantizing when ctx crosses this length",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantization/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize the model's KV cache. This may result in faster generation and lower memory footprint,\nat the expense of the quality of the model output.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV cache quantization bits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Number of bits to quantize the KV cache to",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Group size strategy",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Accuracy",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanced",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Speedy",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Advanced: Quantized 'matmul group size' configuration\n\n‚Ä¢ Accuracy = group size 32\n‚Ä¢ Balanced = group size 64\n‚Ä¢ Speedy = group size 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Start quantizing when ctx reaches this length",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "When the context reaches this amount of tokens,\nbegin quantizing the KV cache",

  "embedding.load.contextLength/title": "Context Length",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Try mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "embedding.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random seed",

  "embedding.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",

  "presetTooltip": {
    "included/title": "Preset Values",
    "included/description": "The following fields will be applied",
    "included/empty": "No fields of this preset apply in this context.",
    "included/conflict": "You will be asked to choose whether to apply this value",
    "separateLoad/title": "Load-time Configuration",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "to apply to",
    "separateLoad/description.3": ".",
    "excluded/title": "May not apply",
    "excluded/description": "The following fields are included in the preset but does not apply in the current context.",
    "legacy/title": "Legacy Preset",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable.",
    "button/publish": "Publish to Hub",
    "button/pushUpdate": "Push Changes to Hub",
    "button/export": "Export"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "llamaCacheQuantizationType": {
      "off": "OFF"
    },
    "mlxKvCacheBits": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Template",
      "jinja/error": "Failed to parse Jinja template: {{error}}",
      "jinja/empty": "Please enter a Jinja template above.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Before System",
      "manual.subfield.beforeSystem/placeholder": "Enter System prefix...",
      "manual.subfield.afterSystem/label": "After System",
      "manual.subfield.afterSystem/placeholder": "Enter System suffix...",
      "manual.subfield.beforeUser/label": "Before User",
      "manual.subfield.beforeUser/placeholder": "Enter User prefix...",
      "manual.subfield.afterUser/label": "After User",
      "manual.subfield.afterUser/placeholder": "Enter User suffix...",
      "manual.subfield.beforeAssistant/label": "Before Assistant",
      "manual.subfield.beforeAssistant/placeholder": "Enter Assistant prefix...",
      "manual.subfield.afterAssistant/label": "After Assistant",
      "manual.subfield.afterAssistant/placeholder": "Enter Assistant suffix...",
      "stopStrings/label": "Additional Stop Strings",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "This is the maximum number of tokens the model was trained to handle. Click to set the context to this value",
      "maxValueTextStart": "Model supports up to",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "While a model may support up to a certain number of tokens, performance may deteriorate if your machine's resources cannot handle the load - use caution when increasing this value"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Stop at Limit",
      "stopAtLimitSub": "Stop generating once the model's memory gets full",
      "truncateMiddle": "Truncate Middle",
      "truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
      "rollingWindow": "Rolling Window",
      "rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Evenly",
      "favorMainGpu": "Favor Main GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Read how it works",
      "placeholder": "Select a compatible draft model",
      "noCompatible": "No compatible draft models found for your current model selection",
      "stillLoading": "Identifying compatible draft models...",
      "notCompatible": "The selected draft model (<draft/>) is not compatible with the current model selection (<current/>).",
      "off": "OFF",
      "loadModelToSeeOptions": "Load model <keyboard-shortcut /> to see compatible options",
      "compatibleWithNumberOfModels": "Recommended for at least {{dynamicValue}} of your models",
      "recommendedForSomeModels": "Recommended for some models",
      "recommendedForLlamaModels": "Recommended for Llama models",
      "recommendedForQwenModels": "Recommended for Qwen models",
      "onboardingModal": {
        "introducing": "Introducing",
        "speculativeDecoding": "Speculative Decoding",
        "firstStepBody": "Inference speedup for <custom-span>llama.cpp</custom-span> and <custom-span>MLX</custom-span> models",
        "secondStepTitle": "Inference Speedup with Speculative Decoding",
        "secondStepBody": "Speculative Decoding is a technique involving the collaboration of two models:\n - A larger \"main\" model\n - A smaller \"draft\" model\n\nDuring generation, the draft model rapidly proposes tokens for the larger main model to verify. Verifying tokens is a much faster process than actually generating them, which is the source of the speed gains. **Generally, the larger the size difference between the main model and the draft model, the greater the speed-up**.\n\nTo maintain quality, the main model only accepts tokens that align with what it would have generated itself, enabling the response quality of the larger model at faster inference speeds. Both models must share the same vocabulary.",
        "draftModelRecommendationsTitle": "Draft model recommendations",
        "basedOnCurrentModels": "Based on your current models",
        "close": "Close",
        "next": "Next",
        "done": "Done"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Please load a model first <model-badge /> ",
      "errorEngineNotSupported": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine (<key/>) and reload the model to use this feature.",
      "errorEngineNotSupported/noKey": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine and reload the model to use this feature."
    },
    "llmReasoningParsing": {
      "startString/label": "Start String",
      "startString/placeholder": "Enter the start string...",
      "endString/label": "End String",
      "endString/placeholder": "Enter the end string..."
    }
  },
  "saveConflictResolution": {
    "title": "Choose which values to include in the Preset",
    "description": "Pick and choose which values to keep",
    "instructions": "Click on a value to include it",
    "userValues": "Previous Value",
    "presetValues": "New Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "applyConflictResolution": {
    "title": "Which values to keep?",
    "description": "You have uncommitted changes which overlap with the incoming Preset",
    "instructions": "Click on a value to keep it",
    "userValues": "Current Value",
    "presetValues": "Incoming Preset Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "empty": "<Empty>",
  "noModelSelected": "No models selected",
  "apiIdentifier.label": "API Identifier",
  "apiIdentifier.hint": "Optionally provide an identifier for this model. This will be used in API requests. Leave blank to use the default identifier.",
  "idleTTL.label": "Auto Unload If Idle (TTL)",
  "idleTTL.hint": "If set, the model will be automatically unloaded after being idle for the specified amount of time.",
  "idleTTL.mins": "mins",

  "presets": {
    "title": "Preset",
    "commitChanges": "Commit Changes",
    "commitChanges/description": "Commit your changes to the preset.",
    "commitChanges.manual": "New fields detected. You will be able to choose which changes to include in the preset.",
    "commitChanges.manual.hold.0": "Hold",
    "commitChanges.manual.hold.1": "to choose which changes to commit to the preset.",
    "commitChanges.saveAll.hold.0": "Hold",
    "commitChanges.saveAll.hold.1": "to save all changes.",
    "commitChanges.saveInPreset.hold.0": "Hold",
    "commitChanges.saveInPreset.hold.1": "to only save changes to fields that are already included in the preset.",
    "commitChanges/error": "Failed to commit changes to the preset.",
    "commitChanges.manual/description": "Choose which changes to include in the preset.",
    "saveAs": "Save As New...",
    "presetNamePlaceholder": "Enter a name for the preset...",
    "cannotCommitChangesLegacy": "This is a legacy preset and cannot be modified. You can create a copy by using \"Save As New...\".",
    "cannotCommitChangesNoChanges": "No changes to commit.",
    "emptyNoUnsaved": "Select a Preset...",
    "emptyWithUnsaved": "Unsaved Preset",
    "saveEmptyWithUnsaved": "Save Preset As...",
    "saveConfirm": "Save",
    "saveCancel": "Cancel",
    "saving": "Saving...",
    "save/error": "Failed to save preset.",
    "deselect": "Deselect Preset",
    "deselect/error": "Failed to deselect preset.",
    "select/error": "Failed to select preset.",
    "delete/error": "Failed to delete preset.",
    "discardChanges": "Discard Unsaved",
    "discardChanges/info": "Discard all uncommitted changes and restore the preset to its original state",
    "newEmptyPreset": "+ New Preset",
    "importPreset": "Import",
    "contextMenuSelect": "Apply Preset",
    "contextMenuDelete": "Delete...",
    "contextMenuShare": "Publish...",
    "contextMenuOpenInHub": "View on Hub",
    "contextMenuPushChanges": "Push changes to Hub",
    "contextMenuPushingChanges": "Pushing...",
    "contextMenuPushedChanges": "Changes pushed",
    "contextMenuExport": "Export File",
    "contextMenuRevealInExplorer": "Reveal in File Explorer",
    "contextMenuRevealInFinder": "Reveal in Finder",
    "share": {
      "title": "Publish Preset",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "Owner",
      "uploadAs": "Your preset will be created as {{name}}",
      "presetNameLabel": "Preset Name",
      "descriptionLabel": "Description (optional)",
      "loading": "Publishing...",
      "success": "Preset Successfully Pushed",
      "presetIsLive": "<preset-name /> is now live on the Hub!",
      "close": "Close",
      "confirmViewOnWeb": "View on web",
      "confirmCopy": "Copy URL",
      "confirmCopied": "Copied!",
      "pushedToHub": "Your preset was pushed to the Hub",
      "descriptionPlaceholder": "Enter a description...",
      "willBePublic": "Publishing your preset will make it public",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "confirmShareButton": "Publish",
      "error": "Failed to publish preset",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "Push Changes to Hub",
      "title/success": "Preset Successfully Updated",
      "subtitle": "Make changes to <custom-preset-name /> and push them to the Hub",
      "descriptionLabel": "Description",
      "descriptionPlaceholder": "Enter a description...",
      "loading": "Pushing...",
      "cancel": "Cancel",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "Failed to push update",
      "confirmUpdateButton": "Push"
    },
    "import": {
      "title": "Import a Preset from File",
      "dragPrompt": "Drag and drop preset JSON files or <custom-link>select from your computer</custom-link>",
      "remove": "Remove",
      "cancel": "Cancel",
      "importPreset_zero": "Import Preset",
      "importPreset_one": "Import Preset",
      "importPreset_other": "Import {{count}} Presets",
      "selectDialog": {
        "title": "Select Preset File (.json)",
        "button": "Import"
      },
      "error": "Failed to import preset",
      "resultsModal": {
        "titleSuccessSection_one": "Imported 1 preset successfully",
        "titleSuccessSection_other": "Imported {{count}} presets successfully",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} failed)",
        "titleFailSection_other": "({{count}} failed)",
        "titleAllFailed": "Failed to import presets",
        "importMore": "Import More",
        "close": "Done",
        "successBadge": "Success",
        "alreadyExistsBadge": "Preset already exists",
        "errorBadge": "Error",
        "invalidFileBadge": "Invalid file",
        "otherErrorBadge": "Failed to import preset",
        "errorViewDetailsButton": "View Details",
        "seeError": "See Error",
        "noName": "No preset name",
        "useInChat": "Use in Chat"
      },
      "importFromUrl": {
        "button": "Import from URL...",
        "title": "Import from URL",
        "back": "Import from File...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "Import",
        "cancel": "Cancel",
        "loading": "Importing...",
        "error": "Failed to download preset."
      }
    },
    "download": {
      "title": "Pull <preset-name /> from LM Studio Hub",
      "subtitle": "Save <custom-name /> to your presets. Doing so you will allow you to use this preset in the app",
      "button": "Pull",
      "button/loading": "Pulling...",
      "cancel": "Cancel",
      "error": "Failed to download preset."
    },
    "inclusiveness": {
      "speculativeDecoding": "Include in Preset"
    }
  },

  "flashAttentionWarning": "Flash Attention is an experimental feature that may cause issues with some models. If you encounter problems, try disabling it.",
  "llamaKvCacheQuantizationWarning": "KV Cache Quantization is an experimental feature that may cause issues with some models. Flash Attention must be enabled for V cache quantization. If you encounter problems, reset to the default \"F16\".",

  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",

  "hardware": {
    "advancedGpuSettings": "Advanced GPU Settings",
    "advancedGpuSettings.info": "If you're unsure, leave these at their default values",
    "advancedGpuSettings.reset": "Reset to default",
    "environmentVariables": {
      "title": "Env. Variables",
      "description": "Active environment variables during model lifetime.",
      "key.placeholder": "Select var...",
      "value.placeholder": "Value"
    },
    "mainGpu": {
      "title": "Main GPU",
      "description": "The GPU to prioritize for model computation.",
      "placeholder": "Select main GPU..."
    },
    "splitStrategy": {
      "title": "Split Strategy",
      "description": "How to split model computation across GPUs.",
      "placeholder": "Select split strategy..."
    }
  }
}
