{
  "noInstanceSelected": "Cap inst√†ncia del model seleccionada",
  "resetToDefault": "Reinicia",
  "showAdvancedSettings": "Mostra configuraci√≥ avan√ßada",
  "showAll": "Tots",
  "basicSettings": "B√†sics",
  "configSubtitle": "Carrega o guarda plantilles i experimenta amb els par√†metres de sobrec√†rrega del model",
  "inferenceParameters/title": "Par√†metres de Predicci√≥",
  "inferenceParameters/info": "Experimenta amb els par√†metres que afecten a la predicci√≥.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Sampling",
  "basicTab": "B√†sics",
  "advancedTab": "Avan√ßat",
  "advancedTab/title": "üß™ Configuraci√≥ avan√ßada",
  "advancedTab/expandAll": "Mostrar totes",
  "advancedTab/overridesTitle": "Configurar Sobrec√†rregues",
  "advancedTab/noConfigsText": "No tens canvis guardats - edita els valors de dalt per veure sobreescriptures aqu√≠.",
  "loadInstanceFirst": "Carrega un model per veure els par√†metres configurables",
  "noListedConfigs": "Cap par√†metre configurable",
  "generationParameters/info": "Experimenta amb par√†metres b√†sics que impactin en la generaci√≥ de text.",
  "loadParameters/title": "Carrega els Par√†metres",
  "loadParameters/description": "Settings to control the way the model is initialized and loaded into memory.",
  "loadParameters/reload": "Recarrega per aplicar els canvis",
  "discardChanges": "Descarta els canvis",
  "loadModelToSeeOptions": "Load a model to see options",
  "llm.prediction.systemPrompt/title": "Plantilla del Sistema",
  "llm.prediction.systemPrompt/description": "Usa aquest camp per donar instruccions de fons al model, com unes regle, limitacions, o requisits generals.",
  "llm.prediction.systemPrompt/subTitle": "Directrius per la IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Qu√® tan aleat√≤ria √©s la resposta. 0 provocar√† les mateixes respostes sempre, mentre que valors m√©s alts augmentaran la creativitat i diferenciaci√≥",
  "llm.prediction.temperature/info": "De la documentaci√≥ de llama.cpp: \"El valor predeterminat √©s <{{dynamicValue}}>, el qual dona un equilibri entre aleatorietat i determinisme. En l'extrem, la temperatura 0 sempre escollir√† el token m√©s probable, causant resultats id√®ntics cada vegada que s'executi\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.topKSampling/title": "Top K Sampling",
  "llm.prediction.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.topKSampling/info": "De la documentaci√≥ de llama.cpp:\n\nTop-k sampling is a text generation method that selects the next token only from the top k most likely tokens predicted by the model.\n\nIt helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit the diversity of the output.\n\nA higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text.\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Fils de la CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Nombre de fils de la CPU a utilitzar durant la generaci√≥",
  "llm.prediction.llama.cpuThreads/info": "El nombre de fils a utilitzar durant la computaci√≥. Augmentar aquest valor no sempre √©s igual a un millor rendiment. El valor predeterminat √©s <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Longitud M√†xima de Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Limita la longitud de la resposta de la IA (OPCIONAL)",
  "llm.prediction.maxPredictedTokens/info": "Controla la longitud m√†xima de la resposta del xatbot. Activa per definir un limit al llarg de les respoestes, o desactiva per deixar decidir al xatbot quan aturar-se.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longitud M√†xima de Resposta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadament {{maxWords}} paraules",
  "llm.prediction.repeatPenalty/title": "Penalitzar per Repetici√≥",
  "llm.prediction.repeatPenalty/subTitle": "Qu√® tant s'ha de descoratjar la repetici√≥ de tokens",
  "llm.prediction.repeatPenalty/info": "De la documentaci√≥ de llama.cpp: \"Ajuda a evitar que el model generi texts repetitius o mon√≤tons.\n\nUn valor alt (p.e., 1.5) penalitzar√† m√©s les repeticions, mentre que un valor baix (p.e., 0.9) ser√† m√©s lax.\" ‚Ä¢ El valor predeterminat √©s <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P Sampling",
  "llm.prediction.minPSampling/subTitle": "Minimum base probability for a token to be selected for output",
  "llm.prediction.minPSampling/info": "De la documentaci√≥ de llama.cpp:\n\nThe minimum probability for a token to be considered, relative to the probability of the most likely token. Must be in [0, 1].\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P Sampling",
  "llm.prediction.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.topPSampling/info": "De la documentaci√≥ de llama.cpp:\n\nTop-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p.\n\nThis method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from.\n\nA higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. Must be in (0, 1].\n\n‚Ä¢ The default value is <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenes d'Aturament",
  "llm.prediction.stopStrings/subTitle": "Strings that should stop the model from generating more tokens",
  "llm.prediction.stopStrings/info": "Specific strings that when encountered will stop the model from generating more tokens",
  "llm.prediction.stopStrings/placeholder": "Enter a string and press ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Context de Sobrec√†rrega",
  "llm.prediction.contextOverflowPolicy/subTitle": "Com s'hauria de comportar el model quan la conversa √©s tan gran que deixa de ser controlable",
  "llm.prediction.contextOverflowPolicy/info": "Decide what to do when the conversation exceeds the size of the model's working memory ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Frequency Penalty",
  "llm.prediction.llama.presencePenalty/title": "Presence Penalty",
  "llm.prediction.llama.tailFreeSampling/title": "Tail-Free Sampling",
  "llm.prediction.llama.locallyTypicalSampling/title": "Locally Typical Sampling",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/info": "From ONNX documentation:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.onnx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.onnx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.onnx.repeatPenalty/info": "A higher value discourages the model from repeating itself",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topPSampling/info": "De la documentaci√≥ d'ONNX:\n\nNom√©s els tokens amb m√©s probabilitat de sumar al TopP o m√©s √©s conserven per generar respostes\n\n‚Ä¢ El filtre est√† desactivat per defecte",
  "llm.prediction.seed/title": "Llavor",
  "llm.prediction.structured/title": "Output Estructurat",
  "llm.prediction.structured/info": "Output Estructurat",
  "llm.prediction.structured/description": "Avan√ßat: pots carregar un JSON Schema per estructurar cert format a l'hora d'obtenir un output del model. Llegeix-te la [documentaci√≥](https://lmstudio.ai/docs/advanced/structured-output) per aprendre'n m√©s",
  "llm.prediction.promptTemplate/title": "Plantilla del Prompt",
  "llm.prediction.promptTemplate/subTitle": "El format en qu√® els missatges s'envien al model. Canviar-ho pot provocar comportaments inesperats - assegura't de saber qu√® coi est√†s fent!",

  "llm.load.contextLength/title": "Llargada del Context",
  "llm.load.contextLength/subTitle": "El nombre m√†xim de tokens que al qual el model pot respondre per pregunta. Mira les opcions de Sobrec√†rrega de la Conversa sota \"Par√†metres d'Inference\" per gestionar-ho de m√©s maneres",
  "llm.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "llm.load.contextLength/warning": "Setting a high value for context length can significantly impact memory usage",
  "llm.load.seed/title": "Llavor",
  "llm.load.seed/subTitle": "La llavor pel RNG utilitzada en aquesta generaci√≥. -1 √©s aleat√≤ria",
  "llm.load.seed/info": "Llavor Aleat√≤ria: Estableix la llavor del RNG per obtenir resultats reproduibles",

  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "llm.load.llama.evalBatchSize/info": "Sets the number of examples processed together in one batch during evaluation, affecting speed and memory usage",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "llm.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Decreases memory usage and generation time on some models",
  "llm.load.llama.flashAttention/info": "Accelerates attention mechanisms for faster and more efficient processing",
  "llm.load.numExperts/title": "Nombre d'Experts",
  "llm.load.numExperts/subTitle": "Nombre d'experts a utilitzar en el model",
  "llm.load.numExperts/info": "Nombre d'experts a utilitzar en el model",
  "llm.load.llama.keepModelInMemory/title": "Mant√© el Model a la Mem√≤ria",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "llm.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "llm.load.llama.useFp16ForKVCache/title": "Utilitza FP16 Per La KV Cau",
  "llm.load.llama.useFp16ForKVCache/info": "Redueix l'√∫s de mem√≤ria cau emmagatzemant-la amb semi-precisi√≥ (FP16)",
  "llm.load.llama.tryMmap/title": "prova mmap()",
  "llm.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "llm.load.llama.tryMmap/info": "Carrega fitxers del model directament del disc a la mem√≤ria",

  "embedding.load.contextLength/title": "Llargada del Context",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avan√ßat] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avan√ßat] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Mant√© Model a la Mem√≤ria",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Prova mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Millora el temps de c√†rrega del model. Desactivar-ho pot millorar el rendiment quan el model √©s major a la mem√≤ria RAM del sistema",
  "embedding.load.llama.tryMmap/info": "Carrega els fitxers del model directament del disc a la mem√≤ria",
  "embedding.load.seed/title": "Llavor",
  "embedding.load.seed/subTitle": "La llavor del RNG utilitzada en aquesta generaci√≥. -1 √©s una llavor aleat√≤ria",

  "embedding.load.seed/info": "Llavor Aleat√≤ria: Estableix la llavor del Generador de Nombres Aleatoris (RNG) per assegurar resultats reproduibles",

  "presetTooltip": {
    "included/title": "Valors Predeterminats",
    "included/description": "Els seg√ºents camps seran aplicats",
    "included/empty": "Cap camp d'aquesta plantilla aplica en aquest context.",
    "included/conflict": "Ser√†s preguntat si vols aplicar aquest valor",
    "separateLoad/title": "Temps de c√†rrega de la Configuraci√≥",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "per aplicar a",
    "separateLoad/description.3": ".",
    "excluded/title": "Potser no aplica",
    "excluded/description": "Els seg√ºents camps estan inclosos en la plantilla per√≤ no s'apliquen en l'actual context.",
    "legacy/title": "Plantilla llegada",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable."
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Buit>"
    },
    "checkboxNumeric": {
      "off": "NO"
    },
    "stringArray": {
      "empty": "<Buit>"
    },
    "llmPromptTemplate": {
      "type": "Tipus",
      "types.jinja/label": "Plantilla (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Plantilla",
      "jinja/error": "Failed to parse Jinja template: {{error}}",
      "jinja/empty": "Please enter a Jinja template above.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Abans que el Sistema",
      "manual.subfield.beforeSystem/placeholder": "Introdueix prefixe de Sistema...",
      "manual.subfield.afterSystem/label": "Despr√©s que el Sistema",
      "manual.subfield.afterSystem/placeholder": "Introdueix sufixe de Sistema...",
      "manual.subfield.beforeUser/label": "Abans que l'Usuari",
      "manual.subfield.beforeUser/placeholder": "Introdueix prefixe d'Usuari...",
      "manual.subfield.afterUser/label": "Despr√©s que l'Usuari",
      "manual.subfield.afterUser/placeholder": "Introdueix sufixe d'Usuari...",
      "manual.subfield.beforeAssistant/label": "Abans que l'Assistent",
      "manual.subfield.beforeAssistant/placeholder": "Introdueix prefixe d'Assistent...",
      "manual.subfield.afterAssistant/label": "Despr√©s que l'Assistent",
      "manual.subfield.afterAssistant/placeholder": "Introdueix sufixe d'Assistent...",
      "stopStrings/label": "Additional Stop Strings",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "Aquest √©s el m√†xim de tokens amb qu√® s'ha entrenat al model. Prem per decidir el context d'aquest valor",
      "maxValueTextStart": "El model soporta fins a",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "Tot i que un model pot suportar fins a certa quantitat de tokens, el rendiment pot deteriorar-se si els recursos del maquinari no estan preparats per controlar la c√†rrega - ves amb precauci√≥ en augmentar aquest valor"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Para al L√≠mit",
      "stopAtLimitSub": "Para de generar en quant la mem√≤ria del model es pleni",
      "truncateMiddle": "Truncar al Mig",
      "truncateMiddleSub": "Elimina missatges del mig de la conversa per fer espai a nous. El model encara recordar√† l'inici de la conversa",
      "rollingWindow": "Rolling Window",
      "rollingWindowSub": "El model sempre s'enrecordar√† dels missatges m√©s recents, per√≤ es pot oblidar de l'inici"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "M√ÄX",
      "off": "NO"
    }
  },
  "saveConflictResolution": {
    "title": "Esculls els canvis a incloure en la Plantilla",
    "description": "Selecciona i escull els valors a incloure",
    "instructions": "Prem un valor per incloure'l",
    "userValues": "Valor Anterior",
    "presetValues": "Nou Valor",
    "confirm": "Confirmar",
    "cancel": "Cancel¬∑lar"
  },
  "applyConflictResolution": {
    "title": "Quins valors es conserven?",
    "description": "Tens canvis per enviar que se sobreposen a la Plantilla entrant",
    "instructions": "Prem en un valor per mantenir-lo",
    "userValues": "Valor Actual",
    "presetValues": "Incoming Preset Value",
    "confirm": "Confirmar",
    "cancel": "Cancel¬∑lar"
  },
  "empty": "<Buit>",
  "presets": {
    "title": "Plantilla",
    "commitChanges": "Envia els Canvis",
    "commitChanges/description": "Envia els canvis fets a la plantilla.",
    "commitChanges.manual": "Nous camps detectats. Podr√†s escollir quins canvis s'inclouran a la plantilla.",
    "commitChanges.manual.hold.0": "Mant√©",
    "commitChanges.manual.hold.1": "per elegir quins canvis enviar a la plantilla.",
    "commitChanges.saveAll.hold.0": "Mant√©",
    "commitChanges.saveAll.hold.1": "per desar tots els canvis.",
    "commitChanges.saveInPreset.hold.0": "Mant√©",
    "commitChanges.saveInPreset.hold.1": "per desar nom√©s els canvis a camps que estan inclosos en la plantilla.",
    "commitChanges/error": "Error en enviar els canvis a la plantilla.",
    "commitChanges.manual/description": "Escogeix les lleng√ºes a incloure en la plantilla.",
    "saveAs": "Guarda Com Nova...",
    "presetNamePlaceholder": "Insereix nom per la plantilla...",
    "cannotCommitChangesLegacy": "Aquesta √©s una plantilla legacy i no pot ser modificada. Pots crear-ne una c√≤pia utilitzant \"Guarda Com Nova...\".",
    "cannotCommitChangesNoChanges": "Sense canvis a enviar.",
    "emptyNoUnsaved": "Selecciona una Plantilla...",
    "emptyWithUnsaved": "Plantilla Sense Desar",
    "saveEmptyWithUnsaved": "Desa Plantilla Com...",
    "saveConfirm": "Desa",
    "saveCancel": "Cancel¬∑la",
    "saving": "Desant...",
    "save/error": "Error en desar la plantilla.",
    "deselect": "Deseleccionar Plantilla",
    "deselect/error": "Error en deseleccionar la plantilla.",
    "select/error": "Error en seleccionar la plantilla.",
    "delete/error": "Error en eliminar la plantilla.",
    "discardChanges": "Descarta No Desats",
    "discardChanges/info": "Descarta tots els canvis no desats i restaura la plantilla a l'estat inicial",
    "newEmptyPreset": "Crea nova plantilla buida...",
    "contextMenuSelect": "Selecciona Plantilla",
    "contextMenuDelete": "Elimina"
  },

  "flashAttentionWarning": "Atenci√≥ Flash √©s una caracter√≠stica experimental que pot presentar problemes en alguns models. En cas de trobar-ne, prova desactivant la caracter√≠stica.",

  "seedUncheckedHint": "Llavor aleat√≤ria",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto"
}
