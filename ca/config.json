{
  "noInstanceSelected": "Cap model seleccionat",
  "resetToDefault": "Reinicia",
  "showAdvancedSettings": "Mostra configuraci√≥ avan√ßada",
  "showAll": "Tot",
  "basicSettings": "B√†sic",
  "configSubtitle": "Load or save presets and experiment with model parameter overrides [TODO]",
  "inferenceParameters/title": "Par√†metres de predicci√≥",
  "inferenceParameters/info": "Experimenta amb par√†metres que impacten a la predicci√≥.",
  "generalParameters/title": "General",
  "samplingParameters/title": "Mostreig",
  "basicTab": "B√†sic",
  "advancedTab": "Avan√ßat",
  "advancedTab/title": "üß™ Configuraci√≥ Avan√ßada",
  "advancedTab/expandAll": "Mostra tot",
    "advancedTab/overridesTitle": "Config Overrides [TODO]",
  "advancedTab/noConfigsText": "You have no unsaved changes - edit values above to see overrides here. [TODO]",
  "loadInstanceFirst": "Carrega un model per veure'n els par√†metres configurables",
  "noListedConfigs": "Cap par√†metre configurable",
  "generationParameters/info": "Experimenta amb par√†mentres b√†sics que afectin a la generaci√≥ de text.",
  "loadParameters/title": "Carrega Par√†metres",
  "loadParameters/description": "La configuraci√≥ que controla com el model s'inicialitza i carrega en la mem√≤ria.",
  "loadParameters/reload": "Reinicia per aplicar els canvis",
  "loadParameters/reload/error": "No s'ha pogut reiniciar el model",
  "discardChanges": "Descarta els canvis",
  "loadModelToSeeOptions": "Carrega un model per veure'n opcions",
  "schematicsError.title": "L'esquema de configuraci√≥ cont√© errors als seg√ºents camps:",
  "manifestSections": {
    "structuredOutput/title": "Resposta Estructurada",
    "speculativeDecoding/title": "Decodificaci√≥ Especulativa",
    "sampling/title": "Mostreig",
    "settings/title": "Configuraci√≥",
    "toolUse/title": "√ös d'Eines",
    "promptTemplate/title": "Plantilla de Prompts",
    "customFields/title": "Camps Personalitzats"
  },

  "llm.prediction.systemPrompt/title": "Prompt del Sistema",
  "llm.prediction.systemPrompt/description": "Utilitza aquest camp per donar al model instruccions de fons, com un conjunt de regles, l√≠mits o instruccions en general.",
  "llm.prediction.systemPrompt/subTitle": "Indicacions per a la IA",
  "llm.prediction.systemPrompt/openEditor": "Editor",
  "llm.prediction.systemPrompt/closeEditor": "Tanca l'Editor",
  "llm.prediction.systemPrompt/openedEditor": "Obert a l'Editor...",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "Quant aleatori ha de ser. Amb 0 sempre obtindr√†s el mateix resultat, mentre que valors m√©s alts incrementen la creativitat i la varietat",
  "llm.prediction.temperature/info": "De la documentaci√≥ de llama.cpp: \"El valor per defecte √©s <{{dynamicValue}}>. Aix√≠ s'obt√© un equilibri entre aleatorietat i determinisme. Als extrems, una temperatura de 0 escollir√† sempre el token m√©s probable, resultant en outputs id√®ntics cada vegada que s'executi\"",
  "llm.prediction.llama.sampling/title": "Mostreig",
  "llm.prediction.topKSampling/title": "Mostreig Top K",
  "llm.prediction.topKSampling/subTitle": "Limita el pr√≤xim token a un dels top-k m√©s probables. √âs semblant a la temperatura",
  "llm.prediction.topKSampling/info": "De la documentaci√≥ de llama.cpp:\n\nEl mostreig top-k √©s un m√®tode de generaci√≥ de text que nom√©s selecciona el pr√≤xim token d'entre els top-k m√©s probables segons el model.\n\nAix√≤ ajuda a reduir el risc de generar tokens poc probables o sense sentit, per√≤ √©s possible que limiti la diversitat del resultat.\n\nUn valor m√©s alt pel top-k (p.ex., 100) far√† que consideri m√©s tokens i resultar√† en textos m√©s diversos, mentre que un valor m√©s baix (p.ex., 10) se centrar√† en el token m√©s probable i generar√† textos m√©s reservats.\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Fils de la CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Nombre de fils de la CPU a utilitzar durant el raonament",
  "llm.prediction.llama.cpuThreads/info": "El nombre de fils a utilitzar durant la computaci√≥. Augmentar el nombre de fils no sempre resultar√† amb un millor rendiment. El valor per defecte √©s <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Llargada M√†xima de la Resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Talla volunt√†riament la llargada de les respostes de la IA",
  "llm.prediction.maxPredictedTokens/info": "Controla la llargada m√†xima de les respostes del xatbot. Activa-ho per establir un l√≠mit a la llargada m√†xima que pot tenir una resposta, o desactiva-ho per deixar al xatbot controlar la llargada de les respostes.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Llargada M√†xima de les Respostes (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Aproximadament {{maxWords}} paraules",
  "llm.prediction.repeatPenalty/title": "Penalitzaci√≥ per Repetici√≥",
  "llm.prediction.repeatPenalty/subTitle": "Quant s'ha de penalitzar la repetici√≥ del mateix token",
  "llm.prediction.repeatPenalty/info": "De la documentaci√≥ de llama.cpp: \"Ajuda a evitar que el model generi textos repetitius o mon√≤tons.\n\nUn valor m√©s alt (p.ex., 1.5) penalitzar√† les repeticions severament, mentre que un valor m√©s baix (p.ex., 0.9) ser√† m√©s permisiu.\" ‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Mostreig Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilitat base m√≠nima que un token necessita per escollir-lo",
  "llm.prediction.minPSampling/info": "De la documentaci√≥ de llama.cpp:\n\nLa probabilitat m√≠nima necessaria per considerar un token, relativa a la probabiliat m√©s alta. Ha d'estar entre [0, 1].\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Mostreig Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilitat acumulada m√≠nima que els pr√≤xims tokens necessiten. √âs semblant a la temperatura",
  "llm.prediction.topPSampling/info": "De la documetaci√≥ de llama.cpp:\n\nEl mostreig top-p, tamb√© conegut com nucleus sampling, √©s un altre m√®tode de generaci√≥ de text que selecciona el pr√≤xim token d'un subconjunt de tokens que junts tenen una probabilitat acumuluada de, com a m√≠nim, p.\n\nAquest m√®tode dona un equilibri entre diversitat i qualitat considerant tant les probabilitats dels tokens com el nombre de tokens a mostrejar.\n\nUn valor m√©s alt pel top-p (p.ex., 0.95) resultar√† en textos m√©s diversos, mentre que un valor m√©s baix (p.ex., 0.5) generar√† un text m√©s centrat i reservat. Ha d'estar entre (0, 1].\n\n‚Ä¢ El valor per defecte √©s <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cadenes d'Aturada",
  "llm.prediction.stopStrings/subTitle": "Cadenes de text que haurien d'aturar al model",
  "llm.prediction.stopStrings/info": "Cadenes de text espec√≠fiques que haurien d'aturar al model quan se les trobi",
  "llm.prediction.stopStrings/placeholder": "Insereix una cadena i pulsa ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Sobrec√†rrega de Context",
  "llm.prediction.contextOverflowPolicy/subTitle": "Com ha de reaccionar el model quan la conversa sigui massa gran perqu√® la pugui gestionar",
  "llm.prediction.contextOverflowPolicy/info": "Decideix qu√® fer quan la conversa excedeix la quantitat de mem√≤ria utilitzable pel model ('context')",
  "llm.prediction.llama.frequencyPenalty/title": "Penalitzaci√≥ per Freq√º√®ncia",
  "llm.prediction.llama.presencePenalty/title": "Penalitzaci√≥ per Pres√®ncia",
  "llm.prediction.llama.tailFreeSampling/title": "Mostreig Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Mostreig T√≠pic Localment",
  "llm.prediction.llama.xtcProbability/title": "Probabilitat del Mostreig XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "El mostreig XTC (Exclou Opcions Top) nom√©s s'activar√† quan un token generat tingui aquesta probabilitat. El mostreig XTC pot millorar la creativitat i reduir els clix√©s",
  "llm.prediction.llama.xtcProbability/info": "El mostreig XTC (Exclou Opcions Top) nom√©s s'activar√† quan un token generat tingui aquesta probabilitat. El mostreig XTC pot millorar la creativitat i reduir clix√©s",
  "llm.prediction.llama.xtcThreshold/title": "Llindar de Mostreig XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Un llindar pel mostreig XTC (Exclou Opcions Top). Donada una probabilitat `xtc`, cerca tokens dins el llindar entre la probabilitat 'xtc' i 0.5, i el¬∑limina'ls tots excepte el menys probable",
  "llm.prediction.llama.xtcThreshold/info": "Un llindar pel mostreig XTC (Exclou Opcions Top). Donada una probabilitat `xtc`, cerca tokens dins el llindar entre la probabilitat 'xtc' i 0.5, i el¬∑limina'ls tots excepte el menys probable",
  "llm.prediction.mlx.topKSampling/title": "Mostreig Top K",
  //
  "llm.prediction.mlx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.mlx.topKSampling/info": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/title": "Top K Sampling",
  "llm.prediction.onnx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/info": "From ONNX documentation:\n\nNumber of highest probability vocabulary tokens to keep for top-k-filtering\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.onnx.repeatPenalty/title": "Repeat Penalty",
  "llm.prediction.onnx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.onnx.repeatPenalty/info": "A higher value discourages the model from repeating itself",
  "llm.prediction.onnx.topPSampling/title": "Top P Sampling",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topPSampling/info": "From ONNX documentation:\n\nOnly the most probable tokens with probabilities that add up to TopP or higher are kept for generation\n\n‚Ä¢ This filter is turned off by default",
  "llm.prediction.seed/title": "Seed",
  "llm.prediction.structured/title": "Structured Output",
  "llm.prediction.structured/info": "Structured Output",
  "llm.prediction.structured/description": "Advanced: you can provide a [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) to enforce a particular output format from the model. Read the [documentation](https://lmstudio.ai/docs/advanced/structured-output) to learn more",
  "llm.prediction.tools/title": "Tool Use",
  "llm.prediction.tools/description": "Advanced: you can provide a JSON-compliant list of tools for the model to request calls to. Read the [documentation](https://lmstudio.ai/docs/advanced/tool-use) to learn more",
  "llm.prediction.tools/serverPageDescriptionAddon": "Pass this through the request body as `tools` when using the server API",
  "llm.prediction.promptTemplate/title": "Prompt Template",
  "llm.prediction.promptTemplate/subTitle": "The format in which messages in chat are sent to the model. Changing this may introduce unexpected behavior - make sure you know what you're doing!",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Draft Tokens to Generate",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "The number of tokens to generate with the draft model per main model token. Find the sweet spot of compute vs. reward",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Drafting Probability Cutoff",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continue drafting until a token's probability falls below this threshold. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Min Draft Size",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Drafts smaller than this will be ignored by the main model. Higher values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Max Draft Size",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Max number of tokens allowed in a draft. Ceiling if all token probs are > the cutoff. Lower values generally mean lower risk, lower reward",
  "llm.prediction.speculativeDecoding.draftModel/title": "Draft Model",
  "llm.prediction.reasoning.parsing/title": "Reasoning Section Parsing",
  "llm.prediction.reasoning.parsing/subTitle": "How to parse reasoning sections in the model's output",

  "llm.load.mainGpu/title": "Main GPU",
  "llm.load.mainGpu/subTitle": "The GPU to prioritize for model computation",
  "llm.load.mainGpu/placeholder": "Select main GPU...",
  "llm.load.splitStrategy/title": "Split Strategy",
  "llm.load.splitStrategy/subTitle": "How to split model computation across GPUs",
  "llm.load.splitStrategy/placeholder": "Select split strategy...",
  "llm.load.offloadKVCacheToGpu/title": "Offload KV Cache to GPU Memory",
  "llm.load.offloadKVCacheToGpu/subTitle": "Offload the KV cache to GPU memory. Improves performance but requires more GPU memory",
  "load.gpuStrictVramCap/title": "Limit Model Offload to Dedicated GPU Memory",
  "load.gpuStrictVramCap.customSubTitleOff": "OFF: Allow model weights to offload to shared memory if dedicated GPU memory is full",
  "load.gpuStrictVramCap.customSubTitleOn": "ON: The system will limit offload of model weights to dedicated GPU memory and RAM only. Context may still use shared memory",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "Model offload limited to dedicated GPU memory. Actual number of offloaded layers may differ",
  "load.allGpusDisabledWarning": "All GPUs are currently disabled. Enable at least one to offload",

  "llm.load.contextLength/title": "Context Length",
  "llm.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "llm.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "llm.load.contextLength/warning": "Setting a high value for context length can significantly impact memory usage",
  "llm.load.seed/title": "Seed",
  "llm.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random",
  "llm.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",

  "llm.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "llm.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "llm.load.llama.evalBatchSize/info": "Sets the number of examples processed together in one batch during evaluation, affecting speed and memory usage",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "llm.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "Decreases memory usage and generation time on some models",
  "llm.load.llama.flashAttention/info": "Accelerates attention mechanisms for faster and more efficient processing",
  "llm.load.numExperts/title": "Number of Experts",
  "llm.load.numExperts/subTitle": "Number of experts to use in the model",
  "llm.load.numExperts/info": "The number of experts to use in the model",
  "llm.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "llm.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "llm.load.llama.useFp16ForKVCache/title": "Use FP16 For KV Cache",
  "llm.load.llama.useFp16ForKVCache/info": "Reduces memory usage by storing cache in half-precision (FP16)",
  "llm.load.llama.tryMmap/title": "Try mmap()",
  "llm.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "llm.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU Thread Pool Size",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Number of CPU threads to allocate to the thread pool used for model computation",
  "llm.load.llama.cpuThreadPoolSize/info": "The number of CPU threads to allocate to the thread pool used for model computation. Increasing the number of threads does not always correlate with better performance. The default is <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "K Cache Quantization Type",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/title": "V Cache Quantization Type",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Lower values reduce memory usage but may decrease quality. The effect varies significantly between models.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è You must disable this value if Flash Attention is not enabled",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Can only be turned on when Flash Attention is enabled",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è You must disable flash attention when using F32",
  "llm.load.mlx.kvCacheBits/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheBits/subTitle": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Context Length setting is ignored when using KV Cache Quantization",
  "llm.load.mlx.kvCacheGroupSize/title": "KV Cache Quantization: Group Size",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Group size during quantization operation for the KV cache. Higher group size reduces memory usage but may decrease quality",
  "llm.load.mlx.kvCacheGroupSize/info": "Number of bits that the KV cache should be quantized to",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV Cache Quantization: Start quantizing when ctx crosses this length",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Context length threshold to start quantizating the KV cache",
  "llm.load.mlx.kvCacheQuantization/title": "KV Cache Quantization",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantize the model's KV cache. This may result in faster generation and lower memory footprint,\nat the expense of the quality of the model output.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV cache quantization bits",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Number of bits to quantize the KV cache to",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Group size strategy",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Accuracy",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "Balanced",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Speedy",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Advanced: Quantized 'matmul group size' configuration\n\n‚Ä¢ Accuracy = group size 32\n‚Ä¢ Balanced = group size 64\n‚Ä¢ Speedy = group size 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "Start quantizing when ctx reaches this length",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "When the context reaches this amount of tokens,\nbegin quantizing the KV cache",

  "embedding.load.contextLength/title": "Context Length",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Specifies the maximum number of tokens the model can consider at once, impacting how much context it retains during processing",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE Frequency Base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Advanced] Adjusts the base frequency for Rotary Positional Encoding, affecting how positional information is embedded",
  "embedding.load.llama.evalBatchSize/title": "Evaluation Batch Size",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Sets the number of tokens processed together in one batch during evaluation",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE Frequency Scale",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Advanced] Modifies the scaling of frequency for Rotary Positional Encoding to control positional encoding granularity",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Keep Model in Memory",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Prevents the model from being swapped out to disk, ensuring faster access at the cost of higher RAM usage",
  "embedding.load.llama.tryMmap/title": "Try mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "embedding.load.llama.tryMmap/info": "Load model files directly from disk to memory",
  "embedding.load.seed/title": "Seed",
  "embedding.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random seed",

  "embedding.load.seed/info": "Random Seed: Sets the seed for random number generation to ensure reproducible results",

  "presetTooltip": {
    "included/title": "Preset Values",
    "included/description": "The following fields will be applied",
    "included/empty": "No fields of this preset apply in this context.",
    "included/conflict": "You will be asked to choose whether to apply this value",
    "separateLoad/title": "Load-time Configuration",
    "separateLoad/description.1": "The preset also includes the following load-time configuration. Load time config are model-wide and requires reloading the model to take effect. Hold",
    "separateLoad/description.2": "to apply to",
    "separateLoad/description.3": ".",
    "excluded/title": "May not apply",
    "excluded/description": "The following fields are included in the preset but does not apply in the current context.",
    "legacy/title": "Legacy Preset",
    "legacy/description": "This preset is a legacy preset. It includes the following fields which are either handled automatically now, or are no longer applicable.",
    "button/publish": "Publish to Hub",
    "button/pushUpdate": "Push Changes to Hub",
    "button/noChangesToPush": "No changes to push",
    "button/export": "Export",
    "hubLabel": "Preset from the Hub by {{user}}",
    "ownHubLabel": "Your preset from the Hub"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Empty>"
    },
    "checkboxNumeric": {
      "off": "OFF"
    },
    "llamaCacheQuantizationType": {
      "off": "OFF"
    },
    "mlxKvCacheBits": {
      "off": "OFF"
    },
    "stringArray": {
      "empty": "<Empty>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Template (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "Template",
      "jinja/error": "Failed to parse Jinja template: {{error}}",
      "jinja/empty": "Please enter a Jinja template above.",
      "jinja/unlikelyToWork": "The Jinja template you provided above is unlikely to work as it does not reference the variable \"messages\". Please double check if you have entered a correct template.",
      "types.manual/label": "Manual",
      "manual.subfield.beforeSystem/label": "Before System",
      "manual.subfield.beforeSystem/placeholder": "Enter System prefix...",
      "manual.subfield.afterSystem/label": "After System",
      "manual.subfield.afterSystem/placeholder": "Enter System suffix...",
      "manual.subfield.beforeUser/label": "Before User",
      "manual.subfield.beforeUser/placeholder": "Enter User prefix...",
      "manual.subfield.afterUser/label": "After User",
      "manual.subfield.afterUser/placeholder": "Enter User suffix...",
      "manual.subfield.beforeAssistant/label": "Before Assistant",
      "manual.subfield.beforeAssistant/placeholder": "Enter Assistant prefix...",
      "manual.subfield.afterAssistant/label": "After Assistant",
      "manual.subfield.afterAssistant/placeholder": "Enter Assistant suffix...",
      "stopStrings/label": "Additional Stop Strings",
      "stopStrings/subTitle": "Template specific stop strings that will be used in addition to user-specified stop strings."
    },
    "contextLength": {
      "maxValueTooltip": "This is the maximum number of tokens the model was trained to handle. Click to set the context to this value",
      "maxValueTextStart": "Model supports up to",
      "maxValueTextEnd": "tokens",
      "tooltipHint": "While a model may support up to a certain number of tokens, performance may deteriorate if your machine's resources cannot handle the load - use caution when increasing this value"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Stop at Limit",
      "stopAtLimitSub": "Stop generating once the model's memory gets full",
      "truncateMiddle": "Truncate Middle",
      "truncateMiddleSub": "Removes messages from the middle of the conversation to make room for newer ones. The model will still remember the beginning of the conversation",
      "rollingWindow": "Rolling Window",
      "rollingWindowSub": "The model will always get the most recent few messages but may forget the beginning of the conversation"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "OFF"
    },
    "gpuSplitStrategy": {
      "evenly": "Evenly",
      "favorMainGpu": "Favor Main GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "Read how it works",
      "placeholder": "Select a compatible draft model",
      "noCompatible": "No compatible draft models found for your current model selection",
      "stillLoading": "Identifying compatible draft models...",
      "notCompatible": "The selected draft model (<draft/>) is not compatible with the current model selection (<current/>).",
      "off": "OFF",
      "loadModelToSeeOptions": "Load model <keyboard-shortcut /> to see compatible options",
      "compatibleWithNumberOfModels": "Recommended for at least {{dynamicValue}} of your models",
      "recommendedForSomeModels": "Recommended for some models",
      "recommendedForLlamaModels": "Recommended for Llama models",
      "recommendedForQwenModels": "Recommended for Qwen models",
      "onboardingModal": {
        "introducing": "Introducing",
        "speculativeDecoding": "Speculative Decoding",
        "firstStepBody": "Inference speedup for <custom-span>llama.cpp</custom-span> and <custom-span>MLX</custom-span> models",
        "secondStepTitle": "Inference Speedup with Speculative Decoding",
        "secondStepBody": "Speculative Decoding is a technique involving the collaboration of two models:\n - A larger \"main\" model\n - A smaller \"draft\" model\n\nDuring generation, the draft model rapidly proposes tokens for the larger main model to verify. Verifying tokens is a much faster process than actually generating them, which is the source of the speed gains. **Generally, the larger the size difference between the main model and the draft model, the greater the speed-up**.\n\nTo maintain quality, the main model only accepts tokens that align with what it would have generated itself, enabling the response quality of the larger model at faster inference speeds. Both models must share the same vocabulary.",
        "draftModelRecommendationsTitle": "Draft model recommendations",
        "basedOnCurrentModels": "Based on your current models",
        "close": "Close",
        "next": "Next",
        "done": "Done"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Please load a model first <model-badge /> ",
      "errorEngineNotSupported": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine (<key/>) and reload the model to use this feature.",
      "errorEngineNotSupported/noKey": "Speculative decoding requires at least version {{minVersion}} of the engine {{engineName}}. Please update the engine and reload the model to use this feature."
    },
    "llmReasoningParsing": {
      "startString/label": "Start String",
      "startString/placeholder": "Enter the start string...",
      "endString/label": "End String",
      "endString/placeholder": "Enter the end string..."
    }
  },
  "saveConflictResolution": {
    "title": "Choose which values to include in the Preset",
    "description": "Pick and choose which values to keep",
    "instructions": "Click on a value to include it",
    "userValues": "Previous Value",
    "presetValues": "New Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "applyConflictResolution": {
    "title": "Which values to keep?",
    "description": "You have uncommitted changes which overlap with the incoming Preset",
    "instructions": "Click on a value to keep it",
    "userValues": "Current Value",
    "presetValues": "Incoming Preset Value",
    "confirm": "Confirm",
    "cancel": "Cancel"
  },
  "empty": "<Empty>",
  "noModelSelected": "No models selected",
  "apiIdentifier.label": "API Identifier",
  "apiIdentifier.hint": "Optionally provide an identifier for this model. This will be used in API requests. Leave blank to use the default identifier.",
  "idleTTL.label": "Auto Unload If Idle (TTL)",
  "idleTTL.hint": "If set, the model will be automatically unloaded after being idle for the specified amount of time.",
  "idleTTL.mins": "mins",

  "presets": {
    "title": "Preset",
    "commitChanges": "Commit Changes",
    "commitChanges/description": "Commit your changes to the preset.",
    "commitChanges.manual": "New fields detected. You will be able to choose which changes to include in the preset.",
    "commitChanges.manual.hold.0": "Hold",
    "commitChanges.manual.hold.1": "to choose which changes to commit to the preset.",
    "commitChanges.saveAll.hold.0": "Hold",
    "commitChanges.saveAll.hold.1": "to save all changes.",
    "commitChanges.saveInPreset.hold.0": "Hold",
    "commitChanges.saveInPreset.hold.1": "to only save changes to fields that are already included in the preset.",
    "commitChanges/error": "Failed to commit changes to the preset.",
    "commitChanges.manual/description": "Choose which changes to include in the preset.",
    "saveAs": "Save As New...",
    "presetNamePlaceholder": "Enter a name for the preset...",
    "cannotCommitChangesLegacy": "This is a legacy preset and cannot be modified. You can create a copy by using \"Save As New...\".",
    "cannotCommitChangesNoChanges": "No changes to commit.",
    "emptyNoUnsaved": "Select a Preset...",
    "emptyWithUnsaved": "Unsaved Preset",
    "saveEmptyWithUnsaved": "Save Preset As...",
    "saveConfirm": "Save",
    "saveCancel": "Cancel",
    "saving": "Saving...",
    "save/error": "Failed to save preset.",
    "deselect": "Deselect Preset",
    "deselect/error": "Failed to deselect preset.",
    "select/error": "Failed to select preset.",
    "delete/error": "Failed to delete preset.",
    "discardChanges": "Discard Unsaved",
    "discardChanges/info": "Discard all uncommitted changes and restore the preset to its original state",
    "newEmptyPreset": "+ New Preset",
    "importPreset": "Import",
    "contextMenuCopyIdentifier": "Copy Preset Identifier",
    "contextMenuSelect": "Apply Preset",
    "contextMenuDelete": "Delete...",
    "contextMenuShare": "Publish...",
    "contextMenuOpenInHub": "View on Web",
    "contextMenuPullFromHub": "Pull Latest",
    "contextMenuPushChanges": "Push Changes to Hub",
    "contextMenuPushingChanges": "Pushing...",
    "contextMenuPushedChanges": "Changes pushed",
    "contextMenuExport": "Export File",
    "contextMenuRevealInExplorer": "Reveal in File Explorer",
    "contextMenuRevealInFinder": "Reveal in Finder",
    "share": {
      "title": "Publish Preset",
      "action": "Share your preset for others to download, like, and fork",
      "presetOwnerLabel": "Owner",
      "uploadAs": "Your preset will be created as {{name}}",
      "presetNameLabel": "Preset Name",
      "descriptionLabel": "Description (optional)",
      "loading": "Publishing...",
      "success": "Preset Successfully Pushed",
      "presetIsLive": "<preset-name /> is now live on the Hub!",
      "close": "Close",
      "confirmViewOnWeb": "View on web",
      "confirmCopy": "Copy URL",
      "confirmCopied": "Copied!",
      "pushedToHub": "Your preset was pushed to the Hub",
      "descriptionPlaceholder": "Enter a description...",
      "willBePublic": "This preset will be public. Anyone on the internet will be able to see it.",
      "willBePrivate": "Only you will be able to see this preset",
      "willBeOrgVisible": "This preset will be visible to everyone in the organization.",
      "publicSubtitle": "Your preset is <custom-bold>Public</custom-bold>. Others can download and fork it on lmstudio.ai",
      "privateUsageReached": "Private preset number limit reached.",
      "continueInBrowser": "Continue in Browser",
      "confirmShareButton": "Publish",
      "error": "Failed to publish preset",
      "createFreeAccount": "Create a free account in the Hub to publish presets"
    },
    "update": {
      "title": "Push Changes to Hub",
      "title/success": "Preset Successfully Updated",
      "subtitle": "Make changes to <custom-preset-name /> and push them to the Hub",
      "descriptionLabel": "Description",
      "descriptionPlaceholder": "Enter a description...",
      "loading": "Pushing...",
      "cancel": "Cancel",
      "createFreeAccount": "Create a free account in the Hub to publish presets",
      "error": "Failed to push update",
      "confirmUpdateButton": "Push"
    },
    "resolve": {
      "title": "Resolve conflicts...",
      "tooltip": "Open a modal to resolve differences with the Hub version"
    },
    "loginToManage": {
      "title": "Login to manage..."
    },
    "downloadFromHub": {
      "title": "Download",
      "downloading": "Downloading...",
      "success": "Downloaded!",
      "error": "Failed to download"
    },
    "push": {
      "title": "Push changes",
      "pushing": "Pushing...",
      "success": "Pushed",
      "tooltip": "Push your local changes to the remote version hosted on the Hub",
      "error": "Failed to push"
    },
    "saveAsNewModal": {
      "title": "Oops! Did not find the preset on Hub",
      "confirmSaveAsNewDescription": "Do you want to publish this preset as a new one?",
      "confirmButton": "Publish as New"
    },
    "pull": {
      "title": "Pull Latest",
      "error": "Failed to pull",
      "contextMenuErrorMessage": "Failed to pull",
      "success": "Pulled",
      "pulling": "Pulling...",
      "upToDate": "Up to date!",
      "unsavedChangesModal": {
        "title": "You have unsaved changes.",
        "bodyContent": "Pulling from remote will overwrite your unsaved changes. Continue?",
        "confirmButton": "Overwrite Unsaved Changes"
      }
    },
    "import": {
      "title": "Import a Preset from File",
      "dragPrompt": "Drag and drop preset files (.tar.gz or preset.json) or <custom-link>select from your computer</custom-link>",
      "remove": "Remove",
      "cancel": "Cancel",
      "importPreset_zero": "Import Preset",
      "importPreset_one": "Import Preset",
      "importPreset_other": "Import {{count}} Presets",
      "selectDialog": {
        "title": "Select Preset File (preset.json or .tar.gz)",
        "button": "Import"
      },
      "error": "Failed to import preset",
      "resultsModal": {
        "titleSuccessSection_one": "Imported 1 preset successfully",
        "titleSuccessSection_other": "Imported {{count}} presets successfully",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} failed)",
        "titleFailSection_other": "({{count}} failed)",
        "titleAllFailed": "Failed to import presets",
        "importMore": "Import More",
        "close": "Done",
        "successBadge": "Success",
        "alreadyExistsBadge": "Preset already exists",
        "errorBadge": "Error",
        "invalidFileBadge": "Invalid file",
        "otherErrorBadge": "Failed to import preset",
        "errorViewDetailsButton": "View Details",
        "seeError": "See Error",
        "noName": "No preset name",
        "useInChat": "Use in Chat"
      },
      "importFromUrl": {
        "button": "Import from URL...",
        "title": "Import from URL",
        "back": "Import from File...",
        "action": "Paste the LM Studio Hub URL of the preset you want to import below",
        "invalidUrl": "Invalid URL. Please make sure you are pasting a correct LM Studio Hub URL.",
        "tip": "You can install the preset directly with the {{buttonName}} button in LM Studio Hub",
        "confirm": "Import",
        "cancel": "Cancel",
        "loading": "Importing...",
        "error": "Failed to download preset."
      }
    },
    "download": {
      "title": "Pull <preset-name /> from LM Studio Hub",
      "subtitle": "Save <custom-name /> to your presets. Doing so you will allow you to use this preset in the app",
      "button": "Pull",
      "button/loading": "Pulling...",
      "cancel": "Cancel",
      "error": "Failed to download preset."
    },
    "inclusiveness": {
      "speculativeDecoding": "Include in Preset"
    }
  },

  "flashAttentionWarning": "Flash Attention is an experimental feature that may cause issues with some models. If you encounter problems, try disabling it.",
  "llamaKvCacheQuantizationWarning": "KV Cache Quantization is an experimental feature that may cause issues with some models. Flash Attention must be enabled for V cache quantization. If you encounter problems, reset to the default \"F16\".",

  "seedUncheckedHint": "Random Seed",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",

  "hardware": {
    "environmentVariables": "Environment Variables",
    "environmentVariables.info": "If you're unsure, leave these at their default values",
    "environmentVariables.reset": "Reset to default",

    "gpus.information": "Configure graphics processing units (GPUs) detected on your machine",
    "gpuSettings": {
      "editMaxCapacity": "Edit Max Capacity",
      "hideEditMaxCapacity": "Hide Edit Max Capacity",
      "allOffWarning": "All GPUs are off or disabled, ensure that there is some GPU allocation to enable loading models",
      "split": {
        "title": "Strategy",
        "placeholder": "Select a GPU memory allocation",
        "options": {
          "generalDescription": "Configure how models will be loaded onto your GPUs",
          "evenly": {
            "title": "Split evenly",
            "description": "Allocate memory evenly across GPUs"
          },
          "priorityOrder": {
            "title": "Priority order",
            "description": "Drag to reorder priority. The system will try to allocate more on GPUs listed first"
          },
          "custom": {
            "title": "Custom",
            "description": "Allocate memory",
            "maxAllocation": "Maximum Allocation"
          }
        }
      },
      "deviceId.info": "Unique identifier for this device",
      "changesOnlyAffectNewlyLoadedModels": "Changes will only affect newly loaded models",
      "toggleGpu": "Enable/Disable GPU"
    }
  },

  "load.gpuSplitConfig/title": "GPU Split Configuration",
  "envVars/title": "Set an Environment Variable",
  "envVars": {
    "select": {
      "placeholder": "Select an environment variable...",
      "noOptions": "No more available",
      "filter": {
        "placeholder": "Filter search results",
        "resultsFound_zero": "No results found",
        "resultsFound_one": "1 result found",
        "resultsFound_other": "{{count}} results found"
      }
    },
    "inputValue": {
      "placeholder": "Enter a value"
    },
    "values": {
      "title": "Current Values"
    }
  }
}
